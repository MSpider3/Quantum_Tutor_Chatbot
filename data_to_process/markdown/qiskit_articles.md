# The next evolution of IBM Quantum Platform: How to prepare for the transition

As our progress along the IBM Quantum™ Roadmap accelerates, we’re upgrading IBM Quantum Platform to deliver enterprise-grade cloud services. The new preview version of the Platform includes Heron QPU access for Open Plan users.

Date

26 Feb 2025

Authors

Julianna Roberts

Leron Gil

Kayla Lee

Fran Cabrera

Sean Dague

Robert Davis

Topics

Share this blog

In 2016, IBM® delivered the world’s first accessible, cloud-enabled quantum processor. We’ve spent the near decade since upgrading our cloud-based quantum computers from the small, experimental devices of 2016 to the high-performance, utility-scale devices we have today. Now, it’s time we upgraded the way users access those devices as well.

We’re upgrading IBM Quantum Platformwith enterprise-grade infrastructure, and releasing an early access preview version of the new Platform that you can begin exploring today. Once it’s complete, the final version of the new IBM Quantum Platform will be very similar to the version you use now, and we’ll ensure that you continue to have access to the documentation and learning resources we’ve traditionally housed there. However, this move gives us the ability to boost performance and roll out powerful features that many of you have been requesting for a long time—features that would be difficult or impossible to deploy with our current infrastructure.

The first examples of this will include enhanced data privacy and security features, a streamlined notifications experience, better platform navigation, multiple language options for users who speak languages other than English, and much, much more. You’ll find a detailed list of features and benefits we’re introducing with the upgrade at the bottom of this article.

Get started now.IBM Quantum Open Plan and Pay-as-you-go Plan users can start exploring the early access Platform today.

Please note: If your IBM Quantum Platform access falls under a plan administrator (e.g., Premium Plan, Dedicated service, Startup program, etc.), you will receive guided migration support through your administrator in the coming months. In the meantime, you are welcome to create a free Open plan account to explore the new platform.

“Early access” means that this new version of the Platform is a preview that is still under development. Our hope is that you’ll begin trying it out now, so we can use your feedback to improve its functionality before we release the full version. There’s no need for you to migrate right away if you don’t feel you’re ready yet.However, the current version of IBM Quantum Platform will eventually be sunset.We’ll make sure you have ample notice, but all users will need to migrate to the new Platform before that point, so we strongly encourage you to read this blog closely and begin exploring the new Platform soon.

IBM Quantum Platform is a direct descendant of the IBM Q Experience we launched in 2016, back when our burgeoning quantum community did not require the same powerful, flexible, and highly secure cloud solutions that are necessary in fields like AI and data science. However, the needs of the quantum community are changing fast. Quantum computers have evolved into powerful, high-performance tools that academic, enterprise, and government research organizations are using to explore scientific problems beyond the reach of classical computation. To support this work, to continue scaling our services along the IBM Quantum Roadmap, and to provide all users with access to high-performance quantum systems backed by the latest data privacy and security measures, we must mature our platform to create an enterprise-grade cloud experience.

Below, we’ll share more information on why we’re making this change, the long-term benefits it will bring, and key details to keep in mind for everyone preparing for the migration. For detailed instructions on how to access the upgraded Platform, be sure to refer to the documentation links in the next section.

General Migration guide.Includes details on creating an account for the new Platform and updating runtime provider code.

Migration guide for REST API users.Covers special instructions for users of the Qiskit Runtime REST API.

If you’re an Open Plan or Pay-as-you-go Plan user who is interested in getting a head start on your migration to the new IBM Quantum Platform, you can do so now by following the instructions in the migration guide linked above. We encourage you to make the move as soon as you’re able to do so. Early access users will have the opportunity to provide vital feedback to our developer team as we prepare for the official launch later this year.

Additionally, we are pleased to announce thatthe new Platform will give Open Plan users access to a state-of-the-art IBM Quantum Heron QPU with tunable coupling architecture for the first time.As of today, quantum computers powered by Heron are available only to Premium Plan users. With the new IQP, all users will have access to at least one Heron QPU.

Reminder: If your IBM Quantum Platform access falls under a plan administrator (e.g., Premium Plan, Dedicated service, Startup program, etc.), you will receive guided migration support through your administrator in the coming months. In the meantime, you are welcome to create a free Open plan account to explore the new platform.

The biggest driver behind our decision to upgrade the Platform is the simple fact that our users have spent years asking for important features that we cannot deliver with our current infrastructure. We want you to know that we’ve been listening, and now we’re forging ahead. Our ultimate goal is to build a new platform robust enough to handle all of our current and future services.

Improved stability.To prepare for users’ first demonstrations of quantum advantage, we need to ensure that the platform is stable and reliable enough to run even more complex workflows. Upgraded cloud services provide a much more resilient infrastructure and advanced failover mechanisms that will significantly improve the stability of the new Platform, helping to minimize service interruptions and ensure consistent performance. These new capabilities further enhance reliability, providing a steady and dependable foundation for the Platform over the long term.

Data locality.As part of our security enhancements and in an ongoing effort to boost processing proximity and regulatory compliance for all users, we are planning to introduce new data locality features with the launch of the new Platform. This means you can choose to have your data and experiments live in certain regions—e.g., If you are based in the EU you can now run all your experiments in our European Datacenter.

Enhanced security.One of the most important features we’ll be able to deliver with the new Platform is an enhanced security and compliance posture that will ensure long-term protection for your data.

Enterprise cloud features.IBM Quantum Platform’s new enterprise-grade cloud infrastructure provides you with many features that will be useful for enterprises and other large organizations. These include local language support, SSO integration, Service IDs, advanced permissions and user management, and improved management of API keys and tokens.

Seamless cloud integrations.This upgrade will enable the Platform to offer seamless integration with off-the-shelf cloud compute services like Cloud Object Storage (COS) and Virtual Private Cloud (VPC). These services can give you significantly greater control over how your data and cloud resources are stored, managed, and secured to facilitate AI, HPC, and other workloads.

Integrating new and existing access plans.The move to the upgraded Platform experience allows us to create a unified entry point for all services related to IBM Quantum. This means that Pay-as-you-Go users will now employ the same dashboard as all other users. This also means we can accelerate the introduction of new access plans that give you even more options for how you use our quantum computers.

Streamlined communications.Our newly unified notification system gives you a convenient, centralized location for tracking account updates, service alerts, and software releases, with greater control over how you receive notifications.

Improved developer experience.We’re using the upcoming transition to introduce a number of features that will improve the developer experience. One example of this is API versioning on the Qiskit Runtime REST API, which allows developers to work with specific API versions confidently—avoiding breaking changes while simplifying testing and feature rollouts. This ensures a smoother and more predictable workflow for developers.

This is an exciting time in quantum computing. With the quantum and HPC communities working together, the world now finds itself on the cusp of achieving quantum advantage, and we believe it will be only a few short years before we achieve quantum error correction and fault tolerance. Big things are coming soon, and we need a cloud platform robust enough to carry through this period of quantum utility, and into the fast-approaching era of quantum advantage. Create an account and start exploringthe new IBM Quantum Platformin early access mode today!

28 May 2025• Marina Caputo, Junye Huang, Marcel Pfaffhauser, Sanskriti Deva, Robert Davis

21 May 2025• Olivia Lanes, Robert Davis

5 May 2025• Andrei Constantinescu, Marco Facchini, Leron Gil, Robert Davis

5 Mar 2025• Suhare Nur, Sanket Panda, Francisco Martin, SheshaShayee Raghunathan, Robert Davis

Featured

Get access

Get started

Stay connected


---

# Exploring the potential for quantum advantage in mathematical optimization

Recent publications from members of the Quantum Optimization Working Group deliver a fresh perspective on the potential for quantum computers to demonstrate value for interesting combinatorial optimization problems.

Date

5 Feb 2025

Authors

Stefan Woerner

Ryan Mandelbaum

Robert Davis

Topics

Share this blog

Optimization problems touch every facet of your life. You decide the best order to run errands to save time, or use your maps app to find an optimal driving route that avoids traffic. Optimization problems also appear widely in the industries that touch our everyday lives. They are crucial in the operation of energy grids, supply chains, and more—helping to ensure stability, safety, and efficiency.

In some cases, we solve these optimization problems using our intuition, or by making common-sense judgments based on prior experience. However, when the considered tasks grow too complex, we need to represent them as optimization problems that we can tackle with mathematics. Over the years, computational methods for solving combinatorial optimization problems have come to play an important role in business and science, and help to solve many of these problems efficiently. Nevertheless, some combinatorial optimization problems remain extremely challenging, even for state-of-the-art methods run on the most powerful classical supercomputers.

Quantum computers have the potential to augment our ability to solve optimization problems. However, questions remain as to which quantum methods and optimization problem classes will enable us to achieve practical quantum advantage. So far, those questions have been difficult to answer because the field of quantum optimization has lacked systematic, reproducible benchmarks that allow us to make fair performance comparisons between quantum and purely classical optimization algorithms—especially given the limitations of what researchers can actually run on current quantum hardware.A white paper recently published inNature Reviews Physicsby representatives of theQuantum Optimization Working Groupaims to address these challenges.

Co-authored by 46 members of the working group—including representatives from enterprise and academic research institutions like University of Amsterdam, MIT, Zuse Institute Berlin, IBM, Hartree Centre, E.ON, Wells Fargo, and more—the white paper provides a broad overview of optimization methods and explores the potential for quantum advantage in a variety of problem settings. Moreover, it explores potential strategies to improve existing quantum methods. A notable example of this which the white paper mentions comes frompromising new collaborative researchconducted by IBM, Los Alamos National Lab, and the University of Basel. That research details how applying something known as theConditional Value at Risk (CVaR)to the samples returned from a quantum computer may be useful for various optimization tasks. More on that later.

The white paper also outlines the essential building blocks for quantum optimization algorithms, and proposes clear metrics and benchmarking problems to facilitate meaningful comparisons between quantum and classical optimization techniques. With the ultimate aim of accelerating progress toward quantum advantage in the field of combinatorial optimization, the paper serves as an important reminder that useful quantum advantages for optimization problems remain well within the realm of theoretical possibility, despite doubts that have been expressed by some in the research community.

Quantum optimization is likely one of the most misunderstood and polarizing domains in quantum algorithms research. Early claims suggested quantum computers could efficiently solve hard optimization problems by exploring every possible solution in parallel—a misconception that still shows up occasionally. Others argue that quantum optimization methods can at best provide a quadratic speedup over classical approaches that scale exponentially with problem size—ultimately yielding little value since a quadratic speedup over an exponential runtime is still an exponential runtime.

In the new optimization white paper, the authors explain that the truth, as usual, is more nuanced than either of these extremes. Quantum computers donotexplore every solution in parallel, at least not in a way that would provide immediate value for optimization. However, the assumption that quantum methods provide only a quadratic speedup for optimization problems is usually based on the worst-case version of problems, and based on the assumption that we have been tasked with finding an optimal solution that is provably better than all possible alternatives.

In the real world, worst-case optimization problem instances are rarely relevant to practical use cases. This is one of the key reasons classical algorithms have proven so useful in practical settings, despite the well known exponential runtimes required to generally solve these problems to provable optimality. In practice, classical algorithms and heuristics can obtain good solutions for many useful problems, even at large problem sizes.

However, there is still a lot of potential to be unlocked through better optimization algorithms. In fields like finance and supply chain management, even small improvements can make a huge impact. Solving real-world problems with purely classical methods involves simplifying them first, for instance, by removing certain factors or approximating the involved dynamics, and finding good solutions for more realistic models could make a huge difference. For some problems, classical algorithms fail to find good solutions altogether—even at relatively small problem sizes.

Quantum computing offers a new set of tools and capabilities that may improve our ability to tackle at least some of the real-world optimization problems that are classically challenging. A “good-enough” solution from a quantum algorithm that’s somehow better, that’s less costly, or that can be obtained faster than any classical solution has the potential to be immensely valuable.

As far as we know, quantum computers will never provide exponential speedups for all instances of all optimization problems. However, we do know special cases of problems that gain exponential speedups from quantum optimization techniques over classical alternatives.

For example, some optimization problems can be reduced to integer factoring, where Shor’s algorithm offers an exponential speedup over all known classical methods. This is the most prominent example of exponential advantage in quantum optimization, although not the most useful one, as such problems are unlikely to appear in practice. Still, there are other cases that promise similar speed-ups for efficiently finding better-than-classical solutions to more relevant problem classes,such as this recent result from researchers at Google, and examples like these are encouraging for quantum optimization in general.

While quantum algorithms with provable performance guarantees like the above-mentioned usually require fault tolerance, we also already have quantum optimization algorithms that we can run on today’s noisy hardware, and which are already capable of returning good solutions for at least some problems. It is entirely possible that new algorithms—or new variants of existing algorithms—will be discovered that enable quantum methods to rival or surpass the solution quality of classical methods using noisy hardware.

These algorithms are usually heuristics, particularly if executed on a noisy quantum computer. Heuristics are algorithms without a priori performance guarantees that are often based on a deep intuitive understanding of the considered problem, or which are achieved by terminating exact optimization algorithms prematurely.

Heuristics appear all throughout both quantum and classical computing. In fact,mostclassical optimization algorithms that we use in practice are heuristics. Genetic algorithms, A* search, and simulated annealing are all examples of influential classical optimization methods that developers have used for decades and that often work very well in practice.

All of this is to say that, despite their lack of performance guarantees, heuristics are not a bad thing. In many cases, they are the best we can hope for because we know these problems to be difficult in general for both quantum and classical methods. What we need are new quantum heuristics that work well for some of those problems where classical algorithms struggle. Thus, our job as a community is to find exact, approximate, or heuristic quantum optimization algorithms that work better than any classical technique—at least for some problems—as well as develop systematic benchmarks to understand the path to practical quantum advantage in optimization.

Today, researchers are beginning to demonstrate that quantum computers, with the help of classical post-processing methods called error mitigation, can deliver accurate expectation values for certain valuable problems. Given a system and a property we want to measure, the expectation value is a weighted average of the possible outcomes from the quantum computer. These are very useful calculations for chemistry and physics, but for optimization, we're often more interested in samples outputted from the processors, rather than expectation values.

Now,a paper recently published inNature Computational Sciencehighlights how to extract value when sampling from noisy quantum computers in the near-term. Specifically, the authors formally show how to determine the sampling overhead to compensate for noise in quantum optimization algorithms. In this particular context, the overhead turns out to be only a fraction of the computational overhead we get from error mitigation methods used to obtain unbiased estimators of expectation values.

Learn more about incorporating CVaR into your optimization workflows inour tutorial on IBM Quantum Learning.

In other words, if you have a quantum circuit that you know with reasonable probability will return a good solution to your optimization problem, you can now quantify the additional number of samples you need to draw from that circuit to counteract the effects of noise. This is a powerful insight that is essentially telling you both how to quantify the effect of the noise in your circuit, and what you must do to compensate for it.

Building on this, the authors show that a function called the Conditional Value at Risk (CVarR) can give us provable bounds on the noise-free expectation values—again with significantly less overhead than the error mitigation methods for expectation values.

What is CVaR, exactly? CVaR, or expected shortfall, is an important function used in finance that gives information about the tail of a distribution. In finance, it tells an investor the average amount of money they can expect to lose when the market turns south. In the context of quantum optimization, because we know how much more often we need to sample to get solutions that are at least as good as the noise-free case, we can also use it to derive lower and upper bounds for the expectation value of interest—i.e., we can potentially use it to identify the minimum value that a solution to our optimization problem can achieve.

Where other more common error mitigation methods like probabilistic error cancellation (PEC) and zero noise extrapolation (ZNE) are computationally expensive and provide exact expectation values, CVaR is computationally cheap and provides boundaries on expectation values, rather than an exact output. The result is noise-free information concerning the outputs of a quantum computer—information that we can use in our quantum optimization algorithms.

CVaR was previously proposed in 2019as a robust loss function to train parametrized circuits and is still very popular today. However, back then its usage was motivated solely by intuition. These new results close the gap in our theoretical understanding of the CVaR as a loss function and demonstrate its error mitigating capabilities for training parametrized circuits.

Theoretical results like those seen in the recent CVaR paper show potential directions to scale  quantum optimization methods on noisy hardware. They also highlight how important it is that we begin combining theory work with empirical research to explore the potential of theoretical proposals in practice.

In classical computing, there are many examples of algorithms that are shown to work well empirically long before they are fully understood from a theoretical perspective. Things are different in quantum computing, where the limited capabilities and availability of quantum hardware have historically meant that quantum algorithms are primarily developed theoretically before they are tested empirically.

However, these limitations are becoming less and less relevant every day. Now that the quantum community has access to gate-based quantum devices with hundreds of qubits that are capable of running circuits beyond exact classical simulation methods, we must begin doing more work that combines theoretical and empirical research. This also highlights the importance of systematic benchmarking to understand the path towards quantum advantage in optimization.

We’re eager to see continued progress in this field. For a more detailed perspective on the path toward quantum advantage in combinatorial optimization, be sure to read the Quantum Optimization Working Group’s full optimization white paperinNature Reviews Physicsand the recent CVaR paperinNature Computational Science. You can learn more about this and other quantum working groups in our previous blog,here, and explore our CVaR tutorialhere.

10 Jun 2025• Ryan Mandelbaum, Jay Gambetta, Jerry Chow, Tushar Mittal, Theodore J. Yoder, Andrew Cross, Matthias Steffen

5 Jun 2025• Joachim Schäfer, Jenay Patel, Valerie Chiang, Sanket Panda, Suhare Nur, Robert Davis

28 May 2025• Marina Caputo, Junye Huang, Marcel Pfaffhauser, Sanskriti Deva, Robert Davis

21 May 2025• Olivia Lanes, Robert Davis

Featured

Get access

Get started

Stay connected


---

# Growing the global quantum ecosystem

IBM® isn’t just building quantum computers—we’re collaborating globally to help lay the groundwork for the quantum computing industry. Just look at what’s taking shape in Japan.

Date

11 Mar 2025

Authors

Daiju Nakano

Kouichi Semba (University of Tokyo)

Jerry Chow

Topics

Share this blog

Japan has become one of the major world leaders in quantum computing.

The story began in 2019, when the University of Tokyo together with IBM launched theJapan-IBM Quantum™ Partnership—a partnership which brought to Japan its first IBM Quantum System One. Since then, the university has become a center of quantum computing science, and its IBM Quantum System One has been actively used by members of the Quantum Innovation Initiative Consortium launched jointly by the University of Tokyo and IBM, with the Consortium including leading Japanese companies like JSR, Toyota, Mitsubishi Chemicals, SoftBank, Sony and more.

Through that work, Japanese industry has played a key role in advancing quantum computing technology.

Japan and the University of Tokyo’s contributions go beyond the science of quantum computing. The University of Tokyo with IBM has helped develop the supply chain and ecosystem of Japanese quantum hardware suppliers through the world-firstQuantum Hardware Test Centerestablished in 2021. In an emerging market like quantum computing, growing the supply chain requires a focused approach centered on developing the components required for the industry. We’re beginning to see the flywheel effect in action—the accumulation of new research at Japanese institutions, new technology from industrial leaders, and collaboration between the United States and Japan have generated significant momentum and the seeds of a self-sustaining quantum industry.

Japan has become a model for building that supply chain and enabling quantum ecosystem growth. The University of Tokyo Quantum Hardware Test Center has been a crucial part of that story. As manufacturers such as TDK Corporation, Fujikura, Keycom, and I-PEX work on quantum computing components, the test center gives them the opportunity to access the specialized infrastructure necessary for evaluating the components’ performance. Beyond the test center, Japanese enterprises ULVAC and Kyocera are building important components and infrastructure for quantum computers, anchored by the demand created by IBM.

Just a few years ago, quantum computing was an exploratory technology operating exclusively in the realm of toy problems and academic research. Today, a worldwide community of users benefits from the industry-leading scale, quality, and speed of IBM quantum computers. That community has picked this technology up off the laboratory bench and brought it into the industrial workplace.

IBM has published a comprehensiveroadmap to advance useful quantum computingthat details our timeline for building large scale, fault tolerant quantum computers. As part of that work, IBM provides specifications for key components of current and future quantum computers to potential vendors. Through the University of Tokyo Quantum Hardware Test Center, IBM has worked with many suppliers to test some of these components.

TDK is a Tokyo-based company that builds electronic components, leads the world in magnetic technology, and has the largest market share in small-size batteries like those used in smartphones. Their components can be found in connected cars, industrial robots, smartphones and IoT smart devices powered AI. TDK has made great strides in developing and demonstrating cryogenic microwave isolators. These are key components in quantum computing systems built around superconducting qubits. TDK has used the University of Tokyo Quantum Hardware Test Center for early verification of their progress.

Another crucial component in superconducting qubit technology is the dilution refrigerator. Dilution refrigerators cycle helium isotopes through a mixing chamber to achieve the milliKelvin operating temperatures of superconducting qubits. ULVAC, Inc., a vacuum technology leader based in Chigasaki, is working to build a state-of-the-art dilution refrigerator that meets IBM quantum cryostat specifications. We look forward to testing ULVAC cryostats at our quantum datacenter later in 2025.

"IBM is leading the advancement of quantum computing, and ULVAC is proud to support this effort with our expertise in vacuum and cryogenic technology,” said Setsuo Iwashita, President and CEO of ULVAC, Inc. “By providing cutting-edge cryogenic solutions, we are committed to strengthening IBM’s quantum computing ecosystem and enabling scalable quantum infrastructure."

We are committed to strengthening IBM’s quantum computing ecosystem and enabling scalable quantum infrastructure.

More and more Japanese suppliers are diving into the quantum industry. We have seen impressive work ranging from cryogenic and superconducting wiring technology to high-density connectors and large-area laminates to accommodate larger quantum chips. Kyocera, the Kyoto ceramics and electronics manufacturer, is an important supplier of large-size semiconductor packaging. They now leverage that expertise for quantum applications. Fujikura, the Tokyo electronic equipment manufacturer, and Keycom, the Tokyo electronic measurement technology company, are leveraging their superconducting material expertise to develop the next generation of ultra high-density cryogenic cabling. I-PEX, in Kyoto, builds connector technology that they are evaluating for operation in the cryogenic environment of quantum computers.

This is the world’s quantum computing future: a global ecosystem of enterprises bringing their expertise to bear on quantum’s toughest engineering problems. We are scaling this technology through our joint efforts at the University of Tokyo and IBM. Our growing userbase is figuring out how to apply it to solve real problems. The Japanese quantum ecosystem is growing fast, thanks to the leadership of the University of Tokyo and the Japanese government’s investment in quantum. Japanese industry is beginning see the rewards of that effort. We hope the Japanese model can serve as an example for the rest of the world to follow.

10 Jun 2025• Ryan Mandelbaum, Jay Gambetta, Jerry Chow, Tushar Mittal, Theodore J. Yoder, Andrew Cross, Matthias Steffen

5 Jun 2025• Joachim Schäfer, Jenay Patel, Valerie Chiang, Sanket Panda, Suhare Nur, Robert Davis

28 May 2025• Marina Caputo, Junye Huang, Marcel Pfaffhauser, Sanskriti Deva, Robert Davis

21 May 2025• Olivia Lanes, Robert Davis

Featured

Get access

Get started

Stay connected


---

# Demonstrating a true realization of quantum-centric supercomputing

With utility-scale quantum computers capable of solving problems that brute force classical computing methods cannot, we show how quantum resources will shape the future of high-performance computing.

Date

20 Nov 2024

Authors

Ryan Mandelbaum

Iskandar Sitdikov

Topics

Share this blog

For the first time in history, computing has branched. Now, rather than progressing along a curve governed by an increasing number of transistors representing binary digits, we have access to a fundamentally different underlying computing architecture. This new architecture, the quantum bit, or qubit, has the potential to provide speedups for problems that have long proven intractable for classical supercomputers.

However, we’ve already established that quantum computers won’t solve every problem more efficiently than classical supercomputers. Instead, we expect quantum processing units (QPUs) to serve as accelerators for high-performance computers, assisting CPUs and GPUs for a subset of challenging problems as part of a heterogeneous compute model. We call this new supercomputing paradigm “quantum-centric supercomputing.”

Withour 2023 utility experiment, we demonstrated the first evidence towards the usefulness of today’s quantum computers. That research showed that quantum error mitigation could enable noisy, utility-scale quantum computers with 100+ qubits to run calculations beyond the ability of brute-force classical simulations.

Since then, we’ve shown that pairing utility-scale quantum computers with HPC resources allows us to run workloads that we believe will soon surpass anything we could achieve with classical HPC compute architectures alone. As researchers employ new tools to experiment in this uncharted computational space, we’re seeing more and more examples of how quantum computers are already accelerating scientific discovery.

Today at the SC24 conference, we presented this vision for the future of quantum-centric supercomputing (QCSC)—and invited HPC users to join us for the ride. We have demonstrated quantum utility, and now developers are using these systems to do real scientific work. As quantum technology continues to mature, we are deploying quantum as a resource alongside classical HPC using resource management systems like Slurm so HPC users can participate in this new era of quantum-enabled scientific discovery.

The future of computing is bigger than just quantum versus classical. It is about using both quantum and classical computing together to discover new algorithms.

Our thesis is that the future of computing will require mapping interesting problems to linear algebra problems—those represented by tensors and quantum circuits. And we know that quantum computers are better-equipped to run circuits than classical computers—that’s what we demonstrated by running the quantum utility experiment.

Therefore, at IBM Quantum our goal is to enable fast, accurate circuits which can be used as subroutines for these kinds of problems. We must push the performance of our hardware and software so they can run longer circuits more accurately—and so that quantum does not become a bottleneck for problems that require both quantum and classical resources concurrently.

Though we desire accurate results for quantum circuits run on real quantum computers, today’s quantum computers are biased by noise that we must mitigate or correct. At present, the techniques that would allow for fault-tolerant quantum computing all require a scaled-up physical resource overhead that has yet to be demonstrated by any quantum computer.

Take the modeling of the iron sulfide molecule Fe4S4as an example. We know that a classical computer is incapable of fully modeling this system. We also know that a present-day quantum computer would require millions of years to do so using established methods designed to make the most of noisy, shallow quantum circuits. In the future, a fault tolerant quantum computer with access to 4.53 million physical qubits could use those same methods to tackle this problem, but even that system would require an estimated 13 days’ runtime.

However, with quantum-centric supercomputing, we have access to a new suite of techniques that have the potential to produce better estimates than state-of-the-art classical compute techniques alone.

Sample-based quantum diagonalization (SQD), for example, is a chemistry simulation technique that uses the quantum computer to extract a noisy distribution of possible electronic configurations of a molecule in the form of bitstrings. Then, it runs an iterative correction entirely on a classical computer to cluster these bitstrings around the correct electron configuration. This technique, for a large enough noisy quantum computer with low enough error rates, has the potential to perform more efficiently than classical approximation methods.

IBM partnersRIKENandCleveland Clinic Foundationhave already published the results of experiments employing SQD, demonstrating promising evidence that quantum-centric supercomputing will open new applications previously considered impossible to execute without fault tolerance.

Realizing quantum-centric supercomputing techniques like SQD will require more than connected QPUs and CPUs. We also must combine the expectations of quantum and classical developer personas into a single user experience. We do so by updating HPC workload management systems to accommodate quantum workloads.

Data centers and supercomputing facilities use workload management systems (WMS) to take care of resource management and job scheduling. Workload management systems know what types of resource are available at what time, and can efficiently execute tasks based on resource requirements. Popular resource management systems include Slurm, LSF, PBS, etc., and while they may look different, they incorporate the same core concepts: resource, node, task, queue.

HPC developer personas working with workload management systems might describe a computation using a graph of tasks, as below:

Now, programming quantum computers requires a different programming workflow, in this case based around the Qiskit software stack. Quantum computational scientists use Qiskit primitives to interface with quantum computers. Qiskit primitives are computational abstractions that extract outputs from quantum computations in the form of circuits. There are two Qiskit primitives: Sampler, which samples from a distribution of possible outcomes from a quantum circuit, and Estimator, which calculates an expectation value, or weighted average of the possible outcomes, from the circuit.

Combining these two computing paradigms means calling Qiskit primitives within tasks of a larger computational workflow to drive the quantum part of the computation. Then, the workload manager will use a new type of resource—a quantum processing unit, or QPU—for compute allocation.

Working with a hybrid quantum-classical computational flow requires an appropriate architecture. Here, we will demonstrate example architectures and explore their benefits and drawbacks, and then show how to implement an SQD workflow on a QCSC architecture and run it with Slurm.

In the first architecture, we separate classical from quantum jobs. Quantum jobs are single executions of the Qiskit primitives, Sampler or Estimator. Separating queues by resource type lets us achieve higher utilization of the quantum device, since the queue is always saturated by strongly typed primitives jobs.

However, dueling queues might violate the order of submitted jobs. The start time of a classical job does not guarantee that you will have quantum allocation ready, as the QPUs might be occupied by other Qiskit primitives jobs started earlier. Therefore, QPU utilization has higher priority over classical resources, since quantum resources are rarer.

We could also imagine a hybrid model, where we treat quantum resources like any other resource, and all jobs in the integrated system as hybrid jobs. We differentiate and allocate based on the type of resource required by each job. Here we also define a vQPU resource. Nodes with this resource have access to a quantum computer API.

With this approach, once your quantum-classical job starts running, all resources required for this job are blocked for entire duration of the job. This is useful for algorithms that require a tight loop between classical and quantum resources. The downside of this approach is the blocking of quantum resource for the duration of the entire job, which lowers the utilization of the quantum system.

Finally, we could imagine a mixed model, allowing you to unlock benefits of both of the above mentioned strategies, while mitigating drawbacks through configuration. This type of architecture requires more complex configuration and development, but it offers the possibility of tighter integration between classical and quantum when needed by altering the job submission to hybrid and classical queues.

We can map the above mentioned architectures to existing workload management systems. Let’s take the dueling queue architecture and apply it to Slurm. For a simple implementation we need 3 components: a client that implements primitives, middleware to handle client calls, and primitives job logic to talk to the API of the quantum system.

These are all relatively easy to implement: for the client, you will need to extend the Qiskit primitives base classes. For middleware, we need to implement a microservice that acts like a proxy for Slurm rest or central deamon commands. A job is a call to the quantum system APIs.

Now, let’s write our first hybrid quantum-classical program based around the SQD technique and run it on this integrated environment. As with any quantum computation, it will require us to follow the steps of mapping of the problem, optimization, execution, and postprocessing.

For each step of this process we will create a job script and submission script for Slurm. Let’s start with mapping, converting a molecule’s full-configuration interaction into circuit of interest.

map.py

map.sh

For the optimization step, we will transform the circuits into ISA circuits optimized for the target backend. Here, we will be using an implemented client to get the specifications of available backends.

optimize.py

optimize.sh

For execution, we will again use the client with Sampler and execute the ISA circuit against the backend.

execute.py

execute.sh

In the postprocessing step we will use the SQD procedure, leveraging parallel computation to recover energies.

postprocessing.py

postprocessing.sh

Now, we have everything that we need to run our workflow. For simplicity we will chain jobs, but any workflow management tool can be used to construct the execution graph.

We can explore the Slurm queue to see the execution of our job and the execution of primitives.

This architecture overview provides you a simple yet powerful integration plan to bring quantum into an HPC infrastructure without disturbing existing usage patterns of users. The learning curve for users of this integrated environment is close to zero, as we are keeping side dependencies to a bare minimum and reusing what is already available to our large user base.

Today, IBM is already providing users with the hardware and software tools they need to begin exploring quantum-centric supercomputing.Last week, we showed that IBM quantum computers can run circuits with over 5,000 gates—a range well into the regime beyond exact classical simulation.

We also demonstrated a host of features that are making quantum easier-than-ever to use. These includeQiskit addons, which are packages for implementing state-of-the-art techniques like SQD, andQiskit Functions, which are hosted services to further optimize quantum application development provided by IBM and third parties.

We encourage you to get started exploring quantum-centric supercomputing today, so that together we can bring useful quantum computing to the world.

10 Jun 2025• Ryan Mandelbaum, Jay Gambetta, Jerry Chow, Tushar Mittal, Theodore J. Yoder, Andrew Cross, Matthias Steffen

5 Jun 2025• Joachim Schäfer, Jenay Patel, Valerie Chiang, Sanket Panda, Suhare Nur, Robert Davis

30 May 2025• Robert Davis, Olivia Lanes, John Watrous

22 May 2025• Javier Robledo-Moreno, Gavin Jones, Roberto Lo Nardo, Robert Davis

Featured

Get access

Get started

Stay connected


---

# IBM Quantum delivers on performance challenge made two years ago

At the first-ever IBM Quantum Developer Conference, IBM is enabling algorithm discovery with high-performance quantum computers and easy-to-use quantum software.

Date

13 Nov 2024

Authors

Jay Gambetta

Ryan Mandelbaum

Topics

Share this blog

Two years ago, IBM® set an ambitious challenge for the quantum computing community: Develop quantum algorithms incorporating circuits with 100 qubits and gate depths of 100, while IBM would build a quantum computer capable of returning accurate values for those circuits in less than a day’s runtime.

Today at the first-ever IBM Quantum™ Developer Conference (QDC), IBM researchers announced that they’d successfully delivered on that promise: a quantum computer capable of running quantum circuits with up to 5,000 two-qubit gate operations — powered by the second revision of the IBM Quantum Heron. Thanks to groundbreaking advances in hardware, middleware, and software, Heron is now capable of running accurate calculations employing circuits with 5,000 two-qubit gates. For comparison, theutility experiment, which heralded in theera of quantum utilityand was powered by the IBM Quantum Eagle processor, only went out to a total of 2,880 two-qubit gates.

Users of IBM Quantum computers and services now have access to this performance thanks to the Qiskit software stack, and theQiskit Functionsoffered by third-party partners. The broader quantum community has published dozens of papers at the utility scale, while Functions providers have delivered their own capabilities that push the accuracy and scale of quantum circuits to drive algorithm discovery further.

This announcement arrives alongside a host of hardware and software releases, with IBM researchers reporting continued success advancing along our development and innovation roadmaps. Here’s what this all means for you.

IBM Quantum Heron R2

Accurately running circuits with 5,000 two-qubit gates is the culmination of the 100×100 challenge we announced at IBM Quantum Summit 2022. We selected this number because it is well into the regime of circuits beyond exact classical simulation. Developing quantum routines with 5,000+ gates gives users the opportunity to perform real scientific discovery with quantum computers, and to push forward in the search for quantum advantage.

Achieving this goal first and foremost required significant breakthroughs across our hardware and software. At QDC 2024, we showed off the latest version R2 of our modular Heron chip first introduced in 2023. This new chip features 156 qubits in a heavy-hex layout, and preserves the tunable coupler architecture we introduced last year to suppress crosstalk. We also added new two-level system mitigation to help reduce the impact of an important source of noise. Two-level systems are essentially disturbances to the qubits interacting with the material surrounding them.

Running long circuits also requires speed; we must be able to run circuit layers quickly. Over the course of the last year, we made updates to the quantum system software stack, further optimized data movement, and introduced our latest generation runtime. We also introduced parametric compiling, so you only have to compile iterative circuits once if parameters are the only things changing between iterations. Thanks to these updates, we’ve achieved speeds of over 150,000 circuit layer operations per second (CLOPS).

Increase in circuit layer operations per second (CLOPS), 2022-2024

Perhaps most importantly, we’ve improved the reliability of our computers and services. When we ran the utility experiment last year, we had to do it with custom circuits and software. Now, clients are able to replicate these demonstrations on their own using Qiskit tools, and they can run the corresponding circuits 50x faster than the experiment in the original quantum utility paper.

But the 100×100 challenge was about more than just improving IBM Quantum hardware and software. It was a call to action for the global quantum community to develop algorithms that would take full advantage of such a system. Alongside today’s announcements, we’re thrilled to say that a number of our startup partners are now offering groundbreaking capabilities of their own, which they are making available as part of theQiskit Functions Catalog. Each of these demonstrations are approaching the 5,000 gate threshold as well.

Now that quantum devices can run circuits beyond the ability of brute-force classical methods, we must begin to identify the algorithms that will use these circuits to achieve computational advantage. IBM Quantum offers powerful tools that can help researchers in their discovery.

These tools center around an important thesis—that, as computing matures, algorithms increasingly reliant on linear algebra will need to employ quantum computers in places where classical computers can’t keep up. The accuracy of these algorithms depends on the accuracy of their quantum subroutines, which, in turn, depends on the length of the quantum circuits that we can run.

In this paradigm, where quantum is a subroutine of a larger algorithm, we need to ensure that our quantum computers can run long circuits, and that quantum hardware is enabled by fast and performant software. Quantum cannot be a bottleneck.

IBM Quantum technology stack

This year, we announced several new tools for improving the performance of our quantum software. We’ve previously discussed how recent updates to the Qiskit SDK—including its first stable release, Qiskit SDK v1.0—have shown it to be themost performant quantum softwarefor building and transpiling circuits. However, Qiskit is more than just an SDK. It’s a full software stack of tools for running quantum circuits and abstracting parts of the development workflow, and our efforts to improve Qiskit have extended far beyond the core SDK.

For example, earlier this year, we introducedthe Qiskit Transpiler Serviceas a preview for our Premium Plan users. The Qiskit Transpiler Service allows you to transpile circuits in the cloud, leveraging the power of AI-powered transpiler passes to run circuits more efficiently. Thanks to these new methods, we are already seeing a 30% depth improvement in the circuits we previously benchmarked while testing Qiskit’s performance.

At the same time, it isn’t enough for our software to just be fast. It must also be easy to use. This year, we introducedQiskit addons, research capabilities developed as modular tools that can plug into a workflow to help design new algorithms at the utility scale. Their impact on the R+D process is clear. Last year, RIKEN and IBM used quantum-centric supercomputing environments to model chemistry problems beyond brute force solutions. This work took approximately a year from initial arxiv to publication and now we’ve transformed that research intothe SQD addon. Since then, Cleveland Clinic Foundation has used the SQD addon to publish chemistry simulations of their own—both in just a few months’ time.

We’ve also released a variety of additional tools that are further simplifying the R+D process for our users so they can begin exploring domain-specific applications with quantum computation. We offer new Qiskit Functions that abstract away many of the complexities of quantum software development as part of the Qiskit Functions Catalog. We’ve also released the Qiskit Code Assistant in preview for our Premium Plan users. Qiskit Code Assistant lets you write quantum computing code faster with GenAI, powered by ourgranite-8b-qiskit modeland IBM watsonx™.



IBM Quantum Crossbill (L) and IBM Quantum Condor (R)

IBM Quantum Flamingo

Last year, we updated our development roadmap to track more than just systems slated for release to clients—but also the innovations we must make to realize error-corrected quantum computing at scale. We showed off several of those innovations at this year’s QDC.

Central to our innovation roadmap is our plan to develop new couplers to run gates across multiple quantum chips. This year, we reported the results of two kinds of couplers: l-couplers, which connect chips with cables, and m-couplers, which seam together adjacent chips.

Today, we demonstrated l-couplers with a proof-of-concept called IBM Quantum Flamingo, which connects two Heron R2 chips with four connectors measuring up to a meter long. L-couplers allow us to implement CNOT gates across distant chips. At the moment, the best CNOTs we’ve benchmarked have been on test devices, with errors per gate of 3.5% for a 235 nanosecond operation. We expect these metrics to improve, and will debut a production-ready Flamingo-based system for use by our clients by the end of 2025.

We also prototyped the technologies for m-couplers integrated into a scaled-up device called IBM Quantum Crossbill.  This chip comprises three connected Herons with 548 couplers and 8 interchip m-coupler connections. Last year, we showed IBM Quantum Condor—a 1,121-qubit chip that tested the limits of yield and monolithic scaling. This year, we showed that with m-couplers, we can perform high quality two-qubit gates and also showed integrated connectors with large-scale silicon packaging. Fully assembled in Crossbill, together these produce a device with over 1000 quantum elements with just 1/5 the area of a fully packaged Condor in circuit board. The m-coupler technology provides additional scaling options for the next parts of our roadmap.

Now, we must continue developing the advances that will allow us to implement theerror correcting codewe published earlier this year. This code has the potential to store quantum information with a fraction of the overhead associated with other leading QEC codes. However, it also requires higher qubit connectivity—i.e., qubits connected to more of their neighbors. Additionally, we’ll need to develop c-couplers, or couplers that link distant qubits on the same chip. We’re well on our way toward realizing these technologies, with the hope of demonstrating c-couplers with Kookaburra, slated for 2026.

Finally, we’re beginning to realize our ultimate vision of quantum-centric supercomputing. Today, classical computing facilities are capable of running tremendously complex workflows thanks to systems called workload management systems. They oversee available computational resources and efficiently execute tasks on them. This year, we worked with Rensselaer Polytechnic Institute to bring quantum into the picture of workload management. With the help of our partners at RPI, we were able to demonstrate the first heterogenous workflow in a fully realized quantum-centric supercomputing environment by connecting the AiMOS supercomputer and IBM Quantum System One into a singular computational environment managed by the Slurm resource manager.

We’ve always thought of our quantum mission as a working agreement between IBM and the global quantum research community. We push the performance of our hardware and software, but it’s up to the community to discover the algorithms that will use these tools to achieve quantum advantage.

We are committed to offering the tools that our users need to make real scientific discoveries in this new era, where quantum computers are finally demonstrating utility beyond brute force classical methods. Furthermore, we continue investing in the development ofworld-class learning resourcesso users can extend these tools, and we continue to collaborate with our clients and partners to help bring their ideas to life. Now, with the first-ever QDC, we are offering developers the resources they need to extend quantum computers to their respective domains.

Now, let’s work together to unlock quantum advantage as a community and bring useful quantum computing to the world.

10 Jun 2025• Ryan Mandelbaum, Jay Gambetta, Jerry Chow, Tushar Mittal, Theodore J. Yoder, Andrew Cross, Matthias Steffen

5 Jun 2025• Joachim Schäfer, Jenay Patel, Valerie Chiang, Sanket Panda, Suhare Nur, Robert Davis

28 May 2025• Marina Caputo, Junye Huang, Marcel Pfaffhauser, Sanskriti Deva, Robert Davis

21 May 2025• Olivia Lanes, Robert Davis

Featured

Get access

Get started

Stay connected


---

# What is quantum computing?

Senior Writer

IBM Blog

Senior Editorial Strategist



Quantum computing is an emergent field of computer science and engineering that harnesses the unique qualities of quantum mechanics to solve problems beyond the ability of even the most powerful classical computers.

The field of quantum computing includes a range of disciplines, including quantum hardware and quantum algorithms. While still in development, quantum technology will soon be able to solve complex problems that classicalsupercomputerscan’t solve (or can’t solve fast enough).

By taking advantage of quantum physics, large-scale quantum computers would be able to tackle certain complex problems many times faster than modern classical machines. With a quantum computer, some problems that might take a classical computer thousands of years to solve might be solved in a matter of minutes or hours.

Quantum mechanics, the study of physics at very small scales, reveals surprising fundamental natural principles. Quantum computers specifically harness these phenomena to access mathematical methods of solving problems not available with classical computing alone.

In practice, quantum computers are expected to be broadly useful for two types of tasks: modeling the behavior of physical systems and identifying patterns and structures in information.

Quantum mechanics is a bit like theoperating systemof the universe. A computer that uses quantum mechanical principles to process information has certain advantages in modeling physical systems. Therefore, quantum computing is of particular interest for chemistry and material science applications. For example, quantum computers might help researchers seeking useful molecules for pharmaceutical or engineering applications identify candidates more quickly and efficiently.

Quantum computers can also process data by using mathematical techniques not accessible to classical computers. That means they can give structure to data and help discover patterns that classical algorithms alone might miss. In practice, this might be useful for applications ranging from biology (for example, protein folding) to finance.

Today, much of the work of quantum computing research involves searching for algorithms and applications within these broad categories of expected use. That is in addition to building the new technology itself.

As leading institutions like IBM, Amazon, Microsoft and Google as well as startups like Rigetti and Ionq continue investing heavily in this exciting technology,quantum computing is estimated to become a USD 1.3 trillion industry by 2035.

Research newsletter

Discover emerging research in AI, quantum, hybrid cloud, and more from IBM’s experts with the monthly Future Forward newsletter. See theIBM Privacy Statement.

Your subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribehere. Refer to ourIBM Privacy Statementfor more information.

When discussing quantum computers, it is important to understand that at the smallest scales, the universe behaves very differently from what we are used to in our day-to-day lives. Compared to what we learned in grade-school physics, the behaviors of quantum objects are often bizarre and counterintuitive.

Describing the behaviors of quantum particles presents a unique challenge. Most common-sense paradigms for the natural world lack the vocabulary to communicate the surprising behaviors of quantum particles. But quantum mechanics reveals how the universe really works. Quantum computers take advantage of quantum mechanics by replacing traditional binary bit circuits with quantum particles called quantum bits, or qubits. These particles behave differently from bits, exhibiting unique properties that can be described only with quantum mechanics.

To understand quantum computing, it is important to understand four key quantum mechanics principles:

A qubit itself isn't very useful. But it can place the quantum information it holds into a state of superposition, which represents a combination of all possible configurations of the qubit. Groups of qubits in superposition can create complex, multidimensional computational spaces. Complex problems can be represented in new ways in these spaces.

When a quantum system is measured, its state collapses from a superposition of possibilities into a binary state, which can be registered like binary code as either a zero or a one.

Entanglement is the ability of qubits to correlate their state with other qubits. Entangled systems are so intrinsically linked that when quantum processors measure a single entangled qubit, they can immediately determine information about other qubits in the entangled system.

Interference is the engine of quantum computing. An environment of qubits placed into a state of collective superposition structures information in a way that looks like waves, with amplitudes associated with each outcome.

These amplitudes become the probabilities of the outcomes of a measurement of the system. These waves can build on each other when many of them peak at a particular outcome or cancel each other out when peaks and troughs interact. Amplifying a probability or canceling out others are both forms of interference.

Decoherence is the process in which a system in a quantum state collapses into a nonquantum state. It can be intentionally triggered by measuring a quantum system or by other environmental factors (sometimes these factors trigger it unintentionally). Generally speaking, quantum computing requires avoiding and minimizing decoherence.

To better understand quantum computing, consider that two surprising ideas are both true. The first is that objects that can be measured as having definite states—qubits in superposition with defined probability amplitudes—behave randomly. The second is that distant objects—in this case, entangled qubits—can still behave in ways that, though individually random, are strongly correlated.

A computation on a quantum computer works by preparing a superposition of computational states. A quantum circuit, prepared by the user, uses operations to entangle qubits and generate interference patterns, as governed by a quantum algorithm. Many possible outcomes are canceled out through interference, while others are amplified. The amplified outcomes are the solutions to the computation.

The primary difference between classical and quantum computers is that quantum computers use qubits instead of bits. While quantum computing does use binary code, qubits process information differently from classical computers. But what are qubits and where do they come from?

While classical computers rely on bits (zeros and ones) to store and process data, quantum computers process data differently by usingquantum bits (qubits)in superposition.

A qubit can behave like a bit and store either a zero or a one, but it can also be a weighted combination of zero and one at the same time. When qubits are combined, their superpositions can grow exponentially in complexity: two qubits can be in a superposition of the four possible 2-bit strings, three qubits can be in a superposition of the eight possible 3-bit strings, and so on. With 100 qubits, the range of possibilities is astronomical.

Quantum algorithms work by manipulating information in a way inaccessible to classical computers, which can provide dramatic speed-ups for certain problems—especially when quantum computers and high-performance classical supercomputers work together.

Generally, qubits are created by manipulating and measuring systems that exhibit quantum mechanical behavior, such as superconducting circuits, photons, electrons, trapped ions and atoms.

There are many different ways of making the qubits used in quantum computing today, with some better suited for different types of tasks.A few of the more common types of qubits in use are as follows:

Computers that use quantum bits have certain advantages over computers that use classical bits. Because qubits can hold a superposition and exhibit interference, a quantum computer that uses qubits approaches problems in ways different from classical computers.

As a helpful analogy for understanding how quantum computers use qubits to solve complicated problems, imagine you are standing in the center of a complicated maze. To escape the maze, a traditional classical computing approach would be to “brute force” the problem, trying every possible combination of paths to find the exit. This kind of computer would use bits to explore new paths and remember which ones are dead ends.

A quantum computer might derive the correct path without needing to test all the bad paths, as if it has a bird's-eye view of the maze. However, qubits don't test multiple paths at once. Instead, quantum computers measure the probability amplitudes of qubits to determine an outcome.

These amplitudes function like waves, overlapping and interfering with each other. When asynchronous waves overlap, it effectively eliminates possible solutions to complex problems, and the realized coherent wave or waves present a correct solution.

Use IBM Quantum Platform suite of applications to support your quantum research and development needs. Get access to a free, in-depth, university-level introduction to quantum computing fundamentals.

An IBM quantum processor is a wafer not much bigger than the silicon chips found in a laptop. However, modern quantum hardware systems (used to keep the instruments at an ultracold temperature) and the extra room-temperature electronic components to control the system and process quantum data are about the size of an average car.

While the large footprint of a complete quantum hardware system makes most quantum computers anything but portable, researchers and computer scientists are still able to access off-site quantum computing capabilities throughcloud computing. The main hardware components of a quantum computer are as follows.

Composed of qubits laid out in various configurations to allow for communication, quantum chips—also known as the quantum data plane—act as the brain of the quantum computer.

As the core component in a quantum computer, a quantum processor contains the system’s physical qubits and the structures required to hold them in place.Quantum processing units (QPUs)include the quantum chip, control electronics and classical compute hardware required for input and output.

Your desktop computer likely uses a fan to get cold enough to work. Quantum processors need to be very cold—about a hundredth of a degree above absolute zero—to minimize noise and avoid decoherence in order to retain their quantum states. This ultralow temperature is achieved with supercooled superfluids. At these temperatures, certain materials exhibit an important quantum mechanical effect: electrons move through them without resistance. This effect makes them superconductors.

When materials become superconductors, their electrons match up, forming Cooper pairs. These pairs can carry a charge across barriers, or insulators, through a process known as quantum tunneling. Two superconductors placed on either side of an insulator form a Josephson junction, a crucial piece of quantum computing hardware.

Quantum computers use circuits with capacitors and Josephson junctions as superconducting qubits. By firing microwave photons at these qubits, we can control their behavior and get them to hold, change and read out individual units of quantum information.

Research continues improving quantum hardware components, but that’s only one half of the equation. The crux of users’ discovery of quantum advantage will be a highly performant and stable quantum software stack to enable the next generation of quantum algorithms.

In 2024, IBM introduced thefirst stable version of the Qiskitopen source software development kit (SDK), Qiskit SDK 1.x. With over 600,000 registered users and 700 global universities that use it to develop quantum computing classes, Qiskit has become the preferred software stack for quantum computing.

But Qiskit is more than just the world’s most popular quantum development software to build and construct quantum circuits. We are redefining Qiskit to represent the full-stack software for quantum at IBM, extending the Qiskit SDK with middleware software and services to write, optimize and run programs on IBM Quantum systems—including newgenerative AIcode-assistance tools.

Quantum computing is built on the principles of quantum mechanics, which describe how very small objects behave differently from large objects. But because quantum mechanics provides the foundational laws for our entire universe, on a very small level, every system is a quantum system.

For this reason, we can say that while conventional computers are also built on top of quantum systems, they fail to take full advantage of the quantum-mechanical properties during their calculations. Quantum computers are expected to take better advantage of quantum mechanics to conduct calculations that even high-performance computers cannot.

From antiquated punch-card adders to modern supercomputers, traditional (or classical) computers essentially function in the same way. These machines generally perform calculations sequentially, storing data by using binary bits of information. Each bit represents either a 0 or 1.

When combined into binary code and manipulated by using logic operations, we can use computers to create everything from simple operating systems to the most advanced supercomputing calculations.

Quantum computers, like classical computers, are problem-solving machines. But instead of bits, quantum computing uses qubits. Qubits are used to process data like traditional bits; however, by harnessing quantum phenomena, qubits have access to more complex mathematics for a different type of computation. This is due to quantum mechanical concepts known as superposition and interference, which were discussed earlier.

Quantum processors do not perform mathematical equations the same way classical computers do. Unlike classical computers that must compute every step of a complicated calculation, quantum circuits made from logical qubits can process complex problems more efficiently.

While traditional computers commonly provide singular answers, probabilistic quantum machines often provide ranges of possible answers. This range might make quantum computing seem less precise than traditional computation. However, for the kinds of incredibly complex problems quantum computers might soon solve, this way of computing might potentially save hundreds of thousands of years of traditional computation.

In practice, quantum computers and classical computers work together in combined workflows to solve problems. The most efficient methods distribute the parts of a computation that quantum computers are best at to quantum computing resources and the parts that classical computers are best at to classical computing resources.

Fully realized quantum computers working in concert withhigh-performance classical computerswould be far superior to classical computers alone for certain kinds of problems like integer factorization. But quantum computing is not ideal for every (or even most) problems.

For most kinds of tasks and problems, classical computers are expected to remain the best solution. But when scientists and engineers encounter certain highly complex problems, quantum computing comes into play. For these types of difficult calculations, even the most powerful classical supercomputers pale in comparison to quantum computing. That’s because even the most powerful classical supercomputers are binary code-based machines reliant on 20th-century technology.

Complex problems are problems with lots of variables interacting in complicated ways. For example, modeling the behavior of individual atoms in a molecule is a complex problem because of all the different interactions between electrons. Identifying new physics in a supercollider is also a complex problem. There are some complex problems that we do not know how to solve with classical computers at any practical scale.

A classical computer might be great at difficult tasks like sorting through a large database of molecules. But it struggles to solve more complex problems, like simulating how those molecules behave.

Today, if scientists want to know how a molecule behaves, they must synthesize it and experiment with it in the real world. If they want to know how a slight tweak would impact its behavior, they usually need to synthesize the new version and run their experiment all over again. This is an expensive, time-consuming process that impedes progress in fields as diverse as medicine andsemiconductordesign.

A classical supercomputer might try to simulate molecular behavior with brute force by using its many processors to explore every possible way every part of the molecule might behave. But as it moves past the simplest, most straightforward molecules available, the supercomputer stalls. No classical computer is able to handle all the possible permutations of molecular behavior by using any known methods.

Quantum algorithms take a new approach to these sorts of complex problems by creating multidimensional computational spaces in which to run algorithms that behave much like these molecules themselves. This turns out to be a much more efficient way of solving complex problems like chemical simulations.

One way to think about this: Classical computers need to crunch the numbers to figure out how a molecule will behave. A quantum computer doesn’t need to crunch the numbers. It can mimic the molecular system directly.

Quantum algorithms can also process data in ways classical computers can’t, offering new structure and insights.

First theorized in the early 1980s, it wasn’t until 1994 that mathematician Peter Shor published one of the first practical real-world applications for a hypothetical quantum machine. Shor’s algorithm for integer factorization demonstrated how a quantum mechanical computer could potentially break the most advanced cryptography systems of the time—some of which are still used today. Shor’s findings demonstrated a viable application for quantum systems, with dramatic implications for not justcybersecurity, but many other fields.

Engineering firms, financial institutions and global shipping companies, among others, are exploring use cases where quantum computers might solve important problems in their fields. An explosion of benefits from quantum research and development is taking shape on the horizon. As quantum hardware scales and quantum algorithms advance, we can soon find new solutions to big, important problems like molecular simulation, energy infrastructure management and financial market modeling.

Quantum computers excel at solving certain complex problems with many variables. From thedevelopment of new drugsto advancements insemiconductor developmentand tackling complex energy challenges, quantum computing might hold the key to breakthroughs in several critical industries.

Quantum computers capable of simulating molecular behavior and biochemical reactions could speed up the research and development of life-saving new drugs and medical treatments.

For the same reasons quantum computers could impact medical research, they might also provide undiscovered solutions for mitigating dangerous or destructive chemical byproducts. Quantum computing could lead to improved catalysts that enable petrochemical alternatives or better processes for the carbon breakdown necessary for combating climate-threatening emissions.

As interest and investment inartificial intelligence (AI)and related fields likemachine learningramps up, researchers are pushingAI modelsto new extremes, testing the limits of our existing hardware and demanding tremendous energy consumption. There is some reason to think that quantum algorithms might be able to look at datasets in a new way, providing a speed-up for some machine learning problems.

While no longer simply theoretical, quantum computing is still under development. As scientists around the world strive to discover new techniques to improve the speed, power and efficiency of quantum machines, technology is approaching a turning point. We understand the evolution of useful quantum computing using the concepts of quantum advantage and quantum utility.

Quantum utilityrefers to any quantum computation that provides reliable, accurate solutions to problems that are beyond the reach of brute-force classical computing quantum-machine simulators. Previously, these problems were accessible only to classical approximation methods—usually problem-specific approximation methods carefully crafted to exploit the unique structures of a specific problem. IBM first demonstrated quantum utility in 2023.

Broadly defined, the term quantum advantage describes a situation where quantum can provide a better, faster, or cheaper solution than all known classical methods. An algorithm that exhibits quantum advantage on a quantum computer should be able to deliver a significant, practical benefit beyond all known classical computing methods. IBM expects that the first quantum advantages should be realized by late 2026, if the quantum and high-performance computing communities work together.

Because quantum computing now offers a viable alternative to classical approximation for certain problems, researchers say it is a useful tool for scientific exploration, or that it has utility. Quantum utility does not constitute a claim that quantum methods have achieved a proven speed-up over all known classical methods. This is a key difference from the concept of quantum advantage.

IBM has introduced two metrics to benchmark quantum computers:layer fidelity and circuit layer operations per second (CLOPS).

An extremely valuable benchmark, layer fidelity provides a way to encapsulate the entire quantum processor’s ability to run circuits while revealing information about individual qubits, gates and crosstalk. By running the layer fidelity protocol, researchers can qualify the overall quantum device while also gaining access to granular performance and error information about individual components.

In addition to layer fidelity, IBM also defined a speed metric:circuit layer operations per second (CLOPS). Currently, CLOPS is a measure of how quickly processors can run quantum volume circuits in series, acting as a measure of holistic system speed, incorporating quantum and classical computing.

Together, layer fidelity and CLOPS provide a new way to benchmark systems that’s more meaningful to the people trying to improve and use quantum hardware. These metrics make it easier to compare systems to one another, to compare our systems to other architectures and to reflect performance gains across scales.

Circuit depth is also an essential capability of a quantum processing unit. It is a measure of the number of parallel gate executions—the number of steps in a quantum circuit—that the processing unit can run before the qubits decohere. The greater the circuit depth, the more complex circuits the computer can run.

Today, companies like IBM, Google, Microsoft, D-Wave, Rigetti Computing and more make real quantum hardware. Cutting-edge tools that were merely theoretical four decades ago are now available to hundreds of thousands of developers. Engineers are delivering ever-more-powerful superconducting quantum processors at regular intervals, alongside crucial advances in software and quantum-classical orchestration. This work drives toward the quantum computing speed and capacity necessary to change the world.

Now that the field has achieved quantum utility, researchers are hard at work to make state-of-the-art quantum computers even more useful. Researchers at IBM Quantum and elsewhere have identified some key challenges to improve upon quantum utility and potentially achieve quantum advantage:

Prepare for the most significant computing revolution in 60 years. Read The Quantum Decade to find out how you, too, can be quantum ready — and how this bleeding-edge technology can help you and your business thrive the moment quantum computers come of age. Because that moment is closer than you think.

Explore IBM's quantum computing roadmap, which charts advancements in quantum processors, software and scaling technologies. See how quantum breakthroughs today are driving the future of computation, from cutting-edge research to scalable commercial applications.

Join our community of quantum innovation leaders, researchers, developers, and enthusiasts. IBM Quantum Network provides engagement opportunities and resources to academic institutions, startups, and organizations around the world.

Explore how leading companies such as Boeing, Mercedes-Benz, ExxonMobil and CERN are using IBM Quantum technology to tackle complex problems. Discover real-world applications that are shaping the future of industries with quantum technology.

After nearly three years, 16 video lessons, and thousands of words of accompanying text, we are proud to share that our “Understanding quantum information and computation” educational series has now been released in full!

Learn the basics of quantum computing, and how to use IBM Quantum services and systems to solve real-world problems.

Qiskit is the world’s most popular software stack for quantum computing. Build circuits, leverage Qiskit functions, transpile with AI tools, and execute workloads in an optimized runtime environment.

IBM provides quantum computing technologies including Qiskit SDK and Qiskit Runtime for scalable and performance-oriented quantum computing.

Bringing useful quantum computing to the world through Qiskit Runtime and IBM Quantum Safe.

Safeguard your enterprise against post-quantum cryptography risks with IBM Quantum Safe Transformation Services.

Unlock the power of quantum computing with IBM's high-performance, scalable quantum systems. Explore the capabilities that will shape the future of computing and security.


---

# What Is Quantum Computing? Full Beginner Guide 2025

2025.04.29·Blog



Quantum computing is a revolutionary field of computing that harnesses the principles of quantum mechanics to process information in fundamentally different ways than classical computers.

In classical computing (used in today's computers), information is processed using bits that can be either 0 or 1. However, in quantum computing, the basic unit of information is called a qubit, and it can be 0, 1, or both at the same time (thanks to a concept called superposition).

Quantum computers also use a special property called entanglement, which links qubits together so they can work as a team to solve problems faster.

Imagine trying to find the shortest path through a maze. A classical computer checks one path at a time, but a quantum computer can explore many pathssimultaneously, making it much faster at solving certain kinds of problems.







Quantum bits, orqubits, are the fundamental units of quantum computing. Unlike classical bits that can only be in a state of 0 or 1, qubits leverage quantum mechanics to exist in a combination of states (0, 1, or both simultaneously) due to superposition. A few of the more commontypes of qubitsin use are as follows:



Superposition in quantum computingrefers to the ability of a quantum bit (or qubit) to exist in multiple states simultaneously.

Unlike classical bits, which are either 0 or 1, qubits can be in a superposition of both 0 and 1 at the same time. This fundamental concept of quantum mechanics allows quantum computers to perform complex computations in parallel, potentially making them exponentially more powerful and faster than classical computers for certain tasks.



Entanglementis a unique quantum phenomenon where qubits become interlinked such that the state of one qubit instantly influences the state of the other, even over large distances. This property is key to quantum computing's power, enabling:



Quantum interferenceuses the wave-like nature of quantum systems to amplify correct computational paths while canceling out incorrect ones. This process helps refine the results of quantum algorithms, such asShor's algorithm(used for factoring) andGrover's algorithm(used for searching).



Decoherenceoccurs when a qubit loses its quantum properties due to interaction with the surrounding environment. For example:

Decoherence poses a significant challenge for quantum computers, as it limits the time qubits can remain in their quantum states (coherence time). Overcoming decoherence requires advanced error correction techniques and isolated environments, such as cryogenic systems for superconducting qubits.





In essence, quantum computing uses superposition, entanglement, and interference to perform complex calculations, with decoherence being a key obstacle to overcome.

Quantum computing leverages the principles of quantum mechanics to process information. At its core arequantum bits (qubits), which can exist in a state ofsuperposition, meaning they can represent both 0 and 1 at the same time. This enables quantum computers to explore multiple solutions simultaneously, enhancing computational efficiency.

Entanglementlinks qubits so that the state of one affects the state of another, allowing coordinated operations.Quantum interferencethen amplifies correct paths and cancels out errors, refining the results.

However,decoherenceoccurs when qubits lose their quantum state due to environmental interference, posing a challenge. To counter this, quantum computers must be maintained in isolated, stable environments, oftenat extremely low temperatures.









Aspect

Classical Computing

Quantum Computing

Basic Unit

Bit (0 or 1)

Qubit (0, 1, or both at the same time — superposition)

Information Processing

Deterministic: follows clear, sequential steps

Probabilistic: leverages quantum phenomena like superposition and entanglement

Parallelism

Limited: needs multiple processors to handle tasks simultaneously

Inherent: a qubit can represent multiple states at once, allowing massive parallelism

Hardware

Silicon-based transistors (CPUs, GPUs)

Superconducting circuits, trapped ions, photons, or other quantum systems

Speed

Very fast for everyday tasks but can struggle with highly complex problems (e.g., factoring large numbers)

Potentially exponentially faster for specific problems like factoring, simulating molecules, optimization

Error Handling

Well-established error correction methods

Quantum error correction is complex and still evolving

Applications

General-purpose computing: browsing, gaming, data processing, machine learning, etc.

Specialized: cryptography (breaking/enhancing security), quantum simulation, optimization, materials science

Scaling

Relatively easy to scale (Moore's Law, though slowing)

Very challenging to scale (requires maintaining quantum coherence)

Current Maturity

Mature and widespread

Emerging and experimental, but rapidly advancing



Simple analogy:





Severaltop quantum computing companiesare heavily investing in quantum technology.

On Dec 9, 2024, Google introduced an experimental quantum computer capable of performing in just five minutes a calculation that would take the world's fastest supercomputers approximately ten septillion years—far longer than the age of the known universe. Their latest quantum chip, namedWillow,is designed to address one of the most significant challenges in quantum computing:quantum error correction(QEC).

More recently, in February 2025, Microsoft made headlines by announcing the discovery of a new state of matter, which could pave the way for significant quantum breakthroughs. After 17 years of research, Microsoft revealed itsMajorana 1 quantum chip, which leverages a material known as a "topological qubit." This material exhibits unusual properties—its particles are neither solid, liquid, nor gas.

Although further validation is needed, Microsoft believes that the chip could dramatically accelerate innovation in areas such as drug development, battery technology, and artificial intelligence. Many researchers are optimistic that topological qubits will enable simpler and more effective error correction while mitigating the problem of quantum decoherence, where quantum systems lose their unique quantum behavior and revert to classical physics.





Quantum computing and artificial intelligence (AI) are two cutting-edge fields that can significantly enhance each other. Quantum computing has the potential to dramatically accelerate AI by processing and analyzing vast amounts of data much faster than classical computers. Thanks to quantum properties like superposition and entanglement, quantum computers can explore many possible solutions at once, making tasks such as training complex machine learning models more efficient.

In turn, AI can also help optimize quantum computing. Machine learning algorithms are being used to improve quantum error correction, optimize quantum circuit designs, and even discover new quantum algorithms. As both fields advance, their combination could lead to breakthroughs in areas such as drug discovery, materials science, financial modeling, and the development of more powerful AI systems themselves.

In short, quantum computing can make AI faster and more powerful, while AI can help quantum computers become more reliable and effective.

To learn more about this exciting synergy, check out our detailed article onHow Quantum Computing Will Revolutionize AI Development.





While quantum computing holds incredible promise, it also faces significant challenges that must be overcome before it can reach its full potential.

1. Quantum Decoherence and Noise

Quantum systems are extremely sensitive to their environment. Even the slightest disturbance—such as a tiny vibration or temperature fluctuation—can cause quantum bits (qubits) to lose their quantum state, a phenomenon known as decoherence. Managing and minimizing noise is one of the biggest technical hurdles.

2. Quantum Error Correction

Because qubits are fragile, quantum computers are highly prone to errors. Developing effective quantum error correction methods requires a large number of additional qubits, making scalable, fault-tolerant quantum computing very difficult with today’s technology.

3. Quantum Hardware Scalability

Building a quantum computer with just a few dozen qubits is already challenging. Scaling up to thousands or millions of qubits—necessary for solving practical, real-world problems—requires advances in manufacturing, cooling, and architecture.

4. Quantum Algorithm Development

Quantum algorithms must be specially designed to take advantage of quantum mechanics. Currently, only a handful of quantum algorithms offer clear advantages over classical ones, and creating new, efficient algorithms remains a major area of research.

5. Quantum Talent Shortage

Quantum computing is a highly specialized field that demands expertise in physics, computer science, and engineering. There is a global shortage of skilled researchers, developers, and engineers capable of pushing the field forward.

6. High Costs and Complex Quantum Infrastructure

Quantum computers often require ultra-low temperatures, sophisticated shielding, and specialized environments to operate. Setting up and maintaining such systems is extremely expensive and technically demanding.





Quantum computing is poised to transform a wide range of industries by solving problems that are too complex for classical computers. Here are some of the most promising use cases and business applications:

Quantum computers have the potential to significantly enhance our understanding of complex biological systems, a crucial step toward improving drug discovery and development.

By simulating how drug candidates interact with biological molecules, quantum computing could lead to the creation of more effective treatments.

For instance, Google, working with pharmaceutical company Boehringer Ingelheim, has demonstrated that quantum simulations can model the structure of Cytochrome P450 — a key human enzyme involved in drug metabolism — with greater speed and accuracy than traditional computers. This breakthrough could help optimize how new medications are designed and tested.



Banks and financial institutions are exploring quantum computing to optimize investment portfolios, improve risk analysis, enhance fraud detection, and perform complex simulations for market behavior predictions.

Also Read:How Quantum Computing Benefits Financial Services [2025]



Quantum algorithms can efficiently solve complex optimization problems, such as minimizing delivery times or reducing costs across large-scale supply chain networks, leading to significant operational improvements.



Quantum computing could dramatically enhance machine learning algorithms by speeding up data analysis, improving pattern recognition, and training AI models much faster than classical systems allow.



As global energy demands continue to grow, the need for more efficient and environmentally friendly energy storage solutions becomes critical.

Quantum simulations can help design more efficient solar cells, optimize energy grids, and develop better battery technologies. This can lead to more sustainable and cost-effective energy solutions.

Google has explored how quantum computers could assist in designing advanced battery materials. In collaboration with BASF, Google researchers showed that quantum simulations could more accurately model Lithium Nickel Oxide (LNO), a promising but challenging material used in batteries. Better understanding LNO's chemical behavior could improve manufacturing processes and reduce reliance on less sustainable materials like cobalt, paving the way for more powerful and eco-friendly batteries.



Quantum computing poses both a threat and an opportunity for cybersecurity. While it could break many existing encryption methods, it is also driving the development of quantum-safe cryptography to secure future communications.



Quantum simulations can uncover new materials with unique properties, leading to breakthroughs in industries like aerospace, electronics, and automotive manufacturing.



Fusion energy holds the promise of providing clean, limitless power by replicating the processes that power the stars. However, building practical fusion reactors requires extremely accurate simulations of material behavior under intense conditions — a task that currently demands enormous classical computing resources with limited accuracy.

Google, in collaboration with Sandia National Laboratories, has demonstrated that quantum algorithms running on fault-tolerant quantum computers could model fusion processes far more efficiently. These advancements could be key to overcoming major barriers in achieving sustainable fusion energy on a practical scale.

Curious about more ways quantum computing is shaping real-world solutions? Check out our guide onTop Quantum Computing Applications in Key Industries [2025].





23 Leading Quantum Computing Companies Worldwide [2025 List]

Superconducting Quantum Computing: Breakthroughs & Insights

Cloud-Based Quantum Computing: How it Works?

Who Was the First Quantum Computing Company? A Deep Dive [2025]

Is Practical Quantum Computing Here? Discover the Truth!

Featured Content

SPINQ Gemini Lab

Quantum Computing Experimental Platform

SPINQ Gemini Mini/Mini Pro

2-qubit Portable NMR Quantum Computer

Quantum Education Solution

Products+Resources+Training Services

SPINQ SQC

Superconducting Quantum Computer

SPINQ QPU

Superconducting QPU

Popular Reads

Info us

Call us

Mainland China:

400-150-4472

Non-Mainland China regions:

86-755-23760210

WhatsApp


---

# What Is a Qubit and How It Works in Quantum Computing

2025.03.28·Blog



Curious about quantum computing? Understanding qubits is key! In this guide, we'll explain what qubits are, how they differ from classical bits, and why they're so crucial to the functionality of quantum computers. Dive in to learn more!



A qubit (quantum bit) is the fundamental unit of quantum information, similar to a classical bit in traditional computing, but it can exist in multiple states simultaneously thanks to quantum phenomena like superposition and entanglement.

Mathematically, a qubit's state is represented as:



where α and β are complex probability amplitudes that determine the likelihood of measuring the qubit as 0 or 1 when observed.

Unlike a classical bit, which is either 0 or 1, a qubit can represent both 0 and 1 at the same time, allowing quantum computers to process much more information in parallel.



In classical computing, information is stored as bits, which are binary units of data that can only be in one of two states: 0 or 1. These bits are the foundation of classical computing and are used to perform all computations and store information.

In contrast, qubits can exist not only in the states corresponding to 0 or 1 but also in a superposition of both. This ability to occupy multiple states simultaneously is what makes quantum computing so powerful and vastly different from classical computing.



One of the most remarkable properties of qubits is superposition. In classical systems, bits are either 0 or 1, but a qubit can be in a state that is a combination of both.

Imagine spinning a coin—it can be heads, tails, or in an indeterminate state between the two while it is in motion. Similarly, a qubit in superposition can represent 0, 1, or any quantum superposition of these states. This exponential state space allows quantum computers to process vast amounts of information simultaneously.

Superposition is key to quantum parallelism, where quantum computers can solve problems much faster than classical computers by evaluating many possibilities at once.

For example, with just 10 qubits, a quantum computer can encode 1,024 states at once. With 300 qubits, it could theoretically represent more states than there are atoms in the observable universe.



Another crucial feature of qubits is entanglement, a phenomenon that links qubits in such a way that the state of one qubit cannot be described independently of the state of another, even if they are far apart. When qubits become entangled, the measurement of one qubit immediately affects the state of its entangled partner, regardless of distance. This unique quantum property is fundamental for quantum algorithms and is what enables quantum computers to perform complex calculations at extraordinary speeds.

Entanglement is what makes quantum computing so powerful for tasks such as optimization problems, cryptography, and more. It allows quantum computers to process information in a fundamentally new way, where the interconnected qubits work together to solve problems more efficiently than classical systems could.



In quantum computing,quantum gatesmanipulate qubits to perform operations, just as classical gates perform operations on bits. However, quantum gates are quite different—they act on a qubit by altering its state in a way that is consistent with the principles of quantum mechanics. Quantum gates operate in superposition and entanglement, enabling a quantum computer to solve more complex problems that classical computers cannot.

Quantum gates are used to create quantum circuits, which form the foundation for quantum algorithms. Some common quantum gates include:

Pauli-X: This gate flips a qubit from 0 to 1 or vice versa.

Hadamard Gate (H): Creates superposition, placing a qubit into a state of equal probability for 0 and 1.

CNOT Gate (Controlled-NOT):Flips the target qubit when the control qubit is in state |1⟩, enabling entanglement between qubits.

Pauli-Y Gate:Flips a qubit and adds a 90-degree phase shift, changing the state and affecting quantum interference.

Pauli-Z Gate:Flips the phase of the |1⟩ state, affecting the relative phase between qubits.



Qubits can be realized through various physical systems. Some of the most common implementations include:

Superconducting Qubits: These are created using superconducting circuits that can carry a current without resistance. These circuits are manipulated with microwave pulses to create the quantum states needed for computation.

Trapped Ion Qubits: In this approach, individual ions are trapped in a vacuum and manipulated using lasers to control their quantum states.

Photonic Qubits: Photons (light particles) can also be used as qubits. These are manipulated using beamsplitters and other optical devices.

Topological Qubits: A more theoretical approach to qubits, topological qubits rely on exotic particles known as anyons, which are theorized to be resistant to environmental noise, potentially making them more stable than other qubit types.

Each of these implementations has its advantages and challenges, but all share the same fundamental goal: to harness the unique properties of quantum mechanics to perform powerful quantum computations.





Quantum computers powered by qubits could revolutionize multiple industries:

Cryptography & Cybersecurity

Shor's algorithmcan break classical encryption (RSA, ECC) using quantum computing.

Post-quantum cryptography is now being developed to counter future quantum threats.



Drug Discovery & Material Science

Simulating molecules at the quantum level enables the design of new drugs and materials.

Quantum simulations could accelerate the discovery of new antibiotics and superconductors.



Optimization&AI

Grover's algorithmprovides a quadratic speedup for database searches.

Quantum computers could optimize logistics, supply chains, and financial models.



Quantum Internet & Secure Communication

Quantum key distribution (QKD) offers unbreakable encryption using entangled qubits.

Potential applications include military communications and secure banking transactions.





Qubits are the cornerstone of quantum computing, with their ability to represent multiple states and interact through quantum phenomena like superposition and entanglement. As research and development continue to advance, we may see quantum computers become more powerful and practical, unlocking the potential to solve problems that are currently beyond the reach of classical computers.

The evolution of qubit technology is one of the most exciting areas of quantum computing, and it will be fascinating to see how these challenges are overcome in the years to come.

Featured Content

SPINQ Gemini Lab

Quantum Computing Experimental Platform

SPINQ Gemini Mini/Mini Pro

2-qubit Portable NMR Quantum Computer

Quantum Education Solution

Products+Resources+Training Services

SPINQ SQC

Superconducting Quantum Computer

SPINQ QPU

Superconducting QPU

Popular Reads

Info us

Call us

Mainland China:

400-150-4472

Non-Mainland China regions:

86-755-23760210

WhatsApp


---

# Introduction

Welcome toQuantum Computing in Practice— a course that focuses on today's quantum computers and how to use them to their full potential. It covers realistic potential use cases for quantum computing and best practices for running and experimenting with quantum processors having 100 or more qubits.

It's an exciting time for quantum computing. After many years of theoretical and experimental research and development, we're approaching a point at which quantum computers can begin to compete with classical computers and demonstrateutility.

Utility is not the same thing asquantum advantage, which refers to quantum computersoutperformingclassical computers for meaningful tasks. Classical computers have incredible power and adaptability, and quantum computers can't beat them yet. We've seen decades of advancements in classical computation — not only in computing hardware but also in algorithms for classical computers — and we can observe with clarity that electronic digital computing has radically changed our world.

Quantum computing, on the other hand, is at a different stage in its development. Quantum computing places extreme demands on our control of quantum mechanical systems and pushes the boundaries of today's technology — and we cannot realistically expect to master this new technology and beat classical computing immediately. But we are seeing suggestive signs that quantum computers are starting to compete with classical computing methods for selected tasks, which is a natural step in the technological evolution of quantum computing known asquantum utility.

As the technology advances and new quantum computing methods are developed, we can reasonably expect that its advantages will become increasingly pronounced — but this will take time. As this happens we'll likely see a back-and-forth interaction with classical computing: quantum computing demonstrations will be performed and classical computing will respond, quantum computing will take another turn, and the pattern will repeat. And one day, when a quantum computer's performance can't be matched classically, we'll hypothesize that we've seen a quantum advantage — but even then we won't know for sure! Proving impossibility results for classical computers is itself an impossibly difficult problem as far as we know.

Classical simulators — computer programs running on classical computers that simulate physical systems — can make predictions about quantum mechanical systems. But classical simulators are not quantum and cannot directly emulate quantum systems. Instead, they use mathematical calculations to approximate quantum behavior. As the sizes of the simulated systems grow the overhead required to do this increases dramatically, placing limits on which quantum systems can be simulated classically, how long the simulations take, and the accuracy of the results.

Quantum computers, on the other hand, can emulate quantum systems more directly — and as a result the overhead they require scales significantly better as the system size grows. This, in fact, was Richard Feynman's idea in the 1980s that first motivated an investigation into the potential of quantum computers. We'll have more to say about this later!

IBM researcherspublished a paper in 2023that showed, for the first time, that a quantum computer can compete with state-of-the-art classical techniques for simulating a particular physical model. Its results can still be matched by advanced techniques running on classical computers — but it bested brute-force algorithms, and it also offers a new data point to which different simulation methods can be compared.

Prior users of IBM quantum hardware may have noticed that the smaller processors we previously made available to the public have been taken offline, making way for larger processors (having 100+ qubits). Those smaller processors could easily be simulated classically. So, although they were stepping stones in an advancing technology, they could not possibly demonstrate quantum utility: anything that could be done with them could just as easily be done with a classical simulation.

At around 100 qubits, however, this is no longer the case; quantum processors of this size can no longer be simulated classically. This represents a phase-transition of sorts, into a new era of quantum computing technology where thepotentialfor outperforming classical computation exists. This is where IBM has chosen to focus — to look for quantum computational power and reach toward an eventual quantum advantage.

We encourage our users to use these new devices to their full potential, to experiment with them and push their limits, and to carry forward lessons learned to the next generation of quantum processors currently in development. The purpose of this course is to enable you to do this!

This course is for anyone that aims to develop new applications for quantum computers, wants to scale up their current work in quantum computing, or learn how to use quantum processors within their workflow. This includes not just physicists and computer scientists, but also engineers, chemists, materials scientists, and anyone else interested in mastering quantum computing hardware.

The course will be hands-on and focused on the practical use of quantum computers. The following topics and skills are among those it covers:

This course does not cover the introductory theory of quantum computing and assumes a basic familiarity with qubits and quantum circuits. TheBasics of quantum informationcourse covers this material and is recommended first for those new to quantum computing.

Before we begin, please take a moment to complete ourpre-course survey, which is important to help improve our content offerings and user experience.

Quantum computing is an exciting new technology in an early stage of development — but it's just one chapter in a story that goes back thousands of years. It's the story ofcomputationand its multifaceted connections to thephysical world.

Since ancient times, we as humans have needed to performcomputations— or, in other words, to process information according to certain rules and constraints — to enable communication, construction, commerce, science, and other aspects of our lives. We've looked to the physical world for assistance, and through ingenious discoveries we've constructed devices to help us compute.

Long ago, devices made from wood, bone, and knotted ropes stored information and facilitated calculations. Mechanical devices built from levers, gears, and other machinery advanced from early astronomical clocks, to calculators, to sophisticated computing devices such as differential analyzers that solved equations using wheels and rotating disks. Even the technology of writing has played an important part in this story by allowingpeopleto perform computations they wouldn't be able to otherwise.

When we think about computers today, we tend to think about electronic digital computers. But this is in fact a fairly recent technology: electronic digital computers were first built in the 1940s. (In contrast, the Sumerian abacus is believed to have been invented somewhere between 2700 and 2300 BC.) The technology has advanced dramatically since then and computers are now ubiquitous. They're found in homes, places of work, and the vehicles that transport us between them, and many of us carry them with us wherever we go.

We also havesupercomputers, which are large collections of powerful classical processors hooked up in parallel. They're among the best tools humankind has ever built for solving difficult problems, and their power and reliability continues to advance. But still, there are important computational problems that even these behemoths will never be able to solve, due to the inherent computational difficulty of these problems.

Computers have many uses. One important use for computers is tolearnabout the physical world and better understand its patterns. Historical uses in this category have included the prediction of eclipses and tides, understanding the movement of astronomical bodies, and (in somewhat more recent times) modeling explosions. Today there's scarcely a physics lab in the world without a computer.

More generally, physics and computation have always been intertwined. Computation can't exist in a vacuum: information requires a medium, and to compute we need to harness the physical world in some regard. Rolf Landauer, a physicist (and IBMer), recognized decades ago that information is physical, existing only through a physical representation.Landauer's principleestablishes a connection between information and the laws of thermodynamics, but in fact there are many connections.

The aim of physics is to understand the physical world, but it's a two-way street. Through this understanding, we're able to harness new technologies to help us compute, and through them we continue to learn about the physical world — essentially pulling physics and computational technology up by the bootstraps.

Moore’s law is an observation that the maximum number of transistors in an integrated circuit doubles approximately every two years. Over the past five decades or so, we've observed this trend and reaped its rewards. With more transistors on a chip we can perform more complex computations and we can do them faster. This is why computers have become more and more powerful over time.

However, Moore’s law is, by necessity, coming to an end. Experts disagree on when this will happen, and some argue that it already has. But we know for certain that it must inevitably end because there's a theoretical limit to the miniaturization of computing components. We can't make a transistor smaller than an atom! While that may sound exaggerated, this is the wall we're approaching.

The solution is not to give up and say, "Well, that’s as good as it gets." This goes against human nature. Instead, we must look to the physical world for new computational tools, which is where quantum computing comes into play.

Quantum mechanics was discovered in the early 20th century, and it's already played an important role in computation. Indeed, our understanding of quantum mechanics has, in part, made modern day computers possible. Without quantum mechanics, for instance, it is difficult to imagine the solid-state hard drive having been invented.

When Richard Feynman first proposed the notion of a quantum computer in 1982, his focus was on simulating quantum mechanical systems. The calculations required to do this seemed too hard for ordinary computers — but perhaps, with a computer that operates according to a quantum mechanical description of the world, the systems could be emulated directly.

Today this is one of the most promising avenues for quantum computing. To the best of our understanding, Nature is not classical — it's quantum. And so, quantum computers may be valuable tools for understanding it. Classical computers, on the other hand, can only approximate what actually occurs in Nature, and in some cases those approximations are very limited.

One way to think about this is through an analogy to wind tunnels. Fluid dynamics are notoriously hard to simulate and predict mathematically. For example, it's too costly and impractical to simulate a car driving through wind, so instead car manufactures actually build tunnels with blowing wind and drive cars through them to test their performance. That is, they create wind rather than simulating it. Building a quantum computer to study the physical world is kind of like building a wind tunnel to study how wind affects cars. Quantum computers can directly emulate the laws of Nature on a molecular level because they act in accordance with those laws: they emulate Nature rather than simulating it through formulas and calculations.

Others followed up on Feynman's ideas — and they linked these ideas with a theory ofquantum informationthat was already being developed. The field ofquantum information and computationwas born. It has since developed into a rich, multidisciplinary field of study, and numerous advantages of quantum over classical information and computation have been identified in a wide variety oftheoreticalsettings involving communication, computation, and cryptography.

In practical terms, two things are needed to transfer these sorts of theoretical advantages to real world advantages: the devices themselves and the methodologies to unlock their potential.

Unlike classical computers, quantum computers aren't stashed away in anyone's back pocket. Until very recently, if you wanted to experiment with a quantum computer, you had to build and maintain one yourself (usually in a sad, basement lab in a university or research facility), and you would have just a few, very noisy qubits at most. No longer is this the case. In 2016, IBM put the first quantum processor on the cloud. It had only five qubits and fairly high rates of errors, but we've come a long way since then. We'll summarize the current state of the technology in a section below.

In addition to building quantum computers, we also need to develop methodologies for using them effectively. While theoretical advances in quantum algorithms and protocols suggest a strong potential, we still need to find practical uses for quantum computing. Today's quantum computers cannot yet perform the fault-tolerant computations required to transfer known theoretical advantages into practical advantages. But they are beyond the reach of classical computer simulations, and we can attempt to leverage this fact for computational power.

With these advances we find ourselves with a new tool for computation, and it's up to us to figure out what we can do with it.

We don't expect to use quantum computers to study how cars perform in wind. But there are other physical processes — such as ones involved in the design of batteries or in certain chemical reactions — where a quantum computer's ability to emulate Nature could lead to a quantum advantage. More generally, there are many problems that are too difficult or costly even for cutting-edge supercomputers, including problems that are highly relevant to our society. Quantum computing may not offer solutions to all of them, but it could offer solutions to some.

The following three application areas represent targets in the noisy quantum computing era, prior to the implementation of quantum error correction and fault-tolerance.

We'll discuss these topics in greater detail later in the course.

Building quantum computers is a difficult technological challenge, and it's only been eight years since small quantum computers have been publicly available. In those eight years, we have made progress on many fronts.

Numerous IBM quantum processors are now accessible through the cloud, all of which have over 100 qubits. But it isn’t just the size of the processors that is important — that's just one metric we care about. Gate quality has dramatically improved, and we've also introduced methods of reducing and mitigating errors intrinsic to quantum systems, even as we push forward to the creation of fault-tolerant systems. Three basic metrics —scale,quality, andspeed— are vital to tracking performance improvement.

Size.More qubits are obviously better, but only if increasing the number doesn't degrade performance (which can happen). We want moregood qualityqubits that don’t interfere with one another through crosstalk when we don’t want them to. The way the qubits are connected to one another is also important, and figuring out how best to do this represents a challenge for superconducting qubit circuits.

Quality.Another important performance metric we track is two-qubit gate fidelity. Gates that run on single qubits are not as prone to errors as two-qubit gates, which are therefore the larger concern. (Two-qubit gates are also crucial because they're responsible for creating entanglement between the qubits, which helps give quantum computing its power.)

Speed.Last is speed and efficiency. In short, the time spent running a program (including both quantum and classical parts) should be as short as possible.



It really is an exciting time to be working in quantum computing: for the first time in history we can begin to explore a region of computing that lies beyond classical computation.

T.J. Watson once famously predicted a world market for just a few computers. We may laugh now at how far off he was — but we have the benefit of hindsight. And we should acknowledge that, as humans, we have a general tendency to grossly underestimate the potential of future technologies. Now that it's our turn to be the early pioneers — of quantum computing — we should keep this in mind.

Quantum computing is often contrasted with classical computing, as something distinctly different from it and in competition with it. But from a broader lens we can see quantum computing as simply one more chapter in a long story. It is human nature to seek out new ways to compute and to harnessing the power of the natural world to do this. We've been doing this for centuries. Quantum computing is a new tool in the endeavor, and we must discover how we can leverage its power.

Was this page helpful?


---

# Running Quantum Circuits

This lesson will be a high-level look at the basics of running a utility-scale quantum computation, from the quantum hardware used to principles to consider when designing a quantum circuit. Ideally, by the end of this lesson, you'll know:

What IBM Quantum computers actually are. You'll need to know the basics of the hardware features to optimally design your quantum circuits to run on it.

What Qiskit is, what primitives are, and how we can use them to create and execute quantum circuits.

The typical workflow we follow to run experiments at scale. This includes selecting the best primitives for your use-case, mapping a problem to a quantum circuit, and applying error mitigation and suppression, which enable us to squeeze as much power out of these machines as possible.

To understand how we can make optimal choices in designing large-scale quantum circuits, we need to know a little bit about the actual hardware that will run these circuits. So let's briefly discuss physical qubits and IBM quantum processors.

IBM quantum processors are built using superconducting transmon qubits, which are electrical circuits composed of aJosephson junctionand a capacitor connected in parallel. The Josephson junction is a nonlinear inductor created from two overlapping layers of superconducting metal with an insulating barrier between them. At very low temperatures, the electrons in superconductors pair up to form what is known as a cooper pair. Cooper pairs can spontaneously tunnel through the insulating barrier from one side of the junction to the other. This tunneling behavior gives rise to the nonlinear properties which create our qubit.

Microwave transmission lines are fabricated on the chip to deliver microwave signals to the qubits. When we apply highly calibrated microwave pulses — with specific frequency, amplitude, shape, and duration — to the lines, we can make the qubits do specific things. This forms the basis of our quantum gates. We fabricate the chip so that neighboring qubits are connected in a specific lattice structure called a heavy-hex lattice. This connectivity — the so-calledtopology— of our processors is an important factor to consider when designing a circuit, as we'll discuss later in the lesson.

The instructions for the microwave pulse go from your computer, through the cloud, and to room-temperature control electronics, which interpret those instructions and physically generate the pulses. After the room-temperature control boxes create the pulses, they travel through cables into a dilution refrigerator and eventually to the quantum chip. The signal goes into the resonators, through a wirebond, and then flows down the transmission line into our qubits.

IBM has dozens of quantum computers around the world, and we've recently upgraded our fleet to exclusively have processors larger than 100 qubits. Some of them are located in an IBM Quantum data center in upstate New York and deployed over the cloud for everyone's use — and some of them are dedicated, on-premises systems that support partners in the IBM Quantum Network. You can log into quantum.ibm.com to see which processors you have access to.

Each processor lists three performance metrics, which we discussed in the previous lesson, but as a reminder, they are: qubit count, EPLG, and CLOPS.

Qubit count.This is self-explanatory: it's the number of total qubits available to use on a sole quantum processor. For a relatively large, utility-scale problem, you'll need to make sure you are using a processor with enough qubits to be able to tackle the problem. But the qubit count alone is not the only thing that matters.

EPLG,or “errors per layered gate.” This is a measure of the quality of the qubits and quantum gates. It measures the average error each gate introduces in a circuit that entangles neighboring qubits in a chain of 100 qubits. You want this to be as small as possible.

CLOPS,or “circuit layer operations per second.” This quantifies the speed of the processor. It measures how many layers of a certain benchmarking circuit called a quantum volume circuit a quantum processing unit (QPU) can execute per unit of time. The higher the number, the faster we can compute.

The importance of each of these metrics varies depending on the specific application, and in future lessons, we'll look at real examples to see how each of these factors can affect the outcome of a calculation.

To turn your quantum problem into instructions for a quantum computer, you'll use Qiskit, the open-source software development kit designed for work on quantum computers developed by IBM. There's also the Qiskit Ecosystem — a collection of software tutorials and functions that build or extend upon the core functionalities of Qiskit — and Qiskit Runtime — a quantum computing service and programming model that allows users to design and optimize their quantum workloads and execute them efficiently by using Qiskit Runtime Primitives.

A primitive is a small building block that you can use to design a larger circuit or job. The two primitives that are most important to us are the sampler and the estimator, which we'll discuss in more depth shortly.

With the recent release of Qiskit 1.0, Qiskit has become more performative and stable than ever before. So, for those of you just getting started, you came in at the perfect time! For those of you that are already familiar with Qiskit, you'll need to download and reinstall the newest version. For a full installation guide, visit thepage over in Docs.

Now we're ready to discuss the foundation of quantum programs: quantum circuits. This section will only serve as a refresher — if you aren't familiar with quantum circuits, we recommend learning about them in more depth by visitingLesson 03: Quantum Circuitsin the Basic of quantum information course before continuing.

A quantum circuit is a network of quantum gates and measurements linked by wires that represent qubits, as shown below. Quantum circuits can be read like sheet music, from left to right, starting at time 0 on the left. Virtual qubits — those that haven't yet been assigned to a physical qubit on a processor — are listed in increasing order from top to bottom.

Gates are represented by different symbols on the wires of the involved qubit(s). Single-qubit gates — like a Hadamard gate, depicted below (the box with the H) — affect only the qubit whose wire it's placed on. Multi-qubit gates — like a CNOT gate, also shown below (the plus sign in the circle with a line connected to q0) — affect two or more qubits. In the depicted CNOT gate, the state of the q1 changes according to the state of q0. After all the gates are performed, we can measure the qubits, indicated by the black gates with the measurement symbol. The outcomes of the measurements are written onto classical registers, the double-lined “meas” bus below.



One important characteristic of a circuit is its depth. The depth of a quantum circuit is the minimum number of “layers” of quantum gates, executed in parallel, required to complete the circuit. Quantum gates can be executed in parallel (at the same time) whenever they don't have any qubits in common. But if two or more gates act of the same qubit then we can't perform them in parallel — they must be performed in two separate layers, one after the other.

There's another, less obvious, way to determine the depth of a circuit, by playing a sort of game. The rules are simple: starting from any qubit wire on the left, you must travel to the right and count the number of gates you encounter in your path. You may hop to a neighboring wire only when it is connected to your current wire by a multi-qubit gate. The goal is to maximize the number of gates you encounter along your path. This maximal number also happens to be the depth of the circuit.



Because quantum gates take time to implement, the depth of a circuit roughly corresponds to the amount of time needed for a quantum computer to execute the circuit. Some machines are better suited for large depth circuits than others due to the decoherence times of the qubits on the processor. So, we need to know the depth of a circuit to know if it can be run on a particular device.

So, how do we go about designing and running a quantum circuit? The easiest way to understand a typical quantum computing workflow is through Qiskit patterns. Qiskit patterns are a conceptual framework allowing users to run quantum workloads by implementing certain steps with modular tooling. This allows quantum computing tasks to be performed by a powerful heterogenous (CPU/GPU/QPU) computing infrastructure. The steps can be performed as-a-service and can incorporate resource management, which allows for the seamless composability of new capabilities as they're developed.

Here are the main steps, which experienced Qiskit users will likely recognize.

Map.This step formalizes how we take a general problem we're interested in and figure out how to map it onto a quantum computer in the form of a quantum circuit.

Optimize.In this step we use Qiskit's transpiler to route and lay out the circuit onto an actual, physical qubit hardware. This includes translating the individual quantum gates into sequences of operations that are performed on the hardware as well as an optimization in the layout of the gates.

Execute.Qiskit Runtime primitives provide the interface to IBM Quantum hardware that allows transpiled circuits to run. This step also includes using error suppression and mitigation techniques, which can largely be abstracted away from the user.

Post-process.In this step the data from the quantum processor itself is processed, providing the user with useful results on the original problem. Basically, this encompasses any further analysis of the data acquired.

The Map step essentially asks the question: “How do I translate my problem into a quantum circuit that can reasonably be run on quantum hardware?” There's no doubt about it: mapping is a hard problem and an active area of research. There isn't a foolproof method that guarantees success, but there are recommended guidelines and examples of problems we already know how to map.

The first guideline is to let classical computers do whatever work they're better at. Tasks that are easy for classical computers probably won't benefit from a quantum computer. Quantum computers are for problems that are classically hard. Of course, if this is your very first time using Qiskit or a quantum computer, don't worry about finding a problem that is computationally complex. Break it down into smaller, bite-sized problems that you can learn to address before going straight for a utility-scale project.

Next, translate the outcomes for your problem that you want to measure or understand into an expectation value or a cost function. A cost function is a problem-specific function that defines the problem's goal as something to be minimized or maximized. It can be used to see how well a trial state or solution is performing with respect to that goal. This notion can be applied to various applications in chemistry, machine learning, finance, optimization, and so on — it doesn't necessarily matter from what subject you're approaching the problem.

Also keep in mind that the hardware you're going to be using has a specific topology, as we discussed in the hardware section. Some qubits are connected, and some aren't — you'll need to map your problem to a circuit that respects IBM quantum processors' heavy-hex topology.

For now, the most important thing to keep in mind is that this stage requires practice. You need to have a good understanding of not only your problem, but also the hardware capabilities — and we'll go through specific examples and use-cases in future lessons to see how to balance all these considerations.

Next, we'll need to choose a quantum processor that has enough qubits of high enough quality that we can run our quantum circuit. Make these decisions guided by the three metrics that we discussed in the hardware section: qubit count, EPLG, and CLOPS.

Then we optimize our circuit for the selected hardware. First, we need to lay out and route our circuit efficiently. Layout refers to mapping the virtual qubits in the circuit to the physical qubits on the processor. Routing refers to tweaking the circuit so that the connectivity between virtual qubits in the circuit matches the connectivity of the physical qubits on the processor. There are a couple of things to keep in mind during the layout and routing stage.

Not all qubits are connected. Some are very far away from one another on the chip, and we need to reduce or eliminate long-distance interactions wherever possible. You could apply a sequence of SWAP gates between neighboring qubits to move the qubits information around, but SWAP gates are costly and prone to errors, so there may be betters ways of doing this. Try to avoid too many costly SWAP gates.

Layout and routing are iterative processes. You can do it by hand, but there's also a Qiskit tool calledmapomatic, which can make recommendations for a physical qubit layout based on approximate error rates. The transpiler (which we'll discuss shortly) can also make an informed suggestion.

Next, we can compose sequences of one-qubit gates acting on the same qubit into single gates — and we can also sometimes get rid of unnecessary gates or combination of gates. For example, some combinations of gates can be reduced to simpler combinations — and in fact, sometimes a combination of gates might equate to the identity operation, so we can simply eliminate them. You can do this automatically using the Qiskit transpiler — but you can also do it manually on a gate-by-gate basis if you want more control.

Once we've improved the circuit layout, routing and gate counts — either by hand or using the transpiler — we usually want to visualize our circuit to make sure the timing of all the gates makes sense. There is an argument you can flag in the transpiler to visualize the timeline of your circuit, and make sure everything is lined up the way you would expect.

As mentioned previously, the Qiskit Transpiler can be used to help in the early stages of the patterns workflow. Now let's dig into its capabilities in more detail. It can rewrite a given input circuit so that it matches the topology of a specific quantum device and optimize the circuit for execution and resilience against noise. It also rewrites a given circuit into basis gates of the specific quantum processor you have selected to use.

Qiskit has four built-in transpilation pipelines corresponding to different optimization levels, and unless you're already familiar with quantum circuit optimization, we recommend using one of them. By default, the transpilation process includes these six steps:

Initialization. This stage runs any initial passes that are required before we start embedding the circuit onto the backend. This typically involves unrolling custom instructions and converting the circuit to just single- and two-qubit gates.

Layout. This stage maps the virtual qubits in the circuit to the physical qubits on a backend. SeeLayout Stagefor more details.

Routing. This stage runs after a layout has been applied and injects gates (such as swap gates) into the original circuit to make it compatible with the backend's connectivity. SeeRouting Stagefor more details.

Translation. This stage translates the gates in the circuit to the target backend's basis set. SeeTranslation Stagefor more details.

Optimization. This stage runs the main optimization loop repeatedly until a condition (such as reaching a certain target depth) is reached. We have four different optimization levels to choose from, described below.

Scheduling. This stage is for any hardware-aware scheduling passes. At a high level, the scheduling can be thought of as inserting delays into the circuit to account for idle time on the qubits between the execution of instructions.

There are four optimization levels ranging from 0 to 3, where higher optimization levels take more time and computational effort but may yield a better circuit. Optimization level 0 is intended for device characterization experiments and, as such, only maps the input circuit to the constraints of the target backend, without performing any optimizations. Optimization level 3 spends the most effort to optimize the circuit. However, as many of the optimization techniques in the transpiler are based on heuristics, spending more computational effort doesn't always result in an improvement in the quality of the output circuit. If this is of further interest, please check out the transpiler pages in the Qiskit documentation.

The first step in reducing errors in a circuit is optimizing the layout, routing, and minimizing the gate count, which we've already done, either using the transpiler or on our own. Now let's talk about some more sophisticated methods of error suppression.

Error suppression refers to a class of techniques that transform a circuit during compilation to minimize errors. It is distinct from error mitigation, which we'll discuss later in the “Execute” section below. The two most common forms of error suppression that we use are dynamical decoupling and Pauli twirling:

Now we're ready to execute the quantum program. The Qiskit Runtime primitives provide an interface to IBM Quantum hardware, and they also abstract error suppression and mitigation away from the user. There are two primitives to choose from: the Sampler and the Estimator.

Qiskit Runtime's Samplerruns the circuit multiple times on a quantum device, performing measurements on each run, and reconstructing the probability distribution from the recovered bit strings. The more runs (or shots) it performs, the more accurate the results will be, but this requires more time and quantum resources. Specifically, it calculates the probability of obtaining each possible standard basis state by measuring the state prepared by the circuit.

Qiskit Runtime's Estimatoruses a complex algebraic process to estimate the expectation value on a real quantum device by breaking down the observable into a combination of other observables with known eigenbases.

The Execute step is also when we can selecterror mitigationstrategy.Error mitigationrefers to techniques that allow users to reduce circuit errors by modeling the device noise that was present at the time of execution. Typically, this results in quantum pre-processing overhead related to model training and classical post-processing overhead to mitigate errors in the raw results by using the generated model. In exchange for this overhead, we're able to get much more accurate results.

There are multiple techniques we can implement for error mitigation. We'll discuss three, in increasing order of resilience to errors, but also, consequently, in increasing order of computational cost. Be aware, however, that this is an active area of research — so we'll likely continue to invent new ones and improve upon old ones.

At resilience level 0, the transpiler doesn't do anything to your circuit.

At level 1, it introduces a method called Twirled Readout Error eXtinction (T-REX). T-REX uses Pauli twirling, as was discussed in the error suppression section. As mentioned, inserting random gates into the circuit can make even very complicated, difficult-to-model noise look stochastic, and much easier to account for or subtract out in post-processing.

At resilience level 2, Zero Noise Extrapolation (ZNE) is added. This is a popular technique that we've had a lot of recent success with. The idea behind ZNE might be a bit surprising — we actually add noise on top of what's already there! But this allows us to extrapolate in the reverse direction, to predict what the results would look like if there were less and less noise.

Adding noise can be accomplished in a few different ways. For instance, we can stretch out the gates to make them longer and thus, more prone to error, or run more gates that ultimately result in an identity operation, so the circuit doesn't change functionally but we purposefully sample more noise. You do have to do this for every circuit and every expectation value you want to keep track of, though — so you can see how it can end up being computationally expensive.

One specific type of ZNE is called Probabilistic Error Amplification (PEA). Once we've learned a noise model for a gate, PEA works by sampling errors from that noise model and deliberately injecting them into the circuit. This isn't available in Qiskit yet, but it will be later this year.

The final form of error mitigation we'll discuss is Probabilistic Error Cancellation (PEC). Instead of being at the 3rd resilience level, PEC is a special capability you must turn on manually in Qiskit, because the required computational resources don't scale very well compared to the other error mitigation techniques. You begin by learning about the noise that's affecting your circuit — run noise-learning or noise-characterization circuits for each unique layer of two-qubit gates in your circuit. These results let you describe the noise in terms of Pauli operators. Once you know these noise terms, you can modify your circuits so that they effectively have the opposite Pauli gates built in to cancel these noise channels. In some ways, the process is similar to the way noise-cancelling headphones work. However, this way of undoing the noise is very costly, with a time to run that grows quickly and exponentially in the number of gates, so it may not be the best choice for a very large circuit.

The post-process stage is where we visualize and analyze the output of our quantum circuit. There are a number of Qiskit tools available for you to do this, such as the visualization and quantum-info modules. We won't cover these here, but we'll see these modules in action as we dive into some application examples in future lessons.

Hopefully this lesson gave you a whirlwind tour of the main considerations and workflow we use when we want to run a utility-scale quantum computation. It was packed with information, and much of it won't sink in until we see some actual examples where these theoretical concepts are put into practice. So, that's what the remainder of the course will be. After all, this course isn't called Quantum Computing in Practice for nothing!

Next time, we'll look at a specific example of how to use the Qiskit patterns workflow to design and run a quantum circuit that solves the classic problem from graph theory called MaxCut.

Was this page helpful?


---

# Utility-Scale QAOA

So far in this course, we hope we've given you a solid foundation of the framework and tools needed to solve utility-scale problems on a quantum computer. Now, we're finally going to see these tools in action.

In this lesson, we'll get our hands dirty with a utility-scale example of the Max-Cut problem, which is a famous problem in graph theory involving how to best partition a graph into two. We'll start with a simple, five-node graph to build our intuition for how a quantum computer can help us solve the problem, then apply this to a utility-scale version of the problem.

This lesson will serve as a broad overview of the approach we take to solving this problem. This won't be a code walk-through. Accompanying this lesson, though, there is atutorialwith actual code that you can run to solve the Max-Cut problem on a quantum computer.

As a reminder, not all computational problems are suitable for quantum computing. “Easy problems” won't gain any advantages from this technology because classical computers are perfectly good at solving them already.

The three use-cases we're most optimistic about exploring are:

Today, we'll be focusing on the third use-case, optimization. In an optimization problem, we're generally looking for the largest or smallest possible value for a given function. The difficulty of finding these extrema with classical methods can increase exponentially as the problem size grows.

The optimization problem of interest today is called Max-Cut, which we're going to solve using an algorithm called Quantum Approximate Optimization Algorithm (QAOA).

We start with a graph, which consists of a collection of vertices (or nodes), some of which are connected by edges. In the problem, we're asked to divide the nodes of the graph into two subsets by “cutting” the edges that connect them. We want to find the partition that maximizes the number of edges that are cut in this way – hence the name, “Max-Cut.”



For instance, the figure above shows a five-node graph with a Max-cut solution on the right. It cuts through five edges, which is the best one can do with this graph.

Because a five-node graph is so small, it isn't too hard to work out the Max-Cut in your head or by trying a few cuts on a piece of paper. But as you can imagine, the problem becomes more and more difficult as the number of vertices grows — in part because the number of possible cuts to consider grows exponentially in the number of nodes. And at a certain point, this becomes hard even for supercomputers to do with the any known classical algorithms.

We'd like a way to solve the Max-Cut problem for these larger, more complicated graphs, because the problem has many practical applications, including fraud detection in finance, graph clustering, network design, and social media analysis. Max-Cut is often encountered as subproblem within a particular approach to a larger problem. So, it's much more common than we might naively think.

Now, we're going to walk through the approach we use to solve the Max-Cut problem on a quantum computer. We'll do this with a simple, five-node graph. You can follow along using the python notebook tutorial. After that simple example, the tutorial will walk you through a utility-scale example of the problem.

The first step is to create our graph by defining the number of nodes and the edges that connect two nodes. You can do this by importing a package calledrustworkx, as demonstrated in the tutorial. The result will be a graph that looks like this:



We'll use the Qiskit patterns framework to find the Max-Cut solutions for this graph on our quantum computer.

We need to map the problem onto our quantum computer. To do this, let's first note that maximizing the number of cuts in a graph can be mathematically written as:

max⁡x∈{0,1}n∑(i,j)xi+xj−2xixj\max\limits_{x\in\{0,1\}^n} \sum\limits_{(i,j)} {x_i + x_j - 2x_ix_j}x∈{0,1}nmax​(i,j)∑​xi​+xj​−2xi​xj​

Whereiiiandjjjare nodes in the graph, andxix_ixi​andxjx_jxj​are either 0 or 1, depending on which side of the partition each node is on (one group is labelled “0” and one is labelled “1”). Whenxix_ixi​andxjx_jxj​are on the same side of the partition, the expression in the sum is equal to zero. When they are on opposite sides, so there is a cut between them, the expression is equal to one. So, maximizing the number of cuts will maximize the sum.

We can also flip this around, and search for the minimum by multiplying each of the values by negative one.

min⁡x∈{0,1}n∑(i,j)2xixj−xi−xj\min\limits_{x\in\{0,1\}^n} \sum\limits_{(i,j)} {2x_ix_j - x_i - x_j}x∈{0,1}nmin​(i,j)∑​2xi​xj​−xi​−xj​

Now, we're ready to map. It can be kind of daunting to think about how to go from a graph like we one we just drew to a quantum circuit. But we'll take it one step at a time.

Remember, we're going to try to solve Max-Cut using QAOA. In the QAOA methodology, we ultimately want to have an operator (or in other words a Hamiltonian) that will be used to represent the cost function of our hybrid algorithm, as well as a parametrized circuit (the ansatz) that we use to represent possible solutions to the problem.

We can sample from these candidate solutions and then evaluate them using the cost function. To do this, we take advantage of a series of mathematical reformulations, including the Quadratic Unconstrained Binary Optimization notation — or QUBO for short — which is a useful way to encode combinatorial optimization problems. In QUBO, we want to find:

min⁡x∈{0,1}nxTQx\min\limits_{x\in\{0,1\}^n} x^TQxx∈{0,1}nmin​xTQx

whereQQQis ann×nn\times nn×nmatrix of real numbers,nnncorresponds to number of nodes in our graph, here, five.

To apply QAOA, we need to formulate our problem as a Hamiltonian — which is a function or matrix that represents the total energy of a system. Specifically, we want to create a cost function Hamiltonian that has the property that the ground state corresponds to the minimum value of the function. So, to solve our optimization problem, we'll try to prepare the ground state ofHHHon a quantum computer. Then, sampling from this state will yield the solution tomin⁡𝑓(𝑥)\min 𝑓(𝑥)minf(x)with a high probability.

As it turns out, we are in luck, because the QUBO problem is very closely related to, and actually computationally equivalent to, one of the most famous and ubiquitous Hamiltonians in physics: the Ising Hamiltonian.

In order to write the QUBO problem as the Ising Hamiltonian, all we actually need to do is do a simple change of variables:

xi=1−zi2x_i = \frac{1-z_i}{2}xi​=21−zi​​.

We won't walk through all the steps here, but they are explained in the attached notebook. In the end, the minimization of the QUBO expression is the same as the minimization of this expression:

min⁡x∈{0,1}nxTQx⟺min⁡z∈{−1,1}nzTQz+bTz\min_{x\in\{0,1\}^n} x^TQx\Longleftrightarrow \min_{z\in\{-1,1\}^n}z^TQz + b^Tzminx∈{0,1}n​xTQx⟺minz∈{−1,1}n​zTQz+bTz

Rewriting again slightly and we have our cost function Hamiltonian, where the minimum of the expression represents the ground state,Zis the Pauli Z operator, andbbbis a real scalar coefficient:

HC=∑ijQijZiZj+∑ibiZiH_C=\sum_{ij}Q_{ij}Z_iZ_j + \sum_i b_i Z_iHC​=∑ij​Qij​Zi​Zj​+∑i​bi​Zi​

Now that we have our Hamiltonian, we need to rewrite it in terms of two-local Pauli ZZ operators, that we can easily convert to two-qubit gates in our quantum circuit. We'll end up with six objects — or Pauli strings — where each corresponds to each of the six edges in the graph. Each of the five elements in a string represents an operation on a node — the identity if the node is not connected to that particular edge, and the Pauli Z operator if it is. In Qiskit, bitstrings representing qubits are indexed backwards. For example, an edge between nodes 0 and 1 is encoded asIIIZZ, and an edge between 2 and 4 is encoded asZIZII.

With our Hamiltonian written in terms of Pauli operators, we're ready to construct our quantum circuit, which allows us to sample good solutions using a quantum computer:



The QAOA algorithm takes inspiration from the Adiabatic Theorem, which states that if we start in the ground state of a time-dependent Hamiltonian, if the Hamiltonian evolves slowly enough, and given enough time, the final state will be the ground state of the final Hamiltonian. QAOA can be thought of as the discrete, trotterized version of this Quantum Adiabatic Algorithm, where each trotter step represents a layer of the QAOA algorithm. So instead of evolving from one state to the other, in each layer, we will switch back and forth between our cost function Hamiltonian and a so-called “mixer” Hamiltonian, which we'll cover later in this lesson.

The advantage of QAOA is that it's faster than the quantum adiabatic algorithm, but it returns approximate solutions rather than the optimal. In the limit where the number of layers goes to infinity, QAOA converges to the QAA case, but of course this is very computationally expensive.

To create our quantum circuit, we'll apply alternating operators, parameterized byγ\gammaγandβ\betaβ, which will represent the discretization of the time evolution.

So, the three main parts of the QAOA circuit are:

Our starting Hamiltonian is called the Mixer because its ground state is the superposition of all possible bitstrings of interest: hence enforcing a mixture of all possible solutions at the start.

The mixer Hamiltonian is the simple, sum of Pauli-X operations on each node of the graph. Qiskit allows you to use a different, custom mixer operator if you wish, but we are going to use the standard one here. So again, you can see that with Qiskit, a lot of the work is removed for us, making coming up with the mixer Hamiltonian and the starting state trivial. The only work we had to do was to find the cost function.

Each iteration of these operators is called a layer. These layers can be seen as discretization of the time evolution of the system, as previously described. The alternating pattern comes from the trotter decomposition and approximates the exponential functions of non-commuting matrices. In general, the more layers or steps we include, the closer we will be to continuous time evolution, like in QAA, so in theory, the more accurate the result will be. But for this example, we'll begin by just sampling with one layer. Remember, both the cost function Hamiltonian and the mixer are parameterized, we still need to come up with optimal values forγ\gammaγandβ\betaβ.

While the circuit we just created looks pretty simple and is useful to build up an intuitive understanding, remember, the quantum chip doesn't understand what the QAOA gate is. We need to turn this into a series of single- and two-qubit “native” gates that can be performed directly on the hardware. Native gates are those that can be performed directly on the qubits. Such circuits are said to be written in the backend's Instruction Set Architecture (ISA).

The Qiskit library offers a series of transpilation passes that cater to a wide range of circuit transformations. We want to make sure that the circuit is optimized for our purpose.

Recall from our previous lesson that the transpilation process involves several steps: • Initial mapping of the qubits in the circuit (i.e. decision variables) to physical qubits on the device. • Unrolling of the instructions in the quantum circuit to the hardware native instructions that the backend understands. • Routing of any qubits in the circuit that interact to physical qubits that are adjacent with one another.

And as always, more details about this can be found on the documentation site.

Before transpiling, though, we need to choose which backend we'll run our circuit on, since the transpiler optimizes differently for different processors. This is yet another reason it's important to use an automated transpiler — you wouldn't want to go through the time-consuming process of optimizing your circuit by hand, only to realize you actually want to run your circuit on a different processor with different properties.

Pass your backend of choice through the transpiler function and specify your optimization level. In the tutorial, you'll select level 3, which is the highest and most thorough level.

And with that, we have a transpiled circuit that is ready to be executed on hardware!

So far, we transpiled the circuit leaving the parameters gamma and beta alone — but we can't actually run the circuit without specifying these parameters. In the QAOA workflow, the optimal QAOA parameters are found in an iterative optimization loop, where we run a series of circuit evaluations and then use a classical optimizer to find the optimal 𝛽 and 𝛾 parameters. However, we need to start somewhere, so we make an initial guess ofγ=π/2\gamma=\pi/2γ=π/2andβ=π\beta=\piβ=π.

Execution Modes

Now, we're almost ready to run the circuit — I promise! But first, it is important to note that you can send your job in a variety of different ways, which are called execution modes.

Job mode: A single primitive request of the estimator or the sampler is made without a context manager. Circuits and inputs are packaged as primitive unified blocs (PUBs) and submitted as an execution task on the quantum computer.

Batch mode: A multi-job manager for efficiently running an experiment that is comprised of bundle of independent jobs. Use batch mode to submit multiple primitive jobs simultaneously

Session mode: A dedicated window for running a multi-job workload. This allows users to experiment with variational algorithms in a more predictable way, and even run multiple experiments simultaneously, taking advantage of parallelism in the stack. Use sessions for iterative workloads or experiments that require dedicated access. See Run jobs in a session for examples.

For a QAOA experiment, a session would be a good choice to proceed with if you have access to it, since we need to sample our circuit many times with different parameter values to find the optimum.

Back to the optimization problem. We need to find better values of gamma and beta than just our rough first guesses. We will do this by plugging in our cost function and these initial guesses into a scipy optimizerCOBYLA.



Here you can see the value of the cost function over the iterations. It starts off a little wonky and goes up and down, but then settles to a low value. We will use the values that scipy found that correspond to the lowest evaluation of the cost function.

Now that we were able to reduce our cost function by finding better values of our parameters, we will run our circuit using the new values we found for gamma and beta. I listed the specific values that I am using here, but remember, when you try your hand at this or even just rerun the same tutorial notebook, these values might change slightly. Now we will run our optimized circuit with these values and find our candidate solution to our Max-Cut problem.

In the post-processing stage, we'll analyze the data and display these results to see if our quantum algorithm found the correct solutions.

Now let's plot a histogram of the data to look at the final solution:



The bit strings represent how each of the nodes were partitioned into two groups (labelled “0” and “1”) by the cut. There should be four solutions that all give the maximum value of edges cut. These four are shown in purple. You can see right away that 4 solutions are much more probable than any of the others. The highest, and thus most probable bit string solution is 0,1,0,1,1. (Remember – the order of the qubits is reversed in the plot bitstrings!)

From this plot, we can take the most likely bitstring and represent it as a partitioned graph, with the cut going through five edges:



So, this is indeed a Max-Cut solution. But it's not the only one! Because of the symmetry of this graph, there are multiple correct solutions. Instead of having the nodes 0 and 3 be inside the cut, we could have nodes 2 and 4 be included. You can see that all I had to do was rotate my cut to include these new points. The number of edges cut remains five. There turns out to be four max cut solutions, since each of the two solutions we noted also has an “opposite” partner, where the purple nodes are grey and the grey nodes are purple – so the cut remains the same, but each node effectively swaps to the opposite side of the partition.

Let's look again at the histogram and the four most likely solutions for a minute. Ideally, they would be each of the four true Max-Cut solutions. The problem is that the algorithm actually didn't identify the fourth and final solution as being one of the top 4 most probable answers. It was the fifth most likely. The fourth solution that the algorithm identified is incorrect--if you were to draw it, you would see that the solution only has four cuts.

But remember: this is an approximate algorithm. It is not infallible, and it is not correct 100% of the time. You do have to employ some of your own knowledge and understanding to sanity-check the solutions.

This error can arise from several places:

This kind of error analysis is what it takes to become a practitioner of quantum computing. You need to understand the performance of the hardware and how this can contribute to certain types of errors and how to correct for them.

However, let's not forget that there were 32 possible bit strings, and that the four real solutions were included in the top five best candidates. And we only used two layers to find this. In general, if we wanted to increase our chances of finding the best Max-Cut every time, we could increase the layer depth. There are some subtleties to this, but that's for a later lesson.

Now that you've had a taste of the process of solving a small Max-Cut problem on a quantum computer, I challenge you to do it at scale. Follow along in the linkedtutorialto see how many cuts you can get in a 125-node graph.

Was this page helpful?


---

# Which Problems Are Quantum Computers Good For?

In the previous lesson, we tackled a single problem in-depth – solving the Max-Cut optimization problem using the QUBO formulation. Today we are going to take a different approach and discuss near-term applications more broadly. We'll start by giving you a sense for how we decide on the types of problems we think might benefit from a quantum solution. Then, we'll look at some recent examples of work done in our community. This will help you start to develop an intuition for different types of quantum computing problems and how we approach solving them.

Before jumping into the examples, let's first discuss how we study and categorize the difficulty of various problems. Some problems can easily be solved on a classical computer, and we have no need for a quantum computer to solve them. On the other hand, there are very hard problems for which quantum computers are necessary to solve. One famous example is finding the prime factors of enormous integers. RSA encryption relies on the difficulty of this problem, and Shor's algorithm was designed to solve it on a quantum computer. Another example is finding a solution in an unsorted data set – this can theoretically be solved by the quantum algorithm known as Grover's algorithm. However, most experts agree that these types of algorithms will necessitate the implementation of error correction and the technology just isn't there yet.

So, we are looking for problems that we can tackle somewhere in a sweet spot between the very easy and very hard – ones that today's quantum computers can tackle, but classical computers have trouble with.

The difficulty of these problems is categorized and analyzed in a branch of computer science called computational complexity theory. There are a ton of different complexity classes in classical computing, but some of the most fundamental are:

When the concept of quantum computing was invented, people spent considerable efforts trying to figure out what class of problems these new types of computers would be able to solve efficiently. A new class of problems was invented:



All these classes live in a larger class we call PSPACE. Above is a diagram of the suspected relationships among some of the complexity classes, but this is very hard to definitively prove mathematically. You'll notice that BQP does not necessarily overlap with NP-complete. But you might have still seen some quantum computing approaches that aim to try to solve problems in NP-complete.

One common misconception is that there is no point in exploring quantum solutions to any problems where a mathematical proof for a quantum speed-up has not been found. But a mathematical proof that a quantum algorithm is faster than its classical counterpart is hard to find. Shor's and Grover's are two of only a handful of examples where this has been done so far. In fact, rigorously proving that P and NP are different is one of the most notorious open questions in all of mathematics, even though all intuition tells is they must be.

But the way an algorithm scales with increasing problem size – which is what is reflected in the complexity class – is not always the most relevant feature of an algorithm. This scaling is often the worst-case scenario. It is quite possible that, in practice, the worst-case scenario is not what we most commonly encounter.

Just because hardness proofs are tricky doesn’t mean we can’t make progress. We introduce the idea of heuristic solutions. If you're an experimentalist you likely know and love these types of solutions. A heuristic is any approach to solving a problem that is pragmatic, but not necessarily optimal, since solutions don't need to be optimal to be useful. For instance, think about financial applications. We haven't found an exponential speedup for most financial algorithms that quantum could be used for yet, but we don't need an optimal solution. In finance, even a solution that is just 0.1% more efficient could equate to billions of dollars of profit.

So, how do we know what use cases and problems could be suitable for quantum computing right now? Is there is good reason to believe quantum utility, or even advantage, can be found either now or in the near future?

Maybe it's easier to first name the things the problem should certainly not have. It can't require a huge number of qubits. We don't have processors that have thousands to millions of qubits available yet. That's one of the main reasons that Shor's algorithm and the like are so far from being realized. The circuits also can't be too deep. The limit to circuit depth depends on many factors, but in general, if your experiment requires a depth that you haven't seen achieved in the literature yet, it's probably not going to work. And lastly, any type of algorithm that we know will require error correction cannot be done yet.

All of these limitations are addressed in theIBM Quantum roadmapand we expect to achieve error correction in the early 2030's, but for now, we need to look for experiments that make use of most of the qubits currently available on a given QPU. We also emphasize the importance of error mitigation and suppression. And lastly, there should be an obvious extension to future applications that would be important for society and that we could see eventually leading to quantum advantage.

Now let's talk about some examples of use cases, which fall into three main categories that we have identified as most likely to see favorable outcomes in the near to middle term:

Nature simulations. Current classical methods of atomic and molecular simulations are limited by inefficient mathematical descriptions of the atomic structure. Storing and manipulating a quantum state takes exponentially many resources on a classical computer but can be done efficiently on a quantum computer. This could lead to developments in carbon dioxide sequestration, alternative batteries, or the invention of new drugs. Some algorithms that are especially relevant in this area are: the Variational Quantum Eigensolver (VQE), which is used to estimate certain properties of a material, such as equilibrium or minimum energy states; the Time Dynamics Simulation (TDS) algorithm, which is used to estimate response functions or spectral properties of materials; and a newcomer,Sample-based Quantum Diagonalization (SQD), which we think we will be hearing much more about in the near future.

Optimization. This area is ubiquitous in computing, so the use-cases are numerous and varied. Some examples that we hear about a lot are portfolio optimization in finance, industrial design, and distribution and supply chain. The most common algorithm you will probably hear related to finance is the one we have already covered in some depth: the quantum approximate optimization algorithm, or QAOA.

Quantum machine learning. This area has generated a lot of excitement in the past few years, but it's likely that QML won't be useful as soon as simulation will. But there are nevertheless some impressive algorithms that are being worked on to address some very important uses cases. Some of these possible use cases are natural language processing, network traffic analysis, and even fraud detection in financial transactions. Relevant algorithms in this area are the quantum support vector machine (QSVM), quantum neural networks (QNN), and quantum generative adversarial networks.

Within these broad application areas, the community sees the benefit in groups working together who are focused on a more specific topic. IBM spearheaded an initiative called Working Groups to try to help collaborators meet each other and create productive synergy in four specific areas: healthcare and life science, materials and high-performance computing (HPC), high energy physics, and optimization. And recently, a fifth working group on sustainability was created.

We're now going to zoom in on a few problems that were recently tackled by some of these working groups. The main goal here is not to understand every detail of an experiment – this can be intimidating for even experts if the paper is slightly out of your area of expertise. The goal is simply to help develop an intuition for the types of problems quantum computers are good for and how to handle them. And if you're interested, we encourage you to read the full papers.

First, we're going to delve into a paper by Martin Savage's group at the University of Washington calledQuantum Simulations of Hadron Dynamics in the Schwinger Model Using 112 Qubits.

If you're not a high energy physicist, you still might be familiar with the term “hadron,” as in the Large Hadron Collider (LHC), which is the giant particle accelerator, 27-km in circumference, that made it possible to finally observe the Higgs boson. A hadron is a subatomic composite particle made up of other small particles called quarks. Some examples of hadrons are neutrons and protons.

For a little context, the LHC was built to enable the study of fundamental physics by colliding particles at super high energies. With the LHC, scientists hope to learn more about the early universe and the fundamental laws of nature. In principle, the interactions of these particles could be simulated from start to finish with a sufficiently powerful quantum computer. We're not quite there, but we're making progress.

The Schwinger model is a popular, simple model used to simulate some of these dynamics. It is a model that describes the behavior of electrons and positrons interacting via photons in 1+1D, meaning time and one spatial dimension. The model has a lot of similarities to quantum chromodynamics (QCD), which describes how quarks and hadrons interact, but QCD is extremely hard to simulate. So, the Schwinger model is often used as a toy model to investigate some phenomena that are common to both.

To understand why they tackled this problem, let's ask ourselves a series of questions.

First, why did they have reason to believe simulating this on a quantum computer would work at all? In this case, the electrons and positrons in the Schwinger model have a screening effect, causing correlations between distant fermions to decay exponentially with separation. This means there aren't as many necessary long-range interactions from a qubit on one side of the chip to another, which we know is very error-prone. So, this is great for the hardware we have available today.

Next, why is this topic of interest? High-energy physics in general is of great interest. People were willing to spend billions of dollars to build the LHC, and many thousands of scientists and technicians across the globe have dedicated their careers to this field. Although the Schwinger model is simplistic and isn't designed to cover three spatial dimensions, it is still a useful simplification of the full theory.

Last, how was this work done, or how would we approach the problem if we were looking to continue this work? In simulation-type experiments, VQE is one of the most common approaches, and the first step is almost always the same: prepare the ground state. In this case, it is a vacuum state. In this experiment, they use a new version of VQE called SC-ADAPT-VQE (which stands for Scalable Circuits - Adaptive Derivative-Assembled Pseudo-Trotter ansatz-VQE) to prepare both the ground state and the hadron wave packet on this vacuum. The next step is to allow the hadrons to evolve in time. Lastly, identify the observables you want to measure and measure them.

If those steps sound a bit familiar, minus the hadron wavepacket part, that's because these steps are very similar to what we covered in the QAOA example in the previous lesson. We start in a familiar state (here the vacuum state), and then we let it evolve in time with a series of exponentiated Hamiltonians. Many variational algorithms follow this general approach. A big difference here, though, is that we create the wave packet of hadrons centered within our circuit, before we start letting it evolve.

So, how do we create the wavepacket? On the vacuum, a hadron can be excited by creating a fermion-antifermion pair on adjacent sites. By preparing a superposition of such hadrons at different locations, an arbitrary wavepacket can be prepared. The authors centered their wavepacket in middle of the circuit to observe the evolution without hitting a boundary.

But remember: the name of the game when working with noisy QPUs is to keep the circuit depth manageable. To do this, the SC-ADAPT-VQE protocol uses symmetries and hierarchies in length scales to determine low-depth quantum circuits for state preparation. This will create an ansatz with a smaller number of parameters, and therefore, a shallower depth.

The experiment was run on an IBM Quantum Heron device and included a few different types of error mitigation and suppression: dynamical decoupling, zero noise extrapolation, Pauli twirling, and a recently developed technique called operator decoherence renormalization.



Above is a figure from the paper showing the observable of interest, the chiral condensate, which is basically a superfluid phase of the hadrons. Now, we can see the wavepacket at the center of the sites that have been designated to run this experiment. The black lines are the error-free results from the (computationally expensive) classical simulation, while the points with error bars are the results from the 133-qubit IBM quantum computer, Torino.

We see two different time steps in the wavepacket evolution. At timet=1t=1t=1, you can see that the chiral condensate is narrow and localized, and it also matches the classical simulation well. Att=14t=14t=14, it is much more spread out. The comparison to the simulator isn't quite as perfect now, but you can still obviously see very good agreement between theory and data, which is encouraging.

In conclusion, this is a very cool example of the type of simulation work you might not initially think of applying quantum computing to, but which shows real promise. It's not perfect but you don't have to be a particle physics expert to see that the quantum computer accurately predicts the outward propagation of the wavepacket, which is exactly what we would expect to find. Hopefully future work in this area will continue and high energy physicists will continue to find ways to incorporate quantum computing into their workstreams. The aim is to solve difficult theoretical problems more precisely and use experiments to accept or reject theories in hopes of discovering new physics, building improved detectors, and leading to a better understanding of nature at its most fundamental level.

Our next example focuses on optimization and will be a deep dive into a paper calledBias-Field Digitized Counterdiabatic Quantum Optimization, which was done by members of the Kipu Quantum team and the University of the Basque Country in Spain.

In the paper, the authors developed a new optimization method and applied it to find the ground-state of an Ising spin-glass. As we discussed previously, many combinatorial optimization problems can be reformulated as solving for low-energy states of Ising Hamiltonians. The Ising model describes the interaction of an array of microscopic spins. In some regimes, the model predicts that the spins behave as a glass, in which the magnetic moments are disordered above a so-called "freezing temperature."

We'll begin like we did before with a series of definitions. The first is counterdiabatic, which is a type of evolution that suppresses non-adiabatic effects experienced by a system, regardless of how fast those processes occur. Recall the adiabatic theorem from the last episode – you usually need to evolve a system very slowly if you wish for it to remain in the ground state. This is a big problem because the slower we must evolve things, the more time we have for errors to occur. Counterdiabatic driving (CD) aims to combat this by adding terms that counteract these unwanted excitations. The main idea here is to speed up the whole experiment and reduce the quantum circuit depth by suppressing excitations that could cause spurious transitions.

Now for the other piece of jargon in the title: the bias field. Other iterative algorithms, like VQE, take classical parameters into the states and use classical optimizers to search the many-dimensional parameter space for the set of parameters that yields a minimum expectation value for a fixed Hamiltonian. In this case, they instead vary the Hamiltonian each time, moving adiabatically from a known case to the case of interest. To change the Hamiltonian, they simply directly apply the Pauli-Z expectation value from one iteration as a bias field in the Hamiltonian for the next iteration. In this way, they steer the dynamics toward the actual solution without the need for classical optimizers.

So, why is this experiment of interest? Ising spin-glasses are of fundamental interest in physics, but this new approach is even more general than that. It could be applied to many optimization problems, so the paper is of wide interest.

And why did we think this would work? The algorithm they are proposing speeds up the evolution to reduce the circuit depth, while also suppressing non-adiabatic transitions. Furthermore, it does not rely on any classical optimization subroutines, which can be an issue leading to barren plateaus and getting stuck at local minima. Lastly, the authors also make sure to align the interactions in the problem Hamiltonian with the hardware connectivity in the real QPUs, which is always very important.

So, how does this method work? Again, it does not use any classical optimizers, unlike most other iterative quantum algorithms. Instead, by feeding the solution from each iteration into the input for the next one, the bias-field digitized quantum optimization algorithm incrementally refines the ground state, bringing it closer and closer to the final evolved state. And combined with the counterdiabatic protocols, we can do this even with short-depth quantum circuits that should run smoothly on noisy hardware.

So, when the experiment was performed, the authors chose to run the algorithm on the 127-qubit IBM Quantum computer Brisbane. Below is a figure showing the 8th iteration of the optimization algorithm for a nearest-neighbor, randomly generated spin-glass instance on 100 qubits. They compare idealized classical simulation results from DCQO and BF-DCQO, as well as the experimental result run on the quantum computer. They also show the result from a classical solver called Gurobi as a reference. With just 10 iterations, BF-DCQO provides a drastic enhancement compared to DCQO. Although the experimental result is a bit different than the ideal result due to noise, the performance is still better than the ideal DCQO. This shows that there is still a lot of excellent progress being made with regards to quantum optimization and good results are being reported on over 100 qubits for one of the first times.



Finally, we'll discuss a paper from Moderna Pharmaceuticals calledmRNA Secondary Structure Prediction Using Utility-Scale Quantum Computers.

First, a brief refresher on mRNA. Messenger RNA is a type of RNA involved in protein synthesis. It basically reads instructions given by DNA. The secondary structure of mRNA is how the chain is folded, as shown in the diagram below. And the RNA secondary structure prediction problem is the problem of finding the most stable folding of the sequence of bases or nucleotides that make up RNA: adenine (A), cytosine (C), uracil (U), and guanine (G). The image below shows some common folding structures found in mRNA, each color represents a different type of secondary structure. What makes one structure more favorable than the others isn't well understood; all we can do is calculate which structure yields the lowest free energy compared to the unfolded state. And that's where quantum computers come in.



So, why are mRNA secondary structures important? Accurate prediction of them is crucial to not only understanding DNA and our genes, but also for designing RNA-based therapeutics, like the COVID-19 vaccine.

This has long been known to be a formidable optimization problem for classical computers due to the vast number of possible configurations. For some configurations, it's known to be an NP-complete problem. However, on a quantum computer, we can formulate the secondary structure prediction as a binary optimization problem – something we know how to handle. Furthermore, there had been evidence in the literature already of accurate RNA predictions on small scale quantum devices and quantum simulators. But would this work on larger hardware?

This experiment was performed using something called the conditional value at risk variational quantum eigensolver, which is a modification of a traditional VQE algorithm and is expected to achieve better convergence.



The plot above shows the distribution of measurement probabilities of the sampled bitstrings, with the corresponding energies for a 42-nucleotide, 80-qubit instance. Here, the bitstrings symbolize pairings of nucleotides. It illustrates that the lowest energy bitstring found by the quantum computer matches that of the comparative classical solver, so that is great. Also shown is the optimal folded structure of that nucleotide chain based on the lowest energy bitstring the quantum computer found.

Hopefully, these three use-cases have given you enough context to understand what cutting-edge work in the field looks like right now, and the confidence to attempt new quantum experiments you might not have before.

Remember: quantum computing isn't good for every problem. And really this is just a testament to how good we've gotten at classical computing. Just because you think you could apply quantum computing to a problem doesn't mean it is going to yield interesting results; you must consider the scaling.

Circuit depth is a double-edged sword. We need it to be considerably large to do interesting work that classical computers can't, but right now, we can't increase the depth too much because the hardware noise will cause the fidelity to diminish. It's all about finding that sweet spot and knowing that it's a moving target. So, take some time between now and the next lesson to think about a problem you have come across in your research, and how you might approach it with what we have learned so far. And hey, your solution may not pan out, and that's fine. That's why this is research.

Was this page helpful?


---

