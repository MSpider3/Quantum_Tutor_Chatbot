

## Can all computers take advantage of quantum computers？

[Music] [Music]
no most calculations that we all do on a no most calculations that we all do on a no most calculations that we all do on a
day-to-day basis have no need for day-to-day basis have no need for day-to-day basis have no need for
Quantum computational power because they Quantum computational power because they Quantum computational power because they
are simple however there is a whole are simple however there is a whole are simple however there is a whole
realm of really hard computational realm of really hard computational realm of really hard computational
problems that people working in science problems that people working in science problems that people working in science
and medicine and optimization would love and medicine and optimization would love and medicine and optimization would love
to be able to solve but they can't to be able to solve but they can't to be able to solve but they can't
quantum computers allow us to push that quantum computers allow us to push that quantum computers allow us to push that
barrier a little bit more and it's barrier a little bit more and it's barrier a little bit more and it's
likely that as that technology matures likely that as that technology matures likely that as that technology matures
we will find even more problems that we will find even more problems that we will find even more problems that
they can Aid in

## Do you need to write your own software for quantum computers？

Not at all. IBM and other companies have Not at all. IBM and other companies have
created built-in classes, functions, and created built-in classes, functions, and created built-in classes, functions, and
add-ons to help get you started much add-ons to help get you started much add-ons to help get you started much
more quickly. But if you prefer to have more quickly. But if you prefer to have more quickly. But if you prefer to have
that level of control, you can alter that level of control, you can alter that level of control, you can alter
these functions and of course create these functions and of course create these functions and of course create
your

## How do quantum computers break encryption？ #quantum #quantumphysics #quantumcomputing

[Music] [Music]
well right now they do not break well right now they do not break well right now they do not break
encryption however that is a real encryption however that is a real encryption however that is a real
concern many have because a concern many have because a concern many have because a
theoretically powerful enough quantum theoretically powerful enough quantum theoretically powerful enough quantum
computer could be capable of running computer could be capable of running computer could be capable of running
Shores algorithm which is an efficient Shores algorithm which is an efficient Shores algorithm which is an efficient
algorithm for finding prime factors of algorithm for finding prime factors of algorithm for finding prime factors of
large numbers the difficulty of this large numbers the difficulty of this large numbers the difficulty of this
task is actually the basis of RSA task is actually the basis of RSA task is actually the basis of RSA
encryption one of the standard encryption one of the standard encryption one of the standard
encryption methods that is implemented encryption methods that is implemented encryption methods that is implemented
all over the world but no no one has a all over the world but no no one has a all over the world but no no one has a
machine capable of that yet to prepare machine capable of that yet to prepare machine capable of that yet to prepare
for this however we are already for this however we are already for this however we are already
developing other encryption schemes that developing other encryption schemes that developing other encryption schemes that
would be safe from Quantum hacking

## How do quantum computers process information？ #quantum #quantumphysics #quantumcomputing

[Music] [Music]
quantum computers process information by quantum computers process information by quantum computers process information by
running Quantum Gates on Quantum bits or running Quantum Gates on Quantum bits or running Quantum Gates on Quantum bits or
cubits not dissimilar to electronic cubits not dissimilar to electronic cubits not dissimilar to electronic
Gates ran on regular bits for example Gates ran on regular bits for example Gates ran on regular bits for example
the and or or gate there are quantum the and or or gate there are quantum the and or or gate there are quantum
counterparts that allow the cubits to counterparts that allow the cubits to counterparts that allow the cubits to
take advantage of the uniquely Quantum take advantage of the uniquely Quantum take advantage of the uniquely Quantum
properties that they have putting all properties that they have putting all properties that they have putting all
these Gates together creates a Quantum these Gates together creates a Quantum these Gates together creates a Quantum
circuit which is a series of circuit which is a series of circuit which is a series of
instructions for the quantum computer to instructions for the quantum computer to instructions for the quantum computer to
follow

## If quantum computers are real, why aren’t they changing the world yet？

[Music] Well, quantum computing is a new Well, quantum computing is a new
computing paradigm that presents computing paradigm that presents computing paradigm that presents
difficult technological challenges. And difficult technological challenges. And difficult technological challenges. And
while there has been a lot of exciting while there has been a lot of exciting while there has been a lot of exciting
progress, the technology is still in its progress, the technology is still in its progress, the technology is still in its
infancy. When we discuss the development infancy. When we discuss the development infancy. When we discuss the development
of quantum computing technology, there of quantum computing technology, there of quantum computing technology, there
are a couple different eras people refer are a couple different eras people refer are a couple different eras people refer
to. The one that is furthest off but to. The one that is furthest off but to. The one that is furthest off but
most powerful would be the era of fault most powerful would be the era of fault most powerful would be the era of fault
tolerant quantum computing which means tolerant quantum computing which means tolerant quantum computing which means
we would have machines capable of we would have machines capable of we would have machines capable of
detecting and fixing intrinsic errors detecting and fixing intrinsic errors detecting and fixing intrinsic errors
even in the middle of a calculation. even in the middle of a calculation. even in the middle of a calculation.
We're not there yet. Currently the We're not there yet. Currently the We're not there yet. Currently the
quantum computers that exist are quite a quantum computers that exist are quite a quantum computers that exist are quite a
bit smaller. And while we know how to bit smaller. And while we know how to bit smaller. And while we know how to
mitigate some types of errors, we can't mitigate some types of errors, we can't mitigate some types of errors, we can't
account for all of them. So at this time account for all of them. So at this time account for all of them. So at this time
we are still very much in an era of we are still very much in an era of we are still very much in an era of
exploration where we are exploring what exploration where we are exploring what exploration where we are exploring what
applications could still make use of the applications could still make use of the applications could still make use of the
processors of this size and processors of this size and processors of this size and
functionality. We don't believe the functionality. We don't believe the functionality. We don't believe the
answer is none but it's still a very big answer is none but it's still a very big answer is none but it's still a very big
parameter space that we have only begun parameter space that we have only begun parameter space that we have only begun
to explore while we are still very much to explore while we are still very much to explore while we are still very much
working towards building machines that working towards building machines that working towards building machines that
are capable of fault tolerance.

## More Qubits, More Power？ Not So Fast…

[Music] [Music]
no cubic count is just one metric that no cubic count is just one metric that no cubic count is just one metric that
is important to keep in mind when we're is important to keep in mind when we're is important to keep in mind when we're
looking at the maturation of the looking at the maturation of the looking at the maturation of the
technology just as important are the technology just as important are the technology just as important are the
quality of the cubits which means how quality of the cubits which means how quality of the cubits which means how
well they can store Quantum information well they can store Quantum information well they can store Quantum information
and how fast we can run instructions on and how fast we can run instructions on and how fast we can run instructions on
them citing a processor with a high them citing a processor with a high them citing a processor with a high
Cubit count means little unless these Cubit count means little unless these Cubit count means little unless these
other metrics are also taken into other metrics are also taken into other metrics are also taken into
account

## Quantum Computing： Advanced Insights

if you've at all been paying attention if you've at all been paying attention
to the news or the media recently you to the news or the media recently you to the news or the media recently you
have probably come across the term have probably come across the term have probably come across the term
quantum Computing as Quantum Computing quantum Computing as Quantum Computing quantum Computing as Quantum Computing
becomes more mainstream more and more becomes more mainstream more and more becomes more mainstream more and more
people are understandably struggling to people are understandably struggling to people are understandably struggling to
explain what quantum computers really explain what quantum computers really explain what quantum computers really
are how they work and the fundamental are how they work and the fundamental are how they work and the fundamental
concepts surrounding them IBM Quantum is concepts surrounding them IBM Quantum is concepts surrounding them IBM Quantum is
the world leader in Quantum Computing we the world leader in Quantum Computing we the world leader in Quantum Computing we
have the most sophisticated Quantum have the most sophisticated Quantum have the most sophisticated Quantum
program in the world and lots of program in the world and lots of program in the world and lots of
dedicated free educational content so dedicated free educational content so dedicated free educational content so
that you don't have to feel left in the that you don't have to feel left in the that you don't have to feel left in the
dark about what's going on in the space dark about what's going on in the space dark about what's going on in the space
today I'm going to answer some of the today I'm going to answer some of the today I'm going to answer some of the
most common but technologically advanced most common but technologically advanced most common but technologically advanced
questions we get about quantum computers questions we get about quantum computers questions we get about quantum computers
let's get [Music] [Music]
started number one to use the quantum started number one to use the quantum started number one to use the quantum
computer do you need to build your own computer do you need to build your own computer do you need to build your own
software and programs from the ground up software and programs from the ground up software and programs from the ground up
not at all IBM and other companies have not at all IBM and other companies have not at all IBM and other companies have
created built-in classes function and created built-in classes function and created built-in classes function and
add-ons to help get you started much add-ons to help get you started much add-ons to help get you started much
more quickly but if you prefer to have more quickly but if you prefer to have more quickly but if you prefer to have
that level of control you can alter that level of control you can alter that level of control you can alter
these functions and of course create these functions and of course create these functions and of course create
your own number two what does Quantum your own number two what does Quantum your own number two what does Quantum
utility actually mean and is utility the utility actually mean and is utility the utility actually mean and is utility the
same thing as same thing as same thing as
Advantage no they are different let's Advantage no they are different let's Advantage no they are different let's
start with Quantum Advantage Quantum start with Quantum Advantage Quantum start with Quantum Advantage Quantum
Advantage is achieved when a quantum Advantage is achieved when a quantum Advantage is achieved when a quantum
computer is the best possible choice for computer is the best possible choice for computer is the best possible choice for
completing a computational task completing a computational task completing a computational task
it can solve it faster more efficiently it can solve it faster more efficiently it can solve it faster more efficiently
or more cost effectively or by any other or more cost effectively or by any other or more cost effectively or by any other
metric Quantum utility is not as strong metric Quantum utility is not as strong metric Quantum utility is not as strong
a claim Quantum utility means that the a claim Quantum utility means that the a claim Quantum utility means that the
quantum computer can run certain quantum computer can run certain quantum computer can run certain
calculations more efficiently than a calculations more efficiently than a calculations more efficiently than a
Brute Force classical computation Brute Brute Force classical computation Brute Brute Force classical computation Brute
Force Computing means that the classical Force Computing means that the classical Force Computing means that the classical
computer is more or less exactly computer is more or less exactly computer is more or less exactly
calculating the mathematics required to calculating the mathematics required to calculating the mathematics required to
run the problem it's highly inefficient run the problem it's highly inefficient run the problem it's highly inefficient
but sometimes there's no better option but sometimes there's no better option but sometimes there's no better option
now there are all sorts of classical now there are all sorts of classical now there are all sorts of classical
methods Beyond Brute Force parallel methods Beyond Brute Force parallel methods Beyond Brute Force parallel
computation but at the bleeding edge of computation but at the bleeding edge of computation but at the bleeding edge of
computation you often don't know what computation you often don't know what computation you often don't know what
the right answer looks like there's some the right answer looks like there's some the right answer looks like there's some
degree of uncertainty in all of these degree of uncertainty in all of these degree of uncertainty in all of these
calculated Solutions and they don't calculated Solutions and they don't calculated Solutions and they don't
always agree with one another always agree with one another always agree with one another
either however when you have achieved either however when you have achieved either however when you have achieved
Quantum utility it means you can Quantum utility it means you can Quantum utility it means you can
calculate solutions that fall within the calculate solutions that fall within the calculate solutions that fall within the
range of the other possible solutions range of the other possible solutions range of the other possible solutions
and so using a quantum computer is a and so using a quantum computer is a and so using a quantum computer is a
legitimate way to compute a solution and legitimate way to compute a solution and legitimate way to compute a solution and
it can be a useful comparative marker it can be a useful comparative marker it can be a useful comparative marker
but that doesn't mean it's the ultimate but that doesn't mean it's the ultimate but that doesn't mean it's the ultimate
fastest or more efficient fastest or more efficient fastest or more efficient
method number three do we need room method number three do we need room method number three do we need room
temperature superconductors in order to temperature superconductors in order to temperature superconductors in order to
scale today's quantum computers nah we scale today's quantum computers nah we scale today's quantum computers nah we
don't need this there are actually many don't need this there are actually many don't need this there are actually many
superconductors with a much higher superconductors with a much higher superconductors with a much higher
critical temperature the temperature at critical temperature the temperature at critical temperature the temperature at
which they become superc conducting than which they become superc conducting than which they become superc conducting than
the ones that are used in Quantum the ones that are used in Quantum the ones that are used in Quantum
computing we don't use these because computing we don't use these because computing we don't use these because
they have a variety of other effects and they have a variety of other effects and they have a variety of other effects and
properties that aren't suitable for us properties that aren't suitable for us properties that aren't suitable for us
higher temp superconductors fit in this higher temp superconductors fit in this higher temp superconductors fit in this
category in addition we operate the category in addition we operate the category in addition we operate the
devices at much colder temperatures than devices at much colder temperatures than devices at much colder temperatures than
even the critical temperature of the even the critical temperature of the even the critical temperature of the
superconductor because we need to remove superconductor because we need to remove superconductor because we need to remove
all thermal noise and stray energy that all thermal noise and stray energy that all thermal noise and stray energy that
would disrupt or even destroy the would disrupt or even destroy the would disrupt or even destroy the
quantum States containing the quantum States containing the quantum States containing the
information there could be other information there could be other information there could be other
Technologies or ways to scale in the Technologies or ways to scale in the Technologies or ways to scale in the
future but high temperature future but high temperature future but high temperature
superconductors are not considered superconductors are not considered superconductors are not considered
important important important
elements number four how do we handle elements number four how do we handle elements number four how do we handle
Quantum errors today's quantum computers Quantum errors today's quantum computers Quantum errors today's quantum computers
use a combination of techniques to deal use a combination of techniques to deal use a combination of techniques to deal
with noise at different stages of the with noise at different stages of the with noise at different stages of the
computation at the most basic level we computation at the most basic level we computation at the most basic level we
use techniques that transform a circuit use techniques that transform a circuit use techniques that transform a circuit
during compilation to minimize errors during compilation to minimize errors during compilation to minimize errors
these might include introducing Gates these might include introducing Gates these might include introducing Gates
while a cubit is sitting idle so it while a cubit is sitting idle so it while a cubit is sitting idle so it
doesn't have time to doesn't have time to doesn't have time to
decohere we can also employ techniques decohere we can also employ techniques decohere we can also employ techniques
that occur in the postprocessing stage that occur in the postprocessing stage that occur in the postprocessing stage
typically this starts with typically this starts with typically this starts with
pre-processing overhead related to model pre-processing overhead related to model pre-processing overhead related to model
training to understand noise in the training to understand noise in the training to understand noise in the
system and classical post-processing system and classical post-processing system and classical post-processing
overhead to mitigate errors in the Raw overhead to mitigate errors in the Raw overhead to mitigate errors in the Raw
results by using the generated model in results by using the generated model in results by using the generated model in
exchange for this overhead though we're exchange for this overhead though we're exchange for this overhead though we're
able to get much more accurate able to get much more accurate able to get much more accurate
results however a large scale fault results however a large scale fault results however a large scale fault
tolerant quantum computer computer will tolerant quantum computer computer will tolerant quantum computer computer will
require techniques that detect errors require techniques that detect errors require techniques that detect errors
while an algorithm is running and while an algorithm is running and while an algorithm is running and
correct for them in real time we do this correct for them in real time we do this correct for them in real time we do this
by encoding the information of one what by encoding the information of one what by encoding the information of one what
we call a logical Cubit across multiple we call a logical Cubit across multiple we call a logical Cubit across multiple
physical cubits the basic idea is that physical cubits the basic idea is that physical cubits the basic idea is that
the information will be stored more the information will be stored more the information will be stored more
reliably as we use redundant physical reliably as we use redundant physical reliably as we use redundant physical
cubits to represent a single logical cubits to represent a single logical cubits to represent a single logical
Cubit the downside of this is that a Cubit the downside of this is that a Cubit the downside of this is that a
large overhead is needed since you need large overhead is needed since you need large overhead is needed since you need
more physical cubits on the chip more physical cubits on the chip more physical cubits on the chip
although IBM has recently developed and although IBM has recently developed and although IBM has recently developed and
published a code that reduced the published a code that reduced the published a code that reduced the
overhead by overhead by overhead by
90% we also require components that can 90% we also require components that can 90% we also require components that can
apply Quantum Gates on encoded cubits apply Quantum Gates on encoded cubits apply Quantum Gates on encoded cubits
and all of these components must be and all of these components must be and all of these components must be
scalable there's been a lot of good work scalable there's been a lot of good work scalable there's been a lot of good work
experimenting with these techniques but experimenting with these techniques but experimenting with these techniques but
a lot more work is needed once we've a lot more work is needed once we've a lot more work is needed once we've
demonstrated this however we will have demonstrated this however we will have demonstrated this however we will have
achieved what we refer to as Quantum achieved what we refer to as Quantum achieved what we refer to as Quantum
Computing that is Fault tolerant that it Computing that is Fault tolerant that it Computing that is Fault tolerant that it
can calculate accurately even in the can calculate accurately even in the can calculate accurately even in the
presence of presence of presence of
Errors question five are experts only Errors question five are experts only Errors question five are experts only
interested in exponential speedups like interested in exponential speedups like interested in exponential speedups like
we see from Shores we see from Shores we see from Shores
algorithm while exponential speedups are algorithm while exponential speedups are algorithm while exponential speedups are
most compelling to experts Quantum most compelling to experts Quantum most compelling to experts Quantum
heuristics hold enormous potential even heuristics hold enormous potential even heuristics hold enormous potential even
without a rigorously proven speed up a without a rigorously proven speed up a without a rigorously proven speed up a
heris is a pragmatic approach to solving heris is a pragmatic approach to solving heris is a pragmatic approach to solving
a problem that isn't necessarily a problem that isn't necessarily a problem that isn't necessarily
guaranteed to be optimal or even always guaranteed to be optimal or even always guaranteed to be optimal or even always
correct yet still proves useful in correct yet still proves useful in correct yet still proves useful in
practice in fact classical Computing practice in fact classical Computing practice in fact classical Computing
already relies on powerful heuristics already relies on powerful heuristics already relies on powerful heuristics
large language models for instance use large language models for instance use large language models for instance use
heuristic techniques rather than heuristic techniques rather than heuristic techniques rather than
strictly optimal algorithms and this strictly optimal algorithms and this strictly optimal algorithms and this
suggests that similar approaches in suggests that similar approaches in suggests that similar approaches in
Quantum Computing could be extremely Quantum Computing could be extremely Quantum Computing could be extremely
valuable moreover in the long term even valuable moreover in the long term even valuable moreover in the long term even
polinomial speedups could contribute to polinomial speedups could contribute to polinomial speedups could contribute to
Quantum advantage Quantum advantage Quantum advantage
expanding the range of problems where expanding the range of problems where expanding the range of problems where
Quantum methods provide meaningful Quantum methods provide meaningful Quantum methods provide meaningful
benefits and there you have it that benefits and there you have it that benefits and there you have it that
concludes our three episode series where concludes our three episode series where concludes our three episode series where
we answer the most common Quantum we answer the most common Quantum we answer the most common Quantum
Computing questions that we often see if Computing questions that we often see if Computing questions that we often see if
I didn't answer your question go back I didn't answer your question go back I didn't answer your question go back
and check out the other two videos and and check out the other two videos and and check out the other two videos and
if we still didn't answer your question if we still didn't answer your question if we still didn't answer your question
please drop it in the comments below please drop it in the comments below please drop it in the comments below
we'll see you next time we'll see you next time we'll see you next time
[Music]

## Quantum Computing： Beyond The Basics

if you've at all been paying attention if you've at all been paying attention
to the news or the media recently you've to the news or the media recently you've to the news or the media recently you've
probably come across the term quantum probably come across the term quantum probably come across the term quantum
Computing as Quantum Computing becomes Computing as Quantum Computing becomes Computing as Quantum Computing becomes
more and more mainstream people are more and more mainstream people are more and more mainstream people are
understandably struggling to explain understandably struggling to explain understandably struggling to explain
exactly what quantum computers are how exactly what quantum computers are how exactly what quantum computers are how
they work and the fundamental concepts they work and the fundamental concepts they work and the fundamental concepts
surrounding them IBM Quantum is the surrounding them IBM Quantum is the surrounding them IBM Quantum is the
world leader in Quantum Computing we world leader in Quantum Computing we world leader in Quantum Computing we
have the most sophisticated Quantum have the most sophisticated Quantum have the most sophisticated Quantum
program in the world and lots of program in the world and lots of program in the world and lots of
dedicated free educational content so dedicated free educational content so dedicated free educational content so
that you don't have to feel left in the that you don't have to feel left in the that you don't have to feel left in the
dark about what's going on in the space dark about what's going on in the space dark about what's going on in the space
today in this video I'm going to answer today in this video I'm going to answer today in this video I'm going to answer
some of the most common intermediate some of the most common intermediate some of the most common intermediate
level questions about quantum computers level questions about quantum computers level questions about quantum computers
let's get [Music] [Music]
started number one when will quantum started number one when will quantum started number one when will quantum
computers replace computers replace computers replace
laptops well the most important thing to laptops well the most important thing to laptops well the most important thing to
know is that quantum computers are not know is that quantum computers are not know is that quantum computers are not
just faster regular computers so the just faster regular computers so the just faster regular computers so the
answer is they almost certainly never answer is they almost certainly never answer is they almost certainly never
will it is a misconception that it's will it is a misconception that it's will it is a misconception that it's
quantum computers versus classical quantum computers versus classical quantum computers versus classical
computers it's really quantum computers computers it's really quantum computers computers it's really quantum computers
aiding classical computers versus aiding classical computers versus aiding classical computers versus
classical computers alone quantum classical computers alone quantum classical computers alone quantum
computers are more efficient in theory computers are more efficient in theory computers are more efficient in theory
at only a subset of computational at only a subset of computational at only a subset of computational
problems most computational problems the problems most computational problems the problems most computational problems the
public is familiar with can be computed public is familiar with can be computed public is familiar with can be computed
with no issues whatsoever on a regular with no issues whatsoever on a regular with no issues whatsoever on a regular
computer in our world we call them class computer in our world we call them class computer in our world we call them class
classical computers adding a Quantum classical computers adding a Quantum classical computers adding a Quantum
processor to your phone or your laptop processor to your phone or your laptop processor to your phone or your laptop
would not add any would not add any would not add any
benefit number two if functioning benefit number two if functioning benefit number two if functioning
quantum computers exist today then why quantum computers exist today then why quantum computers exist today then why
aren't they doing all of the things aren't they doing all of the things aren't they doing all of the things
people said quantum computers can do people said quantum computers can do people said quantum computers can do
well Quantum Computing is a new well Quantum Computing is a new well Quantum Computing is a new
Computing Paradigm that presents Computing Paradigm that presents Computing Paradigm that presents
difficult technological challenges and difficult technological challenges and difficult technological challenges and
while there has been a lot of exciting while there has been a lot of exciting while there has been a lot of exciting
progress the technology is still in its progress the technology is still in its progress the technology is still in its
infancy when we discuss the development infancy when we discuss the development infancy when we discuss the development
of quantum Computing technology there of quantum Computing technology there of quantum Computing technology there
are a couple different eras people refer are a couple different eras people refer are a couple different eras people refer
to the one that is furthest off but most to the one that is furthest off but most to the one that is furthest off but most
powerful would be the era of fault powerful would be the era of fault powerful would be the era of fault
tolerant Quantum Computing which means tolerant Quantum Computing which means tolerant Quantum Computing which means
we would have machines capable of we would have machines capable of we would have machines capable of
detecting and fixing intrinsic errors detecting and fixing intrinsic errors detecting and fixing intrinsic errors
even in the middle of a calculation even in the middle of a calculation even in the middle of a calculation
we're not there yet currently the we're not there yet currently the we're not there yet currently the
quantum computers that exist are quite a quantum computers that exist are quite a quantum computers that exist are quite a
bit smaller and while we know how to bit smaller and while we know how to bit smaller and while we know how to
mitigate some types of Errors we can't mitigate some types of Errors we can't mitigate some types of Errors we can't
account for all of them so at this time account for all of them so at this time account for all of them so at this time
we are still very much in an era of we are still very much in an era of we are still very much in an era of
exploration where we are exploring what exploration where we are exploring what exploration where we are exploring what
applications could still make use of the applications could still make use of the applications could still make use of the
processors of this size and processors of this size and processors of this size and
functionality we don't believe the functionality we don't believe the functionality we don't believe the
answer is none but it's still a very big answer is none but it's still a very big answer is none but it's still a very big
parameter space that we have only begun parameter space that we have only begun parameter space that we have only begun
to explore while we are still very much to explore while we are still very much to explore while we are still very much
working towards building machines that working towards building machines that working towards building machines that
are capable of fault are capable of fault are capable of fault
tolerance number three can all Computing tolerance number three can all Computing tolerance number three can all Computing
problems take advantage of quantum problems take advantage of quantum problems take advantage of quantum
Computing no most calculations that we Computing no most calculations that we Computing no most calculations that we
all do on a day-to-day basis have no all do on a day-to-day basis have no all do on a day-to-day basis have no
need for Quantum computational power need for Quantum computational power need for Quantum computational power
because they are simple however there is because they are simple however there is because they are simple however there is
a whole realm of really hard a whole realm of really hard a whole realm of really hard
computational problems that people computational problems that people computational problems that people
working in science and medicine and working in science and medicine and working in science and medicine and
optimization would love to be able to optimization would love to be able to optimization would love to be able to
solve but they can't quantum computers solve but they can't quantum computers solve but they can't quantum computers
allow us to push that barrier a little allow us to push that barrier a little allow us to push that barrier a little
bit more bit more bit more
and it's likely that as that technology and it's likely that as that technology and it's likely that as that technology
matures we will find even more problems matures we will find even more problems matures we will find even more problems
that they can Aid that they can Aid that they can Aid
in four what is a Quantum gate a Quantum in four what is a Quantum gate a Quantum in four what is a Quantum gate a Quantum
gate is a basic operation or instruction gate is a basic operation or instruction gate is a basic operation or instruction
performed by a quantum computer on a performed by a quantum computer on a performed by a quantum computer on a
small number of cubits usually just one small number of cubits usually just one small number of cubits usually just one
or two for IBM quantum computers Quantum or two for IBM quantum computers Quantum or two for IBM quantum computers Quantum
gates are performed by highly calibrated gates are performed by highly calibrated gates are performed by highly calibrated
microwave pulses that give instructions microwave pulses that give instructions microwave pulses that give instructions
to the cubits on how to to the cubits on how to to the cubits on how to
behave five do more cubits automatically behave five do more cubits automatically behave five do more cubits automatically
make a faster quantum computer no cubic make a faster quantum computer no cubic make a faster quantum computer no cubic
count is just one metric that is count is just one metric that is count is just one metric that is
important to keep in mind when we're important to keep in mind when we're important to keep in mind when we're
looking at the maturation of the looking at the maturation of the looking at the maturation of the
technology just as important are the technology just as important are the technology just as important are the
quality of the cubits which means how quality of the cubits which means how quality of the cubits which means how
well they can store Quantum information well they can store Quantum information well they can store Quantum information
and how fast we can run instructions on and how fast we can run instructions on and how fast we can run instructions on
them citing a processor with a high them citing a processor with a high them citing a processor with a high
Cubit count means little unless these Cubit count means little unless these Cubit count means little unless these
other metrics are also taken into other metrics are also taken into other metrics are also taken into
account six what does a cubit actually account six what does a cubit actually account six what does a cubit actually
look like well it's essentially just an look like well it's essentially just an look like well it's essentially just an
electronic circuit like a regular bit electronic circuit like a regular bit electronic circuit like a regular bit
but it has additional components that but it has additional components that but it has additional components that
give it access to Quantum behaviors such give it access to Quantum behaviors such give it access to Quantum behaviors such
as nonlinear inductors in our third and as nonlinear inductors in our third and as nonlinear inductors in our third and
final video in this series we're going final video in this series we're going final video in this series we're going
to dive into even more advanced to dive into even more advanced to dive into even more advanced
questions in the quantum Computing space questions in the quantum Computing space questions in the quantum Computing space
so stay tuned until then if you have so stay tuned until then if you have so stay tuned until then if you have
further questions drop them in the further questions drop them in the further questions drop them in the
comments below and we'll see you next comments below and we'll see you next comments below and we'll see you next
time time time
[Music]

## Quantum Computing： The Basics

if you've at all been paying attention if you've at all been paying attention
to the news or media recently you've to the news or media recently you've to the news or media recently you've
probably come across the term quantum probably come across the term quantum probably come across the term quantum
Computing as Quantum Computing becomes Computing as Quantum Computing becomes Computing as Quantum Computing becomes
more mainstream more and more people are more mainstream more and more people are more mainstream more and more people are
understandably struggling to explain understandably struggling to explain understandably struggling to explain
what quantum computers really are how what quantum computers really are how what quantum computers really are how
they work and the fundamental concepts they work and the fundamental concepts they work and the fundamental concepts
surrounding them IBM Quantum is the surrounding them IBM Quantum is the surrounding them IBM Quantum is the
world leader in Quantum Computing we world leader in Quantum Computing we world leader in Quantum Computing we
have the most sophisticated Quantum have the most sophisticated Quantum have the most sophisticated Quantum
program in the world and lots of program in the world and lots of program in the world and lots of
dedicated free educational content so dedicated free educational content so dedicated free educational content so
that you don't have to feel left in the that you don't have to feel left in the that you don't have to feel left in the
dark about what's going on in the space dark about what's going on in the space dark about what's going on in the space
today I'm going to answer the most today I'm going to answer the most today I'm going to answer the most
common questions about quantum computers common questions about quantum computers common questions about quantum computers
like why they look the way they do how like why they look the way they do how like why they look the way they do how
they work and how they will be used in they work and how they will be used in they work and how they will be used in
the future let's get [Music] [Music]
started question zero because of course started question zero because of course started question zero because of course
here we're going to start from index here we're going to start from index here we're going to start from index
zero what is quantum Computing well zero what is quantum Computing well zero what is quantum Computing well
Quantum Computing is a new way of Quantum Computing is a new way of Quantum Computing is a new way of
performing computations based on the performing computations based on the performing computations based on the
principles of quantum physics unlike principles of quantum physics unlike principles of quantum physics unlike
traditional computers which use bits the traditional computers which use bits the traditional computers which use bits the
smallest unit of information represented smallest unit of information represented smallest unit of information represented
as either zero or one quantum computers as either zero or one quantum computers as either zero or one quantum computers
use quantum bits or use quantum bits or use quantum bits or
cubits cubits introduce a different cubits cubits introduce a different cubits cubits introduce a different
Behavior to computation by leveraging Behavior to computation by leveraging Behavior to computation by leveraging
superposition meaning they can exist in superposition meaning they can exist in superposition meaning they can exist in
a combination of zero and one a combination of zero and one a combination of zero and one
simultaneously rather than being simultaneously rather than being simultaneously rather than being
restricted to just one state another key restricted to just one state another key restricted to just one state another key
property of cubits is entanglement where property of cubits is entanglement where property of cubits is entanglement where
two or more cubits become intrinsically two or more cubits become intrinsically two or more cubits become intrinsically
linked such that the state of one cubit linked such that the state of one cubit linked such that the state of one cubit
instantly influences the state of instantly influences the state of instantly influences the state of
another this phenomenon defines another this phenomenon defines another this phenomenon defines
classical physics and allows quantum classical physics and allows quantum classical physics and allows quantum
computers to process certain types of computers to process certain types of computers to process certain types of
information in a fundamentally different information in a fundamentally different information in a fundamentally different
way by harnessing superposition and way by harnessing superposition and way by harnessing superposition and
entanglement quantum computers have the entanglement quantum computers have the entanglement quantum computers have the
potential to solve certain types of potential to solve certain types of potential to solve certain types of
complex problems far more efficiently complex problems far more efficiently complex problems far more efficiently
than even the fastest classical than even the fastest classical than even the fastest classical
supercomputers they could one day excel supercomputers they could one day excel supercomputers they could one day excel
in areas like cryptography Material in areas like cryptography Material in areas like cryptography Material
Science optimization and simulation Science optimization and simulation Science optimization and simulation
which include tasks that would take which include tasks that would take which include tasks that would take
traditional computers hundreds or even traditional computers hundreds or even traditional computers hundreds or even
thousands of years to thousands of years to thousands of years to
compute number one why are quantum compute number one why are quantum compute number one why are quantum
computers so cold most quantum computers computers so cold most quantum computers computers so cold most quantum computers
are kept at insanely low temperatures are kept at insanely low temperatures are kept at insanely low temperatures
around 10 m Melvin or - around 10 m Melvin or - around 10 m Melvin or -
49° F because any amount of heat or 49° F because any amount of heat or 49° F because any amount of heat or
thermal energy can disrupt their fragile thermal energy can disrupt their fragile thermal energy can disrupt their fragile
Quantum States in addition IBM quantum Quantum States in addition IBM quantum Quantum States in addition IBM quantum
computers also need to be cold because computers also need to be cold because computers also need to be cold because
they run on superconductors which is a they run on superconductors which is a they run on superconductors which is a
type of metal where current can flow type of metal where current can flow type of metal where current can flow
with no resistance below a certain with no resistance below a certain with no resistance below a certain
threshold temperature number two why are threshold temperature number two why are threshold temperature number two why are
quantum computers so hard to build all quantum computers so hard to build all quantum computers so hard to build all
new breakthrough technology is hard to new breakthrough technology is hard to new breakthrough technology is hard to
build at first and even harder to scale build at first and even harder to scale build at first and even harder to scale
but quantum computers are especially but quantum computers are especially but quantum computers are especially
difficult because they combine so many difficult because they combine so many difficult because they combine so many
complicated aspects of Science and complicated aspects of Science and complicated aspects of Science and
Technology there's a lot of new physics Technology there's a lot of new physics Technology there's a lot of new physics
we are learning and need to be able to we are learning and need to be able to we are learning and need to be able to
understand to build them while also understand to build them while also understand to build them while also
incorporating state-of-the-art incorporating state-of-the-art incorporating state-of-the-art
engineering and programming techniques engineering and programming techniques engineering and programming techniques
in addition we're also fighting against in addition we're also fighting against in addition we're also fighting against
Quantum Quantum Quantum
decoherence the natural tendency for decoherence the natural tendency for decoherence the natural tendency for
quantum computers to lose track of their quantum computers to lose track of their quantum computers to lose track of their
Quantum information over a period of Quantum information over a period of Quantum information over a period of
time time sometimes in the middle of a time time sometimes in the middle of a time time sometimes in the middle of a
calculation keeping Quantum processors calculation keeping Quantum processors calculation keeping Quantum processors
isolated but still being able to control isolated but still being able to control isolated but still being able to control
them is one of our biggest engineering them is one of our biggest engineering them is one of our biggest engineering
hurdles number three why are quantum hurdles number three why are quantum hurdles number three why are quantum
computers so big so quantum computers computers so big so quantum computers computers so big so quantum computers
themselves aren't actually all that big themselves aren't actually all that big themselves aren't actually all that big
Quantum processing units or qpu are only Quantum processing units or qpu are only Quantum processing units or qpu are only
about the size of a typical computer about the size of a typical computer about the size of a typical computer
chip however all the other Machinery chip however all the other Machinery chip however all the other Machinery
that goes around it that is needed to that goes around it that is needed to that goes around it that is needed to
keep it cold and stable and allow for keep it cold and stable and allow for keep it cold and stable and allow for
the input and output of signals is what the input and output of signals is what the input and output of signals is what
makes the whole thing look quite large makes the whole thing look quite large makes the whole thing look quite large
however work to create smaller denser however work to create smaller denser however work to create smaller denser
cables and components is already well cables and components is already well cables and components is already well
underway four why are quantum computers underway four why are quantum computers underway four why are quantum computers
being built this is a great question why being built this is a great question why being built this is a great question why
are we even doing all of this in the are we even doing all of this in the are we even doing all of this in the
first place the truth is that while our first place the truth is that while our first place the truth is that while our
regular old computers are truly regular old computers are truly regular old computers are truly
extraordinary machines they can't extraordinary machines they can't extraordinary machines they can't
compute every problem we'd like to solve compute every problem we'd like to solve compute every problem we'd like to solve
specific specifically even the world's specific specifically even the world's specific specifically even the world's
best supercomputers really struggle to best supercomputers really struggle to best supercomputers really struggle to
do things like simulating nature on an do things like simulating nature on an do things like simulating nature on an
atomic scale highly crucial for atomic scale highly crucial for atomic scale highly crucial for
materials and Drug developments there's materials and Drug developments there's materials and Drug developments there's
also room for improvement in the way we also room for improvement in the way we also room for improvement in the way we
perform optimization calculations which perform optimization calculations which perform optimization calculations which
are needed for financial models or are needed for financial models or are needed for financial models or
Supply Chain management in every Supply Chain management in every Supply Chain management in every
industry it's not difficult to think of industry it's not difficult to think of industry it's not difficult to think of
a computational problem that bring a computational problem that bring a computational problem that bring
regular computers basically to their regular computers basically to their regular computers basically to their
limit in essence there exists problems limit in essence there exists problems limit in essence there exists problems
so complex that we don't know how to so complex that we don't know how to so complex that we don't know how to
solve them efficiently with a classical solve them efficiently with a classical solve them efficiently with a classical
computer but with a quantum computer we computer but with a quantum computer we computer but with a quantum computer we
could five how do quantum computers work could five how do quantum computers work could five how do quantum computers work
quantum computers harness the behavior quantum computers harness the behavior quantum computers harness the behavior
of atoms to compute by different rules of atoms to compute by different rules of atoms to compute by different rules
than regular old classical computers do than regular old classical computers do than regular old classical computers do
they don't perform operations faster they don't perform operations faster they don't perform operations faster
than a regular computer but they can than a regular computer but they can than a regular computer but they can
perform different operations that your perform different operations that your perform different operations that your
classical computer can't and sometimes s classical computer can't and sometimes s classical computer can't and sometimes s
those operations offer a faster root to those operations offer a faster root to those operations offer a faster root to
a a a
solution six how do quantum computers solution six how do quantum computers solution six how do quantum computers
break encryption well right now they do break encryption well right now they do break encryption well right now they do
not break encryption however that is a not break encryption however that is a not break encryption however that is a
real concern many have because a real concern many have because a real concern many have because a
theoretically powerful enough quantum theoretically powerful enough quantum theoretically powerful enough quantum
computer could be capable of running computer could be capable of running computer could be capable of running
Shores algorithm which is an efficient Shores algorithm which is an efficient Shores algorithm which is an efficient
algorithm for finding prime factors of algorithm for finding prime factors of algorithm for finding prime factors of
large numbers the difficulty of this large numbers the difficulty of this large numbers the difficulty of this
task is actually the basis of RSA task is actually the basis of RSA task is actually the basis of RSA
encryption one of the standard encryption one of the standard encryption one of the standard
encryption methods that is implemented encryption methods that is implemented encryption methods that is implemented
all over the world but no one has a all over the world but no one has a all over the world but no one has a
machine capable of that yet to prepare machine capable of that yet to prepare machine capable of that yet to prepare
for this however we are already for this however we are already for this however we are already
developing other encryption schemes that developing other encryption schemes that developing other encryption schemes that
would be safe from Quantum hacking seven would be safe from Quantum hacking seven would be safe from Quantum hacking seven
how do quantum computers process how do quantum computers process how do quantum computers process
information quantum computers process information quantum computers process information quantum computers process
information by running Quantum Gates on information by running Quantum Gates on information by running Quantum Gates on
Quantum bits or cubits not dissimilar to Quantum bits or cubits not dissimilar to Quantum bits or cubits not dissimilar to
electronic Gates ran on regular bits for electronic Gates ran on regular bits for electronic Gates ran on regular bits for
example the and or or gate there are example the and or or gate there are example the and or or gate there are
quantum counterparts that allow the quantum counterparts that allow the quantum counterparts that allow the
Cubit to take advantage of the uniquely Cubit to take advantage of the uniquely Cubit to take advantage of the uniquely
Quantum properties that they have Quantum properties that they have Quantum properties that they have
putting all these Gates together creates putting all these Gates together creates putting all these Gates together creates
a Quantum circuit which is a series of a Quantum circuit which is a series of a Quantum circuit which is a series of
instructions for the quantum computer to instructions for the quantum computer to instructions for the quantum computer to
follow eight when will quantum computers follow eight when will quantum computers follow eight when will quantum computers
be available actually they are already be available actually they are already be available actually they are already
available to the public right now making available to the public right now making available to the public right now making
an account on IBM Quantum will give you an account on IBM Quantum will give you an account on IBM Quantum will give you
free access to IBM quantum computers free access to IBM quantum computers free access to IBM quantum computers
that you can use number nine when will that you can use number nine when will that you can use number nine when will
quantum computers break encryption we quantum computers break encryption we quantum computers break encryption we
don't know but all indications tell us don't know but all indications tell us don't know but all indications tell us
that it's probably quite a ways away that it's probably quite a ways away that it's probably quite a ways away
this might sound scary but there are this might sound scary but there are this might sound scary but there are
other positive benefits to building a other positive benefits to building a other positive benefits to building a
quantum computer capable of running quantum computer capable of running quantum computer capable of running
shortes algorithm that same algorithm is shortes algorithm that same algorithm is shortes algorithm that same algorithm is
valuable for other things like phase valuable for other things like phase valuable for other things like phase
estimation and simultaneously the estimation and simultaneously the estimation and simultaneously the
country and world are moving towards country and world are moving towards country and world are moving towards
other encryption schemes that are other encryption schemes that are other encryption schemes that are
quantum safe quantum safe quantum safe
number 10 when will Quantum Computing number 10 when will Quantum Computing number 10 when will Quantum Computing
become common the truth is we can't become common the truth is we can't become common the truth is we can't
predict this for certain hopefully and predict this for certain hopefully and predict this for certain hopefully and
we think pretty soon if you subscribe we think pretty soon if you subscribe we think pretty soon if you subscribe
you'll be one of the first to know and you'll be one of the first to know and you'll be one of the first to know and
if you have any further questions please if you have any further questions please if you have any further questions please
drop them in the comments we'll see you drop them in the comments we'll see you drop them in the comments we'll see you
next time next time next time
[Music]

## Tour a Quantum Lab with Dr. Olivia Lanes

[Music] [Music]
hi my name is Olivia lanes and I'm one hi my name is Olivia lanes and I'm one hi my name is Olivia lanes and I'm one
of the scientists and education leads of the scientists and education leads of the scientists and education leads
working here at IBM Quantum and today I working here at IBM Quantum and today I working here at IBM Quantum and today I
want to welcome you to one of our labs want to welcome you to one of our labs want to welcome you to one of our labs
and Quantum data centers IBM Quantum has and Quantum data centers IBM Quantum has and Quantum data centers IBM Quantum has
the largest number of quantum processors the largest number of quantum processors the largest number of quantum processors
available through the cloud and this is available through the cloud and this is available through the cloud and this is
where we house some of them right here where we house some of them right here where we house some of them right here
so I'm going to walk you through the lab so I'm going to walk you through the lab so I'm going to walk you through the lab
today and I'm going to show you some of today and I'm going to show you some of today and I'm going to show you some of
the room temperature and cryogenic the room temperature and cryogenic the room temperature and cryogenic
equipment and all the Machinery that equipment and all the Machinery that equipment and all the Machinery that
goes into keeping these devices and goes into keeping these devices and goes into keeping these devices and
these machines these machines these machines
[Music] [Music] [Music]
cold so the most important piece of cold so the most important piece of cold so the most important piece of
equipment in this room is the dilution equipment in this room is the dilution equipment in this room is the dilution
refrigerator which is this big cylinder refrigerator which is this big cylinder refrigerator which is this big cylinder
here and this basically works as a here and this basically works as a here and this basically works as a
Russian nesting doll if you've ever seen Russian nesting doll if you've ever seen Russian nesting doll if you've ever seen
those toys it's basically a can and a those toys it's basically a can and a those toys it's basically a can and a
can and a can and a can and that works can and a can and a can and that works can and a can and a can and that works
the exact same way the outer can here is the exact same way the outer can here is the exact same way the outer can here is
obviously at room temperature and then obviously at room temperature and then obviously at room temperature and then
there are four other cans on the inside there are four other cans on the inside there are four other cans on the inside
which get progressively colder and work which get progressively colder and work which get progressively colder and work
to keep the thermal isolation at that to keep the thermal isolation at that to keep the thermal isolation at that
stage of the fridge the inner can which stage of the fridge the inner can which stage of the fridge the inner can which
hosts the IBM Quantum processor and hosts the IBM Quantum processor and hosts the IBM Quantum processor and
lives here at the very bottom is an lives here at the very bottom is an lives here at the very bottom is an
insanely cold temperature of about 15 insanely cold temperature of about 15 insanely cold temperature of about 15
Melvin and every other can basically is Melvin and every other can basically is Melvin and every other can basically is
progressively warmer than that and there progressively warmer than that and there progressively warmer than that and there
are also stages of the fridge as well are also stages of the fridge as well are also stages of the fridge as well
where the top part is at room where the top part is at room where the top part is at room
temperature and and then again the temperature and and then again the temperature and and then again the
bottom part which hosts the processor bottom part which hosts the processor bottom part which hosts the processor
and the cubits themselves lives here at and the cubits themselves lives here at and the cubits themselves lives here at
the very the very the very
bottom so how do we get signal from the bottom so how do we get signal from the bottom so how do we get signal from the
very bottom out to the rest of the world very bottom out to the rest of the world very bottom out to the rest of the world
well we basically have cables that run well we basically have cables that run well we basically have cables that run
to the bottom of the fridge here and to the bottom of the fridge here and to the bottom of the fridge here and
then all the way back up to the top of then all the way back up to the top of then all the way back up to the top of
the fridge into these room temperature the fridge into these room temperature the fridge into these room temperature
amplifiers and then those room amplifiers and then those room amplifiers and then those room
temperature amplifiers make the signal temperature amplifiers make the signal temperature amplifiers make the signal
bigger and it travels through these blue bigger and it travels through these blue bigger and it travels through these blue
cables in into our room temperature cables in into our room temperature cables in into our room temperature
control Electronics over here which control Electronics over here which control Electronics over here which
digitize the signal and turn that digitize the signal and turn that digitize the signal and turn that
waveform into something that is able to waveform into something that is able to waveform into something that is able to
be processed and understood by our be processed and understood by our be processed and understood by our
computers and our computers and our computers and our
laptops um and one thing you'll probably laptops um and one thing you'll probably laptops um and one thing you'll probably
notice is the sound it's very very loud notice is the sound it's very very loud notice is the sound it's very very loud
in here because we have multiple in here because we have multiple in here because we have multiple
dilution refrigerators all turned on and dilution refrigerators all turned on and dilution refrigerators all turned on and
working at the same time uh that's a working at the same time uh that's a working at the same time uh that's a
compressor that noise um that's a compressor that noise um that's a compressor that noise um that's a
compressor pump which basically is compressor pump which basically is compressor pump which basically is
pumping on these fridges at all times pumping on these fridges at all times pumping on these fridges at all times
and keeps them at that insanely cold and keeps them at that insanely cold and keeps them at that insanely cold
temperature um so back here you can also temperature um so back here you can also temperature um so back here you can also
see a whole lot of more pumps and see a whole lot of more pumps and see a whole lot of more pumps and
compressors that are working uh to keep compressors that are working uh to keep compressors that are working uh to keep
the fridge and the flow uh of air and the fridge and the flow uh of air and the fridge and the flow uh of air and
the cryogens static and working at all the cryogens static and working at all the cryogens static and working at all
times and if you come over here a little times and if you come over here a little times and if you come over here a little
bit more you can see we have a drawer of bit more you can see we have a drawer of bit more you can see we have a drawer of
liquid nitrogen which is the first liquid nitrogen which is the first liquid nitrogen which is the first
cryogen that we use in the cryostat in cryogen that we use in the cryostat in cryogen that we use in the cryostat in
order to cool it down to 15 mvin the order to cool it down to 15 mvin the order to cool it down to 15 mvin the
other important one is liquid helium but other important one is liquid helium but other important one is liquid helium but
you can't really see that we keep it you can't really see that we keep it you can't really see that we keep it
inside the dilution unit but we might inside the dilution unit but we might inside the dilution unit but we might
need to make sure that this dwar of need to make sure that this dwar of need to make sure that this dwar of
liquid nitrogen is full at all times we liquid nitrogen is full at all times we liquid nitrogen is full at all times we
have a little scale here to make sure have a little scale here to make sure have a little scale here to make sure
that we can weigh it and see how much that we can weigh it and see how much that we can weigh it and see how much
nitrogen is left in the door and it nitrogen is left in the door and it nitrogen is left in the door and it
never goes empty because if we don't never goes empty because if we don't never goes empty because if we don't
have enough nitrogen left in the door have enough nitrogen left in the door have enough nitrogen left in the door
then things start to warm up on us and then things start to warm up on us and then things start to warm up on us and
this is the control panel which this is the control panel which this is the control panel which
basically controls all of the valve basically controls all of the valve basically controls all of the valve
and the Turbo pumps that are working and the Turbo pumps that are working and the Turbo pumps that are working
inside the fridge and outside the fridge inside the fridge and outside the fridge inside the fridge and outside the fridge
you can control them and turn them on or you can control them and turn them on or you can control them and turn them on or
open them by basically just pressing open them by basically just pressing open them by basically just pressing
these buttons the blue color means these these buttons the blue color means these these buttons the blue color means these
valves are open and the gray color means valves are open and the gray color means valves are open and the gray color means
these ones are still these ones are still these ones are still
closed so you can control that like I closed so you can control that like I closed so you can control that like I
said it's just a button with the Press said it's just a button with the Press said it's just a button with the Press
of a button and then the laptop here is of a button and then the laptop here is of a button and then the laptop here is
also an interface for all of the also an interface for all of the also an interface for all of the
cryogenic equipment in the dilution cryogenic equipment in the dilution cryogenic equipment in the dilution
refrigerator as well you can log on on refrigerator as well you can log on on refrigerator as well you can log on on
and basically press one button that says and basically press one button that says and basically press one button that says
full cool down or one button that says full cool down or one button that says full cool down or one button that says
full warmup and then you can basically full warmup and then you can basically full warmup and then you can basically
cool down your device and warm it back cool down your device and warm it back cool down your device and warm it back
up in uh two presses two clicks on a up in uh two presses two clicks on a up in uh two presses two clicks on a
computer so it's really pretty simple um computer so it's really pretty simple um computer so it's really pretty simple um
like I said this is more room like I said this is more room like I said this is more room
temperature control boxes that digitize temperature control boxes that digitize temperature control boxes that digitize
the signal these cables carry the signal the signal these cables carry the signal the signal these cables carry the signal
into more room temperature amplifiers into more room temperature amplifiers into more room temperature amplifiers
that you can see here just makes the that you can see here just makes the that you can see here just makes the
signal bigger so the computers can see signal bigger so the computers can see signal bigger so the computers can see
the quantum signal above the noise floor the quantum signal above the noise floor the quantum signal above the noise floor
this is a hemp box which controls all of this is a hemp box which controls all of this is a hemp box which controls all of
the cryogenic amplifiers inside the the cryogenic amplifiers inside the the cryogenic amplifiers inside the
fridge that live at around 4 Kelvin um fridge that live at around 4 Kelvin um fridge that live at around 4 Kelvin um
these are network these are network these are network
analyzers um and these are current analyzers um and these are current analyzers um and these are current
signals which send current or voltage signals which send current or voltage signals which send current or voltage
into the fridge to control some of the into the fridge to control some of the into the fridge to control some of the
amplifiers or other cryogenic components amplifiers or other cryogenic components amplifiers or other cryogenic components
inside the delution refrigerator one inside the delution refrigerator one inside the delution refrigerator one
other thing I want to show you in this other thing I want to show you in this other thing I want to show you in this
lab is is this device over here which is lab is is this device over here which is lab is is this device over here which is
a BNA or a vector Network analyzer and a BNA or a vector Network analyzer and a BNA or a vector Network analyzer and
this is a device that basically can tell this is a device that basically can tell this is a device that basically can tell
us the frequency and the phase of the us the frequency and the phase of the us the frequency and the phase of the
signal that we're sending into our signal that we're sending into our signal that we're sending into our
cubits and we like to use this as a cubits and we like to use this as a cubits and we like to use this as a
first check in the lab when we're trying first check in the lab when we're trying first check in the lab when we're trying
to see whether a new chip that we've to see whether a new chip that we've to see whether a new chip that we've
made has all the cubits at the right made has all the cubits at the right made has all the cubits at the right
frequency or not um so come with us frequency or not um so come with us frequency or not um so come with us
we're going to go upstairs now because we're going to go upstairs now because we're going to go upstairs now because
like I said all of these solution like I said all of these solution like I said all of these solution
refrigerators are turned on and they're refrigerators are turned on and they're refrigerators are turned on and they're
all working which means you can't really all working which means you can't really all working which means you can't really
see anything interesting inside of them see anything interesting inside of them see anything interesting inside of them
but we have our model upstairs which is but we have our model upstairs which is but we have our model upstairs which is
turned off and I'm going to explain what turned off and I'm going to explain what turned off and I'm going to explain what
you can actually see inside the delution you can actually see inside the delution you can actually see inside the delution
refrigerator in just a minute so here we have our model minute so here we have our model
delusion refrigerator which is obviously delusion refrigerator which is obviously delusion refrigerator which is obviously
not turned on at the moment uh because not turned on at the moment uh because not turned on at the moment uh because
when the fridge is actually running you when the fridge is actually running you when the fridge is actually running you
can't see any of the internal parts or can't see any of the internal parts or can't see any of the internal parts or
any of the cool things but now that it any of the cool things but now that it any of the cool things but now that it
is turned off is turned off is turned off
um you can really see all of the um you can really see all of the um you can really see all of the
different cryogenic components inside different cryogenic components inside different cryogenic components inside
the fridge um we can start with the chip the fridge um we can start with the chip the fridge um we can start with the chip
down here so this chip this quantum down here so this chip this quantum down here so this chip this quantum
computer has 16 cubits and all of these computer has 16 cubits and all of these computer has 16 cubits and all of these
cables here on the bottom and the top cables here on the bottom and the top cables here on the bottom and the top
basically plug into the sides of the basically plug into the sides of the basically plug into the sides of the
chip which is wire bonded to the sides chip which is wire bonded to the sides chip which is wire bonded to the sides
of the packaging and then these cables of the packaging and then these cables of the packaging and then these cables
carry the microwave signal to the carry the microwave signal to the carry the microwave signal to the
isolators and amplifiers that are also isolators and amplifiers that are also isolators and amplifiers that are also
kept at this same plate here and then kept at this same plate here and then kept at this same plate here and then
those carry the microwave signal back up those carry the microwave signal back up those carry the microwave signal back up
to room temperature so every individual to room temperature so every individual to room temperature so every individual
stage of the delution refrigerator is at stage of the delution refrigerator is at stage of the delution refrigerator is at
a different temperature just like the a different temperature just like the a different temperature just like the
cans that I was showing you in the other cans that I was showing you in the other cans that I was showing you in the other
lab that connects right here that's the lab that connects right here that's the lab that connects right here that's the
room temperature uh part of the fridge room temperature uh part of the fridge room temperature uh part of the fridge
and that's the most outer shield and and that's the most outer shield and and that's the most outer shield and
then inside on every single plate that then inside on every single plate that then inside on every single plate that
has its own Shield which would attach has its own Shield which would attach has its own Shield which would attach
from the bottom and gets screwed on here from the bottom and gets screwed on here from the bottom and gets screwed on here
and every single Shield is held at a and every single Shield is held at a and every single Shield is held at a
progressively colder temperature progressively colder temperature progressively colder temperature
starting out here which is the one that starting out here which is the one that starting out here which is the one that
we saw in the lab and you could touch we saw in the lab and you could touch we saw in the lab and you could touch
obviously that was at room temperature obviously that was at room temperature obviously that was at room temperature
down all the way to the mixing chamber down all the way to the mixing chamber down all the way to the mixing chamber
plate which is about 15 millin and that plate which is about 15 millin and that plate which is about 15 millin and that
would be that gold shiny one that we saw would be that gold shiny one that we saw would be that gold shiny one that we saw
in the lab as well that is the lowest in the lab as well that is the lowest in the lab as well that is the lowest
part of the fridge and it's sort of like part of the fridge and it's sort of like part of the fridge and it's sort of like
I saw I said in the lab like a rushing I saw I said in the lab like a rushing I saw I said in the lab like a rushing
nesting doll where every single can is nesting doll where every single can is nesting doll where every single can is
just within another can and that's to just within another can and that's to just within another can and that's to
keep it basically thermally isolated keep it basically thermally isolated keep it basically thermally isolated
from the environment so once these from the environment so once these from the environment so once these
cables go back up to this stage they get cables go back up to this stage they get cables go back up to this stage they get
carried all the way back to the top here carried all the way back to the top here carried all the way back to the top here
and one of the most common questions we and one of the most common questions we and one of the most common questions we
get are why do all of the cables have get are why do all of the cables have get are why do all of the cables have
this little loop on them and it's this little loop on them and it's this little loop on them and it's
basically because when you cool down the basically because when you cool down the basically because when you cool down the
the fridge and then you warm it up the fridge and then you warm it up the fridge and then you warm it up
multiple times over a few cycles of multiple times over a few cycles of multiple times over a few cycles of
doing this these cables because of the doing this these cables because of the doing this these cables because of the
expanding and shrinking of the metal due expanding and shrinking of the metal due expanding and shrinking of the metal due
to the thermal temperature of the to the thermal temperature of the to the thermal temperature of the
environment um can get very brittle and environment um can get very brittle and environment um can get very brittle and
the cables can break if they don't have the cables can break if they don't have the cables can break if they don't have
a little bit of give to them this is the a little bit of give to them this is the a little bit of give to them this is the
dilution unit the mixing chamber itself dilution unit the mixing chamber itself dilution unit the mixing chamber itself
which is responsible for the cooling of which is responsible for the cooling of which is responsible for the cooling of
the mixing chamber plate which is the the mixing chamber plate which is the the mixing chamber plate which is the
lowest plate of the fridge like I said lowest plate of the fridge like I said lowest plate of the fridge like I said
15 Melvin which is colder than outer 15 Melvin which is colder than outer 15 Melvin which is colder than outer
space and then this plate basically has space and then this plate basically has space and then this plate basically has
the first two um cryo pumps that are the first two um cryo pumps that are the first two um cryo pumps that are
responsible for bringing it down to a responsible for bringing it down to a responsible for bringing it down to a
reasonable temperature before the reasonable temperature before the reasonable temperature before the
dilution unit pumps on it and introduces dilution unit pumps on it and introduces dilution unit pumps on it and introduces
more cryogens to bring it down to an more cryogens to bring it down to an more cryogens to bring it down to an
even colder even colder even colder
temperature um other things to note up temperature um other things to note up temperature um other things to note up
here um you'll see a lot of Copp here um you'll see a lot of Copp here um you'll see a lot of Copp
wiring that's because copper has really wiring that's because copper has really wiring that's because copper has really
good thermal properties and is really good thermal properties and is really good thermal properties and is really
good at basically helping to keep all of good at basically helping to keep all of good at basically helping to keep all of
the parts that should be cold cold um the parts that should be cold cold um the parts that should be cold cold um
these parts right here are just to these parts right here are just to these parts right here are just to
connect the plates to one another um connect the plates to one another um connect the plates to one another um
these uh are for magnets if you had any these uh are for magnets if you had any these uh are for magnets if you had any
magnet that you wanted to introduce into magnet that you wanted to introduce into magnet that you wanted to introduce into
your experiment you would plug those in your experiment you would plug those in your experiment you would plug those in
here here here
um and I think that that's basically it um and I think that that's basically it um and I think that that's basically it
yeah so the important things to note are yeah so the important things to note are yeah so the important things to note are
the actual cooling parts of the fridge the actual cooling parts of the fridge the actual cooling parts of the fridge
that's what these parts are for and then that's what these parts are for and then that's what these parts are for and then
the parts that actually carry the signal the parts that actually carry the signal the parts that actually carry the signal
to the processor are the ones that go in to the processor are the ones that go in to the processor are the ones that go in
the middle straight down through here the middle straight down through here the middle straight down through here
and then the signal leaves coming out and then the signal leaves coming out and then the signal leaves coming out
the other side and then those cables the other side and then those cables the other side and then those cables
basically are distributed to all of basically are distributed to all of basically are distributed to all of
these amplifiers and isolators these these amplifiers and isolators these these amplifiers and isolators these
basically direct just the flow of basically direct just the flow of basically direct just the flow of
traffic the flow of signal to make sure traffic the flow of signal to make sure traffic the flow of signal to make sure
that it goes back up the direction that that it goes back up the direction that that it goes back up the direction that
we want it to and not backwards into the we want it to and not backwards into the we want it to and not backwards into the
quantum chip where it can interfere with quantum chip where it can interfere with quantum chip where it can interfere with
the cubits and cause noise and like I the cubits and cause noise and like I the cubits and cause noise and like I
said all of these cables on the outside said all of these cables on the outside said all of these cables on the outside
would carry the signal back up to room would carry the signal back up to room would carry the signal back up to room
temperature and there would typically be temperature and there would typically be temperature and there would typically be
a bunch of cables although this is just a bunch of cables although this is just a bunch of cables although this is just
a demonstration fridge so it doesn't a demonstration fridge so it doesn't a demonstration fridge so it doesn't
have those cables on the top of the have those cables on the top of the have those cables on the top of the
fridge that would then take that signal fridge that would then take that signal fridge that would then take that signal
and put it back into those room and put it back into those room and put it back into those room
temperature boxes that you saw in the temperature boxes that you saw in the temperature boxes that you saw in the
big rack next to the fridges that we big rack next to the fridges that we big rack next to the fridges that we
looked at in the lab thanks for coming looked at in the lab thanks for coming looked at in the lab thanks for coming
on the tour with us and sticking around on the tour with us and sticking around on the tour with us and sticking around
if you have any other questions about if you have any other questions about if you have any other questions about
what you saw here on the tour please what you saw here on the tour please what you saw here on the tour please
feel free to leave it in the comments feel free to leave it in the comments feel free to leave it in the comments
and we'll see you next and we'll see you next and we'll see you next
[Music] [Music] [Music]
time

## What is a Bit-Flip？ Quantum Jargon Explained

a bit flip in classical Computing refers a bit flip in classical Computing refers
to the process by which the value of a to the process by which the value of a to the process by which the value of a
bit the smallest increment of bit the smallest increment of bit the smallest increment of
information is flipped from 0 to 1 or information is flipped from 0 to 1 or information is flipped from 0 to 1 or
from one to zero in Quantum Computing a from one to zero in Quantum Computing a from one to zero in Quantum Computing a
bit flip does the same but for a Quantum bit flip does the same but for a Quantum bit flip does the same but for a Quantum
bit a qubit bit a qubit bit a qubit
a bit flip can be triggered by a bit a bit flip can be triggered by a bit a bit flip can be triggered by a bit
flip operator otherwise known as the not flip operator otherwise known as the not flip operator otherwise known as the not
operator or X gate operator or X gate operator or X gate
this is one of the most important this is one of the most important this is one of the most important
operations in Computing as the basis for operations in Computing as the basis for operations in Computing as the basis for
all information you input an output from all information you input an output from all information you input an output from
your computer or words and numbers and your computer or words and numbers and your computer or words and numbers and
strings are reduced to zeros and ones strings are reduced to zeros and ones strings are reduced to zeros and ones
in Kiss kit you can apply a bit flip to in Kiss kit you can apply a bit flip to in Kiss kit you can apply a bit flip to
a Quantum Circuit by appending an X gate a Quantum Circuit by appending an X gate a Quantum Circuit by appending an X gate
to your Quantum circuit object see the to your Quantum circuit object see the to your Quantum circuit object see the
links in the description for more links in the description for more links in the description for more
details on kiss kit Quantum circuits and details on kiss kit Quantum circuits and details on kiss kit Quantum circuits and
gate operations

## What is a Hamiltonian？ Quantum Jargon Explained

- [Narrator] In quantum mechanics, a Hamiltonian is a
mathematical description of the total energy of a system incorporating both
kinetic and potential energy. The system being described
could be anything, a molecule, a single cubit, or a bouncy ball. A Hamiltonian can take different forms. It might incorporate different
types of energy contributions or be written in different
coordinates or dimensions, which often makes them look very different In quantum mechanics,
knowing the Hamiltonian of your system is very important because it will allow you to solve the Schrodinger equation, the heart of quantum mechanics. The Schrodinger equation calculates how the system will
evolve or change in time, which basically tells us everything useful we would want to know. That is why the first step of almost any problem
or experiment boils down to calculating or
identifying the Hamiltonian.

## What is a quantum gate？ #quantum #science #quantumcomputing

[Music] [Music]
a Quantum gate is a basic operation or a Quantum gate is a basic operation or a Quantum gate is a basic operation or
instruction performed by a quantum instruction performed by a quantum instruction performed by a quantum
computer on a small number of cubits computer on a small number of cubits computer on a small number of cubits
usually just one or two for IBM quantum usually just one or two for IBM quantum usually just one or two for IBM quantum
computers Quantum gates are performed by computers Quantum gates are performed by computers Quantum gates are performed by
highly calibrated microwave pulses that highly calibrated microwave pulses that highly calibrated microwave pulses that
give instructions to the cubits on how give instructions to the cubits on how give instructions to the cubits on how
to behave

## What is an Ansatz？ Quantum Jargon Explained

and anzas is an educated guess about the and anzas is an educated guess about the
value or form of an unknown function and value or form of an unknown function and value or form of an unknown function and
is used to help derive the real solution is used to help derive the real solution is used to help derive the real solution
of an equation in Quantum Computing and of an equation in Quantum Computing and of an equation in Quantum Computing and
anzas is usually a parameterized circuit anzas is usually a parameterized circuit anzas is usually a parameterized circuit
often used in variational algorithms often used in variational algorithms often used in variational algorithms
such as vqe this ansats is used as a such as vqe this ansats is used as a such as vqe this ansats is used as a
starting point or trial state which is starting point or trial state which is starting point or trial state which is
then iteratively updated as more then iteratively updated as more then iteratively updated as more
information is calculated information is calculated information is calculated
picking a good answer is important when picking a good answer is important when picking a good answer is important when
running Quantum experiments as a running Quantum experiments as a running Quantum experiments as a
well-chosen ansats can drastically well-chosen ansats can drastically well-chosen ansats can drastically
improve the accuracy of your results

## What is Quantum Advantage？ Cutting Through the Hype.

Quantum Computing Quantum Computing Quantum Computing Quantum Computing
Quantum Computing quantum computer Quantum Computing quantum computer Quantum Computing quantum computer
quantum computer chip this is a quantum quantum computer chip this is a quantum quantum computer chip this is a quantum
computer the UN has named 2025 as the computer the UN has named 2025 as the computer the UN has named 2025 as the
international year of quantum Science international year of quantum Science international year of quantum Science
and Technology on the occasion of 100 and Technology on the occasion of 100 and Technology on the occasion of 100
Years of quantum mechanics and it's an Years of quantum mechanics and it's an Years of quantum mechanics and it's an
exciting time in Quantum Computing as we exciting time in Quantum Computing as we exciting time in Quantum Computing as we
reach towards the Longs sought goal of reach towards the Longs sought goal of reach towards the Longs sought goal of
quantum Advantage this is a critically quantum Advantage this is a critically quantum Advantage this is a critically
important Concept in Quantum Computing important Concept in Quantum Computing important Concept in Quantum Computing
it's basically the reason we're building it's basically the reason we're building it's basically the reason we're building
quantum computers in the first place but quantum computers in the first place but quantum computers in the first place but
what is is quantum Advantage exactly in what is is quantum Advantage exactly in what is is quantum Advantage exactly in
this video we'll help you understand this video we'll help you understand this video we'll help you understand
exactly what Quantum Advantage is and exactly what Quantum Advantage is and exactly what Quantum Advantage is and
what so many people get wrong about it classical computers like your phone it classical computers like your phone
or your laptop are extraordinary or your laptop are extraordinary or your laptop are extraordinary
machines and they're capable of doing machines and they're capable of doing machines and they're capable of doing
truly amazing things but there are some truly amazing things but there are some truly amazing things but there are some
computational tasks that classical computational tasks that classical computational tasks that classical
computers struggle with one example of a computers struggle with one example of a computers struggle with one example of a
problem that we think is hard for problem that we think is hard for problem that we think is hard for
classical computers is factoring where classical computers is factoring where classical computers is factoring where
we're given a positive integer and the we're given a positive integer and the we're given a positive integer and the
goal is to express that number as a goal is to express that number as a goal is to express that number as a
product of prime numbers for example the product of prime numbers for example the product of prime numbers for example the
factorization of the number 91 is 7 * 13 factorization of the number 91 is 7 * 13 factorization of the number 91 is 7 * 13
that one is easy because 91 isn't very that one is easy because 91 isn't very that one is easy because 91 isn't very
big you could even solve it on paper now big you could even solve it on paper now big you could even solve it on paper now
how about a larger number what about how about a larger number what about how about a larger number what about
2,595 2,595 2,595
3177 well you'd probably need a computer 3177 well you'd probably need a computer 3177 well you'd probably need a computer
to help you solve that one like I did to help you solve that one like I did to help you solve that one like I did
although it didn't take too long to although it didn't take too long to although it didn't take too long to
compute turns out the prime factors are compute turns out the prime factors are compute turns out the prime factors are
1609 and 1609 and 1609 and
1613 now here's another one the number 1613 now here's another one the number 1613 now here's another one the number
shown on the screen is called RSA shown on the screen is called RSA shown on the screen is called RSA
1024 and it has 309 decimal digits or 1024 and it has 309 decimal digits or 1024 and it has 309 decimal digits or
1,24 bits if it's written in binary 1,24 bits if it's written in binary 1,24 bits if it's written in binary
notation Nobody Knows the prime notation Nobody Knows the prime notation Nobody Knows the prime
factorization of this one in fact in factorization of this one in fact in factorization of this one in fact in
1991 RSA Laboratories offered a cash 1991 RSA Laboratories offered a cash 1991 RSA Laboratories offered a cash
prize of prize of prize of
$100,000 to anyone who could figure it $100,000 to anyone who could figure it $100,000 to anyone who could figure it
out and to this day nobody has because out and to this day nobody has because out and to this day nobody has because
every classical algorithm for factoring every classical algorithm for factoring every classical algorithm for factoring
that anyone has ever come up with takes that anyone has ever come up with takes that anyone has ever come up with takes
too long people just don't live long too long people just don't live long too long people just don't live long
enough to be able to wait for these enough to be able to wait for these enough to be able to wait for these
algorithms to finish factoring numbers algorithms to finish factoring numbers algorithms to finish factoring numbers
of this size and that's not for a lack of this size and that's not for a lack of this size and that's not for a lack
of trying a lot of very smart people of trying a lot of very smart people of trying a lot of very smart people
have tried to come up with efficient have tried to come up with efficient have tried to come up with efficient
algorithms for this problem simply put algorithms for this problem simply put algorithms for this problem simply put
it's a highly complex problem for a it's a highly complex problem for a it's a highly complex problem for a
classical computer to solve or so we classical computer to solve or so we classical computer to solve or so we
think in 1994 however Peter Shore came think in 1994 however Peter Shore came think in 1994 however Peter Shore came
up with a method that allows a quantum up with a method that allows a quantum up with a method that allows a quantum
computer to solve this problem quickly computer to solve this problem quickly computer to solve this problem quickly
so long as we can build one that's up to so long as we can build one that's up to so long as we can build one that's up to
the task so how can a quantum computer the task so how can a quantum computer the task so how can a quantum computer
succeed where classical computers succeed where classical computers succeed where classical computers
fail quantum computers harness quantum fail quantum computers harness quantum fail quantum computers harness quantum
mechanics to compute by different rules mechanics to compute by different rules mechanics to compute by different rules
than classical computers do they don't than classical computers do they don't than classical computers do they don't
perform operations faster than a perform operations faster than a perform operations faster than a
classical computer but they perform classical computer but they perform classical computer but they perform
different operations that a classical different operations that a classical different operations that a classical
computer can't and sometimes those computer can't and sometimes those computer can't and sometimes those
operations offer a faster route to a operations offer a faster route to a operations offer a faster route to a
solution the details are complicated but solution the details are complicated but solution the details are complicated but
that's the that's the that's the
idea as it turns out it's an extremely idea as it turns out it's an extremely idea as it turns out it's an extremely
difficult technological challenge to difficult technological challenge to difficult technological challenge to
build quantum computers and after over build quantum computers and after over build quantum computers and after over
30 years we're still a long ways away 30 years we're still a long ways away 30 years we're still a long ways away
from being able to build one capable of from being able to build one capable of from being able to build one capable of
factoring a number like RSA 1024 using factoring a number like RSA 1024 using factoring a number like RSA 1024 using
shor's method shor's method shor's method
but the technology is developing but the technology is developing but the technology is developing
rapidly and we expect quantum computers rapidly and we expect quantum computers rapidly and we expect quantum computers
to be useful for a lot more than just to be useful for a lot more than just to be useful for a lot more than just
factoring large numbers another task factoring large numbers another task factoring large numbers another task
that we believe quantum computers will that we believe quantum computers will that we believe quantum computers will
be useful for is simulating nature which be useful for is simulating nature which be useful for is simulating nature which
appears to obey the laws of quantum appears to obey the laws of quantum appears to obey the laws of quantum
mechanics quantum mechanics is complex mechanics quantum mechanics is complex mechanics quantum mechanics is complex
and sometimes classical computers can and sometimes classical computers can and sometimes classical computers can
struggle to Crunch the numbers to figure struggle to Crunch the numbers to figure struggle to Crunch the numbers to figure
out what nature is doing out what nature is doing out what nature is doing
but quantum computers play by different but quantum computers play by different but quantum computers play by different
rules quantum computers don't need to rules quantum computers don't need to rules quantum computers don't need to
Crunch these numbers per say they can Crunch these numbers per say they can Crunch these numbers per say they can
simply mimic nature rather than simply mimic nature rather than simply mimic nature rather than
approximate it and that's because just approximate it and that's because just approximate it and that's because just
like nature quantum computers are like nature quantum computers are like nature quantum computers are
quantum and the potential here is quantum and the potential here is quantum and the potential here is
enormous not just for understanding enormous not just for understanding enormous not just for understanding
physics but for Designing new materials physics but for Designing new materials physics but for Designing new materials
and medicine for and medicine for and medicine for
instance okay so that's it's all fine instance okay so that's it's all fine instance okay so that's it's all fine
and good but do today's quantum and good but do today's quantum and good but do today's quantum
computers actually allow us to solve computers actually allow us to solve computers actually allow us to solve
problems that we can't using classical problems that we can't using classical problems that we can't using classical
computers the answer is no not today not computers the answer is no not today not computers the answer is no not today not
yet but the technology is advancing yet but the technology is advancing yet but the technology is advancing
daily and soon we expected to start daily and soon we expected to start daily and soon we expected to start
paying paying paying
dividends this brings us to the subject dividends this brings us to the subject dividends this brings us to the subject
of quantum Advantage now the term of quantum Advantage now the term of quantum Advantage now the term
quantum advantage means exactly what it quantum advantage means exactly what it quantum advantage means exactly what it
sounds like Quantum Computing actually sounds like Quantum Computing actually sounds like Quantum Computing actually
paying off paying off paying off
so when a quantum computer allows us to so when a quantum computer allows us to so when a quantum computer allows us to
solve computational problems more solve computational problems more solve computational problems more
efficiently more cost effectively or efficiently more cost effectively or efficiently more cost effectively or
more accurately than we can with more accurately than we can with more accurately than we can with
classical computers classical computers classical computers
alone that's simple enough in principle alone that's simple enough in principle alone that's simple enough in principle
but there are some important but there are some important but there are some important
subtleties first it is a misconception subtleties first it is a misconception subtleties first it is a misconception
that Quantum Advantage is about quantum that Quantum Advantage is about quantum that Quantum Advantage is about quantum
computers versus classical computers in computers versus classical computers in computers versus classical computers in
reality it's quantum computers plus reality it's quantum computers plus reality it's quantum computers plus
classical computers versus classical classical computers versus classical classical computers versus classical
computers alone just look at these racks computers alone just look at these racks computers alone just look at these racks
of classical computers here which are of classical computers here which are of classical computers here which are
all needed to make this quantum computer all needed to make this quantum computer all needed to make this quantum computer
work the point is not to beat classical work the point is not to beat classical work the point is not to beat classical
computers with quantum computers it's to computers with quantum computers it's to computers with quantum computers it's to
enhance classical Computing with quantum enhance classical Computing with quantum enhance classical Computing with quantum
computers quantum computers won't computers quantum computers won't computers quantum computers won't
replace classical computers they need replace classical computers they need replace classical computers they need
classical computers to classical computers to classical computers to
work second it's another misconception work second it's another misconception work second it's another misconception
that Quantum Advantage can be be that Quantum Advantage can be be that Quantum Advantage can be be
definitively proven with one definitively proven with one definitively proven with one
demonstration that's not true because demonstration that's not true because demonstration that's not true because
that's not how science works the that's not how science works the that's not how science works the
philosophy of science tells us that philosophy of science tells us that philosophy of science tells us that
scientific claims are not provable scientific claims are not provable scientific claims are not provable
statements they are hypotheses subject statements they are hypotheses subject statements they are hypotheses subject
to falsification through repeated to falsification through repeated to falsification through repeated
attempts to challenge and disprove a attempts to challenge and disprove a attempts to challenge and disprove a
hypothesis we may gain confidence in its hypothesis we may gain confidence in its hypothesis we may gain confidence in its
truth but absolute certainty remains truth but absolute certainty remains truth but absolute certainty remains
impossible for all scientific theories impossible for all scientific theories impossible for all scientific theories
naturally if we have a particular naturally if we have a particular naturally if we have a particular
problem we'd like to solve we pick the problem we'd like to solve we pick the problem we'd like to solve we pick the
best method available to us whether it's best method available to us whether it's best method available to us whether it's
the best in terms of the time required the best in terms of the time required the best in terms of the time required
the cost in dollars to find a solution the cost in dollars to find a solution the cost in dollars to find a solution
or some combination of the two or or some combination of the two or or some combination of the two or
something even entirely different but we something even entirely different but we something even entirely different but we
never know for sure that a better never know for sure that a better never know for sure that a better
solution isn't just around the corner solution isn't just around the corner solution isn't just around the corner
waiting to be discovered for example we waiting to be discovered for example we waiting to be discovered for example we
don't know for certain that an efficient don't know for certain that an efficient don't know for certain that an efficient
classical algorithm for factoring won't classical algorithm for factoring won't classical algorithm for factoring won't
be discovered tomorrow we really don't be discovered tomorrow we really don't be discovered tomorrow we really don't
think that will happen but it could you think that will happen but it could you think that will happen but it could you
can see how the dynamic landscape of can see how the dynamic landscape of can see how the dynamic landscape of
algorithms can change the usefulness of algorithms can change the usefulness of algorithms can change the usefulness of
a technology very quickly as we have a technology very quickly as we have a technology very quickly as we have
seen in the AI space this applies seen in the AI space this applies seen in the AI space this applies
equally well to problems having Quantum equally well to problems having Quantum equally well to problems having Quantum
Solutions when we reach the point when Solutions when we reach the point when Solutions when we reach the point when
we believe that a Quantum device may we believe that a Quantum device may we believe that a Quantum device may
have provided an advantage over have provided an advantage over have provided an advantage over
classical Computing alone this belief classical Computing alone this belief classical Computing alone this belief
will be a will be a will be a
hypothesis we must then allow classical hypothesis we must then allow classical hypothesis we must then allow classical
Computing to respond and under no Computing to respond and under no Computing to respond and under no
circumstances should it be circumstances should it be circumstances should it be
underestimated in fact we fully expect underestimated in fact we fully expect underestimated in fact we fully expect
classical Computing to put up a strong classical Computing to put up a strong classical Computing to put up a strong
fight not only because classical fight not only because classical fight not only because classical
computers are extraordinary machines but computers are extraordinary machines but computers are extraordinary machines but
because people continue to find creative because people continue to find creative because people continue to find creative
new ways to use them initially new ways to use them initially new ways to use them initially
hypotheses of quantum Advantage will hypotheses of quantum Advantage will hypotheses of quantum Advantage will
probably be eclipsed by new classical probably be eclipsed by new classical probably be eclipsed by new classical
methods and this should not be seen as a methods and this should not be seen as a methods and this should not be seen as a
failure of quantum failure of quantum failure of quantum
Computing rather what it means is that Computing rather what it means is that Computing rather what it means is that
further work on the quantum solution is further work on the quantum solution is further work on the quantum solution is
needed so that it once again becomes the needed so that it once again becomes the needed so that it once again becomes the
better better better
alternative this sort of back and forth alternative this sort of back and forth alternative this sort of back and forth
is very healthy in fact and it should be is very healthy in fact and it should be is very healthy in fact and it should be
encouraged because it will lead to encouraged because it will lead to encouraged because it will lead to
improvements in both classical and improvements in both classical and improvements in both classical and
Quantum Computing so don't expect a Quantum Computing so don't expect a Quantum Computing so don't expect a
definitive incontrovertible claim of definitive incontrovertible claim of definitive incontrovertible claim of
quantum advantage to stand instead there quantum advantage to stand instead there quantum advantage to stand instead there
will be some competition and don't will be some competition and don't will be some competition and don't
imagine classical Computing will just imagine classical Computing will just imagine classical Computing will just
roll over here at IBM we expect to start roll over here at IBM we expect to start roll over here at IBM we expect to start
seeing serious claims of quantum seeing serious claims of quantum seeing serious claims of quantum
Advantage within the next 2 years and Advantage within the next 2 years and Advantage within the next 2 years and
then we'll have to wait and see how then we'll have to wait and see how then we'll have to wait and see how
classical Computing classical Computing classical Computing
responds and the last point I want to responds and the last point I want to responds and the last point I want to
make is that for a reasonable hypothesis make is that for a reasonable hypothesis make is that for a reasonable hypothesis
of quantum Advantage we need to trust of quantum Advantage we need to trust of quantum Advantage we need to trust
that the quantum computer is giving us that the quantum computer is giving us that the quantum computer is giving us
the correct answer if you and I solve the correct answer if you and I solve the correct answer if you and I solve
some math problem and I solve it faster some math problem and I solve it faster some math problem and I solve it faster
than you but I get the wrong answer than you but I get the wrong answer than you but I get the wrong answer
there's no advantage in that the same there's no advantage in that the same there's no advantage in that the same
goes for Quantum Computing except that goes for Quantum Computing except that goes for Quantum Computing except that
sometimes it can be quite tricky to know sometimes it can be quite tricky to know sometimes it can be quite tricky to know
if a result is correct for a problem if a result is correct for a problem if a result is correct for a problem
like factoring it's easy to check the like factoring it's easy to check the like factoring it's easy to check the
correctness of a solution but for other correctness of a solution but for other correctness of a solution but for other
problems it's not always that simple a problems it's not always that simple a problems it's not always that simple a
great example is the random circuit great example is the random circuit great example is the random circuit
sampling demonstration recently sampling demonstration recently sampling demonstration recently
performed by Google with their new performed by Google with their new performed by Google with their new
Quantum chip this is the task that you Quantum chip this is the task that you Quantum chip this is the task that you
may have heard about in the news that may have heard about in the news that may have heard about in the news that
they estimate would take 10 subti years they estimate would take 10 subti years they estimate would take 10 subti years
for a classical computer to solve what for a classical computer to solve what for a classical computer to solve what
this task is quite literally is to run this task is quite literally is to run this task is quite literally is to run
randomly generated instructions on a randomly generated instructions on a randomly generated instructions on a
quantum computer on its own that's quantum computer on its own that's quantum computer on its own that's
probably not a very useful thing to do probably not a very useful thing to do probably not a very useful thing to do
but that's not the point the point is but that's not the point the point is but that's not the point the point is
that we don't know how to get a that we don't know how to get a that we don't know how to get a
classical computer alone to predict what classical computer alone to predict what classical computer alone to predict what
happens when random instructions are run happens when random instructions are run happens when random instructions are run
on a quantum computer but on a quantum on a quantum computer but on a quantum on a quantum computer but on a quantum
computer you can just run them so it's a computer you can just run them so it's a computer you can just run them so it's a
hypothesis that the quantum computer is hypothesis that the quantum computer is hypothesis that the quantum computer is
doing something beyond the capabilities doing something beyond the capabilities doing something beyond the capabilities
of classical computers of classical computers of classical computers
alone the trouble however is that for alone the trouble however is that for alone the trouble however is that for
this particular problem running random this particular problem running random this particular problem running random
instructions on a quantum computer we instructions on a quantum computer we instructions on a quantum computer we
don't actually have any way to verify don't actually have any way to verify don't actually have any way to verify
that the quantum computer ran the random that the quantum computer ran the random that the quantum computer ran the random
instructions instructions instructions
correctly that's why before making correctly that's why before making correctly that's why before making
claims of Advantage we first need to claims of Advantage we first need to claims of Advantage we first need to
establish Trust trust in the device establish Trust trust in the device establish Trust trust in the device
itself just like in any experiment in itself just like in any experiment in itself just like in any experiment in
science where we must validate the science where we must validate the science where we must validate the
instrument before using it to make instrument before using it to make instrument before using it to make
discoveries quantum computers require discoveries quantum computers require discoveries quantum computers require
rigorous demonstrations that build rigorous demonstrations that build rigorous demonstrations that build
confidence that their outputs are confidence that their outputs are confidence that their outputs are
accurate especially for problems where accurate especially for problems where accurate especially for problems where
classical verification isn't feasible classical verification isn't feasible classical verification isn't feasible
this is what IBM demonstrated in 2023 this is what IBM demonstrated in 2023 this is what IBM demonstrated in 2023
and what we called the evidence of and what we called the evidence of and what we called the evidence of
utility of quantum Computing in the utility of quantum Computing in the utility of quantum Computing in the
prefault tolerant era the demonstration prefault tolerant era the demonstration prefault tolerant era the demonstration
that we can trust the outputs of a noisy that we can trust the outputs of a noisy that we can trust the outputs of a noisy
quantum computer whose outputs lie quantum computer whose outputs lie quantum computer whose outputs lie
Beyond Brute Force classical simulation Beyond Brute Force classical simulation Beyond Brute Force classical simulation
this is huge since this ensures that this is huge since this ensures that this is huge since this ensures that
these methods can be relied upon for these methods can be relied upon for these methods can be relied upon for
exploring Quantum Advantage but don't exploring Quantum Advantage but don't exploring Quantum Advantage but don't
take my word for it read the paper I'll take my word for it read the paper I'll take my word for it read the paper I'll
link it link it link it
below now we've already talked a bit below now we've already talked a bit below now we've already talked a bit
about how the search for Quantum about how the search for Quantum about how the search for Quantum
Advantage will likely play out there Advantage will likely play out there Advantage will likely play out there
will be demonstrations of what quantum will be demonstrations of what quantum will be demonstrations of what quantum
computers can do but then classical computers can do but then classical computers can do but then classical
Computing gets its turn there will be Computing gets its turn there will be Computing gets its turn there will be
hypotheses of quantum advantage and at hypotheses of quantum advantage and at hypotheses of quantum advantage and at
the start it's reasonable to expect the start it's reasonable to expect the start it's reasonable to expect
those hypotheses to be falsified as those hypotheses to be falsified as those hypotheses to be falsified as
creative people come up with new ways of creative people come up with new ways of creative people come up with new ways of
approaching the relevant problems with approaching the relevant problems with approaching the relevant problems with
classical computers but quantum classical computers but quantum classical computers but quantum
computers continue to develop and people computers continue to develop and people computers continue to develop and people
find new ways to use them as well and find new ways to use them as well and find new ways to use them as well and
one day perhaps not too long from now a one day perhaps not too long from now a one day perhaps not too long from now a
hypothesis of quantum Advantage will be hypothesis of quantum Advantage will be hypothesis of quantum Advantage will be
met with silence as classical Computing met with silence as classical Computing met with silence as classical Computing
fails to respond and as long as fails to respond and as long as fails to respond and as long as
classical Computing falls short will classical Computing falls short will classical Computing falls short will
have a Quantum have a Quantum have a Quantum
Advantage do I have a bias of course I Advantage do I have a bias of course I Advantage do I have a bias of course I
have a bias I work for IBM but what I'm have a bias I work for IBM but what I'm have a bias I work for IBM but what I'm
saying now should not be saying now should not be saying now should not be
controversial scientific progress is controversial scientific progress is controversial scientific progress is
sometimes slow sometimes it's sometimes slow sometimes it's sometimes slow sometimes it's
unpredictable but it is always rewarding unpredictable but it is always rewarding unpredictable but it is always rewarding
thank you for watching and be sure to thank you for watching and be sure to thank you for watching and be sure to
subscribe to this channel if you want to subscribe to this channel if you want to subscribe to this channel if you want to
learn more about Quantum Computing and learn more about Quantum Computing and learn more about Quantum Computing and
stay up toate on the exciting new stay up toate on the exciting new stay up toate on the exciting new
advances that are surely on the way

## What is quantum computing？ #quantum #quantumcomputing #science

[Music] [Music]
Quantum Computing is a new way of Quantum Computing is a new way of Quantum Computing is a new way of
Performing computations based on the Performing computations based on the Performing computations based on the
principles of quantum physics unlike principles of quantum physics unlike principles of quantum physics unlike
traditional computers which use bits the traditional computers which use bits the traditional computers which use bits the
smallest unit of information represented smallest unit of information represented smallest unit of information represented
as either zero or one quantum computers as either zero or one quantum computers as either zero or one quantum computers
use quantum bits or use quantum bits or use quantum bits or
cubits cubits introduce a different cubits cubits introduce a different cubits cubits introduce a different
Behavior to computation by leveraging Behavior to computation by leveraging Behavior to computation by leveraging
superposition meaning they can exists in superposition meaning they can exists in superposition meaning they can exists in
a combination of zero and one a combination of zero and one a combination of zero and one
simultaneously rather than being simultaneously rather than being simultaneously rather than being
restricted to just one state another key restricted to just one state another key restricted to just one state another key
property of cubits is entanglement where property of cubits is entanglement where property of cubits is entanglement where
two or more cubits become intrinsically two or more cubits become intrinsically two or more cubits become intrinsically
linked such that the state of one cubit linked such that the state of one cubit linked such that the state of one cubit
instantly influences the state of instantly influences the state of instantly influences the state of
another this phenomenon defies classical another this phenomenon defies classical another this phenomenon defies classical
physics and allows quantum computers to physics and allows quantum computers to physics and allows quantum computers to
process certain types of information in process certain types of information in process certain types of information in
a fundamentally different way a fundamentally different way a fundamentally different way
by harnessing superp position and by harnessing superp position and by harnessing superp position and
entanglement quantum computers have the entanglement quantum computers have the entanglement quantum computers have the
potential to solve certain types of potential to solve certain types of potential to solve certain types of
complex problems far more efficiently complex problems far more efficiently complex problems far more efficiently
than even the fastest classical than even the fastest classical than even the fastest classical
supercomputers they could one day excel supercomputers they could one day excel supercomputers they could one day excel
in areas like cryptography Material in areas like cryptography Material in areas like cryptography Material
Science optimization and simulation Science optimization and simulation Science optimization and simulation
which include tasks that would take which include tasks that would take which include tasks that would take
traditional computers hundreds or even traditional computers hundreds or even traditional computers hundreds or even
thousands of years to compute

## What is Superposition？ Quantum Jargon Explained

superposition is a fundamental principle superposition is a fundamental principle
of both Quantum Computing and quantum of both Quantum Computing and quantum of both Quantum Computing and quantum
mechanics mechanics mechanics
the principle of superposition states the principle of superposition states the principle of superposition states
that a physical system can exist in a that a physical system can exist in a that a physical system can exist in a
sum of its possible measurable outcomes sum of its possible measurable outcomes sum of its possible measurable outcomes
when talking about the superposition when talking about the superposition when talking about the superposition
state of a Quantum bit the only possible state of a Quantum bit the only possible state of a Quantum bit the only possible
measurement outcomes are 0 and 1. measurement outcomes are 0 and 1. measurement outcomes are 0 and 1.
writing this down mathematically would writing this down mathematically would writing this down mathematically would
look like this where the coefficient look like this where the coefficient look like this where the coefficient
squared would represent the probability squared would represent the probability squared would represent the probability
of measuring the qubit in either 0 or 1. of measuring the qubit in either 0 or 1. of measuring the qubit in either 0 or 1.
once we physically measure the qubit you once we physically measure the qubit you once we physically measure the qubit you
will never find it in a superposition will never find it in a superposition will never find it in a superposition
state it will only ever be in the state it will only ever be in the state it will only ever be in the
classical States 1 or 0. we can never classical States 1 or 0. we can never classical States 1 or 0. we can never
directly measure a superposition State directly measure a superposition State directly measure a superposition State
quantum mechanics forbids this but we quantum mechanics forbids this but we quantum mechanics forbids this but we
can deduce it from the outcome of can deduce it from the outcome of can deduce it from the outcome of
specific experiments specific experiments specific experiments
in Kiss kit you can put a qubit into in Kiss kit you can put a qubit into in Kiss kit you can put a qubit into
superposition by applying a hadamard superposition by applying a hadamard superposition by applying a hadamard
gate see the links in the description to gate see the links in the description to gate see the links in the description to
learn more

## When will quantum computers be available？ #quantum #quantumphysics #quantumphysics

[Music] [Music]
actually they are already available to actually they are already available to actually they are already available to
the public right now making an account the public right now making an account the public right now making an account
on IBM Quantum will give you free access on IBM Quantum will give you free access on IBM Quantum will give you free access
to IBM quantum computers that you can to IBM quantum computers that you can to IBM quantum computers that you can
use

## When will quantum computers break encryption？ #quantum #quantumphysics #quantumcomputing

[Music] [Music]
we don't know but all indications tell we don't know but all indications tell we don't know but all indications tell
us that it's probably quite a ways away us that it's probably quite a ways away us that it's probably quite a ways away
this might sound scary but there are this might sound scary but there are this might sound scary but there are
other positive benefits to building a other positive benefits to building a other positive benefits to building a
quantum computer capable of running quantum computer capable of running quantum computer capable of running
Short's algorithm that same algorithm is Short's algorithm that same algorithm is Short's algorithm that same algorithm is
valuable for other things like phase valuable for other things like phase valuable for other things like phase
estimation and simultaneously the estimation and simultaneously the estimation and simultaneously the
country and world are moving towards country and world are moving towards country and world are moving towards
other encryption schemes that are other encryption schemes that are other encryption schemes that are
quantum safe

## When will quantum computing be common？ #quantum #quantumcomputing #quantumphysics

[Music] [Music]
the truth is we can't predict this for the truth is we can't predict this for the truth is we can't predict this for
certain hopefully and we think pretty certain hopefully and we think pretty certain hopefully and we think pretty
soon if you subscribe you'll be one of soon if you subscribe you'll be one of soon if you subscribe you'll be one of
the first to know and if you have any the first to know and if you have any the first to know and if you have any
further questions please drop them in further questions please drop them in further questions please drop them in
the comments

## Why are quantum computers so big？ #quantum #quantumcomputing #ibm

[Music] [Music]
quantum computers themselves aren't quantum computers themselves aren't quantum computers themselves aren't
actually all that big Quantum processing actually all that big Quantum processing actually all that big Quantum processing
units or qpu are only about the size of units or qpu are only about the size of units or qpu are only about the size of
a typical computer chip however all the a typical computer chip however all the a typical computer chip however all the
other Machinery that goes around it that other Machinery that goes around it that other Machinery that goes around it that
is needed to keep it cold and stable and is needed to keep it cold and stable and is needed to keep it cold and stable and
allow for the input and output of allow for the input and output of allow for the input and output of
signals is what makes the whole thing signals is what makes the whole thing signals is what makes the whole thing
look quite large however work to create look quite large however work to create look quite large however work to create
smaller denser cables and components is smaller denser cables and components is smaller denser cables and components is
already well underway

## Why are quantum computers so cold？ #quantum #quantumphysics #quantumcomputing

most quantum computers are kept at most quantum computers are kept at
insanely low temperatures around 10 insanely low temperatures around 10 insanely low temperatures around 10
Melvin or - Melvin or - Melvin or -
49° F because any amount of heat or 49° F because any amount of heat or 49° F because any amount of heat or
thermal energy can disrupt their fragile thermal energy can disrupt their fragile thermal energy can disrupt their fragile
Quantum States in addition IBM quantum Quantum States in addition IBM quantum Quantum States in addition IBM quantum
computers also need to be cold because computers also need to be cold because computers also need to be cold because
they run on they run on they run on
superconductors which is a type of metal superconductors which is a type of metal superconductors which is a type of metal
where current can flow with no where current can flow with no where current can flow with no
resistance below a certain threshold resistance below a certain threshold resistance below a certain threshold
temperature

## Why are quantum computers so hard to build？ #quantum #quantumphysics #quantumcomputing

all new breakthrough technology is hard all new breakthrough technology is hard
to build at first and even harder to to build at first and even harder to to build at first and even harder to
scale but quantum computers are scale but quantum computers are scale but quantum computers are
especially difficult because they especially difficult because they especially difficult because they
combine so many complicated aspects of combine so many complicated aspects of combine so many complicated aspects of
Science and Technology there's a lot of Science and Technology there's a lot of Science and Technology there's a lot of
new physics we are learning and need to new physics we are learning and need to new physics we are learning and need to
be able to understand to build them be able to understand to build them be able to understand to build them
while also incorporating while also incorporating while also incorporating
state-of-the-art engineering and state-of-the-art engineering and state-of-the-art engineering and
programming techniques in addition we're programming techniques in addition we're programming techniques in addition we're
also fighting against Quantum also fighting against Quantum also fighting against Quantum
decoherence the natural tendency for decoherence the natural tendency for decoherence the natural tendency for
quantum computers to lose track of their quantum computers to lose track of their quantum computers to lose track of their
Quantum information over a period of Quantum information over a period of Quantum information over a period of
time sometimes in the middle of a time sometimes in the middle of a time sometimes in the middle of a
calculation keeping Quantum processors calculation keeping Quantum processors calculation keeping Quantum processors
isolated but still being able to control isolated but still being able to control isolated but still being able to control
them is one of our biggest engineering them is one of our biggest engineering them is one of our biggest engineering
hurdles

## Will quantum computers ever replace laptops？ #quantum #quantumphysics #quantumcomputing

[Music] [Music]
well the most important thing to know is well the most important thing to know is well the most important thing to know is
that quantum computers are not just that quantum computers are not just that quantum computers are not just
faster regular computers so the answer faster regular computers so the answer faster regular computers so the answer
is they almost certainly never will it is they almost certainly never will it is they almost certainly never will it
is a misconception that it's quantum is a misconception that it's quantum is a misconception that it's quantum
computers versus classical computers computers versus classical computers computers versus classical computers
it's really quantum computers aiding it's really quantum computers aiding it's really quantum computers aiding
classical computers versus classical classical computers versus classical classical computers versus classical
computers alone quantum computers are computers alone quantum computers are computers alone quantum computers are
more efficient in theory at only a Subs more efficient in theory at only a Subs more efficient in theory at only a Subs
of computational problems most of computational problems most of computational problems most
computational problems the public is computational problems the public is computational problems the public is
familiar with can be computed with no familiar with can be computed with no familiar with can be computed with no
issues whatsoever on a regular computer issues whatsoever on a regular computer issues whatsoever on a regular computer
in our world we call them classical in our world we call them classical in our world we call them classical
computers adding a Quantum processor to computers adding a Quantum processor to computers adding a Quantum processor to
your phone or your laptop would not add your phone or your laptop would not add your phone or your laptop would not add
any benefit

## What is Quantum Computing？

An ideal quantum computer can break the encryption
standards we use today by finding prime factors of a large integer in just minutes instead
of the thousands of years it would take for a classical computer to do. But before you start to panic, while we have
real quantum hardware today, it's not quite powerful enough to do that
just yet. However, technologies are advancing faster
than ever. The cell phones we have today are more powerful
than the mainframes that we used to send people to the moon. And the researchers believe that we will soon
be entering an era of quantum advances where quantum computers will be used to accelerate
classical computers, just like GPUs. In this video, I'm going to talk about five
foundational topics in quantum computers: superposition, gates, measurement, interference,
and entanglement. But before we dive into that, let's first talk
about bits. Classical computers that use bits which are
like switches that can be a 0 or a 1. This way of computation has served us well.
So well, in fact, that almost all modern computers work this way. However, this approach doesn't solve all the
problems that we have today -- problems that can blow up exponentially and would take a
classical computers decades or more to solve. We already talked about the algorithm we
use for encryption. Other types of difficult problems include
optimization, chemistry simulation, and machine learning. Now let's talk about our first topic, superposition. A quantum computer does not use the simple 0
and 1 bits. Instead, it uses qubits. Qubits can be a 0, a 1, or any linear combination
of the two. This spectrum of states is what we called
a superposition. Our next next topic is about gates. Similar to classical computers, we use -- we
string together qubits using a construct called gates that can alter the states of qubits into
circuits. For example, we can have a qubit that's at the state of 0. Then we can use Hadamard gate, or H gate for short,
to put it in a superposition between 0 and 1. And, of course, you can have multiple qubits
with multiple gates in a circuit. For the circuit will be useful, at some point
that you need to read about its outputs. Which brings us to our next topic, measurement. When a qubit is measured, it loses its superposition
and collapses into just a simple 0 or 1. That means an arrow pointing this way does not measure
a 0.5, instead, it has a 50% chance of measuring
a 0 and 50% chance of measuring a 1. It is this in-between state that sometimes
people say that a qubit can be a 0 and a 1 at the same time. It also means that just a small number of
qubits can represent a large amount of information. So for our next topic, interference, we begin by addressing a common problem -- common question -- why is it that quantum
computers can outperform classical ones? So, if you remember, a quantum state is
a linear combination of the 0 state and the 1 state. So, an operation applied to this can be seen
as applying to the 0 state and the 1 state, doing two calculations at once. It is this parallel computation that gives
quantum its unique advantage. However, as you may recall, when a qubit is
measured, it loses its superposition and collapses into 0 or 1. That means we can only get a single answer
instead of all the answers from this parallel computation. And to make sure the single answer we get
is a correct one, quantum gates need to be arranged in a way so that it would amplify the correct answer and cancel
all the incorrect ones. A process called interference. Now this leads us to our last topic, entanglement. When qubits are entangled, their states become
strongly correlated. That is, changing the state of just one qubit would change
the state of another. For example, we can entangle two qubits so
that their states have 50% chance of measuring a 00 and 50% chance of measuring a 11, but
never a 01 or a 10. In this case, if we just -- if we change just
the state of one, the other one would also change. So with the combined power of superposition, interference, and entanglement, quantum computers can solve things that classical
computers simply cannot do today. It can lead to better drug -- better drug discovery --
or enhance the stock portfolio or even artificial intelligence. Now we just need to wait for the quantum hardware
to catch up. Thanks for watching. If you have any questions, please leave them
in the comments below. Also, please remember to Like this video and
Subscribe to our channel so we can continue to bring you content that matters to you.

## Correcting Quantum Errors ｜ Understanding Quantum Information & Computation ｜ Lesson 13

welcome back to understanding Quantum welcome back to understanding Quantum
information and computation my name is information and computation my name is information and computation my name is
John watus and I'm the technical John watus and I'm the technical John watus and I'm the technical
director for education at IBM Quantum director for education at IBM Quantum director for education at IBM Quantum
this is the 13th lesson of the series this is the 13th lesson of the series this is the 13th lesson of the series
and it's the first lesson in the fourth and it's the first lesson in the fourth and it's the first lesson in the fourth
and final unit which is on Quantum error and final unit which is on Quantum error and final unit which is on Quantum error
correction Quantum Computing has the correction Quantum Computing has the correction Quantum Computing has the
potential to enable efficient solutions potential to enable efficient solutions potential to enable efficient solutions
to computational tasks for which to computational tasks for which to computational tasks for which
efficient classical algorithms are not efficient classical algorithms are not efficient classical algorithms are not
known and possibly don't exist there are known and possibly don't exist there are known and possibly don't exist there are
however very significant challenges that however very significant challenges that however very significant challenges that
will need to be overcome if were to will need to be overcome if were to will need to be overcome if were to
implement the sorts of large scale implement the sorts of large scale implement the sorts of large scale
Quantum computations that we hope will Quantum computations that we hope will Quantum computations that we hope will
one day be one day be one day be
possible the heart of the matter is that possible the heart of the matter is that possible the heart of the matter is that
Quantum information is extremely fragile Quantum information is extremely fragile Quantum information is extremely fragile
you can literally ruin it just by you can literally ruin it just by you can literally ruin it just by
looking at it and so to operate looking at it and so to operate looking at it and so to operate
correctly quantum computers need to correctly quantum computers need to correctly quantum computers need to
isolate the quantum information they isolate the quantum information they isolate the quantum information they
store from the environment around them store from the environment around them store from the environment around them
to an extreme to an extreme to an extreme
degree but at the same time the quantum degree but at the same time the quantum degree but at the same time the quantum
computer itself must provide us with computer itself must provide us with computer itself must provide us with
very precise control over this Quantum very precise control over this Quantum very precise control over this Quantum
information and that includes proper information and that includes proper information and that includes proper
initialization accurate and reliable initialization accurate and reliable initialization accurate and reliable
unitary operations and of course the unitary operations and of course the unitary operations and of course the
ability to perform measurements so that ability to perform measurements so that ability to perform measurements so that
we can actually obtain the results of we can actually obtain the results of we can actually obtain the results of
the the the
computation there's clearly some tension computation there's clearly some tension computation there's clearly some tension
between these requirements and in the between these requirements and in the between these requirements and in the
early days of quantum Computing there early days of quantum Computing there early days of quantum Computing there
were some who viewed that the fragility were some who viewed that the fragility were some who viewed that the fragility
of quantum information and its of quantum information and its of quantum information and its
susceptibility to both inaccuracies and susceptibility to both inaccuracies and susceptibility to both inaccuracies and
environmental noise would ultimately environmental noise would ultimately environmental noise would ultimately
make Quantum Computing make Quantum Computing make Quantum Computing
impossible today there's little doubt impossible today there's little doubt impossible today there's little doubt
that building an accurate and reliable that building an accurate and reliable that building an accurate and reliable
large scale quantum computer is a large scale quantum computer is a large scale quantum computer is a
Monumental Monumental Monumental
challenge but we have a key tool to help challenge but we have a key tool to help challenge but we have a key tool to help
us in this endeavor that leads most us in this endeavor that leads most us in this endeavor that leads most
people who are knowledgeable about the people who are knowledgeable about the people who are knowledgeable about the
field to be optimistic about large scale field to be optimistic about large scale field to be optimistic about large scale
Quantum Computing one day becoming a Quantum Computing one day becoming a Quantum Computing one day becoming a
reality and that tool is quantum error reality and that tool is quantum error reality and that tool is quantum error
correction for the next four lessons correction for the next four lessons correction for the next four lessons
we'll discuss Quantum error correction we'll discuss Quantum error correction we'll discuss Quantum error correction
with a focus on the with a focus on the with a focus on the
fundamentals in this lesson we'll take a fundamentals in this lesson we'll take a fundamentals in this lesson we'll take a
first look at Quantum error correction first look at Quantum error correction first look at Quantum error correction
including one of the very first quantum including one of the very first quantum including one of the very first quantum
error correcting codes discovered the 9 error correcting codes discovered the 9 error correcting codes discovered the 9
Cubit Shore code and we'll also discuss Cubit Shore code and we'll also discuss Cubit Shore code and we'll also discuss
another fundamental Concept in Quantum another fundamental Concept in Quantum another fundamental Concept in Quantum
error correction which is known as the error correction which is known as the error correction which is known as the
discretization of errors in the discretization of errors in the discretization of errors in the
subsequent lessons of the unit we'll see subsequent lessons of the unit we'll see subsequent lessons of the unit we'll see
more sophisticated and ultimately better more sophisticated and ultimately better more sophisticated and ultimately better
Quantum error correcting code Quantum error correcting code Quantum error correcting code
constructions and we'll also discuss the constructions and we'll also discuss the constructions and we'll also discuss the
problem of quantum computation on problem of quantum computation on problem of quantum computation on
encoded Quantum information using encoded Quantum information using encoded Quantum information using
computational components that are computational components that are computational components that are
themselves not perfect and prone to themselves not perfect and prone to themselves not perfect and prone to
errors here's a brief overview of the errors here's a brief overview of the errors here's a brief overview of the
lesson we'll Begin by discussing lesson we'll Begin by discussing lesson we'll Begin by discussing
classical repetition codes which are classical repetition codes which are classical repetition codes which are
perhaps the simplest and most basic type perhaps the simplest and most basic type perhaps the simplest and most basic type
of classical error correcting code of classical error correcting code of classical error correcting code
classical repetition codes aren't classical repetition codes aren't classical repetition codes aren't
particularly good codes by classical particularly good codes by classical particularly good codes by classical
coding Theory standards at least on coding Theory standards at least on coding Theory standards at least on
their own but that's okay here we're their own but that's okay here we're their own but that's okay here we're
going for Simplicity and in particular going for Simplicity and in particular going for Simplicity and in particular
we'll take a look at just about the we'll take a look at just about the we'll take a look at just about the
simplest classical error correcting code simplest classical error correcting code simplest classical error correcting code
you can possibly imagine which we'll you can possibly imagine which we'll you can possibly imagine which we'll
then use as a basis for a simple example then use as a basis for a simple example then use as a basis for a simple example
of a Quantum error correcting of a Quantum error correcting of a Quantum error correcting
code that example is the 9 cbit Shore code that example is the 9 cbit Shore code that example is the 9 cbit Shore
code which we'll discuss in detail code which we'll discuss in detail code which we'll discuss in detail
including a description of the code including a description of the code including a description of the code
itself as well as procedures for itself as well as procedures for itself as well as procedures for
encoding cubits using this code encoding cubits using this code encoding cubits using this code
detecting errors on encoded cubits and detecting errors on encoded cubits and detecting errors on encoded cubits and
correcting those errors and in the third correcting those errors and in the third correcting those errors and in the third
part of the lesson we'll discuss the part of the lesson we'll discuss the part of the lesson we'll discuss the
discretization of Errors which allows us discretization of Errors which allows us discretization of Errors which allows us
to think about arbitrary errors on to think about arbitrary errors on to think about arbitrary errors on
cubits which in principle form a cubits which in principle form a cubits which in principle form a
Continuum of possibilities in discrete Continuum of possibilities in discrete Continuum of possibilities in discrete
terms essentially boiling errors down to terms essentially boiling errors down to terms essentially boiling errors down to
so-called bit flips and phase so-called bit flips and phase so-called bit flips and phase
flips before we get into the Technic flips before we get into the Technic flips before we get into the Technic
details of quantum error correction it's details of quantum error correction it's details of quantum error correction it's
worth taking just a moment to consider worth taking just a moment to consider worth taking just a moment to consider
the need for Quantum error correction as the need for Quantum error correction as the need for Quantum error correction as
well as classical error well as classical error well as classical error
correction as I suggested at the start correction as I suggested at the start correction as I suggested at the start
of the lesson quantum computers are of the lesson quantum computers are of the lesson quantum computers are
highly susceptible to errors owing to highly susceptible to errors owing to highly susceptible to errors owing to
the fragility of quantum the fragility of quantum the fragility of quantum
information in particular unwanted information in particular unwanted information in particular unwanted
interactions with the environment or in interactions with the environment or in interactions with the environment or in
other words the rest of the world can other words the rest of the world can other words the rest of the world can
disturb Quantum disturb Quantum disturb Quantum
information these unwanted interactions information these unwanted interactions information these unwanted interactions
can cause decoherence where information can cause decoherence where information can cause decoherence where information
essentially leaks into the environment essentially leaks into the environment essentially leaks into the environment
causing the information stored by the causing the information stored by the causing the information stored by the
quantum computer to behave more and more quantum computer to behave more and more quantum computer to behave more and more
classically it's also critically classically it's also critically classically it's also critically
important that we recognize that Quantum important that we recognize that Quantum important that we recognize that Quantum
operations can only be performed with operations can only be performed with operations can only be performed with
limited limited limited
accuracy for example we might wish to accuracy for example we might wish to accuracy for example we might wish to
perform a particular Quantum gate on a perform a particular Quantum gate on a perform a particular Quantum gate on a
cubit but because physical devices are cubit but because physical devices are cubit but because physical devices are
never perfect what we actually Implement never perfect what we actually Implement never perfect what we actually Implement
might be just an approximation to that might be just an approximation to that might be just an approximation to that
gate that might not be a big deal for a gate that might not be a big deal for a gate that might not be a big deal for a
single gate but if Our intention is to single gate but if Our intention is to single gate but if Our intention is to
perform thousands or millions of quantum perform thousands or millions of quantum perform thousands or millions of quantum
Gates these small inaccuracies will Gates these small inaccuracies will Gates these small inaccuracies will
start to acrew and when the computation start to acrew and when the computation start to acrew and when the computation
is finished we could end up with a very is finished we could end up with a very is finished we could end up with a very
different state from what we different state from what we different state from what we
expected so this is a rather different expected so this is a rather different expected so this is a rather different
source of errors but it's certainly one source of errors but it's certainly one source of errors but it's certainly one
that we can't that we can't that we can't
ignore people have been studying ignore people have been studying ignore people have been studying
classical error correcting codes for classical error correcting codes for classical error correcting codes for
close to a century and these codes have close to a century and these codes have close to a century and these codes have
many applications including in many applications including in many applications including in
information transmission and storage but information transmission and storage but information transmission and storage but
because classical Computing components because classical Computing components because classical Computing components
are so reliable we we don't actually are so reliable we we don't actually are so reliable we we don't actually
need error correction to build classical need error correction to build classical need error correction to build classical
computers now it might be the case that computers now it might be the case that computers now it might be the case that
error correction isn't actually needed error correction isn't actually needed error correction isn't actually needed
to build reliable large scale quantum to build reliable large scale quantum to build reliable large scale quantum
computers either for example the main computers either for example the main computers either for example the main
idea behind topological Quantum idea behind topological Quantum idea behind topological Quantum
Computing is that it might be possible Computing is that it might be possible Computing is that it might be possible
to build robust Quantum systems that are to build robust Quantum systems that are to build robust Quantum systems that are
inherently protected against noise due inherently protected against noise due inherently protected against noise due
to topological properties of the systems to topological properties of the systems to topological properties of the systems
but we don't know if this will be but we don't know if this will be but we don't know if this will be
possible and it's pretty widely believed possible and it's pretty widely believed possible and it's pretty widely believed
that error correction will probably be that error correction will probably be that error correction will probably be
essential in one form or another for essential in one form or another for essential in one form or another for
large- scale Quantum large- scale Quantum large- scale Quantum
Computing now we'll discuss classical Computing now we'll discuss classical Computing now we'll discuss classical
repetition codes which form the basis repetition codes which form the basis repetition codes which form the basis
for our first Quantum error correcting for our first Quantum error correcting for our first Quantum error correcting
code the nine cbit Shore code which code the nine cbit Shore code which code the nine cbit Shore code which
we'll see we'll see we'll see
shortly repetition codes are extremely shortly repetition codes are extremely shortly repetition codes are extremely
basic examples of error correcting codes basic examples of error correcting codes basic examples of error correcting codes
the idea is that we can protect bits the idea is that we can protect bits the idea is that we can protect bits
against errors by simply repeating each against errors by simply repeating each against errors by simply repeating each
bit some number of bit some number of bit some number of
times in particular let's take a look at times in particular let's take a look at times in particular let's take a look at
the three-bit repetition code the three-bit repetition code the three-bit repetition code
here we encode one bit into three by here we encode one bit into three by here we encode one bit into three by
simply repeating the bit three times so simply repeating the bit three times so simply repeating the bit three times so
zero is encoded as 00 0 and one is zero is encoded as 00 0 and one is zero is encoded as 00 0 and one is
encoded as encoded as encoded as
111 of course if nothing goes wrong we 111 of course if nothing goes wrong we 111 of course if nothing goes wrong we
can easily distinguish the two can easily distinguish the two can easily distinguish the two
possibilities but if there was an error possibilities but if there was an error possibilities but if there was an error
and one of the bits flipped let's say and one of the bits flipped let's say and one of the bits flipped let's say
meaning that a zero changes into a one meaning that a zero changes into a one meaning that a zero changes into a one
or a one changes into a zero then we can or a one changes into a zero then we can or a one changes into a zero then we can
still figure out what the original bit still figure out what the original bit still figure out what the original bit
was by seeing which of the two binary was by seeing which of the two binary was by seeing which of the two binary
values appears twice equivalently we can values appears twice equivalently we can values appears twice equivalently we can
decode by Computing the majority value decode by Computing the majority value decode by Computing the majority value
or in other words the binary value that or in other words the binary value that or in other words the binary value that
appears most appears most appears most
frequently and it's not hard to see that frequently and it's not hard to see that frequently and it's not hard to see that
if we have at most one bit flip error if we have at most one bit flip error if we have at most one bit flip error
then the decoding will be then the decoding will be then the decoding will be
correct of course if two bits Flip or if correct of course if two bits Flip or if correct of course if two bits Flip or if
all three bits flip then it won't work all three bits flip then it won't work all three bits flip then it won't work
and we'll get the wrong value when we and we'll get the wrong value when we and we'll get the wrong value when we
decode and that's a general property decode and that's a general property decode and that's a general property
that every error correcting code that every error correcting code that every error correcting code
possesses you may be able to correct possesses you may be able to correct possesses you may be able to correct
some errors but you'll never be able to some errors but you'll never be able to some errors but you'll never be able to
correct every error and ultimately it's correct every error and ultimately it's correct every error and ultimately it's
about decreasing the chance of an error about decreasing the chance of an error about decreasing the chance of an error
as opposed to guaranteeing correctness as opposed to guaranteeing correctness as opposed to guaranteeing correctness
which is generally not possible as an example of a situation in possible as an example of a situation in
which we can decrease the chance of which we can decrease the chance of which we can decrease the chance of
making an error let's suppose that our making an error let's suppose that our making an error let's suppose that our
goal is to communicate a single bit to a goal is to communicate a single bit to a goal is to communicate a single bit to a
hypothetical receiver and to do this hypothetical receiver and to do this hypothetical receiver and to do this
we're able to transmit bits through a we're able to transmit bits through a we're able to transmit bits through a
so-called binary symmetric Channel which so-called binary symmetric Channel which so-called binary symmetric Channel which
flips each bit sent through it flips each bit sent through it flips each bit sent through it
independently with some probability independently with some probability independently with some probability
P so with probability 1 minus P the P so with probability 1 minus P the P so with probability 1 minus P the
receiver will get whichever bit we receiver will get whichever bit we receiver will get whichever bit we
intended to send but with probability P intended to send but with probability P intended to send but with probability P
the bit flips and the receiver gets the the bit flips and the receiver gets the the bit flips and the receiver gets the
opposite value to whatever we opposite value to whatever we opposite value to whatever we
sent so if we don't use the repetition sent so if we don't use the repetition sent so if we don't use the repetition
code and instead we just send whatever code and instead we just send whatever code and instead we just send whatever
bit we have in mind through the channel bit we have in mind through the channel bit we have in mind through the channel
the receiver will get the wrong bit with the receiver will get the wrong bit with the receiver will get the wrong bit with
probability p on the other hand if we probability p on the other hand if we probability p on the other hand if we
first encode the bit we want to send first encode the bit we want to send first encode the bit we want to send
using the three-bit repetition code and using the three-bit repetition code and using the three-bit repetition code and
then send each of the three bits of the then send each of the three bits of the then send each of the three bits of the
encoding through the channel then each encoding through the channel then each encoding through the channel then each
one of them flips with probability P so one of them flips with probability P so one of them flips with probability P so
the chances of a bit flip are now the chances of a bit flip are now the chances of a bit flip are now
greater because there are now three bits greater because there are now three bits greater because there are now three bits
that could possibly flip rather than one that could possibly flip rather than one that could possibly flip rather than one
but if at most one of the bits flips but if at most one of the bits flips but if at most one of the bits flips
then the receiver will be able to then the receiver will be able to then the receiver will be able to
correctly correctly correctly
decode so we'll only get an error after decode so we'll only get an error after decode so we'll only get an error after
decoding if two or more of the bits flip decoding if two or more of the bits flip decoding if two or more of the bits flip
during during during
transmission and it's not too hard to transmission and it's not too hard to transmission and it's not too hard to
calculate that this happens with calculate that this happens with calculate that this happens with
probability 3 * P ^2 - 2 * P probability 3 * P ^2 - 2 * P probability 3 * P ^2 - 2 * P
Cub for values of P smaller than 1/ 12 Cub for values of P smaller than 1/ 12 Cub for values of P smaller than 1/ 12
this leads to a decrease in the this leads to a decrease in the this leads to a decrease in the
probability that the receiver ends up probability that the receiver ends up probability that the receiver ends up
with the wrong bit there will still be a with the wrong bit there will still be a with the wrong bit there will still be a
chance of an error in this case but the chance of an error in this case but the chance of an error in this case but the
code decreases the likelihood of this code decreases the likelihood of this code decreases the likelihood of this
and if you wanted to decrease the chance and if you wanted to decrease the chance and if you wanted to decrease the chance
even further you could do that using a even further you could do that using a even further you could do that using a
different code including but not limited different code including but not limited different code including but not limited
to the possibility of using a repetition to the possibility of using a repetition to the possibility of using a repetition
code where the encoded bit is repeated code where the encoded bit is repeated code where the encoded bit is repeated
more than three more than three more than three
times for values of P greater than 1/ 12 times for values of P greater than 1/ 12 times for values of P greater than 1/ 12
by by the way the code actually makes by by the way the code actually makes by by the way the code actually makes
things worse in the sense that it things worse in the sense that it things worse in the sense that it
increases the likelihood that the increases the likelihood that the increases the likelihood that the
receiver gets the wrong bit so we would receiver gets the wrong bit so we would receiver gets the wrong bit so we would
certainly not use this code in that certainly not use this code in that certainly not use this code in that
situation the three-bit repetition code situation the three-bit repetition code situation the three-bit repetition code
is a classical error correcting code but is a classical error correcting code but is a classical error correcting code but
we can also consider what happens if we we can also consider what happens if we we can also consider what happens if we
try to use it to protect cubits against try to use it to protect cubits against try to use it to protect cubits against
errors it's not a very good Quantum errors it's not a very good Quantum errors it's not a very good Quantum
error correcting code as we'll see it error correcting code as we'll see it error correcting code as we'll see it
can correct against some errors but it can correct against some errors but it can correct against some errors but it
makes other errors more likely it is makes other errors more likely it is makes other errors more likely it is
however the first step toward the shore however the first step toward the shore however the first step toward the shore
code so let's see how it works for code so let's see how it works for code so let's see how it works for
cubits first to be clear when we refer cubits first to be clear when we refer cubits first to be clear when we refer
to the 3-bit repetition code being used to the 3-bit repetition code being used to the 3-bit repetition code being used
for cubits what we have in mind is an for cubits what we have in mind is an for cubits what we have in mind is an
encoding of a cubit as is shown on the encoding of a cubit as is shown on the encoding of a cubit as is shown on the
screen where Alpha 0 plus beta 1 is screen where Alpha 0 plus beta 1 is screen where Alpha 0 plus beta 1 is
encoded as Alpha 0000 0 plus beta 111 or encoded as Alpha 0000 0 plus beta 111 or encoded as Alpha 0000 0 plus beta 111 or
in other words the standard basis state in other words the standard basis state in other words the standard basis state
is repeated three is repeated three is repeated three
times notice in particular that this is times notice in particular that this is times notice in particular that this is
not the same thing as repeating the not the same thing as repeating the not the same thing as repeating the
quantum state three times as in a given quantum state three times as in a given quantum state three times as in a given
Quantum state s being encoded as s Quantum state s being encoded as s Quantum state s being encoded as s
tensor s tensor s which is not an tensor s tensor s which is not an tensor s tensor s which is not an
encoding that we would be able to encoding that we would be able to encoding that we would be able to
implement for an unknown Quantum State s implement for an unknown Quantum State s implement for an unknown Quantum State s
by the no cloning by the no cloning by the no cloning
theorem the encoding that we're talking theorem the encoding that we're talking theorem the encoding that we're talking
about on the other hand is very easily about on the other hand is very easily about on the other hand is very easily
implemented here's a very simple Quantum implemented here's a very simple Quantum implemented here's a very simple Quantum
circuit that does it using two circuit that does it using two circuit that does it using two
initialized workspace cubits and two initialized workspace cubits and two initialized workspace cubits and two
controlled knot Gates now let's suppose that an error Gates now let's suppose that an error
takes place after the encoding has been takes place after the encoding has been takes place after the encoding has been
performed and in particular let's performed and in particular let's performed and in particular let's
suppose that a polyx operation or in suppose that a polyx operation or in suppose that a polyx operation or in
other words a bit flip happens on one of other words a bit flip happens on one of other words a bit flip happens on one of
the cubits and in this figure it's the the cubits and in this figure it's the the cubits and in this figure it's the
middle middle middle
Cubit of course this isn't the only sort Cubit of course this isn't the only sort Cubit of course this isn't the only sort
of error that could occur and you might of error that could occur and you might of error that could occur and you might
object to the idea that an error takes object to the idea that an error takes object to the idea that an error takes
the form of this perfect unitary the form of this perfect unitary the form of this perfect unitary
operation but we'll come back to this operation but we'll come back to this operation but we'll come back to this
later in the lesson and for now we can later in the lesson and for now we can later in the lesson and for now we can
just think about it as an just think about it as an just think about it as an
example here's the resulting State it's example here's the resulting State it's example here's the resulting State it's
the same as the original encoding except the same as the original encoding except the same as the original encoding except
that the middle bit within each K has that the middle bit within each K has that the middle bit within each K has
been flipped been flipped been flipped
obviously we can see this from the obviously we can see this from the obviously we can see this from the
mathematical expressions for the state mathematical expressions for the state mathematical expressions for the state
but suppose that we only had the three but suppose that we only had the three but suppose that we only had the three
cubits and that we suspected that a bit cubits and that we suspected that a bit cubits and that we suspected that a bit
flip might have occurred but we didn't flip might have occurred but we didn't flip might have occurred but we didn't
know which Cubit might have been know which Cubit might have been know which Cubit might have been
affected one way to figure out if a bit affected one way to figure out if a bit affected one way to figure out if a bit
flip occurred would be to perform a flip occurred would be to perform a flip occurred would be to perform a
standard basis measurement and examine standard basis measurement and examine standard basis measurement and examine
the outcome in this case we would see the outcome in this case we would see the outcome in this case we would see
either 0 one0 or one one with either 0 one0 or one one with either 0 one0 or one one with
probabilities depending on Alpha and probabilities depending on Alpha and probabilities depending on Alpha and
beta and in either case our conclusion beta and in either case our conclusion beta and in either case our conclusion
would be that the middle bit flipped would be that the middle bit flipped would be that the middle bit flipped
but unfortunately that ruins the but unfortunately that ruins the but unfortunately that ruins the
original Quantum State and this state original Quantum State and this state original Quantum State and this state
might be precious to us after all the might be precious to us after all the might be precious to us after all the
whole point is to protect this Quantum whole point is to protect this Quantum whole point is to protect this Quantum
state so this is not a good state so this is not a good state so this is not a good
option what we can do instead is to use option what we can do instead is to use option what we can do instead is to use
the quantum circuit shown here on the the quantum circuit shown here on the the quantum circuit shown here on the
screen where we feed the encoded State screen where we feed the encoded State screen where we feed the encoded State
into the top three into the top three into the top three
cubits basically what the circuit does cubits basically what the circuit does cubits basically what the circuit does
is to compute the parity of the standard is to compute the parity of the standard is to compute the parity of the standard
basis States for the top two cubits as basis States for the top two cubits as basis States for the top two cubits as
well as for the bottom two cubits of the well as for the bottom two cubits of the well as for the bottom two cubits of the
encoding and and this will reveal the encoding and and this will reveal the encoding and and this will reveal the
location of the bit flip in particular location of the bit flip in particular location of the bit flip in particular
in this case we'll get the outcome one in this case we'll get the outcome one in this case we'll get the outcome one
for both of these parody checks and from for both of these parody checks and from for both of these parody checks and from
that information we can conclude that that information we can conclude that that information we can conclude that
it's the middle bit that's different it's the middle bit that's different it's the middle bit that's different
from the other two so that's evidently from the other two so that's evidently from the other two so that's evidently
the one that the one that the one that
flipped and crucially this does not flipped and crucially this does not flipped and crucially this does not
cause the state to collapse so we now cause the state to collapse so we now cause the state to collapse so we now
have a chance to try to correct this bit have a chance to try to correct this bit have a chance to try to correct this bit
flip so that eventually we can perhaps flip so that eventually we can perhaps flip so that eventually we can perhaps
decode and get our original state decode and get our original state decode and get our original state
back we'll come back to the correction back we'll come back to the correction back we'll come back to the correction
in a moment but first let's briefly in a moment but first let's briefly in a moment but first let's briefly
consider the other possibilities for a consider the other possibilities for a consider the other possibilities for a
bit flip bit flip bit flip
error If instead the bit flip error error If instead the bit flip error error If instead the bit flip error
occurred on the bottom or left Mouse occurred on the bottom or left Mouse occurred on the bottom or left Mouse
Cubit then we would get the outcome one Cubit then we would get the outcome one Cubit then we would get the outcome one
Zer reading from bottom to top and again Zer reading from bottom to top and again Zer reading from bottom to top and again
the state wouldn't the state wouldn't the state wouldn't
change and if the bit flip happened to change and if the bit flip happened to change and if the bit flip happened to
the top Cubit then the output would be the top Cubit then the output would be the top Cubit then the output would be
01 and once again the state doesn't change and by the way if there was no change and by the way if there was no
bit flip error at all bit flip error at all bit flip error at all
then the measurement outcome is 0 0 and then the measurement outcome is 0 0 and then the measurement outcome is 0 0 and
once again the state comes out unchanged once again the state comes out unchanged once again the state comes out unchanged
so let's summarize all this in the form so let's summarize all this in the form so let's summarize all this in the form
of a table where we list each of the of a table where we list each of the of a table where we list each of the
possible states that we obtain from at possible states that we obtain from at possible states that we obtain from at
most one bit flip the measurement most one bit flip the measurement most one bit flip the measurement
outcomes which we refer to as the outcomes which we refer to as the outcomes which we refer to as the
syndrome in the context of error syndrome in the context of error syndrome in the context of error
correction and the correction we need to correction and the correction we need to correction and the correction we need to
make in order to get the original make in order to get the original make in order to get the original
encoding back if there was no error the encoding back if there was no error the encoding back if there was no error the
syndrome is z Zer and nothing needs to syndrome is z Zer and nothing needs to syndrome is z Zer and nothing needs to
be done to fix the be done to fix the be done to fix the
state if the bottom or leftmost Cubit state if the bottom or leftmost Cubit state if the bottom or leftmost Cubit
experience to bit flip the syndrome is experience to bit flip the syndrome is experience to bit flip the syndrome is
one Zer and the appropriate correction one Zer and the appropriate correction one Zer and the appropriate correction
is to apply an xgate to the bottom or is to apply an xgate to the bottom or is to apply an xgate to the bottom or
leftmost Cubit and the other two cases leftmost Cubit and the other two cases leftmost Cubit and the other two cases
are are are
similar so if we apply the appropriate similar so if we apply the appropriate similar so if we apply the appropriate
correction based on the syndrome then correction based on the syndrome then correction based on the syndrome then
we'll be certain to recover the original we'll be certain to recover the original we'll be certain to recover the original
encoding in all four encoding in all four encoding in all four
cases once again we're only considering cases once again we're only considering cases once again we're only considering
the possibility that at most one bit the possibility that at most one bit the possibility that at most one bit
flip error occurred this wouldn't work flip error occurred this wouldn't work flip error occurred this wouldn't work
correctly if two or three bits flipped correctly if two or three bits flipped correctly if two or three bits flipped
and we also haven't considered the and we also haven't considered the and we also haven't considered the
possibility of other possibility of other possibility of other
errors now let's consider a different errors now let's consider a different errors now let's consider a different
type of error in the quantum setting bit type of error in the quantum setting bit type of error in the quantum setting bit
flip errors aren't the only errors that flip errors aren't the only errors that flip errors aren't the only errors that
we need to worry about for instance we we need to worry about for instance we we need to worry about for instance we
also have to worry about phase flip also have to worry about phase flip also have to worry about phase flip
errors or just phase flips for short errors or just phase flips for short errors or just phase flips for short
which are described by Z Gates or which are described by Z Gates or which are described by Z Gates or
equivalently poly Z equivalently poly Z equivalently poly Z
operations unfortunately the three-bit operations unfortunately the three-bit operations unfortunately the three-bit
repetition code completely fails to repetition code completely fails to repetition code completely fails to
detect phase detect phase detect phase
flips for instance supposing that we flips for instance supposing that we flips for instance supposing that we
encode a cubit using the 3-bit encode a cubit using the 3-bit encode a cubit using the 3-bit
repetition code just like before and a repetition code just like before and a repetition code just like before and a
phase flip error occurs on the middle phase flip error occurs on the middle phase flip error occurs on the middle
Cubit we'll obtain the state that's Cubit we'll obtain the state that's Cubit we'll obtain the state that's
shown on the screen which is exactly the shown on the screen which is exactly the shown on the screen which is exactly the
encoding that we would have gotten if a encoding that we would have gotten if a encoding that we would have gotten if a
phase flip had happened to the original phase flip had happened to the original phase flip had happened to the original
Cubit before we Cubit before we Cubit before we
encoded and so if we perform the same encoded and so if we perform the same encoded and so if we perform the same
circuit as before to try to detect an circuit as before to try to detect an circuit as before to try to detect an
error we would get the syndrome z0 which error we would get the syndrome z0 which error we would get the syndrome z0 which
wrongly suggests that no errors took wrongly suggests that no errors took wrongly suggests that no errors took
place so the 3 bit repetition code works place so the 3 bit repetition code works place so the 3 bit repetition code works
for at most one bit flip error but it for at most one bit flip error but it for at most one bit flip error but it
fails to detect phase flip errors in fails to detect phase flip errors in fails to detect phase flip errors in
fact it actually makes things worse in fact it actually makes things worse in fact it actually makes things worse in
some sense because now we have three some sense because now we have three some sense because now we have three
times as many cubits that could times as many cubits that could times as many cubits that could
potentially experience phase flip potentially experience phase flip potentially experience phase flip
errors we've observed that the three-bit errors we've observed that the three-bit errors we've observed that the three-bit
repetition code is completely oblivious repetition code is completely oblivious repetition code is completely oblivious
to phase flip errors so it doesn't seem to phase flip errors so it doesn't seem to phase flip errors so it doesn't seem
to be very helpful for dealing with this to be very helpful for dealing with this to be very helpful for dealing with this
sort of sort of sort of
error but in fact we can modify the error but in fact we can modify the error but in fact we can modify the
three- bit repetition code so that it three- bit repetition code so that it three- bit repetition code so that it
does detect phase flip errors and I'll does detect phase flip errors and I'll does detect phase flip errors and I'll
explain now how this can be explain now how this can be explain now how this can be
done here's the original circuit for done here's the original circuit for done here's the original circuit for
encoding a cubit using the 3-bit encoding a cubit using the 3-bit encoding a cubit using the 3-bit
repetition code and the way that we'll repetition code and the way that we'll repetition code and the way that we'll
modify it is very simple we just apply a modify it is very simple we just apply a modify it is very simple we just apply a
hadamar gate to each Cubit of the hadamar gate to each Cubit of the hadamar gate to each Cubit of the
encoding a hadamar gate transforms a encoding a hadamar gate transforms a encoding a hadamar gate transforms a
zero State into a plus State and a one zero State into a plus State and a one zero State into a plus State and a one
state into a minus state so the net state into a minus state so the net state into a minus state so the net
effect is that we obtain the state effect is that we obtain the state effect is that we obtain the state
that's shown on the screen where cat that's shown on the screen where cat that's shown on the screen where cat
Plus+ plus and Cat minus- minus are Plus+ plus and Cat minus- minus are Plus+ plus and Cat minus- minus are
shorthand notation for the tensor of shorthand notation for the tensor of shorthand notation for the tensor of
three plus States or three minus three plus States or three minus three plus States or three minus
States a face flip error or in other States a face flip error or in other States a face flip error or in other
words a zgate transforms plus States words a zgate transforms plus States words a zgate transforms plus States
into minus States and minus States into into minus States and minus States into into minus States and minus States into
plus States so this encoding will be plus States so this encoding will be plus States so this encoding will be
useful for detecting and correcting useful for detecting and correcting useful for detecting and correcting
phase flip errors and to do that we can phase flip errors and to do that we can phase flip errors and to do that we can
modify the error detection circuit from modify the error detection circuit from modify the error detection circuit from
before in the following way here's the before in the following way here's the before in the following way here's the
circuit from before and what we'll do is circuit from before and what we'll do is circuit from before and what we'll do is
simply to put hadamar gates at both the simply to put hadamar gates at both the simply to put hadamar gates at both the
beginning and the end as is shown on the beginning and the end as is shown on the beginning and the end as is shown on the
screen screen screen
the idea is very simple the first three the idea is very simple the first three the idea is very simple the first three
hadamar gates transform plus and minus hadamar gates transform plus and minus hadamar gates transform plus and minus
States back into zero and one States the States back into zero and one States the States back into zero and one States the
same parody checks us before take place same parody checks us before take place same parody checks us before take place
and then the second layer of hadamar and then the second layer of hadamar and then the second layer of hadamar
gates brings us back to our gates brings us back to our gates brings us back to our
encoding another way of thinking about encoding another way of thinking about encoding another way of thinking about
this is that we're basically just this is that we're basically just this is that we're basically just
transforming phase flip errors into bit transforming phase flip errors into bit transforming phase flip errors into bit
flip Errors By conjugating by hatar flip Errors By conjugating by hatar flip Errors By conjugating by hatar
operations we can by the way simplify operations we can by the way simplify operations we can by the way simplify
the circuit just a little bit this the circuit just a little bit this the circuit just a little bit this
circuit here is completely equivalent circuit here is completely equivalent circuit here is completely equivalent
and you can check this for yourself if and you can check this for yourself if and you can check this for yourself if
you're so inclined so if we put our you're so inclined so if we put our you're so inclined so if we put our
encoda Cubit into this circuit assuming encoda Cubit into this circuit assuming encoda Cubit into this circuit assuming
no face flip errors have occurred we get no face flip errors have occurred we get no face flip errors have occurred we get
the syndrome 0 0 indicating that there the syndrome 0 0 indicating that there the syndrome 0 0 indicating that there
were no phase flip were no phase flip were no phase flip
errors and if we do have a phase flip errors and if we do have a phase flip errors and if we do have a phase flip
error on the top Cubit let's say then error on the top Cubit let's say then error on the top Cubit let's say then
the rightmost pluses and minuses flip the rightmost pluses and minuses flip the rightmost pluses and minuses flip
and that results in a different error and that results in a different error and that results in a different error
syndrome 01 in this case that identifies syndrome 01 in this case that identifies syndrome 01 in this case that identifies
the location of the phase flip just like the location of the phase flip just like the location of the phase flip just like
we had for bit flip we had for bit flip we had for bit flip
and we can then correct it by applying a and we can then correct it by applying a and we can then correct it by applying a
zgate to the top Cubit and the situation zgate to the top Cubit and the situation zgate to the top Cubit and the situation
is similar for face flip errors on is similar for face flip errors on is similar for face flip errors on
either of the other two either of the other two either of the other two
cubits so we can in fact detect and cubits so we can in fact detect and cubits so we can in fact detect and
correct up to one phase flip error using correct up to one phase flip error using correct up to one phase flip error using
this code but unfortunately as you may this code but unfortunately as you may this code but unfortunately as you may
have guessed we can now no longer have guessed we can now no longer have guessed we can now no longer
correct bit flip correct bit flip correct bit flip
errors all is not lost however because errors all is not lost however because errors all is not lost however because
we can in fact combine these two codes we can in fact combine these two codes we can in fact combine these two codes
together to obtain a code that can together to obtain a code that can together to obtain a code that can
correct both bit flip and fade flip correct both bit flip and fade flip correct both bit flip and fade flip
errors and in fact it will be able to errors and in fact it will be able to errors and in fact it will be able to
correct for any error on a single correct for any error on a single correct for any error on a single
Cubit it's now time to discuss the 9 Cubit it's now time to discuss the 9 Cubit it's now time to discuss the 9
Cubit Shore code which is a Quantum Cubit Shore code which is a Quantum Cubit Shore code which is a Quantum
error correcting code that we can obtain error correcting code that we can obtain error correcting code that we can obtain
by combining together the two codes that by combining together the two codes that by combining together the two codes that
we just saw meaning the three bit we just saw meaning the three bit we just saw meaning the three bit
repetition code that allows us to repetition code that allows us to repetition code that allows us to
correct a single bit flip error and the correct a single bit flip error and the correct a single bit flip error and the
modified version of that code that modified version of that code that modified version of that code that
allows us to correct a single phase flip allows us to correct a single phase flip allows us to correct a single phase flip
error to be more precise the 9 Cubit sh error to be more precise the 9 Cubit sh error to be more precise the 9 Cubit sh
code is the code that we obtain by code is the code that we obtain by code is the code that we obtain by
concatenating the two codes which means concatenating the two codes which means concatenating the two codes which means
that we first apply one encoding which that we first apply one encoding which that we first apply one encoding which
encodes one cubit into three and then we encodes one cubit into three and then we encodes one cubit into three and then we
apply the other encoding to each of the apply the other encoding to each of the apply the other encoding to each of the
three cubits separately so we'll get three cubits separately so we'll get three cubits separately so we'll get
nine cubits in nine cubits in nine cubits in
total we can express this in the form of total we can express this in the form of total we can express this in the form of
a circuit diagram as a circuit diagram as a circuit diagram as
follows first we'll apply the modified follows first we'll apply the modified follows first we'll apply the modified
version of the 3-bit repetition code version of the 3-bit repetition code version of the 3-bit repetition code
that detects phase flip errors and then that detects phase flip errors and then that detects phase flip errors and then
we'll encode each of the resulting we'll encode each of the resulting we'll encode each of the resulting
cubits independently using the original cubits independently using the original cubits independently using the original
3-bit repetition code 3-bit repetition code 3-bit repetition code
so if we shrink this circuit down just a so if we shrink this circuit down just a so if we shrink this circuit down just a
little bit to make some more room we can little bit to make some more room we can little bit to make some more room we can
specify the code in somewhat different specify the code in somewhat different specify the code in somewhat different
terms by describing how the two standard terms by describing how the two standard terms by describing how the two standard
basis States for our original Cubit get basis States for our original Cubit get basis States for our original Cubit get
encoded and once we know this we can encoded and once we know this we can encoded and once we know this we can
determine by linearity how an arbitrary determine by linearity how an arbitrary determine by linearity how an arbitrary
Cubit State Vector gets Cubit State Vector gets Cubit State Vector gets
encoded so that is the 9 Cubit Shore encoded so that is the 9 Cubit Shore encoded so that is the 9 Cubit Shore
code and the idea is that one of the code and the idea is that one of the code and the idea is that one of the
codes allows us to correct phase flip codes allows us to correct phase flip codes allows us to correct phase flip
errors and the other code allows us to errors and the other code allows us to errors and the other code allows us to
correct bit flip errors and as we'll see correct bit flip errors and as we'll see correct bit flip errors and as we'll see
that will in fact imply that we can that will in fact imply that we can that will in fact imply that we can
correct an arbitrary error on a single correct an arbitrary error on a single correct an arbitrary error on a single
Cubit now let's see how we can detect Cubit now let's see how we can detect Cubit now let's see how we can detect
and correct errors using the N9 Cubit and correct errors using the N9 Cubit and correct errors using the N9 Cubit
Shore code starting with bit flip errors Shore code starting with bit flip errors Shore code starting with bit flip errors
which are also called X which are also called X which are also called X
errors here's the circuit that performs errors here's the circuit that performs errors here's the circuit that performs
the encoding and just to make it easier the encoding and just to make it easier the encoding and just to make it easier
to talk about we'll think about the nine to talk about we'll think about the nine to talk about we'll think about the nine
cubits we get from this code as being cubits we get from this code as being cubits we get from this code as being
grouped into three blocks of three grouped into three blocks of three grouped into three blocks of three
cubits where each block represents the cubits where each block represents the cubits where each block represents the
encoding that we get from applying the encoding that we get from applying the encoding that we get from applying the
second encoding step which is the second encoding step which is the second encoding step which is the
ordinary 3-bit repetition ordinary 3-bit repetition ordinary 3-bit repetition
code that's called the inner code in code that's called the inner code in code that's called the inner code in
this context whereas the outer code is this context whereas the outer code is this context whereas the outer code is
the code used for the first encoding the code used for the first encoding the code used for the first encoding
step which in this case is the modified step which in this case is the modified step which in this case is the modified
version of the three-bit repetition code version of the three-bit repetition code version of the three-bit repetition code
that works for phase flip that works for phase flip that works for phase flip
errors to correct bit flip errors we can errors to correct bit flip errors we can errors to correct bit flip errors we can
simply treat each of the blocks simply treat each of the blocks simply treat each of the blocks
separately each block is an encoding of separately each block is an encoding of separately each block is an encoding of
a qbit using the three-bit repetition a qbit using the three-bit repetition a qbit using the three-bit repetition
code and that code protects against bit code and that code protects against bit code and that code protects against bit
flip errors suppose first that an X flip errors suppose first that an X flip errors suppose first that an X
error occurs on any one of the cubits error occurs on any one of the cubits error occurs on any one of the cubits
such as the middle cubit in the middle such as the middle cubit in the middle such as the middle cubit in the middle
block if we independently check each of block if we independently check each of block if we independently check each of
the blocks for bit flip errors and the blocks for bit flip errors and the blocks for bit flip errors and
correct for any errors we find then this correct for any errors we find then this correct for any errors we find then this
error will be corrected and the original error will be corrected and the original error will be corrected and the original
encoding will be encoding will be encoding will be
recovered in fact we could even have recovered in fact we could even have recovered in fact we could even have
multiple X errors and still correct them multiple X errors and still correct them multiple X errors and still correct them
in this way provided that the errors are in this way provided that the errors are in this way provided that the errors are
in separate in separate in separate
blocks if we had two or three x errors blocks if we had two or three x errors blocks if we had two or three x errors
in a single block we'd be in trouble but in a single block we'd be in trouble but in a single block we'd be in trouble but
as long as there's at most One X error as long as there's at most One X error as long as there's at most One X error
per block the inner code will catch them per block the inner code will catch them per block the inner code will catch them
and we'll be able to recover the and we'll be able to recover the and we'll be able to recover the
original encoded original encoded original encoded
state so correcting bit flip errors is a state so correcting bit flip errors is a state so correcting bit flip errors is a
pretty simple matter for this code we pretty simple matter for this code we pretty simple matter for this code we
also need to consider phas flip errors also need to consider phas flip errors also need to consider phas flip errors
or in other words Z errors this time or in other words Z errors this time or in other words Z errors this time
it's not quite so clear what we should it's not quite so clear what we should it's not quite so clear what we should
do because the outer code is the one do because the outer code is the one do because the outer code is the one
that corrects for phase flip errors but that corrects for phase flip errors but that corrects for phase flip errors but
we can't directly detect and correct we can't directly detect and correct we can't directly detect and correct
them because the inner code is somehow them because the inner code is somehow them because the inner code is somehow
in the way but we'll see how to do this in the way but we'll see how to do this in the way but we'll see how to do this
shortly to better understand how x and z shortly to better understand how x and z shortly to better understand how x and z
errors work and the effects that they errors work and the effects that they errors work and the effects that they
have within Quantum circuits it's have within Quantum circuits it's have within Quantum circuits it's
helpful to observe a few simple helpful to observe a few simple helpful to observe a few simple
relationships between these errors and c relationships between these errors and c relationships between these errors and c
not Gates and this will also be helpful not Gates and this will also be helpful not Gates and this will also be helpful
in a later in a later in a later
lesson first let's start with X errors lesson first let's start with X errors lesson first let's start with X errors
or X Gates and how they relate to C not or X Gates and how they relate to C not or X Gates and how they relate to C not
Gates if we apply an X gate to the Gates if we apply an X gate to the Gates if we apply an X gate to the
Target Cubit of a cot and then apply the Target Cubit of a cot and then apply the Target Cubit of a cot and then apply the
C notot then that's equivalent to C notot then that's equivalent to C notot then that's equivalent to
swapping the order and doing the cot swapping the order and doing the cot swapping the order and doing the cot
first on the other hand if we apply an first on the other hand if we apply an first on the other hand if we apply an
xgate to the control Cubit of a cot and xgate to the control Cubit of a cot and xgate to the control Cubit of a cot and
then apply the cot we can't simply then apply the cot we can't simply then apply the cot we can't simply
switch the order but instead we have to switch the order but instead we have to switch the order but instead we have to
apply the xate to both cubits and you apply the xate to both cubits and you apply the xate to both cubits and you
can check all of these relationships on can check all of these relationships on can check all of these relationships on
your own if you wish by thinking about your own if you wish by thinking about your own if you wish by thinking about
what happens to standard basis States what happens to standard basis States what happens to standard basis States
for for for
instance the situation is similar for Z instance the situation is similar for Z instance the situation is similar for Z
errors except that the roles of the errors except that the roles of the errors except that the roles of the
control and Target cubits control and Target cubits control and Target cubits
switches in particular if we perform a switches in particular if we perform a switches in particular if we perform a
zgate on the control Cubit of a c knot zgate on the control Cubit of a c knot zgate on the control Cubit of a c knot
and then perform the C knot then that and then perform the C knot then that and then perform the C knot then that
has the same effect as performing the has the same effect as performing the has the same effect as performing the
zgate after the C zgate after the C zgate after the C
notot but if we perform the zgate on the notot but if we perform the zgate on the notot but if we perform the zgate on the
target Cubit instead then that's target Cubit instead then that's target Cubit instead then that's
equivalent to first performing the cot equivalent to first performing the cot equivalent to first performing the cot
and then applying a zgate to both and then applying a zgate to both and then applying a zgate to both
cubits and finally all of these cubits and finally all of these cubits and finally all of these
relationships are symmetric in the sense relationships are symmetric in the sense relationships are symmetric in the sense
that we can always swap The Ordering of that we can always swap The Ordering of that we can always swap The Ordering of
the gates on both sides of each equation the gates on both sides of each equation the gates on both sides of each equation
which is equivalent to taking the which is equivalent to taking the which is equivalent to taking the
inverse or the conjugate transpose of inverse or the conjugate transpose of inverse or the conjugate transpose of
both sides given that x z and c not both sides given that x z and c not both sides given that x z and c not
gates are all their own gates are all their own gates are all their own
inverses now let's consider phase flip inverses now let's consider phase flip inverses now let's consider phase flip
errors and how we can detect and correct errors and how we can detect and correct errors and how we can detect and correct
them using the 9 Cubit Shore them using the 9 Cubit Shore them using the 9 Cubit Shore
code here's our encoding circuit and code here's our encoding circuit and code here's our encoding circuit and
we'll continue to think about the nine we'll continue to think about the nine we'll continue to think about the nine
cubits used in the encoding as being cubits used in the encoding as being cubits used in the encoding as being
grouped into three blocks of three grouped into three blocks of three grouped into three blocks of three
cubits suppose that a z error occurs on cubits suppose that a z error occurs on cubits suppose that a z error occurs on
any one of the cubits such as the one any one of the cubits such as the one any one of the cubits such as the one
indicated here indicated here indicated here
using what we just observed about Z using what we just observed about Z using what we just observed about Z
Gates and cot Gates that's equivalent to Gates and cot Gates that's equivalent to Gates and cot Gates that's equivalent to
two Z errors prior to the cot one on the two Z errors prior to the cot one on the two Z errors prior to the cot one on the
control and one on the target control and one on the target control and one on the target
Cubit and these errors happen to commute Cubit and these errors happen to commute Cubit and these errors happen to commute
with the other cot in the block so we with the other cot in the block so we with the other cot in the block so we
can swap the ordering and here's what we can swap the ordering and here's what we can swap the ordering and here's what we
obtain the lower Z error doesn't do obtain the lower Z error doesn't do obtain the lower Z error doesn't do
anything because it's acting on a zero anything because it's acting on a zero anything because it's acting on a zero
State and we can slide everything else State and we can slide everything else State and we can slide everything else
over and obtain this diagram so what we over and obtain this diagram so what we over and obtain this diagram so what we
found is that a z a ER somewhere within found is that a z a ER somewhere within found is that a z a ER somewhere within
the middle block is equivalent to a z the middle block is equivalent to a z the middle block is equivalent to a z
error occurring on the middle Cubit error occurring on the middle Cubit error occurring on the middle Cubit
prior to the second encoding phase and prior to the second encoding phase and prior to the second encoding phase and
you can reason exactly the same thing you can reason exactly the same thing you can reason exactly the same thing
for a z error on any of the cubits a for a z error on any of the cubits a for a z error on any of the cubits a
phase flip on any Cubit within a block phase flip on any Cubit within a block phase flip on any Cubit within a block
is equivalent to a phase flip on the is equivalent to a phase flip on the is equivalent to a phase flip on the
corresponding Cubit prior to the inner corresponding Cubit prior to the inner corresponding Cubit prior to the inner
encoding and that's actually pretty encoding and that's actually pretty encoding and that's actually pretty
straightforward and intuitive if you straightforward and intuitive if you straightforward and intuitive if you
think about how zrs affect the three-bit think about how zrs affect the three-bit think about how zrs affect the three-bit
repetition repetition repetition
code so one option for dealing with a z code so one option for dealing with a z code so one option for dealing with a z
error is to decode the inner code and error is to decode the inner code and error is to decode the inner code and
then detect and correct Z errors using then detect and correct Z errors using then detect and correct Z errors using
the outer code which is the modified the outer code which is the modified the outer code which is the modified
version of the 3-bit repetition code version of the 3-bit repetition code version of the 3-bit repetition code
that can correct for a z that can correct for a z that can correct for a z
error so that corrects our Z error and error so that corrects our Z error and error so that corrects our Z error and
we could re-encode with the inner code we could re-encode with the inner code we could re-encode with the inner code
if we wanted to do if we wanted to do if we wanted to do
that we can also correct Z errors in that we can also correct Z errors in that we can also correct Z errors in
place so to speak without decoding and place so to speak without decoding and place so to speak without decoding and
then re-encoding and it's pretty then re-encoding and it's pretty then re-encoding and it's pretty
straightforward but to explain how we straightforward but to explain how we straightforward but to explain how we
can do that we'll need a little bit more can do that we'll need a little bit more can do that we'll need a little bit more
space space space
suppose that we decode the inner code suppose that we decode the inner code suppose that we decode the inner code
and then we compute the syndrome for the and then we compute the syndrome for the and then we compute the syndrome for the
outer code using the same method as outer code using the same method as outer code using the same method as
before where we're only involving the before where we're only involving the before where we're only involving the
three cubits from the outer code and three cubits from the outer code and three cubits from the outer code and
then we then we then we
re-encode well we can simplify all of re-encode well we can simplify all of re-encode well we can simplify all of
these cots just a little bit and in fact these cots just a little bit and in fact these cots just a little bit and in fact
we can use the relationships between not we can use the relationships between not we can use the relationships between not
Gates and cot Gates that we just Gates and cot Gates that we just Gates and cot Gates that we just
observed a few moments ago to help with observed a few moments ago to help with observed a few moments ago to help with
that that that
task and by doing this we can save four task and by doing this we can save four task and by doing this we can save four
cots and this is the circuit that we cots and this is the circuit that we cots and this is the circuit that we
obtain if a z error has occurred on one obtain if a z error has occurred on one obtain if a z error has occurred on one
of the cubits what the syndrome tells us of the cubits what the syndrome tells us of the cubits what the syndrome tells us
now is not which Cubit experienced a z now is not which Cubit experienced a z now is not which Cubit experienced a z
error but rather which block the Z error error but rather which block the Z error error but rather which block the Z error
happened in but that's okay because we happened in but that's okay because we happened in but that's okay because we
know that it doesn't matter which Cubit know that it doesn't matter which Cubit know that it doesn't matter which Cubit
within a block experiences a z error the within a block experiences a z error the within a block experiences a z error the
effect is the same for all three cubits effect is the same for all three cubits effect is the same for all three cubits
within that within that within that
block and so we can in fact correct the block and so we can in fact correct the block and so we can in fact correct the
Z error by applying a zgate to any of Z error by applying a zgate to any of Z error by applying a zgate to any of
the cubits within that the cubits within that the cubits within that
block this by the way is an example of block this by the way is an example of block this by the way is an example of
what's called degeneracy in an error what's called degeneracy in an error what's called degeneracy in an error
correcting code where we don't uniquely correcting code where we don't uniquely correcting code where we don't uniquely
identify a given error but we're able to identify a given error but we're able to identify a given error but we're able to
correct it correct it correct it
anyway finally let's suppose that both a anyway finally let's suppose that both a anyway finally let's suppose that both a
bit flip and a face flip error occur bit flip and a face flip error occur bit flip and a face flip error occur
possibly on the same possibly on the same possibly on the same
Cubit for example let's suppose that a Cubit for example let's suppose that a Cubit for example let's suppose that a
bit flip and a face flip error happens bit flip and a face flip error happens bit flip and a face flip error happens
on this cubbit right on this cubbit right on this cubbit right
here as it turns out these errors can be here as it turns out these errors can be here as it turns out these errors can be
detected and corrected completely detected and corrected completely detected and corrected completely
independently and in fact it doesn't independently and in fact it doesn't independently and in fact it doesn't
matter if we correct each block for a matter if we correct each block for a matter if we correct each block for a
bit flip first and then correct for phas bit flip first and then correct for phas bit flip first and then correct for phas
flips or if we do it the other way flips or if we do it the other way flips or if we do it the other way
around to see this let's first observe around to see this let's first observe around to see this let's first observe
that the ordering of these errors that the ordering of these errors that the ordering of these errors
doesn't matter so we can flip them doesn't matter so we can flip them doesn't matter so we can flip them
around if we around if we around if we
wish the reason for that is not because wish the reason for that is not because wish the reason for that is not because
x and z commute because they don't x * Z x and z commute because they don't x * Z x and z commute because they don't x * Z
is equal to negative Z * X but the is equal to negative Z * X but the is equal to negative Z * X but the
negative 1 Factor has no effect in this negative 1 Factor has no effect in this negative 1 Factor has no effect in this
situation because it's a global phase we situation because it's a global phase we situation because it's a global phase we
can now move the Z error just like can now move the Z error just like can now move the Z error just like
before and at this point it's evident before and at this point it's evident before and at this point it's evident
that if we were first to correct each that if we were first to correct each that if we were first to correct each
block for X errors and then we corrected block for X errors and then we corrected block for X errors and then we corrected
for Z errors we would then have for Z errors we would then have for Z errors we would then have
corrected both corrected both corrected both
errors alternatively if for some reason errors alternatively if for some reason errors alternatively if for some reason
we decided to correct for Z erors first we decided to correct for Z erors first we decided to correct for Z erors first
we could do that and the XR has we could do that and the XR has we could do that and the XR has
absolutely no effect in that process absolutely no effect in that process absolutely no effect in that process
that has to be true because number one that has to be true because number one that has to be true because number one
the X error commutes with every single the X error commutes with every single the X error commutes with every single
gate in the circuit that computes the gate in the circuit that computes the gate in the circuit that computes the
syndrome for the outer code and number syndrome for the outer code and number syndrome for the outer code and number
two the ordering between the X eror and two the ordering between the X eror and two the ordering between the X eror and
the zgate that we apply to make the the zgate that we apply to make the the zgate that we apply to make the
correction doesn't matter just like we correction doesn't matter just like we correction doesn't matter just like we
argued when the zgate was an error so argued when the zgate was an error so argued when the zgate was an error so
that allows for the correction of the Z that allows for the correction of the Z that allows for the correction of the Z
error and we can then correct each block error and we can then correct each block error and we can then correct each block
to eliminate the X to eliminate the X to eliminate the X
error so what we've argued is that we error so what we've argued is that we error so what we've argued is that we
can correct an X error a z error or both can correct an X error a z error or both can correct an X error a z error or both
on any one of the cubits using this code on any one of the cubits using this code on any one of the cubits using this code
in fact we can correct for more errors in fact we can correct for more errors in fact we can correct for more errors
than that such as multiple X errors as than that such as multiple X errors as than that such as multiple X errors as
long as they fall into different blocks long as they fall into different blocks long as they fall into different blocks
or multiple Z errors as long as at most or multiple Z errors as long as at most or multiple Z errors as long as at most
one block gets an odd number of them but one block gets an odd number of them but one block gets an odd number of them but
going forward what will be most relevant going forward what will be most relevant going forward what will be most relevant
is that we can correct an X error a z is that we can correct an X error a z is that we can correct an X error a z
error or both on any one error or both on any one error or both on any one
cubit and in the last part of the lesson cubit and in the last part of the lesson cubit and in the last part of the lesson
we'll see how this is in fact enough to we'll see how this is in fact enough to we'll see how this is in fact enough to
conclude that this code can correct for conclude that this code can correct for conclude that this code can correct for
an arbitrary error on any one an arbitrary error on any one an arbitrary error on any one
cubit before we move on to the last part cubit before we move on to the last part cubit before we move on to the last part
of the lesson in which we'll consider of the lesson in which we'll consider of the lesson in which we'll consider
arbitrary Quantum errors we'll briefly arbitrary Quantum errors we'll briefly arbitrary Quantum errors we'll briefly
consider the performance of the N Cubit consider the performance of the N Cubit consider the performance of the N Cubit
short code when errors represented by short code when errors represented by short code when errors represented by
poly metrices occur randomly on the poly metrices occur randomly on the poly metrices occur randomly on the
cubits to be more concrete let's cubits to be more concrete let's cubits to be more concrete let's
consider a simple noise model where consider a simple noise model where consider a simple noise model where
errors occur independently on the cubits errors occur independently on the cubits errors occur independently on the cubits
with each Cubit experiencing an error with each Cubit experiencing an error with each Cubit experiencing an error
with probability p and with no with probability p and with no with probability p and with no
correlation between errors on different correlation between errors on different correlation between errors on different
cubits the worst case scenario for the 9 cubits the worst case scenario for the 9 cubits the worst case scenario for the 9
Cubit sh code is that a y error or in Cubit sh code is that a y error or in Cubit sh code is that a y error or in
other words both an X error and a z other words both an X error and a z other words both an X error and a z
error occurs on each of fected Cubit and error occurs on each of fected Cubit and error occurs on each of fected Cubit and
we'll assume this worst case scenario we'll assume this worst case scenario we'll assume this worst case scenario
just for the sake of obtaining a bound just for the sake of obtaining a bound just for the sake of obtaining a bound
on the code's on the code's on the code's
performance but you could also consider performance but you could also consider performance but you could also consider
for instance that the three poly errors for instance that the three poly errors for instance that the three poly errors
are equally are equally are equally
likely now suppose that Q is a cubit in likely now suppose that Q is a cubit in likely now suppose that Q is a cubit in
some particular state that we'd like to some particular state that we'd like to some particular state that we'd like to
protect against errors and imagine that protect against errors and imagine that protect against errors and imagine that
we have the option to use the 9 Cubit we have the option to use the 9 Cubit we have the option to use the 9 Cubit
Shore code should we use Shore code should we use Shore code should we use
it well the answer is not necessarily it well the answer is not necessarily it well the answer is not necessarily
yes because if there's two much noise or yes because if there's two much noise or yes because if there's two much noise or
in other words p is too large using the in other words p is too large using the in other words p is too large using the
shore code could actually make things shore code could actually make things shore code could actually make things
worse just like we had for the worse just like we had for the worse just like we had for the
repetition code when p is larger than repetition code when p is larger than repetition code when p is larger than
1/2 but if p is small enough then the 1/2 but if p is small enough then the 1/2 but if p is small enough then the
answer is yes we should use the answer is yes we should use the answer is yes we should use the
code so let's see why this is and what code so let's see why this is and what code so let's see why this is and what
it means for p to be too large or small it means for p to be too large or small it means for p to be too large or small
enough for this enough for this enough for this
code the shore code can correct any poly code the shore code can correct any poly code the shore code can correct any poly
error on a single Cubit and in the worst error on a single Cubit and in the worst error on a single Cubit and in the worst
case where are poly errors are y errors case where are poly errors are y errors case where are poly errors are y errors
this means that the code successfully this means that the code successfully this means that the code successfully
protects Q with a probability given by protects Q with a probability given by protects Q with a probability given by
the expression shown on the the expression shown on the the expression shown on the
screen specifically the code works when screen specifically the code works when screen specifically the code works when
there are no errors and that happens there are no errors and that happens there are no errors and that happens
with probability 1 minus p ^ 9 because with probability 1 minus p ^ 9 because with probability 1 minus p ^ 9 because
there are nine cubits that must each be there are nine cubits that must each be there are nine cubits that must each be
unaffected and it works when there's unaffected and it works when there's unaffected and it works when there's
exactly one error and considering that exactly one error and considering that exactly one error and considering that
there are nine different cubits that there are nine different cubits that there are nine different cubits that
could possibly be affected this happens could possibly be affected this happens could possibly be affected this happens
with probability 9 * P * 1 - p ^ 8 with probability 9 * P * 1 - p ^ 8 with probability 9 * P * 1 - p ^ 8
on the other hand if we didn't bother on the other hand if we didn't bother on the other hand if we didn't bother
using the code our one and only Cubit using the code our one and only Cubit using the code our one and only Cubit
would be unaffected by an error with would be unaffected by an error with would be unaffected by an error with
probability one minus probability one minus probability one minus
P so the code only helps when the first P so the code only helps when the first P so the code only helps when the first
probability is larger than the second probability is larger than the second probability is larger than the second
because these Expressions represent our because these Expressions represent our because these Expressions represent our
probabilities of success in the two probabilities of success in the two probabilities of success in the two
cases when we use the code and when we cases when we use the code and when we cases when we use the code and when we
don't here's a plot of these two don't here's a plot of these two don't here's a plot of these two
functions for small values of p and we functions for small values of p and we functions for small values of p and we
find that the break even Point occurs at find that the break even Point occurs at find that the break even Point occurs at
about 3 and a/4 about 3 and a/4 about 3 and a/4
per. so if p is smaller than this value per. so if p is smaller than this value per. so if p is smaller than this value
then the code then the code then the code
helps at the break even point the helps at the break even point the helps at the break even point the
probabilities are equal so we're just probabilities are equal so we're just probabilities are equal so we're just
wasting our time along with 8 cubits if wasting our time along with 8 cubits if wasting our time along with 8 cubits if
we use the code but otherwise there's no we use the code but otherwise there's no we use the code but otherwise there's no
harm in it and beyond that point we harm in it and beyond that point we harm in it and beyond that point we
should absolutely not be using this code should absolutely not be using this code should absolutely not be using this code
because it's actually increasing the because it's actually increasing the because it's actually increasing the
chance of an error on the encoded Cubit chance of an error on the encoded Cubit chance of an error on the encoded Cubit
which we typically refer to as the which we typically refer to as the which we typically refer to as the
logical cubit in this context and that's logical cubit in this context and that's logical cubit in this context and that's
happening simply because the chance of happening simply because the chance of happening simply because the chance of
two or more errors on nine cubits is two or more errors on nine cubits is two or more errors on nine cubits is
larger than the chance of one error on a larger than the chance of one error on a larger than the chance of one error on a
single Cubit for values of p in this single Cubit for values of p in this single Cubit for values of p in this
range 3 and a/ qu% may not seem very range 3 and a/ qu% may not seem very range 3 and a/ qu% may not seem very
good particularly when compared to 50% good particularly when compared to 50% good particularly when compared to 50%
which is the analogous Break Even point which is the analogous Break Even point which is the analogous Break Even point
for the three-bit repetition code for for the three-bit repetition code for for the three-bit repetition code for
classical information but it's harder to classical information but it's harder to classical information but it's harder to
protect Quantum protect Quantum protect Quantum
information and moreover although this information and moreover although this information and moreover although this
code represents a brilliant Discovery as code represents a brilliant Discovery as code represents a brilliant Discovery as
the very first Quantum error correcting the very first Quantum error correcting the very first Quantum error correcting
code and it happens to be great for code and it happens to be great for code and it happens to be great for
pedagogical purposes the fact of the pedagogical purposes the fact of the pedagogical purposes the fact of the
matter is that it's just not all that matter is that it's just not all that matter is that it's just not all that
good of a code in Practical good of a code in Practical good of a code in Practical
terms so far we've considered X errors terms so far we've considered X errors terms so far we've considered X errors
and Z errors in the context of the 9 and Z errors in the context of the 9 and Z errors in the context of the 9
Cubit Shore Cubit Shore Cubit Shore
code now let's consider arbitrary errors code now let's consider arbitrary errors code now let's consider arbitrary errors
starting first with unitary starting first with unitary starting first with unitary
errors so for example such an error errors so for example such an error errors so for example such an error
could correspond to a very small could correspond to a very small could correspond to a very small
rotation of the block sphere possibly rotation of the block sphere possibly rotation of the block sphere possibly
representing an error that's incurred by representing an error that's incurred by representing an error that's incurred by
a gate that isn't perfect for instance a gate that isn't perfect for instance a gate that isn't perfect for instance
or it could be any other unitary or it could be any other unitary or it could be any other unitary
operation on a cubit and not necessarily operation on a cubit and not necessarily operation on a cubit and not necessarily
something close to the identity something close to the identity something close to the identity
operation it might seem like correcting operation it might seem like correcting operation it might seem like correcting
for such errors is difficult there are for such errors is difficult there are for such errors is difficult there are
infinitely many possible errors like infinitely many possible errors like infinitely many possible errors like
this and it's inconceivable that we this and it's inconceivable that we this and it's inconceivable that we
could somehow identify exactly what could somehow identify exactly what could somehow identify exactly what
error it is and then undo it but we error it is and then undo it but we error it is and then undo it but we
don't actually need to do anything don't actually need to do anything don't actually need to do anything
different from what we've already different from what we've already different from what we've already
discussed as long as we can correct for discussed as long as we can correct for discussed as long as we can correct for
a bit flip a phase Flip or both then we a bit flip a phase Flip or both then we a bit flip a phase Flip or both then we
will in fact suc succeed in correcting will in fact suc succeed in correcting will in fact suc succeed in correcting
for an arbitrary single Cubit unitary for an arbitrary single Cubit unitary for an arbitrary single Cubit unitary
error and I'll now explain why that error and I'll now explain why that error and I'll now explain why that
is first like all 2x two matrices we can is first like all 2x two matrices we can is first like all 2x two matrices we can
express an arbitrary unitary Matrix express an arbitrary unitary Matrix express an arbitrary unitary Matrix
representing an error on a single Cubit representing an error on a single Cubit representing an error on a single Cubit
as a linear combination of the four poly as a linear combination of the four poly as a linear combination of the four poly
matrices including the identity matrices including the identity matrices including the identity
Matrix and by the way as you may have Matrix and by the way as you may have Matrix and by the way as you may have
noticed I'm using capital X Y and Z to noticed I'm using capital X Y and Z to noticed I'm using capital X Y and Z to
denote the non-identity polym matrices denote the non-identity polym matrices denote the non-identity polym matrices
and I'll continue to do that for the and I'll continue to do that for the and I'll continue to do that for the
remainder of this remainder of this remainder of this
series that's conventional in Quantum series that's conventional in Quantum series that's conventional in Quantum
error correction and it also makes error correction and it also makes error correction and it also makes
things a bit easier to read in this things a bit easier to read in this things a bit easier to read in this
context it's also going to be convenient context it's also going to be convenient context it's also going to be convenient
to use subscripts to indicate which to use subscripts to indicate which to use subscripts to indicate which
Cubit a given Cubit unitary operation Cubit a given Cubit unitary operation Cubit a given Cubit unitary operation
acts on so for example if we use Kit's acts on so for example if we use Kit's acts on so for example if we use Kit's
Cubit numbering convention starting from Cubit numbering convention starting from Cubit numbering convention starting from
Cubit zero on the right then we have the Cubit zero on the right then we have the Cubit zero on the right then we have the
Expressions shown on the screen for Expressions shown on the screen for Expressions shown on the screen for
various unitary operations on single various unitary operations on single various unitary operations on single
cubits where in each case we tensor the cubits where in each case we tensor the cubits where in each case we tensor the
unitary Matrix with the identity Matrix unitary Matrix with the identity Matrix unitary Matrix with the identity Matrix
on every other on every other on every other
Cubit so in particular we can express UK Cubit so in particular we can express UK Cubit so in particular we can express UK
meaning U applied to Cubit K as the same meaning U applied to Cubit K as the same meaning U applied to Cubit K as the same
linear combination as before except that linear combination as before except that linear combination as before except that
each poly Matrix is applied to Cubit K each poly Matrix is applied to Cubit K each poly Matrix is applied to Cubit K
we don't need to bother with the we don't need to bother with the we don't need to bother with the
subscript for the identity by the way subscript for the identity by the way subscript for the identity by the way
the understanding here is that this is the understanding here is that this is the understanding here is that this is
the identity on all the the identity on all the the identity on all the
cubits now suppose that SII is the 9 cubits now suppose that SII is the 9 cubits now suppose that SII is the 9
Cubit encoding of a cubit Cubit encoding of a cubit Cubit encoding of a cubit
state if the the error U happens to state if the the error U happens to state if the the error U happens to
Cubit k then we obtain u k * s which we Cubit k then we obtain u k * s which we Cubit k then we obtain u k * s which we
can express as a linear combination of can express as a linear combination of can express as a linear combination of
poly operations acting on S which of poly operations acting on S which of poly operations acting on S which of
course is just s in the case of the course is just s in the case of the course is just s in the case of the
identity at this point it's going to be identity at this point it's going to be identity at this point it's going to be
helpful to observe that the poly y helpful to observe that the poly y helpful to observe that the poly y
Matrix is equal to I * x * Z so we can Matrix is equal to I * x * Z so we can Matrix is equal to I * x * Z so we can
make that substitution in our make that substitution in our make that substitution in our
expression so this error has taken place expression so this error has taken place expression so this error has taken place
and now suppose that we attempt to and now suppose that we attempt to and now suppose that we attempt to
correct it using the same procedure as correct it using the same procedure as correct it using the same procedure as
four the first step is to compute the four the first step is to compute the four the first step is to compute the
syndrome which in the case of the sh syndrome which in the case of the sh syndrome which in the case of the sh
code is 8 Bits long two bits for each of code is 8 Bits long two bits for each of code is 8 Bits long two bits for each of
the three times we use the inner code the three times we use the inner code the three times we use the inner code
along with an additional two bits for along with an additional two bits for along with an additional two bits for
the outer code but it doesn't really the outer code but it doesn't really the outer code but it doesn't really
matter we can just think about it as matter we can just think about it as matter we can just think about it as
some fixed length some fixed length some fixed length
string here's a description of the state string here's a description of the state string here's a description of the state
that we get just prior to the that we get just prior to the that we get just prior to the
measurement of the syndrome where each measurement of the syndrome where each measurement of the syndrome where each
syndrome is whatever eight bits we get syndrome is whatever eight bits we get syndrome is whatever eight bits we get
from the corresponding poly from the corresponding poly from the corresponding poly
error notice at this point that the two error notice at this point that the two error notice at this point that the two
systems are Cor ated in general and systems are Cor ated in general and systems are Cor ated in general and
really this is the key to making all of really this is the key to making all of really this is the key to making all of
this work by measuring the syndrome what this work by measuring the syndrome what this work by measuring the syndrome what
we'll effectively do is to collapse the we'll effectively do is to collapse the we'll effectively do is to collapse the
state into one where one of the four state into one where one of the four state into one where one of the four
poly operations has been applied to the poly operations has been applied to the poly operations has been applied to the
Ki cubi of sigh and we'll know from the Ki cubi of sigh and we'll know from the Ki cubi of sigh and we'll know from the
syndrome which poly operation it was so syndrome which poly operation it was so syndrome which poly operation it was so
we can correct it if we can correct it if we can correct it if
needed in particular if we measure the needed in particular if we measure the needed in particular if we measure the
syndrome and make the appropriate syndrome and make the appropriate syndrome and make the appropriate
correction then we obtain a state of the correction then we obtain a state of the correction then we obtain a state of the
form shown on the screen which form shown on the screen which form shown on the screen which
critically is a product State we have critically is a product State we have critically is a product State we have
our original uncorrupted encoding as the our original uncorrupted encoding as the our original uncorrupted encoding as the
right- hand tensor factor which is what right- hand tensor factor which is what right- hand tensor factor which is what
is most important here and on the left is most important here and on the left is most important here and on the left
we have a density Matrix s that we have a density Matrix s that we have a density Matrix s that
describes a random error syndrome but describes a random error syndrome but describes a random error syndrome but
there's no longer any correlation with there's no longer any correlation with there's no longer any correlation with
the system that we care about which is the system that we care about which is the system that we care about which is
the one on the right because the errors the one on the right because the errors the one on the right because the errors
have been have been have been
corrected at this point we can throw the corrected at this point we can throw the corrected at this point we can throw the
syndrome cubits away or reset them in syndrome cubits away or reset them in syndrome cubits away or reset them in
case we want to use them again and case we want to use them again and case we want to use them again and
that's actually pretty important because that's actually pretty important because that's actually pretty important because
this is how the randomness or entropy this is how the randomness or entropy this is how the randomness or entropy
generated by errors is removed from the generated by errors is removed from the generated by errors is removed from the
system and that's how the discretization system and that's how the discretization system and that's how the discretization
of Errors works at least for unitary of Errors works at least for unitary of Errors works at least for unitary
errors in essence by measuring the errors in essence by measuring the errors in essence by measuring the
syndrome we effectively project the syndrome we effectively project the syndrome we effectively project the
error onto an error that's described by error onto an error that's described by error onto an error that's described by
a poly Matrix at first glance it may a poly Matrix at first glance it may a poly Matrix at first glance it may
seem too good to be true that we can seem too good to be true that we can seem too good to be true that we can
correct for arbitrary unitary errors correct for arbitrary unitary errors correct for arbitrary unitary errors
like this even errors that are tiny and like this even errors that are tiny and like this even errors that are tiny and
hardly noticeable on their hardly noticeable on their hardly noticeable on their
own but what's important to realize here own but what's important to realize here own but what's important to realize here
is that this is a un error on a single is that this is a un error on a single is that this is a un error on a single
Cubit and by the design of the code a Cubit and by the design of the code a Cubit and by the design of the code a
single Cubit operation cannot change the single Cubit operation cannot change the single Cubit operation cannot change the
state of The Logical Cubit that's being state of The Logical Cubit that's being state of The Logical Cubit that's being
encoded all it can possibly do is to encoded all it can possibly do is to encoded all it can possibly do is to
move the state out of the space of valid move the state out of the space of valid move the state out of the space of valid
encodings but then the corrections bring encodings but then the corrections bring encodings but then the corrections bring
it right back to where it it right back to where it it right back to where it
was finally let's consider what happens was finally let's consider what happens was finally let's consider what happens
when an arbitrary not necessarily when an arbitrary not necessarily when an arbitrary not necessarily
unitary error unitary error unitary error
occurs the setup is the same as before occurs the setup is the same as before occurs the setup is the same as before
we've encoded one cubit into 9 using the we've encoded one cubit into 9 using the we've encoded one cubit into 9 using the
short code and some arbitrary error short code and some arbitrary error short code and some arbitrary error
represented by a cubit Channel thi represented by a cubit Channel thi represented by a cubit Channel thi
occurs on one of the occurs on one of the occurs on one of the
cubits as we'll see everything is going cubits as we'll see everything is going cubits as we'll see everything is going
to work out in pretty much the same way to work out in pretty much the same way to work out in pretty much the same way
as it did for unitary errors and the as it did for unitary errors and the as it did for unitary errors and the
first step toward verifying this is to first step toward verifying this is to first step toward verifying this is to
consider a cross representation of our consider a cross representation of our consider a cross representation of our
error what we can do now is to follow error what we can do now is to follow error what we can do now is to follow
exactly the same reasoning as for exactly the same reasoning as for exactly the same reasoning as for
unitary errors except that we apply that unitary errors except that we apply that unitary errors except that we apply that
reasoning to the Cross matrices rather reasoning to the Cross matrices rather reasoning to the Cross matrices rather
than to a single unitary Matrix than to a single unitary Matrix than to a single unitary Matrix
representing an error representing an error representing an error
in particular each cruss Matrix can be in particular each cruss Matrix can be in particular each cruss Matrix can be
expressed as a linear combination of expressed as a linear combination of expressed as a linear combination of
poly matrices and this allows us to poly matrices and this allows us to poly matrices and this allows us to
express the action of this error fi on express the action of this error fi on express the action of this error fi on
whatever cubic K it happens to in the whatever cubic K it happens to in the whatever cubic K it happens to in the
form that's shown here on the form that's shown here on the form that's shown here on the
screen all we're really doing here is to screen all we're really doing here is to screen all we're really doing here is to
expand out the cross matrices as linear expand out the cross matrices as linear expand out the cross matrices as linear
combinations of poly combinations of poly combinations of poly
matrices if we then compute and measure matrices if we then compute and measure matrices if we then compute and measure
the error syndrome and correct for any the error syndrome and correct for any the error syndrome and correct for any
errors that are revealed we'll obtain a errors that are revealed we'll obtain a errors that are revealed we'll obtain a
similar sort of state to what we had in similar sort of state to what we had in similar sort of state to what we had in
the case of a unitary the case of a unitary the case of a unitary
error I won't go through the details error I won't go through the details error I won't go through the details
it's all quite similar to the unitary it's all quite similar to the unitary it's all quite similar to the unitary
case and also a little bit Messier to case and also a little bit Messier to case and also a little bit Messier to
write down but conceptually speaking write down but conceptually speaking write down but conceptually speaking
it's pretty it's pretty it's pretty
straightforward I should mention that straightforward I should mention that straightforward I should mention that
this all generalizes to other Quantum this all generalizes to other Quantum this all generalizes to other Quantum
error correcting codes including codes error correcting codes including codes error correcting codes including codes
that can correct for errors on multiple that can correct for errors on multiple that can correct for errors on multiple
cubits in such a case errors on multiple cubits in such a case errors on multiple cubits in such a case errors on multiple
cubits can be expressed as linear cubits can be expressed as linear cubits can be expressed as linear
combinations of tensor products of poly combinations of tensor products of poly combinations of tensor products of poly
matrices and corresponding ly the matrices and corresponding ly the matrices and corresponding ly the
different possible syndromes will different possible syndromes will different possible syndromes will
describe Corrections that are performed describe Corrections that are performed describe Corrections that are performed
on multiple cubits rather than just one on multiple cubits rather than just one on multiple cubits rather than just one
cubit but the underlying idea is exactly cubit but the underlying idea is exactly cubit but the underlying idea is exactly
the same again by measuring the syndrome the same again by measuring the syndrome the same again by measuring the syndrome
we effectively project or collapse we effectively project or collapse we effectively project or collapse
errors into a discrete set of errors into a discrete set of errors into a discrete set of
possibilities represented by tensor possibilities represented by tensor possibilities represented by tensor
products of poly metrices and by products of poly metrices and by products of poly metrices and by
correcting those errors meaning the poly correcting those errors meaning the poly correcting those errors meaning the poly
errors we can recover the original errors we can recover the original errors we can recover the original
encoded encoded encoded
State meanwhile whatever Randomness is State meanwhile whatever Randomness is State meanwhile whatever Randomness is
generated in the process is moved into generated in the process is moved into generated in the process is moved into
the syndrome cubits which we can then the syndrome cubits which we can then the syndrome cubits which we can then
discard or reset which is crucial discard or reset which is crucial discard or reset which is crucial
because that's how the entropy generated because that's how the entropy generated because that's how the entropy generated
by errors and syndrome measurements is by errors and syndrome measurements is by errors and syndrome measurements is
removed from the removed from the removed from the
system and that is the end of the lesson system and that is the end of the lesson system and that is the end of the lesson
in which we've taken a first look at in which we've taken a first look at in which we've taken a first look at
Quantum error correction including the 9 Quantum error correction including the 9 Quantum error correction including the 9
Cubit Shore code and the discretization Cubit Shore code and the discretization Cubit Shore code and the discretization
of of of
Errors I hope you will join me for the Errors I hope you will join me for the Errors I hope you will join me for the
next lesson in which we'll discuss the next lesson in which we'll discuss the next lesson in which we'll discuss the
stabilizer formalism which provides us stabilizer formalism which provides us stabilizer formalism which provides us
with a powerful tool for describing and with a powerful tool for describing and with a powerful tool for describing and
analyzing a large and interesting class analyzing a large and interesting class analyzing a large and interesting class
of quantum error cting codes including of quantum error cting codes including of quantum error cting codes including
ones that are promising candidates for ones that are promising candidates for ones that are promising candidates for
real quantum computers goodbye until real quantum computers goodbye until real quantum computers goodbye until
then

## Density Matrices ｜ Understanding Quantum Information & Computation ｜ Lesson 09

welcome back to understanding Quantum welcome back to understanding Quantum
information and information and information and
computation my name is John watus and computation my name is John watus and computation my name is John watus and
I'm the technical director for education I'm the technical director for education I'm the technical director for education
at IBM at IBM at IBM
Quantum this is the ninth lesson of the Quantum this is the ninth lesson of the Quantum this is the ninth lesson of the
series and it's the first lesson in the series and it's the first lesson in the series and it's the first lesson in the
third unit which is on the general third unit which is on the general third unit which is on the general
formulation of quantum formulation of quantum formulation of quantum
information up until now we've been information up until now we've been information up until now we've been
using the simplified formulation of using the simplified formulation of using the simplified formulation of
quantum information where Quantum states quantum information where Quantum states quantum information where Quantum states
are represented by complex vectors are represented by complex vectors are represented by complex vectors
having ukian Norm equal to having ukian Norm equal to having ukian Norm equal to
one in in contrast in the general one in in contrast in the general one in in contrast in the general
formulation of quantum information formulation of quantum information formulation of quantum information
Quantum states are represented by Quantum states are represented by Quantum states are represented by
matrices of a certain type known as matrices of a certain type known as matrices of a certain type known as
density density density
matrices this first lesson of the unit matrices this first lesson of the unit matrices this first lesson of the unit
focuses on density matrices specifically focuses on density matrices specifically focuses on density matrices specifically
and we'll get to what they are and how and we'll get to what they are and how and we'll get to what they are and how
they work as the lesson they work as the lesson they work as the lesson
continues density matrices are extremely continues density matrices are extremely continues density matrices are extremely
useful and they offer various advantages useful and they offer various advantages useful and they offer various advantages
that we'll that we'll that we'll
discuss they allow us to represent a discuss they allow us to represent a discuss they allow us to represent a
broader class of States than Quantum broader class of States than Quantum broader class of States than Quantum
State vectors can including States of State vectors can including States of State vectors can including States of
systems that have been affected by noise systems that have been affected by noise systems that have been affected by noise
for instance as well as random for instance as well as random for instance as well as random
selections of quantum selections of quantum selections of quantum
States we'll also take a look at the States we'll also take a look at the States we'll also take a look at the
Block sphere which gives us a very nice Block sphere which gives us a very nice Block sphere which gives us a very nice
and simple geometric way of thinking and simple geometric way of thinking and simple geometric way of thinking
about Cubit States and we'll see how it about Cubit States and we'll see how it about Cubit States and we'll see how it
connects with density matrices in a connects with density matrices in a connects with density matrices in a
pretty remarkable way through the poly pretty remarkable way through the poly pretty remarkable way through the poly
matrices finally we'll discuss density matrices finally we'll discuss density matrices finally we'll discuss density
matrices in the context of multiple matrices in the context of multiple matrices in the context of multiple
systems and in particular we'll see how systems and in particular we'll see how systems and in particular we'll see how
they allow us to describe Quantum States they allow us to describe Quantum States they allow us to describe Quantum States
of systems viewed in isolation even when of systems viewed in isolation even when of systems viewed in isolation even when
they're correlated with other systems they're correlated with other systems they're correlated with other systems
and that includes the possibility that and that includes the possibility that and that includes the possibility that
they're entangled with other they're entangled with other they're entangled with other
systems this is an important thing to be systems this is an important thing to be systems this is an important thing to be
able to do and it's not easily done able to do and it's not easily done able to do and it's not easily done
within the simplified formulation of within the simplified formulation of within the simplified formulation of
quantum quantum quantum
information here's an overview of the information here's an overview of the information here's an overview of the
lesson we'll start with the basics of lesson we'll start with the basics of lesson we'll start with the basics of
density matrices including relevant density matrices including relevant density matrices including relevant
definitions and some examples and we'll definitions and some examples and we'll definitions and some examples and we'll
also discuss how they connect to Quantum also discuss how they connect to Quantum also discuss how they connect to Quantum
State State State
vectors we'll then talk about convex vectors we'll then talk about convex vectors we'll then talk about convex
combinations of density matrices or in combinations of density matrices or in combinations of density matrices or in
other words what happens when we take other words what happens when we take other words what happens when we take
weighted averages of density weighted averages of density weighted averages of density
matrices we'll start with a specific matrices we'll start with a specific matrices we'll start with a specific
example of the so-called completely example of the so-called completely example of the so-called completely
mixed state we'll see how classical mixed state we'll see how classical mixed state we'll see how classical
probabilistic States can be viewed as probabilistic States can be viewed as probabilistic States can be viewed as
special cases of density matrices and special cases of density matrices and special cases of density matrices and
we'll discuss how the spectral theorem we'll discuss how the spectral theorem we'll discuss how the spectral theorem
from linear algebra comes into play in from linear algebra comes into play in from linear algebra comes into play in
this this this
context the third part of the lesson is context the third part of the lesson is context the third part of the lesson is
on the Block sphere which provides a on the Block sphere which provides a on the Block sphere which provides a
geometric way of viewing states of geometric way of viewing states of geometric way of viewing states of
single cubits as I mentioned at the single cubits as I mentioned at the single cubits as I mentioned at the
start of the lesson and in the fourth start of the lesson and in the fourth start of the lesson and in the fourth
part of the lesson we'll talk about how part of the lesson we'll talk about how part of the lesson we'll talk about how
density matrices work for multiple density matrices work for multiple density matrices work for multiple
systems we'll discuss product States and systems we'll discuss product States and systems we'll discuss product States and
correlated states of different sorts correlated states of different sorts correlated states of different sorts
including entangled States as well as including entangled States as well as including entangled States as well as
the notion of a reduced state which is a the notion of a reduced state which is a the notion of a reduced state which is a
density Matrix description of the state density Matrix description of the state density Matrix description of the state
of an isolated part of a compound system of an isolated part of a compound system of an isolated part of a compound system
that can be calculated using an that can be calculated using an that can be calculated using an
operation known as the partial operation known as the partial operation known as the partial
Trace before we get to the technical Trace before we get to the technical Trace before we get to the technical
content of the lesson itself it's content of the lesson itself it's content of the lesson itself it's
fitting to take just a few moments to fitting to take just a few moments to fitting to take just a few moments to
motivate the use of density motivate the use of density motivate the use of density
matrices using the simplified matrices using the simplified matrices using the simplified
formulation of quantum information which formulation of quantum information which formulation of quantum information which
doesn't involve density matrices we're doesn't involve density matrices we're doesn't involve density matrices we're
able to describe Quantum algorithms like able to describe Quantum algorithms like able to describe Quantum algorithms like
shortest factoring algorithm for shortest factoring algorithm for shortest factoring algorithm for
instance without any difficulties so why instance without any difficulties so why instance without any difficulties so why
should we bother with density matrices should we bother with density matrices should we bother with density matrices
or this General formulation of quantum or this General formulation of quantum or this General formulation of quantum
information at information at information at
all well there are multiple all well there are multiple all well there are multiple
reasons for one density matrices reasons for one density matrices reasons for one density matrices
represent a broader class of quantum represent a broader class of quantum represent a broader class of quantum
States than Quantum State vectors do States than Quantum State vectors do States than Quantum State vectors do
including Quantum states where we have including Quantum states where we have including Quantum states where we have
uncertainty or uncertainty or uncertainty or
Randomness this is important when we Randomness this is important when we Randomness this is important when we
want to model the effects of noise on want to model the effects of noise on want to model the effects of noise on
Quantum computations for Quantum computations for Quantum computations for
instance second using density matrices instance second using density matrices instance second using density matrices
we can describe Quantum states of we can describe Quantum states of we can describe Quantum states of
isolated parts of compound systems such isolated parts of compound systems such isolated parts of compound systems such
as the states of systems that are as the states of systems that are as the states of systems that are
entangled with other systems that entangled with other systems that entangled with other systems that
perhaps momentarily we choose to perhaps momentarily we choose to perhaps momentarily we choose to
ignore as I mentioned at the start of ignore as I mentioned at the start of ignore as I mentioned at the start of
the lesson this is not easily or the lesson this is not easily or the lesson this is not easily or
naturally done within the simplified naturally done within the simplified naturally done within the simplified
formulation of quantum formulation of quantum formulation of quantum
information a third reason is that when information a third reason is that when information a third reason is that when
we use density matrices classical we use density matrices classical we use density matrices classical
information actually emerges as a information actually emerges as a information actually emerges as a
special case so instead of having what special case so instead of having what special case so instead of having what
is essentially an analogy between is essentially an analogy between is essentially an analogy between
probabilistic states and operations on probabilistic states and operations on probabilistic states and operations on
one hand and Quantum State vectors and one hand and Quantum State vectors and one hand and Quantum State vectors and
unitary operations on the other hand we unitary operations on the other hand we unitary operations on the other hand we
have Quantum and classical information have Quantum and classical information have Quantum and classical information
being described together within a single being described together within a single being described together within a single
mathematical framework which is pretty mathematical framework which is pretty mathematical framework which is pretty
much Essential to the mathematical study much Essential to the mathematical study much Essential to the mathematical study
of quantum of quantum of quantum
information so if you're serious about information so if you're serious about information so if you're serious about
understanding Quantum information at a understanding Quantum information at a understanding Quantum information at a
mathematical level it's highly mathematical level it's highly mathematical level it's highly
recommended that you learn about density recommended that you learn about density recommended that you learn about density
matrices let's begin with the definition matrices let's begin with the definition matrices let's begin with the definition
of density of density of density
matrices for the sake of the definition matrices for the sake of the definition matrices for the sake of the definition
suppose that X is a system and sigma is suppose that X is a system and sigma is suppose that X is a system and sigma is
its classical State set and recall that its classical State set and recall that its classical State set and recall that
when we use that terminology our when we use that terminology our when we use that terminology our
assumption is that Sigma is finite and assumption is that Sigma is finite and assumption is that Sigma is finite and
non-empty and as always we could choose non-empty and as always we could choose non-empty and as always we could choose
different names if we wanted to different names if we wanted to different names if we wanted to
a density Matrix describing a state of a density Matrix describing a state of a density Matrix describing a state of
the system X is a matrix having complex the system X is a matrix having complex the system X is a matrix having complex
number entries whose rows and columns number entries whose rows and columns number entries whose rows and columns
have been placed in correspondence with have been placed in correspondence with have been placed in correspondence with
its classical State set and to be clear its classical State set and to be clear its classical State set and to be clear
it's the same correspondence for the it's the same correspondence for the it's the same correspondence for the
rows and the columns rows and the columns rows and the columns
simultaneously this means that density simultaneously this means that density simultaneously this means that density
matrices are always Square matrices and matrices are always Square matrices and matrices are always Square matrices and
in addition to being Square there are a in addition to being Square there are a in addition to being Square there are a
couple of conditions that need to be couple of conditions that need to be couple of conditions that need to be
satisfied and we'll get to them satisfied and we'll get to them satisfied and we'll get to them
shortly people usually use certain shortly people usually use certain shortly people usually use certain
lowercase Greek letters to name density lowercase Greek letters to name density lowercase Greek letters to name density
matrices row is often the first choice matrices row is often the first choice matrices row is often the first choice
and other common choices include and other common choices include and other common choices include
lowercase Sigma andai which are shown on lowercase Sigma andai which are shown on lowercase Sigma andai which are shown on
the screen this is just a convention but the screen this is just a convention but the screen this is just a convention but
as usual these sorts of conventions are as usual these sorts of conventions are as usual these sorts of conventions are
helpful because they make things more helpful because they make things more helpful because they make things more
easily easily easily
recognizable and now let's talk about recognizable and now let's talk about recognizable and now let's talk about
the conditions or the requirements that the conditions or the requirements that the conditions or the requirements that
density matrices must satisfy to be density matrices must satisfy to be density matrices must satisfy to be
valid density valid density valid density
matrices there are two of of them and matrices there are two of of them and matrices there are two of of them and
what I'll do is to First just list them what I'll do is to First just list them what I'll do is to First just list them
and then I'll discuss in Greater detail and then I'll discuss in Greater detail and then I'll discuss in Greater detail
what they what they what they
mean the first condition is that density mean the first condition is that density mean the first condition is that density
matrices must have unit trace or in matrices must have unit trace or in matrices must have unit trace or in
other words Trace equal to other words Trace equal to other words Trace equal to
one and the second condition is that one and the second condition is that one and the second condition is that
density matrices must be positive density matrices must be positive density matrices must be positive
semi-definite and in both cases we have semi-definite and in both cases we have semi-definite and in both cases we have
concise symbolic ways of expressing concise symbolic ways of expressing concise symbolic ways of expressing
these two conditions that appear on the these two conditions that appear on the these two conditions that appear on the
screen now let me discuss the two screen now let me discuss the two screen now let me discuss the two
conditions in Greater detail conditions in Greater detail conditions in Greater detail
the first condition refers to the trace the first condition refers to the trace the first condition refers to the trace
which is a very important function which is a very important function which is a very important function
that's defined for all square matrices that's defined for all square matrices that's defined for all square matrices
and it's simply the sum of the diagonal and it's simply the sum of the diagonal and it's simply the sum of the diagonal
entries and here's an equation that entries and here's an equation that entries and here's an equation that
expresses that expresses that expresses that
explicitly now there's quite a lot to be explicitly now there's quite a lot to be explicitly now there's quite a lot to be
said about the trace and why it's said about the trace and why it's said about the trace and why it's
important but for now let's just observe important but for now let's just observe important but for now let's just observe
one very basic property of it which is one very basic property of it which is one very basic property of it which is
that it's a linear that it's a linear that it's a linear
function other properties of the trace function other properties of the trace function other properties of the trace
will come up and will discuss them as will come up and will discuss them as will come up and will discuss them as
they're needed but to keep our focus on they're needed but to keep our focus on they're needed but to keep our focus on
density matrices let's move on to the density matrices let's move on to the density matrices let's move on to the
second second second
requirement and that requirement is that requirement and that requirement is that requirement and that requirement is that
density matrices are positive density matrices are positive density matrices are positive
semi-definite this is also an important semi-definite this is also an important semi-definite this is also an important
concept that arises in many settings in concept that arises in many settings in concept that arises in many settings in
physics chemistry optimization machine physics chemistry optimization machine physics chemistry optimization machine
learning and other subjects and it can learning and other subjects and it can learning and other subjects and it can
be expressed in several different but be expressed in several different but be expressed in several different but
equivalent equivalent equivalent
ways one way to express it is to say ways one way to express it is to say ways one way to express it is to say
that a matrix row is positive that a matrix row is positive that a matrix row is positive
semi-definite if there exists a matrix M semi-definite if there exists a matrix M semi-definite if there exists a matrix M
such that row is equal to M dagger time such that row is equal to M dagger time such that row is equal to M dagger time
M and it turns out that it doesn't M and it turns out that it doesn't M and it turns out that it doesn't
matter if we demand that M itself is a matter if we demand that M itself is a matter if we demand that M itself is a
square Matrix or if we drop that square Matrix or if we drop that square Matrix or if we drop that
restriction another way to express what restriction another way to express what restriction another way to express what
it means for a matrix row to be positive it means for a matrix row to be positive it means for a matrix row to be positive
semi-definite is to say that row is semi-definite is to say that row is semi-definite is to say that row is
herian which means that it's equal to herian which means that it's equal to herian which means that it's equal to
its own conjugate transpose and all of its own conjugate transpose and all of its own conjugate transpose and all of
its I values are non- negative real its I values are non- negative real its I values are non- negative real
numbers in general herian matrices can numbers in general herian matrices can numbers in general herian matrices can
only have real number igen values but only have real number igen values but only have real number igen values but
they can be negative but if we include they can be negative but if we include they can be negative but if we include
the additional restriction that none of the additional restriction that none of the additional restriction that none of
the igen values can be negative we the igen values can be negative we the igen values can be negative we
obtain the positive semi-definite obtain the positive semi-definite obtain the positive semi-definite
matrices and by the way if you're given matrices and by the way if you're given matrices and by the way if you're given
a matrix and you want to know if it's a matrix and you want to know if it's a matrix and you want to know if it's
positive semi-definite then you can positive semi-definite then you can positive semi-definite then you can
determine that by using this determine that by using this determine that by using this
characterization it's quite easy to characterization it's quite easy to characterization it's quite easy to
check whether or not a matrix is heran check whether or not a matrix is heran check whether or not a matrix is heran
that's something that can just be that's something that can just be that's something that can just be
eyeballed it's more difficult to compute eyeballed it's more difficult to compute eyeballed it's more difficult to compute
the igen values but that's something the igen values but that's something the igen values but that's something
that's not too difficult to do with a that's not too difficult to do with a that's not too difficult to do with a
computer because there are very computer because there are very computer because there are very
efficient algorithms to do it and the last characterization that it and the last characterization that
I'll mention although there are others I'll mention although there are others I'll mention although there are others
is that for every column Vector s having is that for every column Vector s having is that for every column Vector s having
a number of entries that agrees with the a number of entries that agrees with the a number of entries that agrees with the
number of rows and the number of columns number of rows and the number of columns number of rows and the number of columns
of row the inequality shown on the of row the inequality shown on the of row the inequality shown on the
screen is true in words drai time row screen is true in words drai time row screen is true in words drai time row
time catchi must be a non- negative real time catchi must be a non- negative real time catchi must be a non- negative real
number it's not obvious by the way that number it's not obvious by the way that number it's not obvious by the way that
these characterizations are all these characterizations are all these characterizations are all
equivalent but they equivalent but they equivalent but they
are and let me note explicitly that the are and let me note explicitly that the are and let me note explicitly that the
notation that we commonly use to notation that we commonly use to notation that we commonly use to
indicate that a matrix row is positive indicate that a matrix row is positive indicate that a matrix row is positive
semi-definite looks like row is greater semi-definite looks like row is greater semi-definite looks like row is greater
than or equal to zero but this is just a than or equal to zero but this is just a than or equal to zero but this is just a
shorthand notation and it doesn't shorthand notation and it doesn't shorthand notation and it doesn't
literally mean that row is greater than literally mean that row is greater than literally mean that row is greater than
or equal to zero row is a matrix so that or equal to zero row is a matrix so that or equal to zero row is a matrix so that
wouldn't make sense and it does not mean wouldn't make sense and it does not mean wouldn't make sense and it does not mean
that each of the entries of row is that each of the entries of row is that each of the entries of row is
greater than or equal to greater than or equal to greater than or equal to
zero in particular there are positive zero in particular there are positive zero in particular there are positive
semi-definite matrices whose entries semi-definite matrices whose entries semi-definite matrices whose entries
aren't all non- negative and there are aren't all non- negative and there are aren't all non- negative and there are
matrices whose entries are all non- matrices whose entries are all non- matrices whose entries are all non-
negative but they aren't positive negative but they aren't positive negative but they aren't positive
semi-definite the idea behind the semi-definite the idea behind the semi-definite the idea behind the
notation is that positive semi-definite notation is that positive semi-definite notation is that positive semi-definite
matrices are like Matrix analoges of matrices are like Matrix analoges of matrices are like Matrix analoges of
non- negative real numbers but this is non- negative real numbers but this is non- negative real numbers but this is
just notation and what it means is just notation and what it means is just notation and what it means is
precisely what we've already precisely what we've already precisely what we've already
discussed now let's take a look at a few discussed now let's take a look at a few discussed now let's take a look at a few
examples starting with an example of a examples starting with an example of a examples starting with an example of a
positive semi-definite positive semi-definite positive semi-definite
Matrix in fact it's quite easy to Matrix in fact it's quite easy to Matrix in fact it's quite easy to
randomly generate an example of a randomly generate an example of a randomly generate an example of a
positive semi-definite matrix by simply positive semi-definite matrix by simply positive semi-definite matrix by simply
choosing a matrix M arbitrarily and then choosing a matrix M arbitrarily and then choosing a matrix M arbitrarily and then
Computing M dagger time Computing M dagger time Computing M dagger time
M for example here's a matrix M that's M for example here's a matrix M that's M for example here's a matrix M that's
not particularly special and in fact I not particularly special and in fact I not particularly special and in fact I
randomly generated this matrix by randomly generated this matrix by randomly generated this matrix by
independently choosing the real and independently choosing the real and independently choosing the real and
imaginary parts of the entries to be imaginary parts of the entries to be imaginary parts of the entries to be
integers between negative 9 and positive integers between negative 9 and positive integers between negative 9 and positive
9 so any patterns or structure here is 9 so any patterns or structure here is 9 so any patterns or structure here is
purely purely purely
coincidental by multiplying the coincidental by multiplying the coincidental by multiplying the
conjugate transpose of M to M we get conjugate transpose of M to M we get conjugate transpose of M to M we get
this Matrix right here which is this Matrix right here which is this Matrix right here which is
therefore positive therefore positive therefore positive
semi-definite of course there's a limit semi-definite of course there's a limit semi-definite of course there's a limit
to how much Insight we can gain from a to how much Insight we can gain from a to how much Insight we can gain from a
random example but it does exhibit some random example but it does exhibit some random example but it does exhibit some
features that are common to positive features that are common to positive features that are common to positive
semi-definite matrices more semi-definite matrices more semi-definite matrices more
generally in particular you'll notice generally in particular you'll notice generally in particular you'll notice
that the diagonal entries meaning the that the diagonal entries meaning the that the diagonal entries meaning the
ones along the line from the upper left ones along the line from the upper left ones along the line from the upper left
to the lower right are all non- negative to the lower right are all non- negative to the lower right are all non- negative
real real real
numbers and moreover the off diagonal numbers and moreover the off diagonal numbers and moreover the off diagonal
entries are somehow not too large entries are somehow not too large entries are somehow not too large
compared to the corresponding diagonal compared to the corresponding diagonal compared to the corresponding diagonal
entries meaning the one in the same row entries meaning the one in the same row entries meaning the one in the same row
and the one in the same column as and the one in the same column as and the one in the same column as
whatever off diagonal entry we're whatever off diagonal entry we're whatever off diagonal entry we're
looking looking looking
at there is more to the story than this at there is more to the story than this at there is more to the story than this
though and it's important to recognize though and it's important to recognize though and it's important to recognize
that being positive semi-definite is a that being positive semi-definite is a that being positive semi-definite is a
property of a matrix as a whole and not property of a matrix as a whole and not property of a matrix as a whole and not
just a condition on individual end just a condition on individual end just a condition on individual end
entries so that's an example of a entries so that's an example of a entries so that's an example of a
positive semi-definite Matrix and you positive semi-definite Matrix and you positive semi-definite Matrix and you
can very easily generate other examples can very easily generate other examples can very easily generate other examples
through a similar methodology now let's take a look at methodology now let's take a look at
some examples of density matrices where some examples of density matrices where some examples of density matrices where
in addition to the positive semidefinite in addition to the positive semidefinite in addition to the positive semidefinite
condition we also have that the trace is condition we also have that the trace is condition we also have that the trace is
equal to equal to equal to
one here are some examples of 2x two one here are some examples of 2x two one here are some examples of 2x two
density matrices the trace is equal to density matrices the trace is equal to density matrices the trace is equal to
one in all four cases and we can also one in all four cases and we can also one in all four cases and we can also
see that these are herian matrices see that these are herian matrices see that these are herian matrices
so using the second characterization of so using the second characterization of so using the second characterization of
positive semi-definite matrices from positive semi-definite matrices from positive semi-definite matrices from
before All That Remains is to check that before All That Remains is to check that before All That Remains is to check that
the igen values are all non- negative the igen values are all non- negative the igen values are all non- negative
I'll leave that for you to check if you I'll leave that for you to check if you I'll leave that for you to check if you
wish for 2x two matrices it isn't too wish for 2x two matrices it isn't too wish for 2x two matrices it isn't too
difficult to compute igen values by hand difficult to compute igen values by hand difficult to compute igen values by hand
but you can also just ask a computer to but you can also just ask a computer to but you can also just ask a computer to
do it for example you can use the igen do it for example you can use the igen do it for example you can use the igen
Val function in the linear algebra Val function in the linear algebra Val function in the linear algebra
package for package for package for
python we can also obtain a density python we can also obtain a density python we can also obtain a density
matrix by simply taking any nonzero matrix by simply taking any nonzero matrix by simply taking any nonzero
positive semi-definite Matrix and positive semi-definite Matrix and positive semi-definite Matrix and
dividing it by its trace for example dividing it by its trace for example dividing it by its trace for example
here's the density Matrix we get by here's the density Matrix we get by here's the density Matrix we get by
dividing the example we saw just a few dividing the example we saw just a few dividing the example we saw just a few
moments ago by its moments ago by its moments ago by its
Trace dividing by the trace like this Trace dividing by the trace like this Trace dividing by the trace like this
will always preserve the positive will always preserve the positive will always preserve the positive
semi-definite property and we'll semi-definite property and we'll semi-definite property and we'll
certainly obtain a matrix having Trace certainly obtain a matrix having Trace certainly obtain a matrix having Trace
equal to one by virtue of the fact that equal to one by virtue of the fact that equal to one by virtue of the fact that
the trace is a linear the trace is a linear the trace is a linear
function that's just a small collection function that's just a small collection function that's just a small collection
of examples and we'll see more as the of examples and we'll see more as the of examples and we'll see more as the
lesson lesson lesson
continues next we take just a few continues next we take just a few continues next we take just a few
moments to discuss the interpretation or moments to discuss the interpretation or moments to discuss the interpretation or
intuitive meaning of density intuitive meaning of density intuitive meaning of density
matrices the first thing to acknowledge matrices the first thing to acknowledge matrices the first thing to acknowledge
is that it may seem a little bit unusual is that it may seem a little bit unusual is that it may seem a little bit unusual
that we're representing the state of a that we're representing the state of a that we're representing the state of a
system with a matrix rather than a system with a matrix rather than a system with a matrix rather than a
vector in contrast in the simplified vector in contrast in the simplified vector in contrast in the simplified
formulation of quantum information we formulation of quantum information we formulation of quantum information we
use vectors to represent States whereas use vectors to represent States whereas use vectors to represent States whereas
matrices specifically unitary matrices matrices specifically unitary matrices matrices specifically unitary matrices
represent actions or represent actions or represent actions or
operations this is just the way the ma operations this is just the way the ma operations this is just the way the ma
mathematics works and although you can mathematics works and although you can mathematics works and although you can
always think about matrices as always think about matrices as always think about matrices as
describing linear mappings we don't describing linear mappings we don't describing linear mappings we don't
typically associate a direct intuitive typically associate a direct intuitive typically associate a direct intuitive
meaning to the linear mapping associated meaning to the linear mapping associated meaning to the linear mapping associated
with a given density with a given density with a given density
Matrix having said that the fact that Matrix having said that the fact that Matrix having said that the fact that
density matrices are indeed matrices density matrices are indeed matrices density matrices are indeed matrices
does turn out to be a critically does turn out to be a critically does turn out to be a critically
important aspect of them and we'll see important aspect of them and we'll see important aspect of them and we'll see
that as we continue on but to briefly that as we continue on but to briefly that as we continue on but to briefly
mention just one example for now the mention just one example for now the mention just one example for now the
igen values of a given density Matrix igen values of a given density Matrix igen values of a given density Matrix
turn not to be very important because turn not to be very important because turn not to be very important because
they describe how much Randomness or they describe how much Randomness or they describe how much Randomness or
uncertainty is inherent to the state uncertainty is inherent to the state uncertainty is inherent to the state
that that density Matrix that that density Matrix that that density Matrix
describes a different way to think about describes a different way to think about describes a different way to think about
density matrices at an intuitive level density matrices at an intuitive level density matrices at an intuitive level
focuses on the entries of these focuses on the entries of these focuses on the entries of these
matrices remember that the rows and matrices remember that the rows and matrices remember that the rows and
Columns of density matrices are placed Columns of density matrices are placed Columns of density matrices are placed
in correspondence with the classical in correspondence with the classical in correspondence with the classical
States of Whatever system we're talking States of Whatever system we're talking States of Whatever system we're talking
about so for each entry we have one about so for each entry we have one about so for each entry we have one
classical State corresponding to the row classical State corresponding to the row classical State corresponding to the row
of that entry and one classical State of that entry and one classical State of that entry and one classical State
corresponding to the column of that corresponding to the column of that corresponding to the column of that
entry for the diagonal entries those two entry for the diagonal entries those two entry for the diagonal entries those two
classical states are the same state and classical states are the same state and classical states are the same state and
that's why the diagonal entries are that's why the diagonal entries are that's why the diagonal entries are
special and one way we can interpret the special and one way we can interpret the special and one way we can interpret the
diagonal entries is that they're diagonal entries is that they're diagonal entries is that they're
probabilities and specifically there are probabilities and specifically there are probabilities and specifically there are
the probabilities that the corresponding the probabilities that the corresponding the probabilities that the corresponding
classical state would appear if we were classical state would appear if we were classical state would appear if we were
to perform a standard basis measurement to perform a standard basis measurement to perform a standard basis measurement
on the system whose Quantum State we're on the system whose Quantum State we're on the system whose Quantum State we're
describing for the the off diagonal describing for the the off diagonal describing for the the off diagonal
entries on the other hand we have one entries on the other hand we have one entries on the other hand we have one
classical State corresponding to the row classical State corresponding to the row classical State corresponding to the row
of that entry and a different classical of that entry and a different classical of that entry and a different classical
State corresponding to the column of State corresponding to the column of State corresponding to the column of
that entry and very informally speaking that entry and very informally speaking that entry and very informally speaking
these entries describe the degree to these entries describe the degree to these entries describe the degree to
which these two classical states are in which these two classical states are in which these two classical states are in
Quantum superp position as well as the Quantum superp position as well as the Quantum superp position as well as the
relative phase between relative phase between relative phase between
them that may make more sense as the them that may make more sense as the them that may make more sense as the
lesson continues on and we look more lesson continues on and we look more lesson continues on and we look more
closely at some specific examples and closely at some specific examples and closely at some specific examples and
the details may not be clear at this the details may not be clear at this the details may not be clear at this
point but please don't let that bother point but please don't let that bother point but please don't let that bother
you this is quite informal and it's just you this is quite informal and it's just you this is quite informal and it's just
meant to be an intuitive idea to keep in meant to be an intuitive idea to keep in meant to be an intuitive idea to keep in
the back of your the back of your the back of your
mind next we'll take a look at the mind next we'll take a look at the mind next we'll take a look at the
connection between density matrices and connection between density matrices and connection between density matrices and
Quantum State vectors which are column Quantum State vectors which are column Quantum State vectors which are column
vectors having ukan Norm equal to one vectors having ukan Norm equal to one vectors having ukan Norm equal to one
just to recall the mathematical just to recall the mathematical just to recall the mathematical
definition supposing that we have a definition supposing that we have a definition supposing that we have a
Quantum State Vector s that represents Quantum State Vector s that represents Quantum State Vector s that represents
the state of some particular system the the state of some particular system the the state of some particular system the
density Matrix that represents the same density Matrix that represents the same density Matrix that represents the same
Quantum state is given by Ki times Quantum state is given by Ki times Quantum state is given by Ki times
brasai as is shown here on the screen to brasai as is shown here on the screen to brasai as is shown here on the screen to
be clear about this the indices of Sai be clear about this the indices of Sai be clear about this the indices of Sai
correspond in some way to the classical correspond in some way to the classical correspond in some way to the classical
States of Whatever system we're talking States of Whatever system we're talking States of Whatever system we're talking
about and that's going to be precisely about and that's going to be precisely about and that's going to be precisely
the same correspondence we have between the same correspondence we have between the same correspondence we have between
the rows and Columns of our density the rows and Columns of our density the rows and Columns of our density
Matrix and the classical states of the Matrix and the classical states of the Matrix and the classical states of the
system if we imagine that there are n system if we imagine that there are n system if we imagine that there are n
classical states of the system and we classical states of the system and we classical states of the system and we
think about column and row vectors as think about column and row vectors as think about column and row vectors as
matrices then Ki is an N by one Matrix matrices then Ki is an N by one Matrix matrices then Ki is an N by one Matrix
and Brasi is a 1 byn Matrix so when we and Brasi is a 1 byn Matrix so when we and Brasi is a 1 byn Matrix so when we
perform the multiplication we get an N perform the multiplication we get an N perform the multiplication we get an N
byn byn byn
Matrix this will always be a positive Matrix this will always be a positive Matrix this will always be a positive
semi-definite Matrix just by its form in semi-definite Matrix just by its form in semi-definite Matrix just by its form in
fact and it will also have Trace equal fact and it will also have Trace equal fact and it will also have Trace equal
to one by virtue of the fact that s is a to one by virtue of the fact that s is a to one by virtue of the fact that s is a
unit vector and we'll see that in more unit vector and we'll see that in more unit vector and we'll see that in more
detail shortly Quantum states that are shortly Quantum states that are
represented by density matrices of this represented by density matrices of this represented by density matrices of this
form are called Pure form are called Pure form are called Pure
States States States
here are a few here are a few here are a few
examples first let's consider this state examples first let's consider this state examples first let's consider this state
right here which is kind of like a plus right here which is kind of like a plus right here which is kind of like a plus
State except that the entry State except that the entry State except that the entry
corresponding to the classical State one corresponding to the classical State one corresponding to the classical State one
is is is
imaginary if we compute the conjugate imaginary if we compute the conjugate imaginary if we compute the conjugate
transpose we'll pick up a minus sign transpose we'll pick up a minus sign transpose we'll pick up a minus sign
because the complex conjugate of I is because the complex conjugate of I is because the complex conjugate of I is
minus I and if we do the multiplication minus I and if we do the multiplication minus I and if we do the multiplication
we'll get the Matrix that's shown here we'll get the Matrix that's shown here we'll get the Matrix that's shown here
on the screen it's heran as we can on the screen it's heran as we can on the screen it's heran as we can
immediately see just by looking at it immediately see just by looking at it immediately see just by looking at it
and it happens to have one ion value and it happens to have one ion value and it happens to have one ion value
equal to one and one ion value equal to equal to one and one ion value equal to equal to one and one ion value equal to
zero so it's positive semi-definite and zero so it's positive semi-definite and zero so it's positive semi-definite and
it Trace is equal to one so it is a it Trace is equal to one so it is a it Trace is equal to one so it is a
density Matrix as expected we can compute the density expected we can compute the density
matrices corresponding to the standard matrices corresponding to the standard matrices corresponding to the standard
basis and the plus minus bases in a basis and the plus minus bases in a basis and the plus minus bases in a
similar way and those density matrices similar way and those density matrices similar way and those density matrices
are shown here and here's one more example of a here and here's one more example of a
Quantum State Vector it happened to Quantum State Vector it happened to Quantum State Vector it happened to
appear way back in the first lesson of appear way back in the first lesson of appear way back in the first lesson of
the series and if we compute the the series and if we compute the the series and if we compute the
conjugate transpose and then multiply conjugate transpose and then multiply conjugate transpose and then multiply
we'll get the density Matrix that's we'll get the density Matrix that's we'll get the density Matrix that's
shown shown shown
here once again it's herian and has here once again it's herian and has here once again it's herian and has
Trace equal to one and if you compute Trace equal to one and if you compute Trace equal to one and if you compute
the IG values you'll find that again it the IG values you'll find that again it the IG values you'll find that again it
has one IG value equal to one and one has one IG value equal to one and one has one IG value equal to one and one
igen value equal to zero and by the way igen value equal to zero and by the way igen value equal to zero and by the way
that's a common characteristic of All that's a common characteristic of All that's a common characteristic of All
Pure States they always have one I value Pure States they always have one I value Pure States they always have one I value
equal to one and the rest of the IG equal to one and the rest of the IG equal to one and the rest of the IG
values are all zero in general for a values are all zero in general for a values are all zero in general for a
completely arbitrary Quantum State completely arbitrary Quantum State completely arbitrary Quantum State
Vector s with any number of entries if Vector s with any number of entries if Vector s with any number of entries if
we multiply Ki to brasai we'll obtain a we multiply Ki to brasai we'll obtain a we multiply Ki to brasai we'll obtain a
density Matrix that looks like density Matrix that looks like density Matrix that looks like
this notice in particular that the this notice in particular that the this notice in particular that the
diagonal entries are always one of the diagonal entries are always one of the diagonal entries are always one of the
entries of s multiplied to its own entries of s multiplied to its own entries of s multiplied to its own
complex conjugate which we can complex conjugate which we can complex conjugate which we can
alternatively Express as the absolute alternatively Express as the absolute alternatively Express as the absolute
value squared of that entry and this is value squared of that entry and this is value squared of that entry and this is
one way to see that the trace will one way to see that the trace will one way to see that the trace will
always be equal to always be equal to always be equal to
one it's also consistent with the remark one it's also consistent with the remark one it's also consistent with the remark
I made earlier in the lesson about the I made earlier in the lesson about the I made earlier in the lesson about the
diagonal entries of a density Matrix diagonal entries of a density Matrix diagonal entries of a density Matrix
telling us what the probabilities would telling us what the probabilities would telling us what the probabilities would
be to see each possible classical state be to see each possible classical state be to see each possible classical state
if we were to perform a standard basis measurement and one final remark there measurement and one final remark there
is no issue concerning Global phases for is no issue concerning Global phases for is no issue concerning Global phases for
density density density
matrices two Quantum states are matrices two Quantum states are matrices two Quantum states are
identical if and only if their density identical if and only if their density identical if and only if their density
matrices are matrices are matrices are
equal in particular suppose that we have equal in particular suppose that we have equal in particular suppose that we have
two Quantum State vectors s and fi that two Quantum State vectors s and fi that two Quantum State vectors s and fi that
are equ equalent up to a global phase are equ equalent up to a global phase are equ equalent up to a global phase
which means that we can express one of which means that we can express one of which means that we can express one of
them F let's say as the other one times them F let's say as the other one times them F let's say as the other one times
some complex number on the unit some complex number on the unit some complex number on the unit
circle if we compute the density Matrix circle if we compute the density Matrix circle if we compute the density Matrix
representation of fi then what we'll representation of fi then what we'll representation of fi then what we'll
find is that the global phase Factor find is that the global phase Factor find is that the global phase Factor
gets multiplied by its own complex gets multiplied by its own complex gets multiplied by its own complex
conjugate and that gives us one so it conjugate and that gives us one so it conjugate and that gives us one so it
effectively disappears and we're left effectively disappears and we're left effectively disappears and we're left
with exactly the same density Matrix with exactly the same density Matrix with exactly the same density Matrix
that we get from s so that's kind of a that we get from s so that's kind of a that we get from s so that's kind of a
nice little bonus and more generally nice little bonus and more generally nice little bonus and more generally
there's no issue concerning equivalence there's no issue concerning equivalence there's no issue concerning equivalence
up to Global phases at all when we're up to Global phases at all when we're up to Global phases at all when we're
working with density matrices that's working with density matrices that's working with density matrices that's
just basically a degeneracy that exists just basically a degeneracy that exists just basically a degeneracy that exists
when we use quantum State vectors rather when we use quantum State vectors rather when we use quantum State vectors rather
than density matrices to represent than density matrices to represent than density matrices to represent
Quantum states density matrices possess Quantum states density matrices possess Quantum states density matrices possess
a key property which is that convex a key property which is that convex a key property which is that convex
combinations of density matrices combinations of density matrices combinations of density matrices
represent probabilistic or random represent probabilistic or random represent probabilistic or random
selections of quantum selections of quantum selections of quantum
States another way of saying that is States another way of saying that is States another way of saying that is
that density matrices can be averaged that density matrices can be averaged that density matrices can be averaged
together in a meaningful way which is together in a meaningful way which is together in a meaningful way which is
not something that we can do with not something that we can do with not something that we can do with
Quantum State Quantum State Quantum State
vectors to explain this in more detail vectors to explain this in more detail vectors to explain this in more detail
let's start with the situation in which let's start with the situation in which let's start with the situation in which
we have two density matrices row and we have two density matrices row and we have two density matrices row and
sigma that represent Quantum states of sigma that represent Quantum states of sigma that represent Quantum states of
some system let's imagine that the some system let's imagine that the some system let's imagine that the
system is prepared in the state row with system is prepared in the state row with system is prepared in the state row with
probability P for some choice of P probability P for some choice of P probability P for some choice of P
between zero and one and otherwise the between zero and one and otherwise the between zero and one and otherwise the
system is prepared in the state system is prepared in the state system is prepared in the state
Sigma for example maybe p is 1/2 so we Sigma for example maybe p is 1/2 so we Sigma for example maybe p is 1/2 so we
we might imagine that someone flips a we might imagine that someone flips a we might imagine that someone flips a
Fair coin and if it's heads they perform Fair coin and if it's heads they perform Fair coin and if it's heads they perform
some process that leaves the system in some process that leaves the system in some process that leaves the system in
the state row and otherwise if the coin the state row and otherwise if the coin the state row and otherwise if the coin
comes up taals they perform a different comes up taals they perform a different comes up taals they perform a different
process that leaves the system in the process that leaves the system in the process that leaves the system in the
state state state
Sigma the question now is what is the Sigma the question now is what is the Sigma the question now is what is the
state of the system and to be clear state of the system and to be clear state of the system and to be clear
we're interested in the state of the we're interested in the state of the we're interested in the state of the
system in isolation disregarding any system in isolation disregarding any system in isolation disregarding any
other information that might exist about other information that might exist about other information that might exist about
which preparation was which preparation was which preparation was
performed for example imagine that we performed for example imagine that we performed for example imagine that we
don't have access to the coin or the don't have access to the coin or the don't have access to the coin or the
memory of whoever it was that performed memory of whoever it was that performed memory of whoever it was that performed
the state preparation and the answer to the state preparation and the answer to the state preparation and the answer to
that question is that it's simply the that question is that it's simply the that question is that it's simply the
weighted average of the two density weighted average of the two density weighted average of the two density
matrices P * row plus 1 - P * matrices P * row plus 1 - P * matrices P * row plus 1 - P *
Sigma this by the way is completely Sigma this by the way is completely Sigma this by the way is completely
analogous to what we would have if row analogous to what we would have if row analogous to what we would have if row
and sigma weren't density matrices but and sigma weren't density matrices but and sigma weren't density matrices but
rather were probability vectors rather were probability vectors rather were probability vectors
representing probabilistic States so representing probabilistic States so representing probabilistic States so
this is not something shocking or this is not something shocking or this is not something shocking or
mysterious but rather it's something mysterious but rather it's something mysterious but rather it's something
that perhaps is familiar from that perhaps is familiar from that perhaps is familiar from
probability Theory we do this kind of probability Theory we do this kind of probability Theory we do this kind of
averaging all the time when we're averaging all the time when we're averaging all the time when we're
calculating probabilities and thinking calculating probabilities and thinking calculating probabilities and thinking
about classical random selections and about classical random selections and about classical random selections and
the point is that this also works for the point is that this also works for the point is that this also works for
density matrices but note that it does density matrices but note that it does density matrices but note that it does
not work for Quantum State not work for Quantum State not work for Quantum State
vectors that is you can't generally vectors that is you can't generally vectors that is you can't generally
average together two Quantum State average together two Quantum State average together two Quantum State
vectors like this and hope to get a vectors like this and hope to get a vectors like this and hope to get a
valid Quantum State vector and if you valid Quantum State vector and if you valid Quantum State vector and if you
try you'll usually get a vector that try you'll usually get a vector that try you'll usually get a vector that
doesn't have ukian Norm equal to doesn't have ukian Norm equal to doesn't have ukian Norm equal to
one with density matrices however one with density matrices however one with density matrices however
weighted averages like this will always weighted averages like this will always weighted averages like this will always
be valid density matrices and more be valid density matrices and more be valid density matrices and more
importantly the density Matrix you get importantly the density Matrix you get importantly the density Matrix you get
will represent that random selection in will represent that random selection in will represent that random selection in
a meaningful a meaningful a meaningful
way this generalizes to random way this generalizes to random way this generalizes to random
selections where there are more than two selections where there are more than two selections where there are more than two
Alternatives in particular if we have M Alternatives in particular if we have M Alternatives in particular if we have M
possible density matrices and a possible density matrices and a possible density matrices and a
probability Vector with M entries one probability Vector with M entries one probability Vector with M entries one
for each density Matrix and a system is for each density Matrix and a system is for each density Matrix and a system is
prepared in each possible state with the prepared in each possible state with the prepared in each possible state with the
corresponding probability corresponding probability corresponding probability
then the weighted average of those then the weighted average of those then the weighted average of those
density matrices describes the resulting density matrices describes the resulting density matrices describes the resulting
state of the system and by the way when state of the system and by the way when state of the system and by the way when
we refer to a convex combination of we refer to a convex combination of we refer to a convex combination of
density matrices this is what we mean a density matrices this is what we mean a density matrices this is what we mean a
convex combination is simply a linear convex combination is simply a linear convex combination is simply a linear
combination where the coefficients form combination where the coefficients form combination where the coefficients form
a probability a probability a probability
Vector this will always be a density Vector this will always be a density Vector this will always be a density
Matrix and a simple and concise way of Matrix and a simple and concise way of Matrix and a simple and concise way of
saying that in mathematical terms is saying that in mathematical terms is saying that in mathematical terms is
that the set of all density matrices that the set of all density matrices that the set of all density matrices
corresponding to a given system is a corresponding to a given system is a corresponding to a given system is a
convex convex convex
set so if we have say m Quantum State set so if we have say m Quantum State set so if we have say m Quantum State
vectors size 0 through S M minus one vectors size 0 through S M minus one vectors size 0 through S M minus one
along with probabilities p 0 through p m along with probabilities p 0 through p m along with probabilities p 0 through p m
minus one that collectively form a minus one that collectively form a minus one that collectively form a
probability vector and a system is probability vector and a system is probability vector and a system is
prepared in the state s k with prepared in the state s k with prepared in the state s k with
probability PK for each k then the probability PK for each k then the probability PK for each k then the
resulting state of the system is resulting state of the system is resulting state of the system is
described by the density Matrix that's described by the density Matrix that's described by the density Matrix that's
shown here on the shown here on the shown here on the
screen in particular we're not averaging screen in particular we're not averaging screen in particular we're not averaging
together together the quantum State together together the quantum State together together the quantum State
vectors themselves but rather we're vectors themselves but rather we're vectors themselves but rather we're
averaging together the corresponding averaging together the corresponding averaging together the corresponding
density density density
matrices in general a state like this matrices in general a state like this matrices in general a state like this
will not have a representation as a will not have a representation as a will not have a representation as a
single Quantum State single Quantum State single Quantum State
Vector unless something trivial is Vector unless something trivial is Vector unless something trivial is
happening like all of the quantum State happening like all of the quantum State happening like all of the quantum State
vectors are equivalent up to a global vectors are equivalent up to a global vectors are equivalent up to a global
phase or all but one of the phase or all but one of the phase or all but one of the
probabilities are zero we will not probabilities are zero we will not probabilities are zero we will not
obtain a pure Quantum state from such a obtain a pure Quantum state from such a obtain a pure Quantum state from such a
process rather we'll obtain what's process rather we'll obtain what's process rather we'll obtain what's
called a mixed Quantum state where called a mixed Quantum state where called a mixed Quantum state where
there's some inherent uncertainty involved let's take a look at a simple involved let's take a look at a simple
example suppose that we have a cubit and example suppose that we have a cubit and example suppose that we have a cubit and
with probability 1/2 the Cubit is with probability 1/2 the Cubit is with probability 1/2 the Cubit is
prepared in the zero State and otherwise prepared in the zero State and otherwise prepared in the zero State and otherwise
it's prepared in the plus it's prepared in the plus it's prepared in the plus
State here's the density Matrix that we State here's the density Matrix that we State here's the density Matrix that we
get by averaging as I've get by averaging as I've get by averaging as I've
described we compute the density described we compute the density described we compute the density
matrices corresponding to the two matrices corresponding to the two matrices corresponding to the two
possible States multiply by the possible States multiply by the possible States multiply by the
corresponding probabilities and add the corresponding probabilities and add the corresponding probabilities and add the
results what we get is indeed a den results what we get is indeed a den results what we get is indeed a den
matrix it's heran it traces equal to one matrix it's heran it traces equal to one matrix it's heran it traces equal to one
and if you compute the igen values of and if you compute the igen values of and if you compute the igen values of
this Matrix you will get two positive this Matrix you will get two positive this Matrix you will get two positive
real numbers and they happen to be two real numbers and they happen to be two real numbers and they happen to be two
numbers that should probably be familiar numbers that should probably be familiar numbers that should probably be familiar
to you by now cosine square of pi over 8 to you by now cosine square of pi over 8 to you by now cosine square of pi over 8
and sin square of pi over 8 or about and sin square of pi over 8 or about and sin square of pi over 8 or about
0.85 and 0.85 and 0.85 and
0.15 there is no single Quantum State 0.15 there is no single Quantum State 0.15 there is no single Quantum State
Vector representation of the state it's Vector representation of the state it's Vector representation of the state it's
not a pure not a pure not a pure
state next we'll take a look at one state next we'll take a look at one state next we'll take a look at one
particular density Matrix that particular density Matrix that particular density Matrix that
represents a Quantum state of a single represents a Quantum state of a single represents a Quantum state of a single
Cubit known as the completely mixed Cubit known as the completely mixed Cubit known as the completely mixed
state the state arises frequently so state the state arises frequently so state the state arises frequently so
it's good to have a name for it and it's it's good to have a name for it and it's it's good to have a name for it and it's
also a good example because it also a good example because it also a good example because it
illustrates an important feature of illustrates an important feature of illustrates an important feature of
density density density
matrices there is in fact an analogously matrices there is in fact an analogously matrices there is in fact an analogously
defined completely mixed state for any defined completely mixed state for any defined completely mixed state for any
system but for now we're going to focus system but for now we're going to focus system but for now we're going to focus
on this state for a single Cubit just to on this state for a single Cubit just to on this state for a single Cubit just to
keep things keep things keep things
simple suppose that we randomly set the simple suppose that we randomly set the simple suppose that we randomly set the
state of a cubit to be either the zero state of a cubit to be either the zero state of a cubit to be either the zero
State or the one state each with State or the one state each with State or the one state each with
probability equal to 1/2 much like a probability equal to 1/2 much like a probability equal to 1/2 much like a
uniform random bit we can calculate the uniform random bit we can calculate the uniform random bit we can calculate the
density Matrix representation of this density Matrix representation of this density Matrix representation of this
state in the usual way we take the state in the usual way we take the state in the usual way we take the
average of the two density Matrix average of the two density Matrix average of the two density Matrix
representations of the states here's how representations of the states here's how representations of the states here's how
that looks in Matrix form and what we that looks in Matrix form and what we that looks in Matrix form and what we
obtain is the identity Matrix divided by obtain is the identity Matrix divided by obtain is the identity Matrix divided by
two this state is known as the two this state is known as the two this state is known as the
completely mixed state and it basically completely mixed state and it basically completely mixed state and it basically
represents complete or maximal represents complete or maximal represents complete or maximal
uncertainty about a cubit uncertainty about a cubit uncertainty about a cubit
just like a uniform probability Vector just like a uniform probability Vector just like a uniform probability Vector
represents maximal uncertainty in the represents maximal uncertainty in the represents maximal uncertainty in the
classical probabilistic classical probabilistic classical probabilistic
setting in general for an arbitrary setting in general for an arbitrary setting in general for an arbitrary
system the completely mixed state is the system the completely mixed state is the system the completely mixed state is the
state we obtain by uniformly choosing a state we obtain by uniformly choosing a state we obtain by uniformly choosing a
standard basis State and the density standard basis State and the density standard basis State and the density
Matrix representation will be the Matrix representation will be the Matrix representation will be the
identity Matrix divided by the dimension identity Matrix divided by the dimension identity Matrix divided by the dimension
or equivalently the number of classical or equivalently the number of classical or equivalently the number of classical
states of that states of that states of that
system at this point this is just system at this point this is just system at this point this is just
terminology but if we change the setup a terminology but if we change the setup a terminology but if we change the setup a
bit we'll see an important feature of bit we'll see an important feature of bit we'll see an important feature of
density density density
matrices in particular let's suppose matrices in particular let's suppose matrices in particular let's suppose
that we change the procedure and in that we change the procedure and in that we change the procedure and in
place of the zero and one standard basis place of the zero and one standard basis place of the zero and one standard basis
States we substitute the plus and minus States we substitute the plus and minus States we substitute the plus and minus
States this time we start with the States this time we start with the States this time we start with the
expression that's shown on the expression that's shown on the expression that's shown on the
screen but if we go through the screen but if we go through the screen but if we go through the
arithmetic we obtain exactly the same arithmetic we obtain exactly the same arithmetic we obtain exactly the same
density matrix it's the identity Matrix density matrix it's the identity Matrix density matrix it's the identity Matrix
divided divided divided
two so it's the same state as before the two so it's the same state as before the two so it's the same state as before the
complete the mixed complete the mixed complete the mixed
state this as they say is a feature and state this as they say is a feature and state this as they say is a feature and
not a bug the two procedures are in fact not a bug the two procedures are in fact not a bug the two procedures are in fact
different but if all you have is the different but if all you have is the different but if all you have is the
resulting Cubit there is no physical way resulting Cubit there is no physical way resulting Cubit there is no physical way
that you can possibly distinguish them that you can possibly distinguish them that you can possibly distinguish them
even in a statistical even in a statistical even in a statistical
sense for example if you performed a sense for example if you performed a sense for example if you performed a
standard basis measurement on this Cubit standard basis measurement on this Cubit standard basis measurement on this Cubit
after the first procedure was performed after the first procedure was performed after the first procedure was performed
you would obviously get the outcome zero you would obviously get the outcome zero you would obviously get the outcome zero
with probability 1/2 and the outcome one with probability 1/2 and the outcome one with probability 1/2 and the outcome one
with probability 1/2 but you'd also get with probability 1/2 but you'd also get with probability 1/2 but you'd also get
exactly the same out probabilities for exactly the same out probabilities for exactly the same out probabilities for
the second procedure and the same is the second procedure and the same is the second procedure and the same is
true for any other test that you might true for any other test that you might true for any other test that you might
devise assuming that you're testing just devise assuming that you're testing just devise assuming that you're testing just
the Cubit itself in general density the Cubit itself in general density the Cubit itself in general density
matrices don't describe how a system was matrices don't describe how a system was matrices don't describe how a system was
prepared they only tell you what will prepared they only tell you what will prepared they only tell you what will
happen if you do things to that system happen if you do things to that system happen if you do things to that system
going forward whether that's performing going forward whether that's performing going forward whether that's performing
operations or operations or operations or
measurements and in fact density measurements and in fact density measurements and in fact density
matrices are perfect in this matrices are perfect in this matrices are perfect in this
regard if two different Alternatives regard if two different Alternatives regard if two different Alternatives
result in the same density Matrix they result in the same density Matrix they result in the same density Matrix they
cannot be discriminated even cannot be discriminated even cannot be discriminated even
statistically but if the two density statistically but if the two density statistically but if the two density
matrices are different then there will matrices are different then there will matrices are different then there will
be some way to discriminate them not be some way to discriminate them not be some way to discriminate them not
necessarily definitively but in a necessarily definitively but in a necessarily definitively but in a
statistical sense when we use the statistical sense when we use the statistical sense when we use the
general formulation of quantum general formulation of quantum general formulation of quantum
information classical information information classical information information classical information
essentially emerges as a special case essentially emerges as a special case essentially emerges as a special case
and I'll now explain how that and I'll now explain how that and I'll now explain how that
works specifically we'll see how works specifically we'll see how works specifically we'll see how
probabilistic States can be represented probabilistic States can be represented probabilistic States can be represented
by density by density by density
matrices let's start with classical matrices let's start with classical matrices let's start with classical
States beginning with a simple States beginning with a simple States beginning with a simple
example here are the density Matrix example here are the density Matrix example here are the density Matrix
representations of the zero and one representations of the zero and one representations of the zero and one
standard basis states of a standard basis states of a standard basis states of a
cubit we associate these states with the cubit we associate these states with the cubit we associate these states with the
classical States 0 and one and so we can classical States 0 and one and so we can classical States 0 and one and so we can
think about these density matrices as think about these density matrices as think about these density matrices as
representing those representing those representing those
States that's very simple and we can States that's very simple and we can States that's very simple and we can
easily generalize the easily generalize the easily generalize the
example if we have a system whose example if we have a system whose example if we have a system whose
classical State set is Sigma then classical State set is Sigma then classical State set is Sigma then
classical State a is represented by or classical State a is represented by or classical State a is represented by or
is associated with the density Matrix k is associated with the density Matrix k is associated with the density Matrix k
a bra a bra a bra
a that Matrix has a single one in the a that Matrix has a single one in the a that Matrix has a single one in the
diagonal entry whose row and column diagonal entry whose row and column diagonal entry whose row and column
corresponds to the classical State a and corresponds to the classical State a and corresponds to the classical State a and
every other entry is zero probabilistic states are zero probabilistic states are
represented by convex combinations of represented by convex combinations of represented by convex combinations of
matrices of this form which is natural matrices of this form which is natural matrices of this form which is natural
given what we know about density given what we know about density given what we know about density
matrices and how they work specifically matrices and how they work specifically matrices and how they work specifically
that convex combinations of density that convex combinations of density that convex combinations of density
matrices represent random or matrices represent random or matrices represent random or
probabilistic selections of quantum probabilistic selections of quantum probabilistic selections of quantum
States for example let's suppose that we States for example let's suppose that we States for example let's suppose that we
have a system X whose classical State have a system X whose classical State have a system X whose classical State
set includes the integers 0 through n set includes the integers 0 through n set includes the integers 0 through n
minus one for some positive integer n of minus one for some positive integer n of minus one for some positive integer n of
course there's little or no generality course there's little or no generality course there's little or no generality
lost in assuming that our classical lost in assuming that our classical lost in assuming that our classical
State set has this form if we had a State set has this form if we had a State set has this form if we had a
different classical State set we could different classical State set we could different classical State set we could
simply rename the classical States in simply rename the classical States in simply rename the classical States in
this way and it's often pretty venient this way and it's often pretty venient this way and it's often pretty venient
to make the assumption that our to make the assumption that our to make the assumption that our
classical State sets look like this in classical State sets look like this in classical State sets look like this in
large part because that makes it easier large part because that makes it easier large part because that makes it easier
to write things explicitly in terms of to write things explicitly in terms of to write things explicitly in terms of
vectors and vectors and vectors and
matrices supposing that we have a matrices supposing that we have a matrices supposing that we have a
probabilistic state of the system X probabilistic state of the system X probabilistic state of the system X
represented by a probability Vector in represented by a probability Vector in represented by a probability Vector in
the usual way we can associate this the usual way we can associate this the usual way we can associate this
probabilistic state with a density probabilistic state with a density probabilistic state with a density
Matrix as is shown on the Matrix as is shown on the Matrix as is shown on the
screen we take the weighted average of screen we take the weighted average of screen we take the weighted average of
the density matrices corresponding to the density matrices corresponding to the density matrices corresponding to
the different classical States and what the different classical States and what the different classical States and what
what we get is a diagonal density Matrix what we get is a diagonal density Matrix what we get is a diagonal density Matrix
in particular we see the probabilities in particular we see the probabilities in particular we see the probabilities
along the diagonal of this Matrix and along the diagonal of this Matrix and along the diagonal of this Matrix and
every other entry is equal to every other entry is equal to every other entry is equal to
zero of course this is a density Matrix zero of course this is a density Matrix zero of course this is a density Matrix
we expect that to be true but we can we expect that to be true but we can we expect that to be true but we can
also reason it also reason it also reason it
directly it's a herian matrix its Trace directly it's a herian matrix its Trace directly it's a herian matrix its Trace
is equal to one because the probability is equal to one because the probability is equal to one because the probability
sum to one and in the case of a diagonal sum to one and in the case of a diagonal sum to one and in the case of a diagonal
matrix the IG values are the diagonal matrix the IG values are the diagonal matrix the IG values are the diagonal
entries and they're all non- negative entries and they're all non- negative entries and they're all non- negative
because they're probab because they're probab because they're probab
abilities also notice that every abilities also notice that every abilities also notice that every
diagonal density Matrix meaning one diagonal density Matrix meaning one diagonal density Matrix meaning one
whose off diagonal entries are all zero whose off diagonal entries are all zero whose off diagonal entries are all zero
must look like this so in a nutshell must look like this so in a nutshell must look like this so in a nutshell
probabilistic States can be associated probabilistic States can be associated probabilistic States can be associated
with diagonal density matrices notice by with diagonal density matrices notice by with diagonal density matrices notice by
the way that this is consistent with the the way that this is consistent with the the way that this is consistent with the
informal interpretation of the entries informal interpretation of the entries informal interpretation of the entries
of density matrices that I mentioned at of density matrices that I mentioned at of density matrices that I mentioned at
the start of the the start of the the start of the
lesson in particular nothing is in lesson in particular nothing is in lesson in particular nothing is in
Quantum superposition here it's Quantum superposition here it's Quantum superposition here it's
basically just classical Randomness and basically just classical Randomness and basically just classical Randomness and
correspondingly all of the off diagonal correspondingly all of the off diagonal correspondingly all of the off diagonal
entries are entries are entries are
zero earlier in this series we discussed zero earlier in this series we discussed zero earlier in this series we discussed
the spectral theorem and we did that the spectral theorem and we did that the spectral theorem and we did that
specifically for unitary matrices in the specifically for unitary matrices in the specifically for unitary matrices in the
context of phase context of phase context of phase
estimation the spectal theorem is more estimation the spectal theorem is more estimation the spectal theorem is more
General than that however meaning that General than that however meaning that General than that however meaning that
it tells us something about a broader it tells us something about a broader it tells us something about a broader
class of matrices than just unitary class of matrices than just unitary class of matrices than just unitary
matrices and in particular it applies to matrices and in particular it applies to matrices and in particular it applies to
a class of matrices called normal a class of matrices called normal a class of matrices called normal
matrices which include all matrices as matrices which include all matrices as matrices which include all matrices as
well as unary well as unary well as unary
matrices what we'll do now is to take a matrices what we'll do now is to take a matrices what we'll do now is to take a
look at a different statement of this look at a different statement of this look at a different statement of this
theorem this time focusing on just theorem this time focusing on just theorem this time focusing on just
positive semi-definite majores and then positive semi-definite majores and then positive semi-definite majores and then
we'll see that it tells us something we'll see that it tells us something we'll see that it tells us something
very interesting and critically very interesting and critically very interesting and critically
important about density matrices suppose p is any positive matrices suppose p is any positive
semi-definite Matrix so that includes semi-definite Matrix so that includes semi-definite Matrix so that includes
any density Matrix as well as any any density Matrix as well as any any density Matrix as well as any
positive semi-definite Matrix that positive semi-definite Matrix that positive semi-definite Matrix that
doesn't happen to have Trace equal to doesn't happen to have Trace equal to doesn't happen to have Trace equal to
one one one
p is positive semi-definite so it has to p is positive semi-definite so it has to p is positive semi-definite so it has to
be square or in other words N by n for be square or in other words N by n for be square or in other words N by n for
some choice of N and we're going to need some choice of N and we're going to need some choice of N and we're going to need
to refer to that number n so let N be to refer to that number n so let N be to refer to that number n so let N be
chosen chosen chosen
appropriately or another way to say all appropriately or another way to say all appropriately or another way to say all
this is assume p is n byn what the this is assume p is n byn what the this is assume p is n byn what the
theorem tells us in this case is that theorem tells us in this case is that theorem tells us in this case is that
there must exist an orthonormal basis of there must exist an orthonormal basis of there must exist an orthonormal basis of
vectors which we'll name s0 through S vectors which we'll name s0 through S vectors which we'll name s0 through S
nus one along with non- negative real nus one along with non- negative real nus one along with non- negative real
numbers Lambda 0 through Lambda nus one numbers Lambda 0 through Lambda nus one numbers Lambda 0 through Lambda nus one
such that P can be expressed as is shown such that P can be expressed as is shown such that P can be expressed as is shown
here on the here on the here on the
screen in words if we multiply each screen in words if we multiply each screen in words if we multiply each
Lambda K to Kai k * brasai K and then Lambda K to Kai k * brasai K and then Lambda K to Kai k * brasai K and then
sum ofall K from 0 to n minus one then sum ofall K from 0 to n minus one then sum ofall K from 0 to n minus one then
we'll get we'll get we'll get
P this is called a spectral P this is called a spectral P this is called a spectral
decomposition of P each Lambda K is decomposition of P each Lambda K is decomposition of P each Lambda K is
necessarily an igen value of p and each necessarily an igen value of p and each necessarily an igen value of p and each
s is a unit igen Vector of P s is a unit igen Vector of P s is a unit igen Vector of P
corresponding to that igen value if you corresponding to that igen value if you corresponding to that igen value if you
look back to lesson 7 on phase look back to lesson 7 on phase look back to lesson 7 on phase
estimation you'll find a similar theorem estimation you'll find a similar theorem estimation you'll find a similar theorem
for unitary for unitary for unitary
matrices there the igen values are matrices there the igen values are matrices there the igen values are
complex numbers on the unit circle complex numbers on the unit circle complex numbers on the unit circle
rather than being non- negative real rather than being non- negative real rather than being non- negative real
numbers but otherwise it's the same idea numbers but otherwise it's the same idea numbers but otherwise it's the same idea
and these are really just two special and these are really just two special and these are really just two special
cases of a more General theorem known as cases of a more General theorem known as cases of a more General theorem known as
the spectral the spectral the spectral
theorem it doesn't work for every Matrix theorem it doesn't work for every Matrix theorem it doesn't work for every Matrix
you need the Matrix to commute with its you need the Matrix to commute with its you need the Matrix to commute with its
own conjugate transpose and that's what own conjugate transpose and that's what own conjugate transpose and that's what
it means for a matrix to be it means for a matrix to be it means for a matrix to be
normal but we do in fact always have normal but we do in fact always have normal but we do in fact always have
that for both unitary matrices and that for both unitary matrices and that for both unitary matrices and
positive semi-definite matrices and for positive semi-definite matrices and for positive semi-definite matrices and for
herian matrices more generally where herian matrices more generally where herian matrices more generally where
we'll always have relig values but not we'll always have relig values but not we'll always have relig values but not
necessarily non- negative necessarily non- negative necessarily non- negative
ones so that's a statement of the ones so that's a statement of the ones so that's a statement of the
spectral theorem for just positive spectral theorem for just positive spectral theorem for just positive
semi-definite matrices and now let's see semi-definite matrices and now let's see semi-definite matrices and now let's see
what it tells us about density what it tells us about density what it tells us about density
matrices what it tells us is that for an matrices what it tells us is that for an matrices what it tells us is that for an
arbitrary n byn density Matrix we can arbitrary n byn density Matrix we can arbitrary n byn density Matrix we can
always express that Matrix as a convex always express that Matrix as a convex always express that Matrix as a convex
combination of n pure States combination of n pure States combination of n pure States
and moreover we can take those n pure and moreover we can take those n pure and moreover we can take those n pure
states to be states to be states to be
orthogonal that follows quite directly orthogonal that follows quite directly orthogonal that follows quite directly
from the theorem together with a basic from the theorem together with a basic from the theorem together with a basic
fact from linear algebra which is that fact from linear algebra which is that fact from linear algebra which is that
the sum of the igen values of any Matrix the sum of the igen values of any Matrix the sum of the igen values of any Matrix
is always equal to the trace of that is always equal to the trace of that is always equal to the trace of that
Matrix in other words all we're really Matrix in other words all we're really Matrix in other words all we're really
doing here is renaming the igen values p doing here is renaming the igen values p doing here is renaming the igen values p
0 through PN minus one and then 0 through PN minus one and then 0 through PN minus one and then
observing that when we do that and observing that when we do that and observing that when we do that and
collect those IG values into an nend collect those IG values into an nend collect those IG values into an nend
dimensional Vector we necessarily get a dimensional Vector we necessarily get a dimensional Vector we necessarily get a
ability Vector because they all have to ability Vector because they all have to ability Vector because they all have to
be non- negative and their sum equals be non- negative and their sum equals be non- negative and their sum equals
the trace of row which is one because the trace of row which is one because the trace of row which is one because
row is a density row is a density row is a density
Matrix so that's pretty simple but it's Matrix so that's pretty simple but it's Matrix so that's pretty simple but it's
really quite really quite really quite
fundamental conceptually what it tells fundamental conceptually what it tells fundamental conceptually what it tells
us is that every density Matrix no us is that every density Matrix no us is that every density Matrix no
matter how complicated it might seem matter how complicated it might seem matter how complicated it might seem
could arise from first selecting an could arise from first selecting an could arise from first selecting an
orthonormal basis of whatever system orthonormal basis of whatever system orthonormal basis of whatever system
we're talking about and then randomly we're talking about and then randomly we're talking about and then randomly
choosing a Quantum state from that basis choosing a Quantum state from that basis choosing a Quantum state from that basis
According to some choice for the prob According to some choice for the prob According to some choice for the prob
prob prob prob
abilities if it's not immediately clear abilities if it's not immediately clear abilities if it's not immediately clear
on the surface why that's interesting or on the surface why that's interesting or on the surface why that's interesting or
important that's okay but we do use this important that's okay but we do use this important that's okay but we do use this
fact very frequently when we're fact very frequently when we're fact very frequently when we're
reasoning about density matrices and in reasoning about density matrices and in reasoning about density matrices and in
fact expressing a given density Matrix fact expressing a given density Matrix fact expressing a given density Matrix
in this way is often the very first in this way is often the very first in this way is often the very first
thing that we do when we want to thing that we do when we want to thing that we do when we want to
understand it understand it understand it
better here's an example recall the better here's an example recall the better here's an example recall the
density Matrix that we came up with density Matrix that we came up with density Matrix that we came up with
earlier where we randomly chose between earlier where we randomly chose between earlier where we randomly chose between
the zero State and in the plus State the zero State and in the plus State the zero State and in the plus State
each with probability 1/2 which as we each with probability 1/2 which as we each with probability 1/2 which as we
calculated has a density Matrix that calculated has a density Matrix that calculated has a density Matrix that
looks like looks like looks like
this the zero State and the plus state this the zero State and the plus state this the zero State and the plus state
are not orthogonal so this isn't a are not orthogonal so this isn't a are not orthogonal so this isn't a
spectral spectral spectral
decomposition but if we compute one decomposition but if we compute one decomposition but if we compute one
which we can do with a computer or by which we can do with a computer or by which we can do with a computer or by
hand in a case like this then what we hand in a case like this then what we hand in a case like this then what we
get is this expression of our density get is this expression of our density get is this expression of our density
Matrix where the IG values are these Matrix where the IG values are these Matrix where the IG values are these
numbers that keep on showing up and numbers that keep on showing up and numbers that keep on showing up and
where the corresponding IG vectors can where the corresponding IG vectors can where the corresponding IG vectors can
be chosen as is shown down here on the be chosen as is shown down here on the be chosen as is shown down here on the
screen and again we've seen these screen and again we've seen these screen and again we've seen these
vectors before for instance in the vectors before for instance in the vectors before for instance in the
context of the chsh game in Lesson context of the chsh game in Lesson context of the chsh game in Lesson
Four so that's interesting according to Four so that's interesting according to Four so that's interesting according to
the original expression this state is the original expression this state is the original expression this state is
what we get by uniformally choosing what we get by uniformally choosing what we get by uniformally choosing
between two states the zero State and between two states the zero State and between two states the zero State and
the plus state but in fact we get the plus state but in fact we get the plus state but in fact we get
exactly the same density matrix by exactly the same density matrix by exactly the same density matrix by
randomly choosing between two pure randomly choosing between two pure randomly choosing between two pure
States but we're very very heavily States but we're very very heavily States but we're very very heavily
biasing towards one of them about 85% biasing towards one of them about 85% biasing towards one of them about 85%
for the first one and only about 15% for for the first one and only about 15% for for the first one and only about 15% for
the second the second the second
one so in some sense there's actually one so in some sense there's actually one so in some sense there's actually
rather less Randomness inherent to the rather less Randomness inherent to the rather less Randomness inherent to the
state than it originally might have state than it originally might have state than it originally might have
seemed there's a very nice geometric way seemed there's a very nice geometric way seemed there's a very nice geometric way
of thinking about and visualizing of thinking about and visualizing of thinking about and visualizing
Quantum states of a single Cubit known Quantum states of a single Cubit known Quantum states of a single Cubit known
as the block sphere it only works for as the block sphere it only works for as the block sphere it only works for
single cubits for systems having three single cubits for systems having three single cubits for systems having three
or more classical States including two or more classical States including two or more classical States including two
or more cubits together it doesn't work or more cubits together it doesn't work or more cubits together it doesn't work
anymore so it is quite limited but anymore so it is quite limited but anymore so it is quite limited but
nevertheless it's it's a great way to nevertheless it's it's a great way to nevertheless it's it's a great way to
think about the states of a single think about the states of a single think about the states of a single
Cubit it could have been introduced way Cubit it could have been introduced way Cubit it could have been introduced way
back in lesson one of the series on back in lesson one of the series on back in lesson one of the series on
single systems but it's not really single systems but it's not really single systems but it's not really
essential to understanding how Quantum essential to understanding how Quantum essential to understanding how Quantum
information works and it also has a information works and it also has a information works and it also has a
really nice connection with density really nice connection with density really nice connection with density
matrices so it fits naturally into this matrices so it fits naturally into this matrices so it fits naturally into this
lesson and in fact I'm going to explain lesson and in fact I'm going to explain lesson and in fact I'm going to explain
it in a somewhat different way than it's it in a somewhat different way than it's it in a somewhat different way than it's
perhaps most typically explained by perhaps most typically explained by perhaps most typically explained by
using density matrices to effectively using density matrices to effectively using density matrices to effectively
derive it so if you're already familiar derive it so if you're already familiar derive it so if you're already familiar
with the block sphere this explanation with the block sphere this explanation with the block sphere this explanation
may be a little bit different from the may be a little bit different from the may be a little bit different from the
way you first learned about it but in way you first learned about it but in way you first learned about it but in
the end it should of course be the end it should of course be the end it should of course be
equivalent we'll start by thinking about equivalent we'll start by thinking about equivalent we'll start by thinking about
the possible pure states of a cubit the possible pure states of a cubit the possible pure states of a cubit
first as represented by Quantum State first as represented by Quantum State first as represented by Quantum State
vectors up to a global phase every Cubit vectors up to a global phase every Cubit vectors up to a global phase every Cubit
Quantum State Vector can be expressed as Quantum State Vector can be expressed as Quantum State Vector can be expressed as
is shown here on the is shown here on the is shown here on the
screen to be precise we have two real screen to be precise we have two real screen to be precise we have two real
numbers Theta and fi and we'll think numbers Theta and fi and we'll think numbers Theta and fi and we'll think
about these two numbers as being angles about these two numbers as being angles about these two numbers as being angles
expressed in expressed in expressed in
radians the first number Theta is radians the first number Theta is radians the first number Theta is
between 0 and Pi or equivalently it's an between 0 and Pi or equivalently it's an between 0 and Pi or equivalently it's an
angle between 0 and angle between 0 and angle between 0 and
180° and the second number five is 180° and the second number five is 180° and the second number five is
between 0 and 2 pi including 0 but not 2 between 0 and 2 pi including 0 but not 2 between 0 and 2 pi including 0 but not 2
pi so that's any angle between 0 and pi so that's any angle between 0 and pi so that's any angle between 0 and
360° the Zero Entry of our Vector is 360° the Zero Entry of our Vector is 360° the Zero Entry of our Vector is
cosine of theta / 2 so that's a real cosine of theta / 2 so that's a real cosine of theta / 2 so that's a real
number between 0 and 1 and in particular number between 0 and 1 and in particular number between 0 and 1 and in particular
it's necessarily non- negative it's necessarily non- negative it's necessarily non- negative
and the one entry of our Vector is e to and the one entry of our Vector is e to and the one entry of our Vector is e to
the I * 5 * the S of th / 2 and that can the I * 5 * the S of th / 2 and that can the I * 5 * the S of th / 2 and that can
be any complex number on or inside the be any complex number on or inside the be any complex number on or inside the
complex unit complex unit complex unit
circle this is always a unit vector or circle this is always a unit vector or circle this is always a unit vector or
in other words it's a valid Quantum in other words it's a valid Quantum in other words it's a valid Quantum
State vector and for any given cubic State vector and for any given cubic State vector and for any given cubic
Quantum State Vector we can always find Quantum State Vector we can always find Quantum State Vector we can always find
a vector of this form that's equivalent a vector of this form that's equivalent a vector of this form that's equivalent
to that Vector up to a global to that Vector up to a global to that Vector up to a global
phase in particular we can always phase in particular we can always phase in particular we can always
multiply the given vector by a global multiply the given vector by a global multiply the given vector by a global
phase to make the Zero Entry a non- phase to make the Zero Entry a non- phase to make the Zero Entry a non-
negative real number choose whatever negative real number choose whatever negative real number choose whatever
Theta Works to give us that entry and Theta Works to give us that entry and Theta Works to give us that entry and
then pick five to match the one entry then pick five to match the one entry then pick five to match the one entry
the magnitude of the one entry will the magnitude of the one entry will the magnitude of the one entry will
always necessarily agree with the sign always necessarily agree with the sign always necessarily agree with the sign
of theta over two by the assumption that of theta over two by the assumption that of theta over two by the assumption that
we started with a unit we started with a unit we started with a unit
Vector now it isn't quite the case that Vector now it isn't quite the case that Vector now it isn't quite the case that
Theta and F are uniquely determined by Theta and F are uniquely determined by Theta and F are uniquely determined by
whatever Quantum State Vector we started whatever Quantum State Vector we started whatever Quantum State Vector we started
with but it's almost the case there's with but it's almost the case there's with but it's almost the case there's
always going to be a unique value of always going to be a unique value of always going to be a unique value of
theta between zero and Pi that works theta between zero and Pi that works theta between zero and Pi that works
and usually there's a unique value of and usually there's a unique value of and usually there's a unique value of
five that works but in two special cases five that works but in two special cases five that works but in two special cases
that's not that's not that's not
true in particular if we started with a true in particular if we started with a true in particular if we started with a
standard basis State then Theta will be standard basis State then Theta will be standard basis State then Theta will be
either zero or pi and in both cases it either zero or pi and in both cases it either zero or pi and in both cases it
doesn't actually matter what f is it's doesn't actually matter what f is it's doesn't actually matter what f is it's
completely completely completely
irrelevant when Theta is zero the cosine irrelevant when Theta is zero the cosine irrelevant when Theta is zero the cosine
of theta / 2 is 1 and the S of theta / 2 of theta / 2 is 1 and the S of theta / 2 of theta / 2 is 1 and the S of theta / 2
is z so we're multiplying e to the I * F is z so we're multiplying e to the I * F is z so we're multiplying e to the I * F
to zero so it doesn't make any to zero so it doesn't make any to zero so it doesn't make any
difference what f is difference what f is difference what f is
and if Theta is pi then this time the and if Theta is pi then this time the and if Theta is pi then this time the
cosine is zero and the S is one so we cosine is zero and the S is one so we cosine is zero and the S is one so we
have e to the I * 5 * kat1 but those have e to the I * 5 * kat1 but those have e to the I * 5 * kat1 but those
vectors are all equivalent up to a vectors are all equivalent up to a vectors are all equivalent up to a
global phase so again it doesn't matter global phase so again it doesn't matter global phase so again it doesn't matter
what f what f what f
is however if Theta lies properly is however if Theta lies properly is however if Theta lies properly
between Z and Pi then there's always between Z and Pi then there's always between Z and Pi then there's always
going to be a unique value of F that going to be a unique value of F that going to be a unique value of F that
works and by the way this possible works and by the way this possible works and by the way this possible
degeneracy in the value of f for the two degeneracy in the value of f for the two degeneracy in the value of f for the two
special cases will make perfect sense special cases will make perfect sense special cases will make perfect sense
when we see the geometric picture in when we see the geometric picture in when we see the geometric picture in
just a little bit now let's compute the just a little bit now let's compute the just a little bit now let's compute the
density Matrix associated with a state density Matrix associated with a state density Matrix associated with a state
Vector of the form that we just Vector of the form that we just Vector of the form that we just
identified so this is going to be a pure identified so this is going to be a pure identified so this is going to be a pure
state of a state of a state of a
cubit to do this we multiply the vector cubit to do this we multiply the vector cubit to do this we multiply the vector
to its conjugate transpose as usual and to its conjugate transpose as usual and to its conjugate transpose as usual and
on the screen is an expression of the on the screen is an expression of the on the screen is an expression of the
Matrix that we get by Computing the Matrix that we get by Computing the Matrix that we get by Computing the
conjugate transpose and doing the conjugate transpose and doing the conjugate transpose and doing the
multiplication now just like every 2x2 multiplication now just like every 2x2 multiplication now just like every 2x2
matrix there's a unique way to obtain matrix there's a unique way to obtain matrix there's a unique way to obtain
this matx matrix by taking a linear this matx matrix by taking a linear this matx matrix by taking a linear
combination of the poly matrices combination of the poly matrices combination of the poly matrices
including the identity including the identity including the identity
Matrix in particular we can get this Matrix in particular we can get this Matrix in particular we can get this
matrix by choosing the coefficients of matrix by choosing the coefficients of matrix by choosing the coefficients of
the poly matrices as is shown here on the poly matrices as is shown here on the poly matrices as is shown here on
the screen we're putting a two in the the screen we're putting a two in the the screen we're putting a two in the
denominator because that's going to make denominator because that's going to make denominator because that's going to make
everything work out nicely nothing everything work out nicely nothing everything work out nicely nothing
forces us to do this but we can and it's forces us to do this but we can and it's forces us to do this but we can and it's
going to be helpful to do that but aside going to be helpful to do that but aside going to be helpful to do that but aside
from this cosmetic Choice there is no from this cosmetic Choice there is no from this cosmetic Choice there is no
Freedom here this is the only way to get Freedom here this is the only way to get Freedom here this is the only way to get
this matrix by taking a linear this matrix by taking a linear this matrix by taking a linear
combination of poly matrices that's combination of poly matrices that's combination of poly matrices that's
because the poly matrices including the because the poly matrices including the because the poly matrices including the
identity Matrix are a basis for the identity Matrix are a basis for the identity Matrix are a basis for the
vector space of all 2x2 matrices one way vector space of all 2x2 matrices one way vector space of all 2x2 matrices one way
to check the correctness of this to check the correctness of this to check the correctness of this
expression is to make use of some basic expression is to make use of some basic expression is to make use of some basic
trigonometric identities and in trigonometric identities and in trigonometric identities and in
particular if I move out of the way for particular if I move out of the way for particular if I move out of the way for
just a moment here we have some just a moment here we have some just a moment here we have some
identities that will do the trick so if identities that will do the trick so if identities that will do the trick so if
you're so inclined pause the video or you're so inclined pause the video or you're so inclined pause the video or
take a screenshot and see if you can take a screenshot and see if you can take a screenshot and see if you can
obtain the second expression of the obtain the second expression of the obtain the second expression of the
density Matrix on the bottom from the density Matrix on the bottom from the density Matrix on the bottom from the
first so in summary any pure state of a first so in summary any pure state of a first so in summary any pure state of a
cubit expressed as a density Matrix can cubit expressed as a density Matrix can cubit expressed as a density Matrix can
be written as a linear combination of be written as a linear combination of be written as a linear combination of
poly matrices like is shown on the poly matrices like is shown on the poly matrices like is shown on the
screen for Theta between 0 and pi and F screen for Theta between 0 and pi and F screen for Theta between 0 and pi and F
between 0 and 2 between 0 and 2 between 0 and 2
pi notice that we have precisely the pi notice that we have precisely the pi notice that we have precisely the
same degeneracy as before which is that same degeneracy as before which is that same degeneracy as before which is that
fi is irrelevant when Theta is either fi is irrelevant when Theta is either fi is irrelevant when Theta is either
zero or pi and this time we can actually zero or pi and this time we can actually zero or pi and this time we can actually
see that more clearly because the S of see that more clearly because the S of see that more clearly because the S of
theta is is zero in those two cases and theta is is zero in those two cases and theta is is zero in those two cases and
otherwise it isn't it's worth pointing otherwise it isn't it's worth pointing otherwise it isn't it's worth pointing
out that in this formula we have Theta out that in this formula we have Theta out that in this formula we have Theta
appearing as an argument to the sign and appearing as an argument to the sign and appearing as an argument to the sign and
cosine functions rather than Theta over cosine functions rather than Theta over cosine functions rather than Theta over
2 as in the original Quantum State 2 as in the original Quantum State 2 as in the original Quantum State
Vector version and that's because we use Vector version and that's because we use Vector version and that's because we use
those trigonometric identities the first those trigonometric identities the first those trigonometric identities the first
three of which are basically three of which are basically three of which are basically
rearrangements of the so-called double rearrangements of the so-called double rearrangements of the so-called double
angle formulas so the angle effectively angle formulas so the angle effectively angle formulas so the angle effectively
gets gets gets
doubled and now we're finally ready to doubled and now we're finally ready to doubled and now we're finally ready to
talk about the block sphere talk about the block sphere talk about the block sphere
itself let's focus on the three itself let's focus on the three itself let's focus on the three
coefficients of the non-identity poly coefficients of the non-identity poly coefficients of the non-identity poly
matrices in the numerator of the matrices in the numerator of the matrices in the numerator of the
expression of our density Matrix so expression of our density Matrix so expression of our density Matrix so
we're ignoring the two in the we're ignoring the two in the we're ignoring the two in the
denominator we're also ignoring the denominator we're also ignoring the denominator we're also ignoring the
identity Matrix because the coefficient identity Matrix because the coefficient identity Matrix because the coefficient
on that one never on that one never on that one never
changes that happens because that changes that happens because that changes that happens because that
coefficient is determined by the trace coefficient is determined by the trace coefficient is determined by the trace
of whatever Matrix we're expressing and of whatever Matrix we're expressing and of whatever Matrix we're expressing and
density matrices always have Trace equal density matrices always have Trace equal density matrices always have Trace equal
to to to
one now imagine that we have three axes one now imagine that we have three axes one now imagine that we have three axes
labeled x y and z oriented according to labeled x y and z oriented according to labeled x y and z oriented according to
the so-called right- hand rule and the so-called right- hand rule and the so-called right- hand rule and
consider the point whose coordinates consider the point whose coordinates consider the point whose coordinates
match with the three match with the three match with the three
coefficients and just to be clear this coefficients and just to be clear this coefficients and just to be clear this
is ordinary three-dimensional space is ordinary three-dimensional space is ordinary three-dimensional space
we're talking about where the we're talking about where the we're talking about where the
coordinates are always real numbers the coordinates are always real numbers the coordinates are always real numbers the
x coordinate corresponds to the x coordinate corresponds to the x coordinate corresponds to the
coefficient of the sigma X poly Matrix coefficient of the sigma X poly Matrix coefficient of the sigma X poly Matrix
in the numerator of our expression which in the numerator of our expression which in the numerator of our expression which
is always a real number because density is always a real number because density is always a real number because density
matrices are herian and likewise for Y matrices are herian and likewise for Y matrices are herian and likewise for Y
and and and
Z what we find is that this point that Z what we find is that this point that Z what we find is that this point that
we obtain is always on the unit sphere we obtain is always on the unit sphere we obtain is always on the unit sphere
or in other words the sphere having or in other words the sphere having or in other words the sphere having
radius equal to radius equal to radius equal to
one and in particular it's the point we one and in particular it's the point we one and in particular it's the point we
get if we start at the point where the get if we start at the point where the get if we start at the point where the
positive z- axis intersects the sphere positive z- axis intersects the sphere positive z- axis intersects the sphere
which we can think about as the North which we can think about as the North which we can think about as the North
Pole rotate down towards the positive Pole rotate down towards the positive Pole rotate down towards the positive
x-axis by an angle of theta and then x-axis by an angle of theta and then x-axis by an angle of theta and then
rotate towards the positive y- AIS or in rotate towards the positive y- AIS or in rotate towards the positive y- AIS or in
other words East by an angle of other words East by an angle of other words East by an angle of
fi in this context the angle Theta is fi in this context the angle Theta is fi in this context the angle Theta is
called the polar angle which is easy to called the polar angle which is easy to called the polar angle which is easy to
remember because it starts at the pole remember because it starts at the pole remember because it starts at the pole
and F is called the azimuthal and F is called the azimuthal and F is called the azimuthal
angle and together these two angles angle and together these two angles angle and together these two angles
represent spherical coordinates for the represent spherical coordinates for the represent spherical coordinates for the
points on the unit sphere and if you points on the unit sphere and if you points on the unit sphere and if you
wanted to you could also allow the wanted to you could also allow the wanted to you could also allow the
radius to vary to represent arbitrary radius to vary to represent arbitrary radius to vary to represent arbitrary
points in threedimensional space but points in threedimensional space but points in threedimensional space but
here we're focusing on points on the here we're focusing on points on the here we're focusing on points on the
unit sphere where the radius is equal to unit sphere where the radius is equal to unit sphere where the radius is equal to
one you can also think about Theta as one you can also think about Theta as one you can also think about Theta as
representing latitude except that we go representing latitude except that we go representing latitude except that we go
from 0 to 180° from the North Pole to from 0 to 180° from the North Pole to from 0 to 180° from the North Pole to
the south pole rather than plus orus 90° the south pole rather than plus orus 90° the south pole rather than plus orus 90°
from the equator and F represents from the equator and F represents from the equator and F represents
longitude where we basically Define the longitude where we basically Define the longitude where we basically Define the
prime meridian to be the Meridian that prime meridian to be the Meridian that prime meridian to be the Meridian that
intersects the positive intersects the positive intersects the positive
xaxis if you're like me by the way and xaxis if you're like me by the way and xaxis if you're like me by the way and
you have trouble keeping straight which you have trouble keeping straight which you have trouble keeping straight which
one is latitude and which one is one is latitude and which one is one is latitude and which one is
longitude it may be helpful to remember longitude it may be helpful to remember longitude it may be helpful to remember
that longitude corresponds to the that longitude corresponds to the that longitude corresponds to the
interval that's longer 2 pi as opposed interval that's longer 2 pi as opposed interval that's longer 2 pi as opposed
to to to
Pi anyway the point we obtain on the Pi anyway the point we obtain on the Pi anyway the point we obtain on the
unit sphere from our Cubit State Vector unit sphere from our Cubit State Vector unit sphere from our Cubit State Vector
s by thinking about coordinates in this s by thinking about coordinates in this s by thinking about coordinates in this
way is the block sphere representation way is the block sphere representation way is the block sphere representation
of the quantum State corresponding to of the quantum State corresponding to of the quantum State corresponding to
SAI and when we think about the entire SAI and when we think about the entire SAI and when we think about the entire
three-dimensional unit sphere in this three-dimensional unit sphere in this three-dimensional unit sphere in this
way we obtain the block way we obtain the block way we obtain the block
sphere geometrically it's just a unit sphere geometrically it's just a unit sphere geometrically it's just a unit
sphere in ordinary three-dimensional sphere in ordinary three-dimensional sphere in ordinary three-dimensional
space but in fact that is the geometry space but in fact that is the geometry space but in fact that is the geometry
of Cubit of Cubit of Cubit
States there's a perfect one toone States there's a perfect one toone States there's a perfect one toone
correspondence here every pure state of correspondence here every pure state of correspondence here every pure state of
a cubit corresponds to a unique point on a cubit corresponds to a unique point on a cubit corresponds to a unique point on
the sphere and every point on the sphere the sphere and every point on the sphere the sphere and every point on the sphere
represents a unique Cubit State up to a represents a unique Cubit State up to a represents a unique Cubit State up to a
global phase when we're working with global phase when we're working with global phase when we're working with
Quantum State Quantum State Quantum State
vectors this correspondence does go by vectors this correspondence does go by vectors this correspondence does go by
other names by the way it's also other names by the way it's also other names by the way it's also
sometimes called the poar a sphere for sometimes called the poar a sphere for sometimes called the poar a sphere for
instance but in the context of quantum instance but in the context of quantum instance but in the context of quantum
information it's generally referred to information it's generally referred to information it's generally referred to
as the block as the block as the block
sphere notice that we can now see the sphere notice that we can now see the sphere notice that we can now see the
degeneracy in the angle fi pretty degeneracy in the angle fi pretty degeneracy in the angle fi pretty
clearly because longitude has no meaning clearly because longitude has no meaning clearly because longitude has no meaning
at the North and South Poles but aside at the North and South Poles but aside at the North and South Poles but aside
from the poles every point on the Earth from the poles every point on the Earth from the poles every point on the Earth
has unique coordinates in terms of has unique coordinates in terms of has unique coordinates in terms of
latitude and longitude and the same latitude and longitude and the same latitude and longitude and the same
thing is happening here so that is how thing is happening here so that is how thing is happening here so that is how
the block sphere Works in the block sphere Works in the block sphere Works in
general now let's take a look at a few general now let's take a look at a few general now let's take a look at a few
specific examples of Cubit States and specific examples of Cubit States and specific examples of Cubit States and
where they lie on the Block where they lie on the Block where they lie on the Block
sphere let's start with the standard sphere let's start with the standard sphere let's start with the standard
basis these are the points we obtain basis these are the points we obtain basis these are the points we obtain
when the polar angle Theta is zero for when the polar angle Theta is zero for when the polar angle Theta is zero for
the zero State and Pi for the one the zero State and Pi for the one the zero State and Pi for the one
state these are the two cases where the state these are the two cases where the state these are the two cases where the
asmuth angle fi is irrelevant as I've asmuth angle fi is irrelevant as I've asmuth angle fi is irrelevant as I've
already already already
discussed we can also Express the discussed we can also Express the discussed we can also Express the
corresponding density matrices as linear corresponding density matrices as linear corresponding density matrices as linear
combinations of poly matrices in either combinations of poly matrices in either combinations of poly matrices in either
way we conclude that these two states way we conclude that these two states way we conclude that these two states
correspond to the North Pole and to the correspond to the North Pole and to the correspond to the North Pole and to the
South Pole of the block sphere the zero South Pole of the block sphere the zero South Pole of the block sphere the zero
state is on the top and the one state is state is on the top and the one state is state is on the top and the one state is
on the bottom next let's find the plus and bottom next let's find the plus and
minus states on the Block minus states on the Block minus states on the Block
sphere again we can either think about sphere again we can either think about sphere again we can either think about
Quantum State vectors or or density Quantum State vectors or or density Quantum State vectors or or density
matrices for Quantum State vectors this matrices for Quantum State vectors this matrices for Quantum State vectors this
time the polar angle Theta has to be Pi time the polar angle Theta has to be Pi time the polar angle Theta has to be Pi
/ 2 in order to get the Zero Entry to be / 2 in order to get the Zero Entry to be / 2 in order to get the Zero Entry to be
1/ &lt; 1/ &lt; 1/ &lt;
tk2 the azimuthal angle F has to be zero tk2 the azimuthal angle F has to be zero tk2 the azimuthal angle F has to be zero
for the plus State because the one entry for the plus State because the one entry for the plus State because the one entry
is positive for the plus State and it is positive for the plus State and it is positive for the plus State and it
has to be Pi or 180° for the minus State has to be Pi or 180° for the minus State has to be Pi or 180° for the minus State
because that one is negative in that because that one is negative in that because that one is negative in that
case so these two points lie on the case so these two points lie on the case so these two points lie on the
equator of the block sphere the plus equator of the block sphere the plus equator of the block sphere the plus
state is where the positive x-axis state is where the positive x-axis state is where the positive x-axis
intersects the sphere and the minor intersects the sphere and the minor intersects the sphere and the minor
state is on the opposite side where the state is on the opposite side where the state is on the opposite side where the
negative x-axis intersects the negative x-axis intersects the negative x-axis intersects the
sphere equivalently we can also see from sphere equivalently we can also see from sphere equivalently we can also see from
the polymatrix expressions for the the polymatrix expressions for the the polymatrix expressions for the
corresponding density matrices that the corresponding density matrices that the corresponding density matrices that the
Y and Z coordinates are both zero Y and Z coordinates are both zero Y and Z coordinates are both zero
whereas the x coordinates are positive whereas the x coordinates are positive whereas the x coordinates are positive
and negative and negative and negative
1 another example is the plus I minus I 1 another example is the plus I minus I 1 another example is the plus I minus I
basis which is similar to the plus minus basis which is similar to the plus minus basis which is similar to the plus minus
basis except that we also multiply the basis except that we also multiply the basis except that we also multiply the
one entry by I the imaginary unit one entry by I the imaginary unit one entry by I the imaginary unit
we saw the plus I State earlier in the we saw the plus I State earlier in the we saw the plus I State earlier in the
lesson and the minus I state is defined lesson and the minus I state is defined lesson and the minus I state is defined
in an analogous in an analogous in an analogous
way the polar angle Theta is the same as way the polar angle Theta is the same as way the polar angle Theta is the same as
it was for the plus and minus States pi/ it was for the plus and minus States pi/ it was for the plus and minus States pi/
2 and this time we need to choose the 2 and this time we need to choose the 2 and this time we need to choose the
athal angle F so that we get I and athal angle F so that we get I and athal angle F so that we get I and
negative I rather than 1 and negative 1 negative I rather than 1 and negative 1 negative I rather than 1 and negative 1
and Pi / 2 and 3 Pi / 2 are the right and Pi / 2 and 3 Pi / 2 are the right and Pi / 2 and 3 Pi / 2 are the right
choices to make that choices to make that choices to make that
happen once again we can also look at happen once again we can also look at happen once again we can also look at
the density Matrix descriptions of these the density Matrix descriptions of these the density Matrix descriptions of these
states and things are pretty similar to states and things are pretty similar to states and things are pretty similar to
the previous two examples except that the previous two examples except that the previous two examples except that
this time is the poly y Matrix in place this time is the poly y Matrix in place this time is the poly y Matrix in place
of Z or of Z or of Z or
X so on the Block sphere these two X so on the Block sphere these two X so on the Block sphere these two
states are located where the y- AIS states are located where the y- AIS states are located where the y- AIS
intersects the sphere and if we clean intersects the sphere and if we clean intersects the sphere and if we clean
things up a bit we can see more clearly things up a bit we can see more clearly things up a bit we can see more clearly
how the Six States we just discussed how the Six States we just discussed how the Six States we just discussed
relate to one relate to one relate to one
another for just one more example or another for just one more example or another for just one more example or
really a class of examples consider the really a class of examples consider the really a class of examples consider the
Cubit State vectors that came up earlier Cubit State vectors that came up earlier Cubit State vectors that came up earlier
and in Lesson Four and in Lesson Four and in Lesson Four
where the Zero Entry is cosine of alpha where the Zero Entry is cosine of alpha where the Zero Entry is cosine of alpha
for some real number Alpha and the one for some real number Alpha and the one for some real number Alpha and the one
entry is the S of alpha we can by the entry is the S of alpha we can by the entry is the S of alpha we can by the
way restrict our attention to values of way restrict our attention to values of way restrict our attention to values of
alpha between zero and Pi because any alpha between zero and Pi because any alpha between zero and Pi because any
other value of alpha will give us a other value of alpha will give us a other value of alpha will give us a
vector that's equivalent up to a global vector that's equivalent up to a global vector that's equivalent up to a global
phase to one where Alpha is in this phase to one where Alpha is in this phase to one where Alpha is in this
range if we compute the density matrices range if we compute the density matrices range if we compute the density matrices
associated with these states expressed associated with these states expressed associated with these states expressed
as linear combinations of poly matrices as linear combinations of poly matrices as linear combinations of poly matrices
we get the expression that is shown here we get the expression that is shown here we get the expression that is shown here
on the screen that computation isn't on the screen that computation isn't on the screen that computation isn't
shown here but that is what you should shown here but that is what you should shown here but that is what you should
get if you go through get if you go through get if you go through
it and what we find is that these states it and what we find is that these states it and what we find is that these states
form a so-called great circle that goes form a so-called great circle that goes form a so-called great circle that goes
through the poles specifically starting through the poles specifically starting through the poles specifically starting
from the North Pole when Alpha is zero from the North Pole when Alpha is zero from the North Pole when Alpha is zero
going down the prime meridian and going down the prime meridian and going down the prime meridian and
hitting the South Pole when Alpha is pi hitting the South Pole when Alpha is pi hitting the South Pole when Alpha is pi
over 2 and then back to the North Pole over 2 and then back to the North Pole over 2 and then back to the North Pole
on the opposite side of the sphere as on the opposite side of the sphere as on the opposite side of the sphere as
Alpha ranges from pi over 2 to Alpha ranges from pi over 2 to Alpha ranges from pi over 2 to
Pi and that all makes sense because Pi and that all makes sense because Pi and that all makes sense because
number one these are real vectors so the number one these are real vectors so the number one these are real vectors so the
imaginary part is always zero and number imaginary part is always zero and number imaginary part is always zero and number
two the angle Alpha effectively gets two the angle Alpha effectively gets two the angle Alpha effectively gets
doubled on the Block sphere and once doubled on the Block sphere and once doubled on the Block sphere and once
again that can be explained by the use again that can be explained by the use again that can be explained by the use
of the double angle of the double angle of the double angle
formulas we've seen that the block formulas we've seen that the block formulas we've seen that the block
sphere provides a very nice geometric sphere provides a very nice geometric sphere provides a very nice geometric
representation for the pure states of a representation for the pure states of a representation for the pure states of a
cubit where each point on the sphere cubit where each point on the sphere cubit where each point on the sphere
corresponds to a unique pure state of a corresponds to a unique pure state of a corresponds to a unique pure state of a
cubit and vice cubit and vice cubit and vice
versa when we refer to the block ball versa when we refer to the block ball versa when we refer to the block ball
we're talking about all of the points on we're talking about all of the points on we're talking about all of the points on
the Block sphere as well as the points the Block sphere as well as the points the Block sphere as well as the points
contained inside of it contained inside of it contained inside of it
points on the interior of the block ball points on the interior of the block ball points on the interior of the block ball
also correspond to Cubit States also correspond to Cubit States also correspond to Cubit States
specifically states that are not pure specifically states that are not pure specifically states that are not pure
States so in other words these are Cubit States so in other words these are Cubit States so in other words these are Cubit
states that can be represented by states that can be represented by states that can be represented by
density matrices but not by Cubit State density matrices but not by Cubit State density matrices but not by Cubit State
vectors a fundamental aspect of the vectors a fundamental aspect of the vectors a fundamental aspect of the
block ball and the way that the interior block ball and the way that the interior block ball and the way that the interior
points represent impure States is that points represent impure States is that points represent impure States is that
convex combinations of points inside of convex combinations of points inside of convex combinations of points inside of
the block ball correspond to convex the block ball correspond to convex the block ball correspond to convex
combinations of density matrices combinations of density matrices combinations of density matrices
intuitively speaking this means that the intuitively speaking this means that the intuitively speaking this means that the
geometry continues to work perfectly geometry continues to work perfectly geometry continues to work perfectly
even inside of the block even inside of the block even inside of the block
ball let's take a look at a couple of ball let's take a look at a couple of ball let's take a look at a couple of
examples beginning with the center point examples beginning with the center point examples beginning with the center point
of the block ball which represents the of the block ball which represents the of the block ball which represents the
completely mixed completely mixed completely mixed
state the completely mixed state is state the completely mixed state is state the completely mixed state is
obtained by averaging the zero and one obtained by averaging the zero and one obtained by averaging the zero and one
states which is consistent with the fact states which is consistent with the fact states which is consistent with the fact
that the center of the sphere is exactly that the center of the sphere is exactly that the center of the sphere is exactly
halfway between the North Pole and the halfway between the North Pole and the halfway between the North Pole and the
South Pole and in fact it's halfway South Pole and in fact it's halfway South Pole and in fact it's halfway
between any two antipodal points such as between any two antipodal points such as between any two antipodal points such as
the plus and minus States and the plus I the plus and minus States and the plus I the plus and minus States and the plus I
and minus I States antipodal points in and minus I States antipodal points in and minus I States antipodal points in
the block sphere always correspond to the block sphere always correspond to the block sphere always correspond to
orthogonal Pure States and what this orthogonal Pure States and what this orthogonal Pure States and what this
reveals or reflects is that we can reveals or reflects is that we can reveals or reflects is that we can
always write the completely mixed state always write the completely mixed state always write the completely mixed state
as the average between any two as the average between any two as the average between any two
orthogonal pure Cubit States another way to see this is to States another way to see this is to
think about the poly Matrix think about the poly Matrix think about the poly Matrix
representation of the completely mixed representation of the completely mixed representation of the completely mixed
state to make this work we have to set state to make this work we have to set state to make this work we have to set
the coefficients on all all three the coefficients on all all three the coefficients on all all three
non-identity poly matrices to be zero non-identity poly matrices to be zero non-identity poly matrices to be zero
and so the corresponding Point inside of and so the corresponding Point inside of and so the corresponding Point inside of
the block ball is right in the center at the block ball is right in the center at the block ball is right in the center at
the the the
origin and by the way thinking about origin and by the way thinking about origin and by the way thinking about
poly Matrix representations of density poly Matrix representations of density poly Matrix representations of density
matrices like this actually makes it matrices like this actually makes it matrices like this actually makes it
pretty clear that convex combinations of pretty clear that convex combinations of pretty clear that convex combinations of
block ball points correspond to convex block ball points correspond to convex block ball points correspond to convex
combinations of density combinations of density combinations of density
matrices if we take an average or a matrices if we take an average or a matrices if we take an average or a
weighted average of density matrices and weighted average of density matrices and weighted average of density matrices and
express everything in terms of linear express everything in terms of linear express everything in terms of linear
combinations of poly matrices then of combinations of poly matrices then of combinations of poly matrices then of
course we're just averaging the course we're just averaging the course we're just averaging the
corresponding points because everything corresponding points because everything corresponding points because everything
is linear in those three coefficients for one more example let's coefficients for one more example let's
consider the average of the zero State consider the average of the zero State consider the average of the zero State
and the plus state which we saw earlier and the plus state which we saw earlier and the plus state which we saw earlier
in the in the in the
lesson we calculated the density Matrix lesson we calculated the density Matrix lesson we calculated the density Matrix
representation which is shown on the representation which is shown on the representation which is shown on the
screen and we have that the point is in screen and we have that the point is in screen and we have that the point is in
the interior of the block ball halfway the interior of the block ball halfway the interior of the block ball halfway
between the zero and plus between the zero and plus between the zero and plus
States we also saw that we could express States we also saw that we could express States we also saw that we could express
this density Matrix in a different way this density Matrix in a different way this density Matrix in a different way
based on its spectral decomposition and based on its spectral decomposition and based on its spectral decomposition and
again that is consistent with the again that is consistent with the again that is consistent with the
diagram and the previous diagram and the previous diagram and the previous
examples and that concludes the examples and that concludes the examples and that concludes the
discussion of the block sphere and block discussion of the block sphere and block discussion of the block sphere and block
ball people often ask if there's a block ball people often ask if there's a block ball people often ask if there's a block
sphere or block ball for systems having sphere or block ball for systems having sphere or block ball for systems having
three or more classical States and the three or more classical States and the three or more classical States and the
unfortunate answer is not unfortunate answer is not unfortunate answer is not
really we can always think about the really we can always think about the really we can always think about the
quantum states of a system in geometric quantum states of a system in geometric quantum states of a system in geometric
terms similar to The Block ball and terms similar to The Block ball and terms similar to The Block ball and
we'll get a convex body in a real Vector we'll get a convex body in a real Vector we'll get a convex body in a real Vector
space but if we have three or more space but if we have three or more space but if we have three or more
classical States it won't be classical States it won't be classical States it won't be
spherical for instance if we have spherical for instance if we have spherical for instance if we have
exactly three classical States or a q exactly three classical States or a q exactly three classical States or a q
trit for short we'll get a convex body trit for short we'll get a convex body trit for short we'll get a convex body
in an eight-dimensional in an eight-dimensional in an eight-dimensional
space but the extreme points of that space but the extreme points of that space but the extreme points of that
body which correspond to the pure body which correspond to the pure body which correspond to the pure
Quantum states of a CIT will only be Quantum states of a CIT will only be Quantum states of a CIT will only be
four-dimensional if you can visualize four-dimensional if you can visualize four-dimensional if you can visualize
taking a four-dimensional manifold in an taking a four-dimensional manifold in an taking a four-dimensional manifold in an
eight-dimensional space and somehow eight-dimensional space and somehow eight-dimensional space and somehow
shrink wrapping it to obtain a convex shrink wrapping it to obtain a convex shrink wrapping it to obtain a convex
body and you find that to be a helpful body and you find that to be a helpful body and you find that to be a helpful
way of thinking about cits then more way of thinking about cits then more way of thinking about cits then more
power to you I guess it's interesting to power to you I guess it's interesting to power to you I guess it's interesting to
think about but it's not a ball it's think about but it's not a ball it's think about but it's not a ball it's
more complicated than that in the final more complicated than that in the final more complicated than that in the final
part of the lesson we'll briefly discuss part of the lesson we'll briefly discuss part of the lesson we'll briefly discuss
how density matrices work for multiple how density matrices work for multiple how density matrices work for multiple
systems density matrices can represent systems density matrices can represent systems density matrices can represent
states of multiple systems and this states of multiple systems and this states of multiple systems and this
works in a completely analogous way to works in a completely analogous way to works in a completely analogous way to
Quantum State Quantum State Quantum State
factors in particular just like we have factors in particular just like we have factors in particular just like we have
in a simplified formulation of quantum in a simplified formulation of quantum in a simplified formulation of quantum
information we can view multiple systems information we can view multiple systems information we can view multiple systems
together as if they're single compound together as if they're single compound together as if they're single compound
systems this time both the rows and systems this time both the rows and systems this time both the rows and
Columns of density matrices that Columns of density matrices that Columns of density matrices that
represent states of multiple systems are represent states of multiple systems are represent states of multiple systems are
placed in correspondence with the placed in correspondence with the placed in correspondence with the
cartisian product of the classical State cartisian product of the classical State cartisian product of the classical State
sets of the individual sets of the individual sets of the individual
systems for example we can consider systems for example we can consider systems for example we can consider
density Matrix representations of the density Matrix representations of the density Matrix representations of the
Four Bell states which we obtain Four Bell states which we obtain Four Bell states which we obtain
precisely as we did for other Quantum precisely as we did for other Quantum precisely as we did for other Quantum
State vectors earlier in the lesson by State vectors earlier in the lesson by State vectors earlier in the lesson by
multiplying State vectors to their multiplying State vectors to their multiplying State vectors to their
conjugate transposes and the results are conjugate transposes and the results are conjugate transposes and the results are
shown here on the shown here on the shown here on the
screen in essence there's really nothing screen in essence there's really nothing screen in essence there's really nothing
new at a conceptual level although we new at a conceptual level although we new at a conceptual level although we
will see momentarily that there are some will see momentarily that there are some will see momentarily that there are some
new things that we can do with density new things that we can do with density new things that we can do with density
matrices for multiple matrices for multiple matrices for multiple
systems the Notions of Independence and systems the Notions of Independence and systems the Notions of Independence and
correlations carry over from Quantum correlations carry over from Quantum correlations carry over from Quantum
State vectors to density matrices and in State vectors to density matrices and in State vectors to density matrices and in
particular we find that tensor products particular we find that tensor products particular we find that tensor products
of dens dity matrices represent of dens dity matrices represent of dens dity matrices represent
Independence between Independence between Independence between
systems we use the same terminology that systems we use the same terminology that systems we use the same terminology that
we did for Quantum State vectors for we did for Quantum State vectors for we did for Quantum State vectors for
instance if we have a system X that's instance if we have a system X that's instance if we have a system X that's
prepared in a state row and a system y prepared in a state row and a system y prepared in a state row and a system y
That's independently prepared in a state That's independently prepared in a state That's independently prepared in a state
Sigma then the state of the pair XY Sigma then the state of the pair XY Sigma then the state of the pair XY
together is given by the tensor product together is given by the tensor product together is given by the tensor product
of row and sigma we call this a product of row and sigma we call this a product of row and sigma we call this a product
State and this generalizes in a natural State and this generalizes in a natural State and this generalizes in a natural
way to more than two way to more than two way to more than two
systems density matrices that can't be systems density matrices that can't be systems density matrices that can't be
expressed as product States represent expressed as product States represent expressed as product States represent
correlations between systems and that correlations between systems and that correlations between systems and that
part is similar to Quantum State vectors part is similar to Quantum State vectors part is similar to Quantum State vectors
but as it turns out there are different but as it turns out there are different but as it turns out there are different
types of correlations that can arise types of correlations that can arise types of correlations that can arise
when we use density matrices even for when we use density matrices even for when we use density matrices even for
just two systems in contrast there's just two systems in contrast there's just two systems in contrast there's
really only one type of correlation we really only one type of correlation we really only one type of correlation we
can have for Quantum State vectors and can have for Quantum State vectors and can have for Quantum State vectors and
that's that's that's
entanglement to say this another way entanglement to say this another way entanglement to say this another way
correlation and entanglement are correlation and entanglement are correlation and entanglement are
synonymous for Quantum State vectors but synonymous for Quantum State vectors but synonymous for Quantum State vectors but
that's not true for density matrices that's not true for density matrices that's not true for density matrices
here things are more here things are more here things are more
interesting for instance we can describe interesting for instance we can describe interesting for instance we can describe
correlated classical states with density correlated classical states with density correlated classical states with density
matrices here's an example of a density matrices here's an example of a density matrices here's an example of a density
Matrix representing a classical random Matrix representing a classical random Matrix representing a classical random
bit shared between Alice and bit shared between Alice and bit shared between Alice and
Bob they always have the same classical Bob they always have the same classical Bob they always have the same classical
State Zer or one and the probability is State Zer or one and the probability is State Zer or one and the probability is
1/2 for each of the 1/2 for each of the 1/2 for each of the
possibilities this is not an entangled possibilities this is not an entangled possibilities this is not an entangled
State entanglement is non-classical but State entanglement is non-classical but State entanglement is non-classical but
it is a correlated state it is a correlated state it is a correlated state
another example is connected with the another example is connected with the another example is connected with the
notion of an ensemble of quantum states notion of an ensemble of quantum states notion of an ensemble of quantum states
which you can think of as a probability which you can think of as a probability which you can think of as a probability
distribution over a finite set of distribution over a finite set of distribution over a finite set of
quantum quantum quantum
States in particular let's imagine that States in particular let's imagine that States in particular let's imagine that
we have M density matrices row zero we have M density matrices row zero we have M density matrices row zero
through row M minus one all representing through row M minus one all representing through row M minus one all representing
possible states of a particular system possible states of a particular system possible states of a particular system
along with a probability to draw each along with a probability to draw each along with a probability to draw each
one if we wanted to we could compute the one if we wanted to we could compute the one if we wanted to we could compute the
weighted average of these density weighted average of these density weighted average of these density
matrices by taking a convex comp com of matrices by taking a convex comp com of matrices by taking a convex comp com of
them but that wouldn't actually tell us them but that wouldn't actually tell us them but that wouldn't actually tell us
what the individual states were or what what the individual states were or what what the individual states were or what
the probabilities were what we can do the probabilities were what we can do the probabilities were what we can do
instead and it's quite useful to do this instead and it's quite useful to do this instead and it's quite useful to do this
sometimes is to consider a density sometimes is to consider a density sometimes is to consider a density
Matrix of the form shown here on the Matrix of the form shown here on the Matrix of the form shown here on the
screen this is a state of two systems screen this is a state of two systems screen this is a state of two systems
where the first system has one classical where the first system has one classical where the first system has one classical
state for each of the Alternatives and state for each of the Alternatives and state for each of the Alternatives and
we take the weighted average of M we take the weighted average of M we take the weighted average of M
product states where the state of the product states where the state of the product states where the state of the
first system is always classical and first system is always classical and first system is always classical and
essentially labels which of the m States essentially labels which of the m States essentially labels which of the m States
we have for the second we have for the second we have for the second
system if it's not clear exactly why system if it's not clear exactly why system if it's not clear exactly why
we'd want to think about States like we'd want to think about States like we'd want to think about States like
this that's okay it's just an example of this that's okay it's just an example of this that's okay it's just an example of
a type of correlation where the first a type of correlation where the first a type of correlation where the first
system is always classical and the system is always classical and the system is always classical and the
second system can be in a Quantum second system can be in a Quantum second system can be in a Quantum
State a third category of possibly State a third category of possibly State a third category of possibly
correlated States is the set of correlated States is the set of correlated States is the set of
so-called separable so-called separable so-called separable
States in simple terms these are convex States in simple terms these are convex States in simple terms these are convex
combinations of product States and they combinations of product States and they combinations of product States and they
essentially represent present Quantum essentially represent present Quantum essentially represent present Quantum
states where any correlations between states where any correlations between states where any correlations between
the systems are classical the systems are classical the systems are classical
correlations one way to think about correlations one way to think about correlations one way to think about
States like this is that we make a States like this is that we make a States like this is that we make a
probabilistic selection for a number K probabilistic selection for a number K probabilistic selection for a number K
and then independently prepare two and then independently prepare two and then independently prepare two
systems in a Quantum state that can systems in a Quantum state that can systems in a Quantum state that can
depend on this choice of depend on this choice of depend on this choice of
K what's perhaps most interesting about K what's perhaps most interesting about K what's perhaps most interesting about
states of this form is that this is states of this form is that this is states of this form is that this is
actually how we Define entanglement in actually how we Define entanglement in actually how we Define entanglement in
mathematical mathematical mathematical
terms in particular entanglement is terms in particular entanglement is terms in particular entanglement is
defined as a lack of separability defined as a lack of separability defined as a lack of separability
so if you have a density Matrix and it's so if you have a density Matrix and it's so if you have a density Matrix and it's
not possible to express it in a form not possible to express it in a form not possible to express it in a form
like this then by definition it's like this then by definition it's like this then by definition it's
entangled I alluded to this back in entangled I alluded to this back in entangled I alluded to this back in
Lesson Four and here it is in somewhat Lesson Four and here it is in somewhat Lesson Four and here it is in somewhat
greater greater greater
detail by the way it can be very detail by the way it can be very detail by the way it can be very
difficult to figure out if a given difficult to figure out if a given difficult to figure out if a given
density Matrix represents an entangled density Matrix represents an entangled density Matrix represents an entangled
State even with the help of a State even with the help of a State even with the help of a
computer mathematically and computer mathematically and computer mathematically and
computationally speaking entanglement is computationally speaking entanglement is computationally speaking entanglement is
actually pretty actually pretty actually pretty
complicated I mentioned at the start of complicated I mentioned at the start of complicated I mentioned at the start of
the lesson that there's a very important the lesson that there's a very important the lesson that there's a very important
thing we can do with density manes and thing we can do with density manes and thing we can do with density manes and
that is to describe the states of that is to describe the states of that is to describe the states of
isolated parts of systems even when isolated parts of systems even when isolated parts of systems even when
they're correlated and possibly they're correlated and possibly they're correlated and possibly
entangled with other entangled with other entangled with other
systems the term reduce state is often systems the term reduce state is often systems the term reduce state is often
used in this context and as a simple used in this context and as a simple used in this context and as a simple
example we'll calculate the reduced example we'll calculate the reduced example we'll calculate the reduced
States for the individual cubits that States for the individual cubits that States for the individual cubits that
form an form an form an
ebit to be precise let's suppose that ebit to be precise let's suppose that ebit to be precise let's suppose that
Alice and Bob share an ebit ALICE holds Alice and Bob share an ebit ALICE holds Alice and Bob share an ebit ALICE holds
a cubbit a Bob holds a cubit B and a cubbit a Bob holds a cubit B and a cubbit a Bob holds a cubit B and
together these two cubits are in a five together these two cubits are in a five together these two cubits are in a five
plus plus plus
State and the question that we'll State and the question that we'll State and the question that we'll
address is what is the state of Alice's address is what is the state of Alice's address is what is the state of Alice's
Cubit a in Cubit a in Cubit a in
isolation of course we could ask the isolation of course we could ask the isolation of course we could ask the
same question for Bob's Cubit but it's same question for Bob's Cubit but it's same question for Bob's Cubit but it's
easy to answer that question once we easy to answer that question once we easy to answer that question once we
know the answer to this know the answer to this know the answer to this
one now it might be tempting to say that one now it might be tempting to say that one now it might be tempting to say that
we can't describe the state of Alice's we can't describe the state of Alice's we can't describe the state of Alice's
cubit in isolation because it's cubit in isolation because it's cubit in isolation because it's
entangled with Bob's Cubit which might entangled with Bob's Cubit which might entangled with Bob's Cubit which might
somehow forbid us from describing or somehow forbid us from describing or somehow forbid us from describing or
even thinking about the state of a in even thinking about the state of a in even thinking about the state of a in
isolation but this is isolation but this is isolation but this is
false it's true that we can't describe false it's true that we can't describe false it's true that we can't describe
the state of a with a Quantum State the state of a with a Quantum State the state of a with a Quantum State
Vector because as we'll see it's not in Vector because as we'll see it's not in Vector because as we'll see it's not in
a pure state but every system has a a pure state but every system has a a pure state but every system has a
state at any given moment that can be state at any given moment that can be state at any given moment that can be
described by a density Matrix whether described by a density Matrix whether described by a density Matrix whether
it's entangled with another system or it's entangled with another system or it's entangled with another system or
not as a thought experiment just to help not as a thought experiment just to help not as a thought experiment just to help
us to see what the answer to this us to see what the answer to this us to see what the answer to this
question is let's suppose that Bob question is let's suppose that Bob question is let's suppose that Bob
decides to take his Cubit and travel to decides to take his Cubit and travel to decides to take his Cubit and travel to
the stars and he's never heard from the stars and he's never heard from the stars and he's never heard from
again but imagine that somewhere along again but imagine that somewhere along again but imagine that somewhere along
the way he decides to perform a standard the way he decides to perform a standard the way he decides to perform a standard
basis measurement on his basis measurement on his basis measurement on his
Cubit well we know how to calculate the Cubit well we know how to calculate the Cubit well we know how to calculate the
probabilities for him to obtain the two probabilities for him to obtain the two probabilities for him to obtain the two
possible outcomes and we also know what possible outcomes and we also know what possible outcomes and we also know what
the state of a will be for each possible the state of a will be for each possible the state of a will be for each possible
outcome and those things are summarized outcome and those things are summarized outcome and those things are summarized
in this in this in this
table in particular with probability 1/2 table in particular with probability 1/2 table in particular with probability 1/2
a will be left in the zero State and a will be left in the zero State and a will be left in the zero State and
with probability 1/2 a a will be left in with probability 1/2 a a will be left in with probability 1/2 a a will be left in
the one the one the one
state and so as was discussed earlier in state and so as was discussed earlier in state and so as was discussed earlier in
the lesson this leaves a in the the lesson this leaves a in the the lesson this leaves a in the
completely mix State and therefore that is the answer State and therefore that is the answer
to our to our to our
question now you might object and say question now you might object and say question now you might object and say
that this is dependent on the fact that that this is dependent on the fact that that this is dependent on the fact that
Bob decided to measure his Cubit and the Bob decided to measure his Cubit and the Bob decided to measure his Cubit and the
assumption that he does this is part of assumption that he does this is part of assumption that he does this is part of
the story that led us to this answer but the story that led us to this answer but the story that led us to this answer but
if Bob is light years away by the time if Bob is light years away by the time if Bob is light years away by the time
that he measures nothing that he does that he measures nothing that he does that he measures nothing that he does
could possibly have any influence on any could possibly have any influence on any could possibly have any influence on any
experiment that Alice might decide to experiment that Alice might decide to experiment that Alice might decide to
perform on her Cubit otherwise we'd have perform on her Cubit otherwise we'd have perform on her Cubit otherwise we'd have
faster than light faster than light faster than light
communication so Alice's Cubit is in the communication so Alice's Cubit is in the communication so Alice's Cubit is in the
completely mixed state regardless of completely mixed state regardless of completely mixed state regardless of
whether or not Bob whether or not Bob whether or not Bob
measures I should make clear at this measures I should make clear at this measures I should make clear at this
point that the notion of a reduced State point that the notion of a reduced State point that the notion of a reduced State
and the way that they're defined doesn't and the way that they're defined doesn't and the way that they're defined doesn't
really have any dependence on relativity really have any dependence on relativity really have any dependence on relativity
or the impossibility of faster than or the impossibility of faster than or the impossibility of faster than
light communication this is just a way light communication this is just a way light communication this is just a way
to explain it in hopefully intuitive to explain it in hopefully intuitive to explain it in hopefully intuitive
terms terms terms
in reality there's a mathematical in reality there's a mathematical in reality there's a mathematical
definition which we'll see in a more definition which we'll see in a more definition which we'll see in a more
General context in a few General context in a few General context in a few
moments it's consistent with this answer moments it's consistent with this answer moments it's consistent with this answer
and coincidentally with the and coincidentally with the and coincidentally with the
impossibility of faster than light impossibility of faster than light impossibility of faster than light
communication and there's also no other communication and there's also no other communication and there's also no other
definition that could be consistent with definition that could be consistent with definition that could be consistent with
Quantum information as we know it now Quantum information as we know it now Quantum information as we know it now
let's consider reduced States in Greater let's consider reduced States in Greater let's consider reduced States in Greater
generality suppose that we have a pair generality suppose that we have a pair generality suppose that we have a pair
of systems A and B not necessarily of systems A and B not necessarily of systems A and B not necessarily
cubits that are together in a pure State cubits that are together in a pure State cubits that are together in a pure State
represented by a Quantum State Vector represented by a Quantum State Vector represented by a Quantum State Vector
s again we're going to focus on the s again we're going to focus on the s again we're going to focus on the
reduce state of the system a and to do reduce state of the system a and to do reduce state of the system a and to do
this it's going to be helpful to think this it's going to be helpful to think this it's going to be helpful to think
about the classical states of the system about the classical states of the system about the classical states of the system
B so let's let gamma be this set of B so let's let gamma be this set of B so let's let gamma be this set of
classical states of classical states of classical states of
b as we've discussed before back in b as we've discussed before back in b as we've discussed before back in
lesson two of the series for instance lesson two of the series for instance lesson two of the series for instance
there's a unique collection of vectors f there's a unique collection of vectors f there's a unique collection of vectors f
b for each B and Gamma for which the b for each B and Gamma for which the b for each B and Gamma for which the
expression of s shown on on the screen expression of s shown on on the screen expression of s shown on on the screen
is is is
correct and in particular there's a correct and in particular there's a correct and in particular there's a
simple formula for these vectors we simple formula for these vectors we simple formula for these vectors we
simply multiply the vector s by the simply multiply the vector s by the simply multiply the vector s by the
tensor product of the identity Matrix tensor product of the identity Matrix tensor product of the identity Matrix
corresponding to the system a with bra B corresponding to the system a with bra B corresponding to the system a with bra B
for whatever B we're interested for whatever B we're interested for whatever B we're interested
in what we're doing here is basically in what we're doing here is basically in what we're doing here is basically
setting ourselves up to calculate what setting ourselves up to calculate what setting ourselves up to calculate what
would happen if we were to perform a would happen if we were to perform a would happen if we were to perform a
standard basis measurement on B and standard basis measurement on B and standard basis measurement on B and
again this was covered in lesson two of again this was covered in lesson two of again this was covered in lesson two of
the the the
series soose we did perform a standard series soose we did perform a standard series soose we did perform a standard
basis measurement on basis measurement on basis measurement on
B each possible outcome would then B each possible outcome would then B each possible outcome would then
appear with probability equal to the appear with probability equal to the appear with probability equal to the
ukan norm squared of the corresponding ukan norm squared of the corresponding ukan norm squared of the corresponding
Vector 5 Vector 5 Vector 5
b and conditioned on getting a b and conditioned on getting a b and conditioned on getting a
particular outcome B we normalize the particular outcome B we normalize the particular outcome B we normalize the
vector 5B to determine the resulting vector 5B to determine the resulting vector 5B to determine the resulting
state of state of state of
a following the same reasoning as in the a following the same reasoning as in the a following the same reasoning as in the
ebit example from a few moments ago we ebit example from a few moments ago we ebit example from a few moments ago we
obtain the expression shown on the obtain the expression shown on the obtain the expression shown on the
screen for the reduced state of a as a screen for the reduced state of a as a screen for the reduced state of a as a
density Matrix and by the way the norm density Matrix and by the way the norm density Matrix and by the way the norm
squared in the denominator comes from squared in the denominator comes from squared in the denominator comes from
multiplying the normalized vector to its multiplying the normalized vector to its multiplying the normalized vector to its
conjugate transpose so we pick up the conjugate transpose so we pick up the conjugate transpose so we pick up the
ukian norm twice in the ukian norm twice in the ukian norm twice in the
denominator the ukian Norms squared denominator the ukian Norms squared denominator the ukian Norms squared
cancel and we're left with a pretty cancel and we're left with a pretty cancel and we're left with a pretty
simple expression and in fact if we substitute expression and in fact if we substitute
in our formula for 5B which works for in our formula for 5B which works for in our formula for 5B which works for
every B we get a formula for the reduced every B we get a formula for the reduced every B we get a formula for the reduced
state of the system a in terms of the state of the system a in terms of the state of the system a in terms of the
density Matrix description of the pure density Matrix description of the pure density Matrix description of the pure
state that we started with namely csai state that we started with namely csai state that we started with namely csai
brasai so this is our formula for the brasai so this is our formula for the brasai so this is our formula for the
reduced state of the system a supposing reduced state of the system a supposing reduced state of the system a supposing
that the compound system AB started in that the compound system AB started in that the compound system AB started in
the pure State sigh but we can also use the pure State sigh but we can also use the pure State sigh but we can also use
the formula more the formula more the formula more
generally in particular let's suppose generally in particular let's suppose generally in particular let's suppose
that the state of ab is given by some that the state of ab is given by some that the state of ab is given by some
density Matrix row which could take the density Matrix row which could take the density Matrix row which could take the
form K SI brai for some Quantum State form K SI brai for some Quantum State form K SI brai for some Quantum State
Vector s but not Vector s but not Vector s but not
necessarily it follows basically by necessarily it follows basically by necessarily it follows basically by
linearity that the reduced state of a is linearity that the reduced state of a is linearity that the reduced state of a is
determined by the same formula as we had determined by the same formula as we had determined by the same formula as we had
for Pure States except that we for Pure States except that we for Pure States except that we
substitute whatever density Matrix row substitute whatever density Matrix row substitute whatever density Matrix row
we we we
wish as a Shand notation it's pretty wish as a Shand notation it's pretty wish as a Shand notation it's pretty
common that we denote this reduced state common that we denote this reduced state common that we denote this reduced state
by whatever name we're using for the by whatever name we're using for the by whatever name we're using for the
density Matrix Row in this case but we density Matrix Row in this case but we density Matrix Row in this case but we
put a subscript indicating the system put a subscript indicating the system put a subscript indicating the system
that we're focusing on so row with a that we're focusing on so row with a that we're focusing on so row with a
subscript a means the reduced state of a subscript a means the reduced state of a subscript a means the reduced state of a
when the compound system is in the the when the compound system is in the the when the compound system is in the the
state state state
row superscripts are also sometimes used row superscripts are also sometimes used row superscripts are also sometimes used
instead of subscripts it's basically instead of subscripts it's basically instead of subscripts it's basically
just a personal preference and it means just a personal preference and it means just a personal preference and it means
the same thing we've been focusing on the system thing we've been focusing on the system
a just for Simplicity but we can also a just for Simplicity but we can also a just for Simplicity but we can also
obtain the reduced state of the system B obtain the reduced state of the system B obtain the reduced state of the system B
analogously but where the formula is analogously but where the formula is analogously but where the formula is
adapted in the natural adapted in the natural adapted in the natural
way everything is symmetric here we're way everything is symmetric here we're way everything is symmetric here we're
essentially just exchanging the roles of essentially just exchanging the roles of essentially just exchanging the roles of
A and B and in particular the sum ranges A and B and in particular the sum ranges A and B and in particular the sum ranges
over the classical states of a this time over the classical states of a this time over the classical states of a this time
rather than b and in the formula on the rather than b and in the formula on the rather than b and in the formula on the
screen we're assuming the classical screen we're assuming the classical screen we're assuming the classical
State set of a is called Sigma all of this generalizes in a Sigma all of this generalizes in a
pretty straightforward way to three or pretty straightforward way to three or pretty straightforward way to three or
more more more
systems for example if we have three systems for example if we have three systems for example if we have three
systems a b and c that are collectively systems a b and c that are collectively systems a b and c that are collectively
in the state described by the density in the state described by the density in the state described by the density
Matrix row then we can obtain the Matrix row then we can obtain the Matrix row then we can obtain the
reduced state of the pair a together by reduced state of the pair a together by reduced state of the pair a together by
the formula that shown here as well as the formula that shown here as well as the formula that shown here as well as
the state of the third system alone the state of the third system alone the state of the third system alone
which is the second formula and you can which is the second formula and you can which is the second formula and you can
consider other combinations and consider other combinations and consider other combinations and
generalize this to however many systems generalize this to however many systems generalize this to however many systems
you wish the sums are always taken over you wish the sums are always taken over you wish the sums are always taken over
the classical State sets of the systems the classical State sets of the systems the classical State sets of the systems
that are being ignored and the pattern that are being ignored and the pattern that are being ignored and the pattern
is always the same with the density is always the same with the density is always the same with the density
Matrix we started with being sandwiched Matrix we started with being sandwiched Matrix we started with being sandwiched
between the tensor product of the between the tensor product of the between the tensor product of the
identity Matrix and bras and cats identity Matrix and bras and cats identity Matrix and bras and cats
corresponding to standard basis corresponding to standard basis corresponding to standard basis
states to conclude the lesson I'll say states to conclude the lesson I'll say states to conclude the lesson I'll say
just a a little bit more about the just a a little bit more about the just a a little bit more about the
definitions we've just definitions we've just definitions we've just
covered the operation on density covered the operation on density covered the operation on density
matrices that describes redu States has matrices that describes redu States has matrices that describes redu States has
a name and it's called the partial a name and it's called the partial a name and it's called the partial
trace for two systems A and B the trace for two systems A and B the trace for two systems A and B the
partial Trace over B and the partial partial Trace over B and the partial partial Trace over B and the partial
Trace over a are as defined here on the Trace over a are as defined here on the Trace over a are as defined here on the
screen these are exactly the same screen these are exactly the same screen these are exactly the same
formulas that we saw a few moments ago formulas that we saw a few moments ago formulas that we saw a few moments ago
and now we're just thinking about them and now we're just thinking about them and now we're just thinking about them
as functions of row and giving them as functions of row and giving them as functions of row and giving them
names the names look like the trace names the names look like the trace names the names look like the trace
except that there's a subscript which except that there's a subscript which except that there's a subscript which
tells us which system is being removed tells us which system is being removed tells us which system is being removed
or in other words which system is the or in other words which system is the or in other words which system is the
one whose classical State set we're one whose classical State set we're one whose classical State set we're
summing over and is getting sandwiched summing over and is getting sandwiched summing over and is getting sandwiched
between a bra and a between a bra and a between a bra and a
cat when we take the partial trace of cat when we take the partial trace of cat when we take the partial trace of
our B for instance which is described by our B for instance which is described by our B for instance which is described by
the first Formula we sometimes say that the first Formula we sometimes say that the first Formula we sometimes say that
b is traced out and likewise for b is traced out and likewise for b is traced out and likewise for
a intuitively speaking tracing out a a intuitively speaking tracing out a a intuitively speaking tracing out a
system just means that we throw it away system just means that we throw it away system just means that we throw it away
or simply ignore it or simply ignore it or simply ignore it
that's one way to define the partial that's one way to define the partial that's one way to define the partial
trace and another equivalent way is to trace and another equivalent way is to trace and another equivalent way is to
say that these mappings are the unique say that these mappings are the unique say that these mappings are the unique
linear mappings that satisfy the linear mappings that satisfy the linear mappings that satisfy the
formulas shown here on the formulas shown here on the formulas shown here on the
right specifically if we have a tensor right specifically if we have a tensor right specifically if we have a tensor
product of two matrices M and N let's product of two matrices M and N let's product of two matrices M and N let's
say and we perform the partial Trace say and we perform the partial Trace say and we perform the partial Trace
over the first system then what we get over the first system then what we get over the first system then what we get
is the trace of the first Matrix M times is the trace of the first Matrix M times is the trace of the first Matrix M times
the second Matrix and the condition is the second Matrix and the condition is the second Matrix and the condition is
symmetric when we Trace over the second symmetric when we Trace over the second symmetric when we Trace over the second
system not every Matrix that we would system not every Matrix that we would system not every Matrix that we would
compute the partial trace of is a tensor compute the partial trace of is a tensor compute the partial trace of is a tensor
product like this but every Matrix can product like this but every Matrix can product like this but every Matrix can
be expressed as a linear combination of be expressed as a linear combination of be expressed as a linear combination of
tensor products and we can determine the tensor products and we can determine the tensor products and we can determine the
partial Trace from these conditions partial Trace from these conditions partial Trace from these conditions
together with the fact that these are together with the fact that these are together with the fact that these are
linear linear linear
functions and this is why it's called functions and this is why it's called functions and this is why it's called
the partial the partial the partial
trace this equivalent definition can be trace this equivalent definition can be trace this equivalent definition can be
quite helpful because we can often use quite helpful because we can often use quite helpful because we can often use
it rather than the original formula here's an example that formula here's an example that
illustrates this suppose that we have a illustrates this suppose that we have a illustrates this suppose that we have a
pair of cubits a in the state whose pair of cubits a in the state whose pair of cubits a in the state whose
density Matrix is shown here on the density Matrix is shown here on the density Matrix is shown here on the
screen this happens to be an example of screen this happens to be an example of screen this happens to be an example of
a correlated state where a is either in a correlated state where a is either in a correlated state where a is either in
the zero State or the one state each the zero State or the one state each the zero State or the one state each
with probability 1/2 so we can think with probability 1/2 so we can think with probability 1/2 so we can think
about a as being about a as being about a as being
classical if a is in the zero State then classical if a is in the zero State then classical if a is in the zero State then
so is B and if a is in the one state so is B and if a is in the one state so is B and if a is in the one state
then B is in the plus state if we want then B is in the plus state if we want then B is in the plus state if we want
to know what the reduced state of a is to know what the reduced state of a is to know what the reduced state of a is
we Trace out B and if we use the second we Trace out B and if we use the second we Trace out B and if we use the second
formula from the equivalent definition formula from the equivalent definition formula from the equivalent definition
together with linearity then we can together with linearity then we can together with linearity then we can
compute this pretty compute this pretty compute this pretty
easily in particular the trace of k0 bra easily in particular the trace of k0 bra easily in particular the trace of k0 bra
0 is one as is the trace of K plus bra 0 is one as is the trace of K plus bra 0 is one as is the trace of K plus bra
plus so those tensor factors disappear plus so those tensor factors disappear plus so those tensor factors disappear
and we're left with the completely mixed and we're left with the completely mixed and we're left with the completely mixed
state along similar lines we can compute state along similar lines we can compute state along similar lines we can compute
the reduce state of B by tracing out the reduce state of B by tracing out the reduce state of B by tracing out
a this time it's the first tensor factor a this time it's the first tensor factor a this time it's the first tensor factor
that we're performing the trace on so that we're performing the trace on so that we're performing the trace on so
we're using the first Formula from the we're using the first Formula from the we're using the first Formula from the
equivalent definition and we're left equivalent definition and we're left equivalent definition and we're left
with 1/2 * cat 0 bra 0 plus 1/2 * cat with 1/2 * cat 0 bra 0 plus 1/2 * cat with 1/2 * cat 0 bra 0 plus 1/2 * cat
plus bra Plus or in other words the same plus bra Plus or in other words the same plus bra Plus or in other words the same
density Matrix that came up a few times density Matrix that came up a few times density Matrix that came up a few times
previously you would get exactly the previously you would get exactly the previously you would get exactly the
same results if you Ed the formulas from same results if you Ed the formulas from same results if you Ed the formulas from
the first definition but this is a much the first definition but this is a much the first definition but this is a much
easier way to do easier way to do easier way to do
it this all generalizes to three or more it this all generalizes to three or more it this all generalizes to three or more
systems but for this video I'm going to systems but for this video I'm going to systems but for this video I'm going to
stop here and leave this for you to stop here and leave this for you to stop here and leave this for you to
consider on your consider on your consider on your
own and that is it for this lesson which own and that is it for this lesson which own and that is it for this lesson which
is introduced density matrices and is introduced density matrices and is introduced density matrices and
describe the basics of how they describe the basics of how they describe the basics of how they
work we also discussed the block sphere work we also discussed the block sphere work we also discussed the block sphere
as well as different types of as well as different types of as well as different types of
correlations and the notion of a reduced correlations and the notion of a reduced correlations and the notion of a reduced
state of a state of a state of a
system I hope you'll join me for the system I hope you'll join me for the system I hope you'll join me for the
next lesson which is about channels next lesson which is about channels next lesson which is about channels
which are linear mappings from density which are linear mappings from density which are linear mappings from density
matrices to density matrices that matrices to density matrices that matrices to density matrices that
describe operations on systems goodbye describe operations on systems goodbye describe operations on systems goodbye
until

## Entanglement in Action ｜Understanding Quantum Information & Computation ｜ Lesson 04

- Welcome back to Understanding
Quantum Information and Computation. My name is John Watrous, and
I'm the Technical Director for IBM Quantum Education. This is the fourth lesson of the series and it's the final
lesson in the first unit of the series on the basics
of quantum information. Thus far, we've talked about
how quantum information works for both single and multiple systems, we've taken a look at the
quantum circuit model, and we've observed some
fundamental limitations on quantum information. Along the way, we
encountered some important mathematical concepts
including tensor products and inner products, and we'll continue to make use of those concepts and we'll introduce some new
ones as well as we need them. In this lesson, we'll be
focusing on three fundamental important examples, quantum teleportation, superdense coding and the CHSH game, which is also called the CHSH inequality and not always described as a game. These are not just random
examples meant to illustrate how quantum information works, they really are key
parts of the foundation of quantum information and
they arise in different and sometimes unexpected settings within quantum information
and computation. Entanglement plays a key role in all three of these
examples, and in fact, it's entanglement that makes them work and makes them interesting. So, this lesson will, in some sense, be our first opportunity to
see entanglement in action and to begin to explore what it is that makes entanglement so interesting. The overview for the
lesson is very simple, we'll go one by one
through the three examples that I just mentioned,
quantum teleportation, superdense coding, and the CHSH game. We'll discuss what these things are and what makes them interesting and we'll go through analyses that explain exactly how they work
at a mathematical level. Before we get to the first
of the three examples, which is quantum teleportation, it's appropriate to first
introduce the characters, Alice and Bob, in case you're
not familiar with them. Alice and and Bob, are names
given to hypothetical entities or agents in systems, protocols, games, and really any other type of interaction you might wish to consider that involves the exchange of information. They first arose in the
context of cryptography in the 1970s, but the
convention to use these names in this way has become very common in a much broader context since then. The idea is that these
are very common names, at least in some parts of the world, and they start with the letters A and B, and it's also convenient
to refer to Alice and Bob using the pronouns, her and him just for the sake of brevity. We typically imagine that Alice and Bob are in different locations and that they have different
goals and different behaviors depending upon whatever it is that we're talking about. For example, if we're
talking about communication, meaning, the transmission of information, then we might decide
that Alice is the sender and Bob is the receiver
of whatever information is being transmitted. Sometimes Alice and Bob
cooperate, that's very typical, but sometimes they
compete or they might have different goals that may
or may not be consistent with one another, but this
all has to be made clear in the situation at hand. Of course, they represent abstractions and not necessarily actual human beings. We can also introduce
additional characters such as Charlie and
Diane or different names that represent different
personas such as Eve for an Eves Strapper or Mallory for someone behaving maliciously
depending upon the setting. In any case, we will see Alice and Bob in all three of the
examples that we'll discuss in this lesson and from time to time throughout the remainder of the series. I'll also make a few
comments about entanglement before moving on to the three examples. In lesson two, we encounter this example of an entangled state of two Qubits. It's one of the four bell states, and you could say that
this particular state represents the archetypal example of an entangled quantum state. We also encountered this
example of a probabilistic state of two bits. It's analogous to the
entangled state in some sense, but it's not entangled,
entanglement is a uniquely quantum phenomenon,
essentially by definition. There is a precise mathematical definition of what entanglement is and basically what that definition
says is that entanglement is a non-classical quantum correlation. But that's not all that
helpful at an intuitive level, it's a definition of entanglement in terms of what entanglement isn't, and it's probably for that
reason that it's sexually pretty difficult to explain what entanglement is and what makes it special
in intuitive terms. When people do try to
explain what entanglement is in intuitive terms, you
often hear explanations that really don't
distinguish these two states in a meaningful way. You may hear the idea
that if you measure one of the two entangled Qubits,
then the state of the other one is somehow instantaneously affected or that the state of the two Qubits can't be described separately or that the two Qubits
somehow maintain a memory of one another or even that
they're emotionally attached in some way. But why are those properties
not true for this state? These two bits are intimately connected, each one has a perfect
memory of the other one in a literal sense, but
it's not an entangled state. Well, one way to explain what
makes entanglement special and what makes this state
very different from this state is to explain what we
can do with entanglement or what we can see happening
because of entanglement that goes beyond the decisions we make about how we represent our
knowledge using vectors. All three of the examples
that we'll discuss in this lesson have this nature in that they illustrate
things that we can do with entanglement that we
cannot do with this state or with any classically correlated state. Indeed, it's very typical in the study of quantum information and computation that we view entanglement as a resource through which different
tasks can be accomplished. And when we do that, we view this state as representing
one unit of entanglement. We refer to this unit of
entanglement as an e-bit, the e stands for
entangled or entanglement. It's true that it's two Qubits, but as a quantity of
entanglement, it's one e-bit. You can also view a pair
of ordinary classical bits in the correlated
probabilistic state from before as being a resource which is
one bit of shared randomness. It can be very useful in
cryptography to share a random bit with somebody presuming
that nobody else knows what that bit is so that it
can be used as a private key or as part of a private key, for example, for the sake of encryption. But in this lesson, our focus will be on this entangled state and
things that we can do with it. And just to clarify the terminology, when we say that Alice
and Bob share an e-bit, what we mean is that
Alice has a qubit named A, Bob has a qubit named B, and together the pair A, B
is in the Phi plus state. It's not important what names
we choose for the two Qubits, of course, we could
choose different names, but throughout this lesson,
we'll stick with these names in the interest of clarity. The first example of the lesson
is quantum teleportation, which hereafter we'll just
refer to as teleportation for the sake of brevity. What teleportation is is a protocol that allows a sender to
transmit quantum information to a receiver using only entanglement and classical communication to
accomplish that transmission. It has absolutely nothing to do with the typical interpretation
of the word teleportation in science fiction, which
is that matter disappears at one location and appears in another. Quantum teleportation is just a name that's meant to conjure that as an image or as a metaphor so to speak. What's getting teleported is
quantum information not matter, and that's happening
through a transmission of classical information
together with entanglement. To understand exactly
how teleportation works, it's helpful to begin by
setting the stage, so to speak. What we'll do is to imagine a scenario involving Alice and Bob and
that scenario is as follows, Alice has a qubit named Q that
she would like to transmit to Bob, meaning that she
wants Bob to hold in his hands a qubit that's in exactly
the same state that Q is in as they begin the protocol. In other words, we're talking about the fundamental problem of
information transmission or communication. We have a sender named Alice
and a receiver named Bob and Alice wants to send
quantum information in the form of a qubit to Bob. There is a constraint, however, which is that Alice is not
able to physically send Q to Bob, all that she can do is to send classical information to Bob. Alice and Bob also have
an additional resource, which is that they share an e-bit. So that's the scenario, it's
easy to imagine situations where the constraint that
Alice can't physically send Q to Bob is a reasonable abstraction. If Alice and Bob were on
different continents for instance, or if Alice didn't know
Bob's exact location, then physically sending a qubit would probably be pretty challenging. But classical communication
is of course not only possible but quite inexpensive
under those circumstances. Now, one might ask whether it's possible for Alice and Bob to accomplish this task without even needing the shared e-bit. In other words, is there
any way to transmit a qubit using classical
communication alone? The answer is no, it's not possible to transmit quantum information using classical communication alone, and that's not too difficult to prove using some basic quantum
information theory, which we'll see in the
third unit of the series. For now, an intuitive
way to think about it is to think about the no-cloning theorem. Imagine that there was a way
to send quantum information using classical communication alone, but classical communication
can easily be copied and broadcast, so that means
the classical communication could be broadcast to two
receivers, Bob and Charlie, let's say. If they both received the
same classical information from Alice, then would
they both end up with Q? That would suggest that we cloned Q, but we know that that's impossible. Just as an aside, let's
observe that it certainly is possible to describe
quantum state vectors using classical information. Every quantum state vector
you'll see in the series is being described to you
using classical information that was transmitted to whatever
device you're watching on. In general, we could imagine
describing whatever state Q is in using classical bits
perhaps to some precision, but that's not quantum communication, that's just describing
quantum states as vectors, and we'll come back to the
distinction between them in just a moment. Getting back to the scenario however, and working under the
assumption that Alice and Bob do share an e-bit, we will
see that Alice and Bob have everything they need
to accomplish the task, and that's what the quantum
teleportation protocol does, specifically using two bits
of classical communication. before getting into
these details, however, let me make a few quick
remarks just to make sure that the assumptions are clear. First, we're assuming
here that the state of Q is unknown to Alice and
Bob, so in particular, Alice can't send Bob a
description of the state Q because she doesn't know it in general. We also have to consider the possibility that Q is correlated with other systems, including the possibility
that it's entangled with other systems and the
transmission of this qubit to Bob must preserve those correlations. Finally, I mentioned
the no-cloning theorem when I was talking about
trying to communicate Qubits classically, but
there's another connection here with the no-cloning theorem, which is that if Bob
receives the transmission of this qubit Q from Alice, then she must not have it anymore
because Q can't be cloned, and that's just something to keep in mind as we consider the teleportation protocol. The teleportation
protocol can be described in a pretty simple and direct way with the quantum circuit diagram as you can see right here. It's slightly stylized in
that it depicts the separation between Alice's part and Bob's part, but otherwise, it's just
a quantum circuit diagram, everything above the dotted
line represents Alice's actions and everything below
represents Bob's actions. Time goes from left to right as always, and we see that we start
with these two Qubits here being in a Phi plus state,
so this is the e-bit that Alice and Bob start with. We can put labels on the Qubits to make clear which one is
which, so this is the qubit A, and this is the qubit
B that form the e-bit, and this is the qubit Q that
Alice wishes to transmit to Bob. Here we're hypothesizing that
the state of this qubit Q is Psi, but a little bit later on we'll consider the possibility
that Q is entangled with some other system. And what the diagram is claiming because we've written psi down here, is that this circuit
effectively does transmit whatever state psi we had
up here on Alice's part down here to Bob's part,
the only communication that we have crossing the
line between Alice and Bob is the two classical
bits that are represented by these double lines right here. Let's shrink this
diagram down a little bit so that we have a little bit more room and let's examine the
circuit more closely. I've already described
the initial conditions, but just to recapitulate, Alice holds a qubit A
and Bob holds a qubit B, where A and B together
are in a Phi plus state, so that's what it means to share an e-bit. Alice also has this qubit Q that she wishes to transmit to Bob, and we're considering the case that Q is in some arbitrary quantum state Psi. Now, we don't know anything
about how Alice and Bob came into possession of this e-bit. It could be that they've
had it for a long time, maybe they created it some time ago when they were in the same location and they've been saving
it for this occasion, but those details aren't relevant here. The steps that happen next, which represent the
teleportation protocol itself are as follows, and just to be clear, these are the steps that
the diagram describes and we're just gonna
write them down in words so that it's clear what
the diagram expresses. Alice first performs a
controlled-NOT operation where the qubit Q that
she wants to send to Bob is the control and the
qubit A is the target. The next step is that she
performs a Hadamard operation on Q, and then she measures
the two Qubits that she holds, A and Q with respect to
standard basis measurements and we let lowercase a be the outcome of the measurement of the qubit A, and we let lowercase b be the outcome of the measurement of the qubit Q. Alice then sends those
two classical bits to Bob and this is the only communication that's involved in this protocol and that's indicated in the diagram by the fact that these two
bits cross the dotted line between Alice and Bob,
which you can think of as representing whatever
distance separates them. After Bob receives the bits
a and b, he does two things. First, if a is equal to one, then he performs an X
operation or in other words, a NOT gate on his qubit B, and otherwise if a is equal to zero, then he doesn't do that. And along similar lines,
if b is equal to one, then Bob performs a Z operation
or a Z-gate or a phase flip also on the qubit b and like
before, if b is equal to zero, then he doesn't do that. This is all indicated in the figure where we have these controlled-X
and controlled-Z gates where the controls are
the two classical bits. You don't have to think of these gates as being control gates
if you don't want to, this is just a handy way
to express what we want in the form of a circuit diagram. The point is that Bob applies
an X-gate to the qubit b if a is equal to one and
then he applies a Z-gate to the qubit b if B equals one. And for the sake of convenience later we can write down explicitly
the combined operation that Bob performs on his qubit b, depending upon the classical
bits that Alice sends. So that is the teleportation protocol, it's pretty simple really, the
fact that it actually works is a different matter
and although the analysis isn't that complicated,
you should not feel that you're missing something if you don't immediately see why it works. That's why we need to
go through an analysis and that's what we will do next. To analyze the teleportation protocol, we'll work through it one step at a time to see what the states
of the three Qubits are at different points in time. We're gonna first consider
the case that the top qubit is in the state AlphaZero plus beta one, where alpha and beta
are any complex numbers that cause this vector
to be a unit vector. This is the qubit Q that
Alice is trying to transmit, but we don't really care what
it's called at this moment. A little bit later, we'll
take a look at the situation in which this qubit is
entangled with another system, but this is a good starting
point just to keep things simple and to see how the analysis
works a little bit more clearly. If we consider the state of
the three Qubits together here at the start, we see
that this state has the form that you see right here. The first two Qubits are
in the Phi plus state, the third qubit is the top one
according to the convention for ordering Qubits that I
described in the previous lesson, and so we take the tensor product to get the state of the
three Qubits together. We can expand this expression
by first substituting the definition of the Phi plus state and then computing the tensor product and we get the state
that you see right here. It's not always the best
strategy by the way, to completely expand
out a vector like this, it's generally a good idea to wait until you have a good reason to do this. But here we do have a reason, which is that we would
like to compute the action of this controlled NOT gate and by writing in the state in this form, we can very easily do that. And in particular, this is
the state of the three Qubits after the controlled NOT gate is applied. The third qubit is the control qubit because that's the one that's
on top by our convention, and the second qubit,
which is the middle one, is the target, so we just
operate on the cats accordingly, if the third bit is a
zero, nothing happens, and if the third bit is a one,
then we flip the second bit. Then we perform the Hadamard
gate on the third qubit, we can write the resulting
vector like you can see right here substituting the
plus and the minus state for the zero and the one state. And if we wanna have enough
room to expend this one out, we're gonna need to shrink
it down a little bit and now, by substituting the
plus and the minus states with their definitions,
bringing the square root of two into the denominator, we get the state that you see right here. And by expending that one out, we get the state that you see right here. So, that is the state of the three Qubits after Alice performs her two operations. Alice then measures
and if we wanna analyze the measurements, what we do is to isolate the standard basis vectors
for the systems being measured just like we talked about in lesson two, and if we do that in this particular case, this is what we obtain. And if you look at that
expression for a moment, it might seem a little bit peculiar because now it looks like the first qubit, which is the bottom qubit
somehow depends upon alpha and beta, even though at
this point there hasn't been any communication across the dotted line, but that's just an illusion. Remember that scalar float
freely through tensor products, so there's really no sense
in which alpha and beta are associated with the first qubit. All we've really done
here is to use parentheses in accordance with the
multilinearity of tensor products. Anyway, that's the form that we need to see what the measurement does, and we can see one
interesting thing right away and that is that the probabilities of the four possible measurement outcomes are always uniform. The probability to measure each
pair of measurement outcomes is one quarter, so, in particular, there's no dependence
whatsoever on alpha and beta. We can see that in each case by computing the Euclidean norm squared of
whatever vector is tensored to the standard basis
vector that we isolated, and in each case we have a one quarter which we get by squaring one half times the absolute value squared of alpha plus the absolute value
squared of beta, which is one, and so we get one quarter in each case. It's quite important in
fact that the probabilities of these different outcomes don't depend in any way on the qubit that
Alice wants to transmit. We couldn't have that an arbitrary state comes in up here and comes out down here and yet a measurement revealed
information about that state. If you wanna gain information about an arbitrary quantum state, you're gonna have to disturb
that state in general. Here's a table that describes not only the probabilities
of the different outcomes, but also the state of
the three Qubits together conditioned on each possible
measurement outcome. We get this by normalizing the vectors tensored to the different
standard basis states, but that's easy in this case, we just eliminate the factor of one half to get a unit vector. So, what we have immediately
after Alice measures is that Alice has two uniform
random bits and that's it. In particular, she no longer
has the original state and she doesn't have any
information about it, she just has random bits. Bob's state now does
depend on alpha and beta as well as on the measurement outcomes. If we imagine momentarily
that Alice hasn't yet sent the measurement outcomes
so that Bob doesn't know what the measurement outcomes are, then he wouldn't be able
to gain any information about alpha and beta
from measuring his qubit. These four possibilities
that we have right here are sufficiently scrambled to ensure that no information
about alpha and beta could ever be gained
through a measurement. That has to be true at an intuitive level because otherwise we'd have
faster than light communication. Alice certainly can't
transmit information to Bob without sending anything at all, but of course Alice does
send the measurement outcomes to Bob, so let's expand the table to see what happens when
Bob performs his operations. In this column right here, we have the different operations that Bob effectively performs for each of the possible
outcomes of the measurements like we observed before,
and if we go one by one and we apply that
operation to Bob's qubit, which is the first one, B, we see that we get the same
outcome in all four cases and that outcome is the original state. So, the original state of the qubit Q has been successfully teleported into B. If Bob wants to rename his qubit and call it Q at this point, he's certainly welcome to do that, but the point is that now
he holds the original state that Alice had and of course by necessity, Alice doesn't have it anymore. Notice also that the e-bit
has effectively been burned up through this process,
teleportation has a cost and the cost is one e-bit and two classical bits of communication, and when the protocol is
over, the e-bit is gone. We've now established that
the teleportation protocol works correctly under the assumption that the top qubit is in a quantum state of the form alpha times cat
zero plus beta times cat one. But what happens if the
top qubit is entangled with some other system such as
one of our additional Qubits as we have here on the screen? We might wish to use teleportation
under such circumstances. Maybe Alice and Bob actually
plan to perform teleportation 100 times so that Alice can transmit some 100 qubit quantum register
to Bob, one qubit at a time. Or maybe these additional
Qubits aren't even in Alice's possession at all
and somebody else has them, maybe Alice doesn't even
know that they exist. The question is, does
teleportation still work even though the qubit being teleported is entangled with another system? Here, the state of the qubit Q together with the additional
system that we're considering is expressed as you can see right up here where gamma zero and
gamma one are unit vectors representing quantum states
of this additional system. We can always write an arbitrary state of the combined system in this way. We don't actually even
need alpha and beta here, but if we include them,
then we can take gamma zero and gamma one to be unit vectors and that'll make the
connection to the simpler case we've already considered more transparent. We can go through a
similar analysis to the one that we already went through, but where we also keep track
of this additional system. It's a little bit messier because we have the additional system, but
the idea is exactly the same, here's a very quick summary. This is the initial state, here's the state after
the controlled NOT gate, and here's the state after
the Hadamard operation. If we expand these things
out, we can express that state as you can see right here, that's the state immediately
before Alice's measurements, and if we go through the
possibilities just like before and we consider what
happens when Bob applies whatever operation he's supposed to apply, then we find that indeed
the correct state comes out and the protocol works as expected. In this table by the way, we're using the letter R to
refer to this additional system, and you can see that we're
using a pretty simple trick, which is to decide that we would rather, at least momentarily, order
the systems as B, R, A, Q because that's more convenient for writing the states right here. According to the diagram, the systems should be
ordered as B, A, Q, R, but as long as we make it clear, we can choose a different
ordering that's more convenient. In a nutshell, everything that's happening in this analysis is the
same as what happened in the simpler analysis, but we happen to have
these additional vectors, gamma zero and gamma one
that basically come along for the ride. We always have gamma zero
when there's a factor of alpha and gamma one when
there's a factor of beta and at the end of the day,
it all works out correctly. This is actually not at all surprising, we have a physical process that acts like the identity
operation on a qubit in an arbitrary quantum state, and there's only one
way that that can happen and that is that the operation
is the identity operation. That's a little bit easier to explain using the general formulation
of quantum information, but the point is that by linearity, we really didn't need to go through this additional analysis. Once we know that
teleportation works correctly for a single qubit in isolation, we can conclude that there's
only one possible thing that can describe what the protocol does, and that is that it implements
the identity operation where a qubit comes in right here and it comes out right here, effectively acting as a perfect
noiseless quantum channel. Just a few quick remarks
about teleportation before moving on to the
second example of the lesson, which is superdense coding. First, I think it's important to clarify that teleportation is not an application of quantum information, it's a protocol and specifically it's a
protocol for performing quantum communication. In order for teleportation
as I've described it to be useful, there has to be some reason for performing quantum communication. Second, it's reasonable to speculate that teleportation could, one
day, become a standard way to communicate quantum information, and specifically a process known as entanglement distillation
might potentially be used as a way to communicate Qubits reliably. If Alice and Bob know they're
going to be communicating quantum information or they
have the need to communicate quantum information, then this process of entanglement distillation
could be used to convert a larger number of noisy e-bits into a smaller number
of high quality e-bits that could then be used for noiseless or near noiseless teleportation. The idea is that the process
of entanglement distillation isn't as delicate in some sense, we could accept losses and
if it doesn't work out, then we could just try it again, whereas the actual Qubits that
we would like to communicate might be considered much more precious. Of course, this is all speculation, but the point is that it isn't difficult to hypothesize situations
in which teleportation could be a very practical
way to communicate Qubits. And the final point
concerning teleportation is that the idea behind teleportation and the way that it works
is quite fundamental in quantum information and computation, teleportation is really a cornerstone of quantum information
theory and variations on the idea also often come
up, it's just one example, there are proposals for
implementing different quantum gates using teleportation, but
where different initial states and measurements are
chosen so that the outcome is to apply a chosen operation
rather than to communicate. The next example is superdense coding. Like quantum teleportation,
superdense coding is a protocol and what it accomplishes is complimentary to teleportation. Whereas teleportation allows
one qubit to be transmitted using an e-bit and two
classical bits of communication, superdense coding allows
two classical bits to be transmitted using
an e-bit and one qubit of quantum communication. So, in short, superdense coding allows for the transmission
of classical information and the reason it's
called superdense coding is that it allows for two
classical bits of communication to be packed into a single qubit, but we have to burn an
e-bit to accomplish this. Here's a more precise
explanation of the setup. Once again, we have Alice and Bob, where Alice is the sender
and Bob is the receiver, and this time Alice has two classical bits that she wants to send to Bob. Alice is able to send Bob a single qubit and Alice and Bob share
an e-bit that they can use to help them to accomplish their task, which is for Bob to receive
the two classical bits. Now, there's a sense in which this task may seem less interesting than the task that
teleportation accomplishes, particularly if we view
that sending Qubits is significantly more
difficult or more challenging than sending classical bits, which is a pretty reasonable point of view but it is definitely interesting that it's possible to do this. One reason why it's interesting
is that it is in fact impossible to accomplish
this task of transmitting two classical bits of information using a single qubit without
making use of the e-bit. We know this because of a famous theorem known as Holevo's theorem. Holevo's theorem is
more general than this, and the precise statement
of Holevo's theorem is a bit technical and it
requires some explanation, but one implication of it
is that you cannot transmit any number of classical
bits of communication using fewer Qubits of
quantum communication alone. If you want to reliably transmit two classical bits of
information using Qubits and you don't have an
e-bit to make use of, you're going to have to send to Qubits, so they might as well store the bits that you wanna transmit. Another reason why superdense
coding is interesting is because of the close
complementary relationship it has to teleportation, and
we'll come back to this a little bit later. Here is a quantum circuit description of the superdense coding protocol. For this one, I won't
list out all the steps, but instead we'll just stick with the circuit diagram description. As you can see, we again
have Alice and Bob, where Alice is on the top
and Bob is on the bottom. Here is the e-bit that's
initially shared between Alice and Bob, which has
somehow been established prior to the protocol being run. Alice has two classical bits, A and B, which you can see up here
and through the protocol that the circuit describes,
these two classical bits will end up in the hands of Bob. There's just one qubit
that crosses the line between Alice and Bob, and of course there aren't
any classical bits being sent, it's the transmission of
the two classical bits that's the point of the protocol. You'll also notice that it has
exactly the same components that are used in teleportation, we have a controlled NOT
gate, a Hadamard gate, and a Z and an X-gate
controlled by classical bits. They're arranged differently however, so let's move on to the
analysis to see how it works. To analyze how superdense coding works, let's start by writing
down the Bell basis, which is very much at the
heart of the analysis. What we're going to do is
to first take a look at the effect that Alice's actions
have on the Phi plus state, and what we'll see is
that Alice transforms this Phi plus state into one
of the four Bell states depending on the bits
that she wants to send. So, let's write down a table so that we can keep
track of what's going on. First, if A and B are both zero, then neither the Z-gate nor
the X-gate gets applied, so Alice's actions don't do anything, which is to say that Phi
plus gets mapped to Phi plus. We can also express that as an equation like you see right here,
nothing happens to Alice's qubit and nothing happens to Bob's qubit and doing nothing is represented
by the identity matrix. Now, what happens if A is equal to zero and B is equal to one? Well, because B is equal to one, we apply the Z-gate and
we don't apply the X-gate because A is equal to zero and we see that the overall effect is that Alice performs a Z-gate. Alice is on the right
because she's on the top, although it wouldn't
actually make any difference in this particular
case, the effect is that the Phi plus date gets
snapped to Phi minus, which you can eyeball
from these expressions if you remember how a Z-gate works works. So, we can add this one to our table, there are two more cases to go through, if A is equal to one
and B is equal to zero, then Alice performs an
X-gate which transforms five Phi into Chi plus, and if A is equal to one
and B is equal to one, then Alice performs Z, then
X and we get Chi minus. So, our table tells us what happens in the four possible cases and that is that Alice effectively chooses which one of the four Bell states she
would like to share with Bob, and she has the freedom to choose by acting on her qubit alone and that's a pretty interesting
aspect of entanglement. Alice then sends her qubit
to Bob, at this point, Bob holds one of the four Bell states and he just needs to
figure out which one it is to recover the bits. And we can check that
indeed a controlled NOT gate followed by a Hadamard gate
as we have in the diagram does the right thing. I won't go through them in detail, but if you choose to do that, you'll find that the effect is
as you see here in the table. One way to check that
is simply to compute. Alternatively, we can recognize that we actually already
went through this in reverse in the previous lesson. In the previous lesson we
had the Hadamard gate first and the controlled NOT gate second and we saw that that circuit
mapped standard basis states to Bell states with a pesky
minus sign in one of the cases. When we reverse the order of the gates and we consider the fact
that both of these two gates are their own inverses,
what we get is the inverse, and so what we have in this
case is the action goes in the opposite direction
and the minus sign can choose whichever
side it wants to be on, but otherwise, it's where it belongs, and at this point it's
immediate that the measurement reveals the correct values, the minus sign here doesn't bother us because one appears with
probability one in this case, and that's all there is to the analysis. In a nutshell, Alice picks a Bell state and Bob figures out which one it is. Just a couple of brief remarks
about superdense coding before we move on; first, we should acknowledge
that superdense coding is probably not likely
to be useful or practical in any sense as a means to
transmit classical information. Classical communication
is very inexpensive and physically sending Qubits is challenging to say the least, and so long as that's the case, we wouldn't expect any
practical advantage at all to come from superdense coding. But just because it isn't
likely to be practical doesn't mean that superdense
coding is not interesting. It's very interesting in fact, for one, the underlying idea is
really quite fundamental, and when I refer to the underlying idea, I mean the idea that if Alice
shares a Bell state with Bob, then she can choose to change that state to any other Bell state that
she chooses all by herself, she doesn't need Bob's
help at all to do that. This is a specific example
that concerns the Bell states, but there's a more general underlying idea that concerns the freedom that one player has to transform an entangled state that's shared with another player, and understanding exactly
what that freedom is is an important step in understanding quantum information and we'll see exactly how to describe that freedom in simple and elegant terms In the
third unit of the series. Superdense coding also
provides a concrete example of what you can do with entanglement, which is to allow two classical
bits to be transmitted by sending a single qubit. As I already mentioned, this
is provably not possible by Holevo's theorem without entanglement. We can also consider teleportation and superdense coding together, and we see that they
establish an equivalence between one qubit of quantum communication and two bits of classical communication. As long as you're willing
to spend one e-bit, you can effectively
convert one to the other. Teleportation allows for the
communication of one qubit using two classical bits of communication and superdense coding goes the other way and allows two classical
bits of communication using one qubit of quantum communication. Without spending the e-bit, however, you cannot transform
either one into the other, it's entanglement that
makes this possible. The third and final example
that we'll cover in this lesson is not a protocol but a
game known as the CHSH game. When we talk about a game in this context, we're not talking about something that's meant to be
played for fun or sport, but rather a mathematical abstraction in the sense of game theory for instance. Mathematical abstractions of games are very commonly studied in
economics and computer science, for instance, and there's
quite a lot of value to be found in the abstraction, so games are not all fun
in games as it turns out, although it is fun to think about them. In this particular case, what we have is essentially
a thought experiment that can be described in a
pretty intuitive way as a game. The letters CHSH refer to the authors, John Clauser, Michael
Horne, Abner Shimony, and Richard Holt of a 1969
paper where the example was first described. They didn't describe
it as a game, in fact, and it's pretty common
to refer to this example not as the CHSH game, but
rather as the CHSH inequality. For a computer scientist like me, however, it's just very natural
to describe it as a game. I've already suggested that
mathematical abstractions of games are both important and useful and there are many different
categories of games that can be considered. The CHSH game is an
example of a class of games called non-local games. Here's the setup in general
for a non-local game. We have two players, Alice and Bob, who cooperate as a team, so they either win together
or they lose together. This means that they have no
incentive to act selfishly, so it's essentially the opposite
extreme to zero sum games, which are purely competitive, here the game is purely cooperative. There's a third entity called the referee. The referee isn't a player in the game, but rather the referee
runs the game, and in fact, you can think about the specification of a particular instance
of a non-local game and the specification of a
referee as being equivalent. There's a critically important constraint that's placed on Alice and Bob, which is that once the game starts, they're forbidden from
communicating with one another. They can agree beforehand on
whatever strategy they choose, but no communication
is allowed between them during the game itself. If you like, you can imagine
the game taking place in a secure facility of some sort as if the referee is playing
the role of a detective and Alice and Bob are suspects located in different
rooms in a police station, but another way of
thinking about the setup is that Alice and Bob are
separated by a vast distance and communication is prohibited because the speed of light
doesn't allow for communication to happen quickly enough. If Alice tries to send a
message to Bob for instance, the game will be over by
the time Bob receives it. The way a non-local game
works is that first, the referee asks both
Alice and Bob a question. Alice's question is labeled X and Bob's question is labeled Y. In general, you can think about X and Y as being classical states
from whatever classical states that you like and in the
specific case of the CHSH game, X and Y are bits. Alice and Bob must then provide answers, Alice's answer is labeled A,
and Bob's answer is labeled B, and again, these answers can be viewed as classical states in general and like the questions, they
will be bits in the CHSH game. At this point, the
referee makes a decision, Alice and Bob either win or they lose depending on whether or not
the pair of answers A, B is deemed correct for the
pair of questions X, Y that the referee asked, and I'll say a little bit more
about this in just a moment. First, let's take a
closer look at the referee to understand what it is
that's potentially challenging for Alice and Bob in a non-local game. And what makes a non-local
game potentially challenging is that the referee can use randomness to choose the questions X and Y. That means that Alice
and Bob have uncertainty, they don't know ahead of time what question they're going to be asked, and perhaps more importantly, they don't know what question
the other one received and it's that uncertainty
together with the fact that they can't communicate
that presents the challenge. So, it's similar to the situation where a detective
interrogates two suspects in different rooms, if
the suspects don't know what questions the other one is asked, it's much harder if they're guilty to keep their story straight. Now, I mentioned a short
time ago that the referee makes a determination
that Alice and and Bob have either answered
correctly or incorrectly depending upon the questions
that they were asked. It's assumed that this
determination is made according to some fixed
rule and you can consider any rule that you want,
but whatever it is, it is assumed that Alice
and Bob know the rule, so at least they have a fighting
chance to answer correctly. We'll take a look at the
rule for the CHSH game in a moment, but we could
consider a different rule and we would have a different
example of a non-local game. Some non-local games are interesting and in fact some are quite fascinating in mysterious to this day and
some aren't so interesting, that's just the nature of
the non-local game model. Anyway, these are the two different things that are needed to describe a referee; how exactly are the questions chosen and which answer pairs are correct for each possible question pair. If we have a description
of those two things, then we have a description for the referee and that effectively describes an instance of a non-local game. Now let's take a look at
the CHSH game specifically. The way that the game is
defined or, equivalently, how the referee is defined is as follows; first, as I've already mentioned, the questions and answers are all bits, so the referee sends a bit X
to Alice and a bit Y to Bob, and they each respond with a bit. Alice responds with A
and Bob responds with B. The way the referee chooses
which questions to ask is simple, he chooses
them uniformly at random, so, the pair of questions X, Y is equally likely to be anyone
of the four possibilities, zero, zero, zero, one,
one, zero, and one, one. Finally, the rule that
determines which answers win and which answers lose is
that a pair of answers A, B is correct for the pair of questions X, Y if and only if the exclusive or of A and B is equal to the logical and of X and Y. That may be a little bit easier to digest if we express those conditions
in the form of a table as you see right here. In the first column, we have the four possible question pairs, and in the second column we
have the conditions on A and B that are required for
Alice and Bob to win. In the first three cases,
the and of A and Y is zero, so the exclusive or must be equal to zero in order for them to win,
and that's the same thing as saying that A and B must be equal because A and B are bits,
and the fourth case, the and of X and Y is equal
to one and so the exclusive or of A and B must be equal to one in order for Alice and Bob to win and that's the same thing
as saying that A and B must be different. So, that's the CHSH game, and now we can think about
different possible strategies for Alice and Bob and how
well they can do in this game. Let's start by thinking about
deterministic strategies where Alice's answer A is a
function of the question X that she receives, and
likewise, Bob's answer B is a function of the
question Y that he receives. And as it turns out, no
deterministic strategy can possibly win every time. One way to reason that is
simply to go one by one through all of the possible
deterministic strategies and to check that every one of them loses for at least one of the four
possible question pairs. Alice and Bob, each have
four possible functions to choose from, we saw those
four functions in lesson one, so there are 16 possibilities
to check overall. Another way to see that is to
think about the equations here on the screen, we're thinking
about deterministic strategies where A and B are functions
of X and Y respectively. So for example, A of zero
represents Alice's answer when her question X is equal to zero and a of one represents Alice's answer when her question X is equal to one. These are the four equations
that have to be true in order for the winning
conditions to be met in each of the four cases. For example, if X is equal to
zero and Y is equal to zero, then the exclusive or of A
and B must be equal to zero, whereas in the case that X is equal to one and Y is equal to one, the
exclusive or of A and B must be equal to one. One way we can see that
there's no way to choose the functions A and B so that
these equations are all true, is simply to take the exclusive
or of all of the equations. The exclusive or of any
binary value with itself is equal to zero, and so it follows that the exclusive or
of the left-hand sides is equal to zero, that's
because we have two instances of a zero, two instances of a one, two instances of B zero
and two instances of B one. So, xing them all together
gives us the final answer of zero, but if we take the exclusive or of the right hand sides of the equations, we get one not zero. So the four equations
can't possibly all be true, at least one of them has to be wrong and that will lead to a
loss for Alice and Bob in at least one of the four cases. And so it follows because the referee chooses the questions
uniformly that the probability for Alice and Bob to win
using a deterministic strategy is at most three quarters or 75%. The fact that Alice and and Bob can win with probability three quarters is quite easy to see
actually, if Alice and and Bob both answer zero all
the time, for instance, they'll be correct three
out of the four times. So Alice and Bob can't do
any better than winning 75% of the time using a
deterministic strategy. But what about a probabilistic strategy? Well, it turns out that using
a probabilistic strategy can't help them at all, that's because every
probabilistic strategy can alternatively be viewed
as a random selection of a deterministic strategy just like probabilistic
operations can be viewed as random selections of
deterministic operations, as we saw in the first lesson. The average is never
larger than the maximum, so it follows that a
probabilistic strategy doesn't offer any advantage
in terms of winning over a deterministic strategy. This even includes the possibility that Alice and Bob's random
choices are correlated. So, the bottom line is that winning with probability three quarters is the best that Alice
and Bob can possibly do using any classical strategy, whether it's deterministic
or probabilistic. The natural question to ask at this point is whether Alice and Bob can do any better using a quantum strategy,
and in particular if they share an entangled quantum state which they could have repaired
prior to playing the game, can that help them? The answer is yes, it can, and that is the point of the example and that's why it's so interesting. So let's see exactly how
Alice and Bob can do better in this game using entanglement. The first thing we need to do is to define a quantum state vector Chi
theta for each angle theta, as you can see here on the screen. So, Chi theta is equal to
cosine theta times cat zero plus sine theta times cat
one, and here by the way, we're thinking about
angles in terms of radians. So for example, here's the vector we get when theta is equal to pi over
four, which is 45 degrees. Cosine of pi over four
and sine of pi over four are both equal to one over
the square root of two, so this is just another
name for the plus state. Here's another example when theta is equal to three times pi over eight, and in case you don't
happen to know the sine and cosine of three times pi over eight off the top of your head,
here's a table of the sines and the cosines that we're
actually going to need throughout this example
and includes the ones that we need for this particular vector. Now, if we wanna take the inner product between any two of these vectors
for angles, alpha and beta let's say, it's
straightforward to do that. We only have real number
entries in these vectors so there are no complex
conjugates to worry about and we obtain the product of the cosines plus the product of the the sines, and we can use one of the
so-called angle addition formulas to simplify that, it's the
cosign of the difference between the angles and
because the cosine function is symmetric around zero,
it doesn't actually matter which order we take them in. If you don't have these sorts
of trigonometric identities committed to memory, by
the way, you're not alone, but you don't have to remember them, you can just look them up. This one does make sense
though when you think about the geometric interpretation
of the inner product between real unit vectors as the cosine of the angle between them. If we compute the inner
product of the tensor product of any two of these vectors
with the Phi plus state, we get a similar looking expression except that there's a squared
of two in the denominator. This is the first time
that the Phi plus state has appeared in the context
of this example by the way, and we'll see exactly why we're interested in this particular inner product shortly, but for the moment let's
just observe that we obtain the same expression that we had for the inner product divided
by the square root of two and so we can simplify as before. Next, we'll define a
unitary matrix, U theta, for each angle theta
like you see right here, it's not difficult to
check that this is indeed a unitary matrix. The key thing we need to
recognize is that the vectors si theta is orthogonal to
the vector si theta plus pi over two, and that makes sense because if we rotate by pi over two, which is 90 degrees, we
get an orthogonal vector. If we then multiply U
theta to U theta dagger, we get the identity matrix. Intuitively speaking,
what this matrix does as an operation is to
map si theta to cat zero and si theta plus pi two to cat one. At this point, we have
everything that we need to describe Alice and Bob's strategy. First, just like we had for teleportation and superdense coding, Alice and Bob will start out by sharing an e-bit. Alice holds a qubit A,
bob holds a qubit B, and together the two Qubits
are in the Phi plus state. Alice's actions are as follows; she looks at the bit X that
she receives from the referee, and if X is equal to zero,
she performs U zero on A, which happens to be the
same thing as doing nothing because U zero is the identity matrix, effects is equal to one on the other hand, she applies U pi over four to A. After she applies one
of the two operations, she measures A using a
standard basis measurement and sends whatever result
she gets to the referee. Bob does something very similar, his qubit is called B of course, and he receives Y from the
referee as opposed to X but the key difference is
that he uses different angles for his unitary operations. If Y is equal to zero, he
applies U pi over eight, and if Y is equaled one, he applies u negative pi over eight. And we'll see why these choices of angles are good choices shortly,
it's not at all obvious. And here is a circuit diagram
that expresses this strategy. Bob is in the top and Alice
is on the bottom this time, here is the e-bit that is coming in and these are the unitary
gates that Alice and Bob might or might not apply
depending upon what X and Y are. For this one and this one, we have gates that are being controlled either by the bit X or by the bit Y and for these two right here, we have what kind of
look like control gates except that the circle isn't filled in. And what this means is
that if the control bit is set to zero, then the gate is performed and if the control is set to
one, then nothing happens. So, it's just like an
ordinary control gate except that the roles of zero and one for the control are swapped. So, effectively, Bob
performs this gate right here if Y is equal to zero, he
performs this gate right here if Y is equal to one, and similarly, Alice performs this gate right
here if X is equal to zero, and this gate right here,
if X is equal to one. So, that is Alice and Bob's strategy and now we just need to
figure out how well it works. For the analysis of the quantum strategy that I just described, we
really only need two things at this point, first, the
unitary matrix U Theta is as you see right here, and
second we had this formula. We don't actually even need the definition of the vector si theta anymore, these two equations are all we need. There are four cases to
consider starting with case one, which is that X equal
zero and Y equals zero. And if we look back at the strategy, we'll find that in this
case Alice performs U zero on her qubit and Bob performs
U pi over eight on his qubit. Remember, Alice is on the
bottom and Bob is on the top, so the state that they
obtain looks like this. And by expanding that out
according to the definition of U theta, we get this
expression you see right here. We don't need to expand
that out any further because we have the other
formula to help us out. And if we apply that formula
for each of the four terms, what we get is this. At this point Alice and Bob both measure and here's a table that
explains the distribution of the outcomes. We take the absolute value
squared of the coefficients to get the probabilities, which happens to be the
squares of the coefficients because the coefficients are
all real numbers in this case. If we make use of the
sine and cosine values from earlier in the lesson
along with some symmetries, we can express these probabilities
as we have right here in the last column. Now, what we really care
about is the probability that they win, and remember that, in the case that X and Y are both zero, they win if A and B are equal and they lose if A and B are not equal. If we calculate those probabilities, we find that the probability
that A and B are equal is the sum of this number and this number, which is equal to two plus
root two all over four, which is about 85%. The probability that A and B are not equal is two minus root two all over four, which is naturally one minus the quantity that we just calculated, so
they win with probability about 85% in this case. There are three more cases to consider, but they're all very similar to this one, in the case that X equals
zero and Y equals one, Bob applies U negative pi over eight rather than U pi over eight, the values in the
formulas change somewhat, but we end up with exactly
the same probabilities that we had in the previous case. When X equals zero and Y equals one, Alice and Bob still win
when A and B are equal and so we have exactly the
same winning probability as before. In the third case, when X is equal to one and Y is equal to zero, we
have a similar situation, the angles are different,
but they have exactly the same relationship to
one another, and in fact, we do get exactly the
same vector that we got in the previous case. So the probabilities are the same and so is the winning probability. The last case is a little bit
different as we might expect because the winning condition
is different in this case. When X and Y are both equal to one, Alice and Bob win when
A and B are different. The basic method is the same, although the angles are different, this time, we get this vector right here and you can check that by going through the required subtractions. The probabilities do change finally, and if we look up the values that we need, we see that the probabilities
have effectively swapped positions from before, so this time it's the probability
that A is not equal to B, that is the larger of the two, but that's good because in this case, Alice and Bob win when
A and B are not equal. So once again, they win with
probability approximately 85%. Given that they win in every
case with the same probability, which is two plus root two
all over four or about 85%, that's the probability
that they win overall. That's better than any classical strategy can do for this game, and that's
what makes it interesting. This happens to be the
optimal winning probability for any quantum strategy, you
can't do any better than this using any entangled state. That fact is known Tsirelson inequality, it's named named for Boris
Tsirelson who first proved it, and to my knowledge was
one of the first people to describe this example as a game. In case the analysis
that we just went through seems a little bit mysterious, it's possible to think
about it geometrically, which might be helpful to
give you a better sense for the choice of the angles. Specifically, if we use the
formula that you see right here, which we can easily deduce
from the expressions we already found for the
inner products involved, then we can alternatively
express the probabilities of the different possible
measurement outcomes in the four cases as follows; first, if X, Y equals zero, zero, then these are the probabilities of the various measurement outcomes, and you have to either think back or look back at the analysis
that we just went through to conclude that using our formula. Over here, we can see the
different state vectors drawn as they were before, and the color coding here
is that blue corresponds to the outcome zero and red
corresponds to the outcome one. You can see that the angle
between the blue vectors and the angle between the red
vectors is relatively small, it's pi over eight in both cases, so the cosine is relatively large, and that means that the
probability that the two outcomes are equal is relatively large. In the case that X, Y equals zero, one, it's a similar situation, the
angles between the vectors having the same color is small, so the probability of
equal outcomes is again, relatively large. The third case is similar, and in the fourth case
when X, Y equals one, one, something interesting happens,
this time, just by the way, these vectors have been chosen, we see that the angles between
vectors having the same color are relatively large, and specifically the angles are three times
pi over eight in both cases. This means that the probability
that the measurement outcomes is equal is relatively small, but this is the one case
in which Alice and Bob win when their answers disagree,
so it's what we want. Once again, here are the vectors: when X, Y equals zero, zero,
when Xx, Y equals zero, one, when X, Y equals one, zero,
and when X, Y equals one, one. And perhaps this geometric
picture is helpful in building some intuition
for how this strategy works and why these various angles were chosen in the way that they were. To finish off the lesson, I'll make a few brief
remarks about the CHSH game. First, as I've already mentioned, this example isn't always
described as a game. In the physics community, for instance, it's more common that
the example is described simply as an experiment rather than a game and Alice and Bob are
viewed as being detectors rather than players. The basic idea of an experiment like this where entanglement leads
to statistical results that are inconsistent with
any classical description or more generally any
theory based on so-called local hidden variables,
is due to John Bell, the namesake of the Bell states, and so people often refer
to experiments of this sort as Bell tests. Sometimes people also
refer to Bell's theorem, which can be formulated in different ways, but it basically says
that quantum mechanics is not compatible with so-called local hidden variable theories. The CHSH game or inequality is essentially a very nice and clean
example of a Bell test, and you can think about
this example as a proof or a demonstration of Bell's theorem. The second remark is that the CHSH game offers us one way to experimentally test that the theory of quantum information is an accurate theory. We can actually set up experiments that implement the CHSH game, including the sort of
strategy based on entanglement that I had just described, and
see if it works, and it does. This provides us with a
high degree of confidence that entanglement is real, and unlike the perhaps
vague or poetic ways that we try to come up with
to explain entanglement, the CHSH game or experiment
gives us a concrete and testable way to observe entanglement. If that sounds important,
it's because it is, and the 2022 Nobel Prize
in physics acknowledges exactly this. The prize was awarded to
Alain Aspect, John Clauser, who is the C in CHSH, and Anton Zeilinger for experiments with entangled photons that established the violation
of Bell inequalities, or in other words, that
showed that quantum strategies of the sort that I described actually work and outperform classical strategies. And if you're interested in
learning more about this, I encourage you to check out
the video on Bells Theorem by my colleague Olivia Lanes, right here on the Qiskit YouTube channel. And finally, the topic of
non-local games more generally is truly fascinating, and it's
an active area of research that brings together
physics, computer science, and pure mathematics and there are still many
interesting questions about non-local games that
remain unanswered to this day. And that is the end of lesson four, as well as the end of
unit one of the series on the Basics of Quantum Information. The first lesson explained
how quantum information works for single systems in isolation, and the second lesson explained
how quantum information works for multiple systems,
which pretty naturally emerges from how it works for single systems. The third lesson introduced
the quantum circuit model of computation, and in the fourth lesson, we took a look at three important examples that illustrate things we
can do with entanglement. Along the way, we saw some very important mathematical concepts,
including tensor products and inner products, and we observed some fundamental limitations
of quantum information. I hope you'll join me for
the next unit of the series, which will be on quantum algorithms, where we'll focus on the question of what computational advantages
quantum information offers. Goodbye until then.

## Fault Tolerant Quantum Computation ｜ Understanding Quantum Information & Computation ｜ Lesson 16

Welcome back to understanding quantum Welcome back to understanding quantum
information and computation. My name is information and computation. My name is information and computation. My name is
John Wattress and I'm the technical John Wattress and I'm the technical John Wattress and I'm the technical
director for education at IBM director for education at IBM director for education at IBM
Quantum. This is the 16th and final Quantum. This is the 16th and final Quantum. This is the 16th and final
lesson of the series and the topic of lesson of the series and the topic of lesson of the series and the topic of
the lesson is fall tolerant quantum the lesson is fall tolerant quantum the lesson is fall tolerant quantum
computing. Over the course of the three computing. Over the course of the three computing. Over the course of the three
previous lessons, we've seen several previous lessons, we've seen several previous lessons, we've seen several
examples of quantum error correcting examples of quantum error correcting examples of quantum error correcting
codes which can detect and allow us to codes which can detect and allow us to codes which can detect and allow us to
correct quantum errors as long as not correct quantum errors as long as not correct quantum errors as long as not
too many of them occur. If we want to too many of them occur. If we want to too many of them occur. If we want to
use error correction for quantum use error correction for quantum use error correction for quantum
computing, however, there are still many computing, however, there are still many computing, however, there are still many
issues that need to be reckoned with, issues that need to be reckoned with, issues that need to be reckoned with,
including the reality that our quantum including the reality that our quantum including the reality that our quantum
gates, measurements, and state gates, measurements, and state gates, measurements, and state
preparations are themselves likely to be preparations are themselves likely to be preparations are themselves likely to be
imperfect. So, for instance, if we're imperfect. So, for instance, if we're imperfect. So, for instance, if we're
going to perform error correction on one going to perform error correction on one going to perform error correction on one
or more cubits that have been encoded or more cubits that have been encoded or more cubits that have been encoded
using a quantum error correcting code, using a quantum error correcting code, using a quantum error correcting code,
then we're going to have to do that with then we're going to have to do that with then we're going to have to do that with
gates and measurements that themselves gates and measurements that themselves gates and measurements that themselves
have a chance of not working correctly. have a chance of not working correctly. have a chance of not working correctly.
And that means not only failing to And that means not only failing to And that means not only failing to
detect or correct errors, but possibly detect or correct errors, but possibly detect or correct errors, but possibly
introducing new errors. In addition, we introducing new errors. In addition, we introducing new errors. In addition, we
also need to perform the actual also need to perform the actual also need to perform the actual
computations we're interested in computations we're interested in computations we're interested in
performing again with gates that aren't performing again with gates that aren't performing again with gates that aren't
perfect. And we certainly can't risk perfect. And we certainly can't risk perfect. And we certainly can't risk
decoding quantum information for the decoding quantum information for the decoding quantum information for the
sake of performing computations and then sake of performing computations and then sake of performing computations and then
re-encoding once we're done because re-encoding once we're done because re-encoding once we're done because
errors might strike when the protection errors might strike when the protection errors might strike when the protection
of a quantum error correcting code is of a quantum error correcting code is of a quantum error correcting code is
absent. absent. absent.
That means that we have to figure out That means that we have to figure out That means that we have to figure out
how to perform quantum gates directly on how to perform quantum gates directly on how to perform quantum gates directly on
logical cubits that never go without the logical cubits that never go without the logical cubits that never go without the
protection of a quantum error correcting protection of a quantum error correcting protection of a quantum error correcting
code. This is a tall order, but it is code. This is a tall order, but it is code. This is a tall order, but it is
known that as long as the level of noise known that as long as the level of noise known that as long as the level of noise
falls below a certain threshold value, falls below a certain threshold value, falls below a certain threshold value,
it is in fact possible to perform it is in fact possible to perform it is in fact possible to perform
arbitrarily large quantum computations arbitrarily large quantum computations arbitrarily large quantum computations
reliably using noisy hardware, at least reliably using noisy hardware, at least reliably using noisy hardware, at least
in theory. And we'll discuss this in theory. And we'll discuss this in theory. And we'll discuss this
critically important fact later in the critically important fact later in the critically important fact later in the
lesson. Here's a brief overview of the lesson. Here's a brief overview of the lesson. Here's a brief overview of the
lesson. We'll start out with a basic lesson. We'll start out with a basic lesson. We'll start out with a basic
model for fault tolerant quantum model for fault tolerant quantum model for fault tolerant quantum
computing including a short discussion computing including a short discussion computing including a short discussion
of noise models along with a general of noise models along with a general of noise models along with a general
methodology for implementing a quantum methodology for implementing a quantum methodology for implementing a quantum
circuit fall circuit fall circuit fall
tolerantly. We'll then move on to the tolerantly. We'll then move on to the tolerantly. We'll then move on to the
issue of error propagation in fault issue of error propagation in fault issue of error propagation in fault
tolerant quantum circuits and how to tolerant quantum circuits and how to tolerant quantum circuits and how to
control it. In particular, I'll discuss control it. In particular, I'll discuss control it. In particular, I'll discuss
transversal gate implementations which transversal gate implementations which transversal gate implementations which
offer a very simple way to control error offer a very simple way to control error offer a very simple way to control error
propagation. Although there is a propagation. Although there is a propagation. Although there is a
fundamental limitation that prevents us fundamental limitation that prevents us fundamental limitation that prevents us
from using this method from using this method from using this method
exclusively. I'll also talk about a exclusively. I'll also talk about a exclusively. I'll also talk about a
different methodology involving different methodology involving different methodology involving
so-called magic states which offers a so-called magic states which offers a so-called magic states which offers a
different path to controlling error different path to controlling error different path to controlling error
propagation in fall tolerant quantum propagation in fall tolerant quantum propagation in fall tolerant quantum
circuits. Although it isn't the only circuits. Although it isn't the only circuits. Although it isn't the only
known method and finally the lesson known method and finally the lesson known method and finally the lesson
concludes with a pretty highle concludes with a pretty highle concludes with a pretty highle
discussion of the threshold theorem discussion of the threshold theorem discussion of the threshold theorem
which I alluded to just a moment ago. which I alluded to just a moment ago. which I alluded to just a moment ago.
This theorem states that arbitrarily This theorem states that arbitrarily This theorem states that arbitrarily
large quantum circuits can be large quantum circuits can be large quantum circuits can be
implemented reliably so long as the implemented reliably so long as the implemented reliably so long as the
error rate of all of the components error rate of all of the components error rate of all of the components
involved falls below a certain finite involved falls below a certain finite involved falls below a certain finite
threshold threshold threshold
value. In general, this threshold value value. In general, this threshold value value. In general, this threshold value
depends on the error correcting code we depends on the error correcting code we depends on the error correcting code we
use as well as the specific choices we use as well as the specific choices we use as well as the specific choices we
make for fault implementations of make for fault implementations of make for fault implementations of
quantum gates and measurements but not quantum gates and measurements but not quantum gates and measurements but not
on the size of the circuit being on the size of the circuit being on the size of the circuit being
implemented. And this gives us a good implemented. And this gives us a good implemented. And this gives us a good
reason to believe that large-scale reason to believe that large-scale reason to believe that large-scale
quantum computations will one day become quantum computations will one day become quantum computations will one day become
a reality. We'll begin by discussing a a reality. We'll begin by discussing a a reality. We'll begin by discussing a
basic model for fault tolerant quantum basic model for fault tolerant quantum basic model for fault tolerant quantum
computing which is naturally based on computing which is naturally based on computing which is naturally based on
quantum circuits and quantum error quantum circuits and quantum error quantum circuits and quantum error
correcting correcting correcting
codes. For the sake of this discussion, codes. For the sake of this discussion, codes. For the sake of this discussion,
let's imagine that we have a quantum let's imagine that we have a quantum let's imagine that we have a quantum
circuit that we hope to implement such circuit that we hope to implement such circuit that we hope to implement such
as the one shown here. This happens to as the one shown here. This happens to as the one shown here. This happens to
be a teleportation circuit including the be a teleportation circuit including the be a teleportation circuit including the
preparation of the EBIT. But the preparation of the EBIT. But the preparation of the EBIT. But the
specifics don't really matter for this specifics don't really matter for this specifics don't really matter for this
discussion. And you could imagine that discussion. And you could imagine that discussion. And you could imagine that
this is any other quantum circuit, this is any other quantum circuit, this is any other quantum circuit,
possibly a much larger circuit that possibly a much larger circuit that possibly a much larger circuit that
includes state preparations, unitary includes state preparations, unitary includes state preparations, unitary
operations, and operations, and operations, and
measurements. This circuit, of course, measurements. This circuit, of course, measurements. This circuit, of course,
represents an ideal, and an actual represents an ideal, and an actual represents an ideal, and an actual
implementation of it won't be implementation of it won't be implementation of it won't be
perfect. The natural question to ask is perfect. The natural question to ask is perfect. The natural question to ask is
what could possibly go what could possibly go what could possibly go
wrong? Well, there's quite a lot that wrong? Well, there's quite a lot that wrong? Well, there's quite a lot that
could go wrong. For instance, the state could go wrong. For instance, the state could go wrong. For instance, the state
initializations generally won't be initializations generally won't be initializations generally won't be
perfect. Of course, the unitary gates perfect. Of course, the unitary gates perfect. Of course, the unitary gates
won't be perfect and neither will the won't be perfect and neither will the won't be perfect and neither will the
measurements. And moreover, the cubits measurements. And moreover, the cubits measurements. And moreover, the cubits
themselves will be susceptible to noise, themselves will be susceptible to noise, themselves will be susceptible to noise,
including decoherence, at every point in including decoherence, at every point in including decoherence, at every point in
the computation, even when nothing is the computation, even when nothing is the computation, even when nothing is
happening to these cubits and they're happening to these cubits and they're happening to these cubits and they're
simply storing quantum simply storing quantum simply storing quantum
information. In other words, pretty much information. In other words, pretty much information. In other words, pretty much
everything could go wrong. everything could go wrong. everything could go wrong.
There is one exception though or at There is one exception though or at There is one exception though or at
least that's a typical assumption and least that's a typical assumption and least that's a typical assumption and
that is that any classical computations that is that any classical computations that is that any classical computations
that are involved are probably not going that are involved are probably not going that are involved are probably not going
to be faulty because for all intents and to be faulty because for all intents and to be faulty because for all intents and
purposes classical computations are purposes classical computations are purposes classical computations are
basically perfect. So for example, if we basically perfect. So for example, if we basically perfect. So for example, if we
run a classical matching algorithm to run a classical matching algorithm to run a classical matching algorithm to
compute corrections for surface code compute corrections for surface code compute corrections for surface code
errors, we really don't need to worry errors, we really don't need to worry errors, we really don't need to worry
that this classical computation might be that this classical computation might be that this classical computation might be
faulty. But anything that directly faulty. But anything that directly faulty. But anything that directly
involves quantum information could involves quantum information could involves quantum information could
be. Of course, we need a precise model be. Of course, we need a precise model be. Of course, we need a precise model
through which we can associate through which we can associate through which we can associate
probabilities for various things to go probabilities for various things to go probabilities for various things to go
wrong. And if we wanted to, we could wrong. And if we wanted to, we could wrong. And if we wanted to, we could
come up with as complicated a model as come up with as complicated a model as come up with as complicated a model as
we like to hopefully reflect whatever we like to hopefully reflect whatever we like to hopefully reflect whatever
actually happens in reality. If the actually happens in reality. If the actually happens in reality. If the
model is too complicated, however, it model is too complicated, however, it model is too complicated, however, it
might end up being difficult or might end up being difficult or might end up being difficult or
impossible to work with or to say impossible to work with or to say impossible to work with or to say
anything interesting anything interesting anything interesting
about. One very simple example of a about. One very simple example of a about. One very simple example of a
noise model that people often consider noise model that people often consider noise model that people often consider
is the independent stochastic noise is the independent stochastic noise is the independent stochastic noise
model where errors or faults affecting model where errors or faults affecting model where errors or faults affecting
different components at different different components at different different components at different
moments or in other words, different moments or in other words, different moments or in other words, different
locations in the circuit are locations in the circuit are locations in the circuit are
independent. independent. independent.
For instance, we could say that every For instance, we could say that every For instance, we could say that every
gate fails with a certain probability, gate fails with a certain probability, gate fails with a certain probability,
that bit flips and phase flips happen to that bit flips and phase flips happen to that bit flips and phase flips happen to
stored cubits with a different stored cubits with a different stored cubits with a different
probability per unit time and so on. But probability per unit time and so on. But probability per unit time and so on. But
the assumption is that there's no the assumption is that there's no the assumption is that there's no
correlation between different correlation between different correlation between different
errors. Now, you might object to that errors. Now, you might object to that errors. Now, you might object to that
model because there probably will be model because there probably will be model because there probably will be
some correlations among some correlations among some correlations among
errors. For instance, there's probably a errors. For instance, there's probably a errors. For instance, there's probably a
small chance of a catastrophic error small chance of a catastrophic error small chance of a catastrophic error
that wipes out all the cubits at once. that wipes out all the cubits at once. that wipes out all the cubits at once.
Or perhaps more likely, there could be Or perhaps more likely, there could be Or perhaps more likely, there could be
errors that are localized but errors that are localized but errors that are localized but
nevertheless affect multiple components. nevertheless affect multiple components. nevertheless affect multiple components.
And nobody suggests And nobody suggests And nobody suggests
otherwise. But nevertheless, this model otherwise. But nevertheless, this model otherwise. But nevertheless, this model
does represent a simple baseline that in does represent a simple baseline that in does represent a simple baseline that in
essence represents the idea that nature essence represents the idea that nature essence represents the idea that nature
is unpredictable, but it isn't malicious is unpredictable, but it isn't malicious is unpredictable, but it isn't malicious
and it's not trying to intentionally and it's not trying to intentionally and it's not trying to intentionally
ruin quantum ruin quantum ruin quantum
computations. There are other arguably computations. There are other arguably computations. There are other arguably
more realistic noise models that are more realistic noise models that are more realistic noise models that are
often considered. And regardless of the often considered. And regardless of the often considered. And regardless of the
model we choose, if it turns out that model we choose, if it turns out that model we choose, if it turns out that
our model leads us astray and we learn our model leads us astray and we learn our model leads us astray and we learn
something new about the errors in something new about the errors in something new about the errors in
whatever system we're working with, we whatever system we're working with, we whatever system we're working with, we
can naturally formulate and consider can naturally formulate and consider can naturally formulate and consider
different error models. Next, we'll take different error models. Next, we'll take different error models. Next, we'll take
a look at a basic strategy for a look at a basic strategy for a look at a basic strategy for
implementing quantum circuits fall implementing quantum circuits fall implementing quantum circuits fall
tolerantly. So let's consider a given tolerantly. So let's consider a given tolerantly. So let's consider a given
logical quantum circuit, meaning an logical quantum circuit, meaning an logical quantum circuit, meaning an
ideal quantum circuit that we're ideal quantum circuit that we're ideal quantum circuit that we're
interested in running. For instance, we interested in running. For instance, we interested in running. For instance, we
could consider the same circuit as could consider the same circuit as could consider the same circuit as
before, which is now stretched out a before, which is now stretched out a before, which is now stretched out a
little bit because we're going to need a little bit because we're going to need a little bit because we're going to need a
little bit more space for the fall little bit more space for the fall little bit more space for the fall
tolerant implementation of it. First tolerant implementation of it. First tolerant implementation of it. First
off, the state preparations, unitary off, the state preparations, unitary off, the state preparations, unitary
gates, and measurements won't be gates, and measurements won't be gates, and measurements won't be
performed directly as single gates or as performed directly as single gates or as performed directly as single gates or as
single cubit measurements or state single cubit measurements or state single cubit measurements or state
preparations, but rather they'll be preparations, but rather they'll be preparations, but rather they'll be
performed by so-called gadgets, which performed by so-called gadgets, which performed by so-called gadgets, which
could each involve multiple cubits and could each involve multiple cubits and could each involve multiple cubits and
multiple multiple multiple
operations. Here in this diagram, I've operations. Here in this diagram, I've operations. Here in this diagram, I've
illustrated this by drawing purple boxes illustrated this by drawing purple boxes illustrated this by drawing purple boxes
around these things. And you can think around these things. And you can think around these things. And you can think
of these purple boxes as being like of these purple boxes as being like of these purple boxes as being like
little circuits or operations that are little circuits or operations that are little circuits or operations that are
merely labeled by whatever state merely labeled by whatever state merely labeled by whatever state
preparation or gate we intend to preparation or gate we intend to preparation or gate we intend to
implement. Second, we need to protect implement. Second, we need to protect implement. Second, we need to protect
the cubits using an error correcting the cubits using an error correcting the cubits using an error correcting
code. So it won't be the logical cubits code. So it won't be the logical cubits code. So it won't be the logical cubits
themselves that connect these different themselves that connect these different themselves that connect these different
gadgets, but rather the gadgets act on gadgets, but rather the gadgets act on gadgets, but rather the gadgets act on
the physical cubits that encode these the physical cubits that encode these the physical cubits that encode these
logical logical logical
cubits. And it's worth stressing here cubits. And it's worth stressing here cubits. And it's worth stressing here
that the logical cubits that we care that the logical cubits that we care that the logical cubits that we care
about are never exposed, meaning that about are never exposed, meaning that about are never exposed, meaning that
these logical cubits will spend their these logical cubits will spend their these logical cubits will spend their
entire lives being protected by whatever entire lives being protected by whatever entire lives being protected by whatever
quantum error correcting code we're quantum error correcting code we're quantum error correcting code we're
using. And finally, we perform error using. And finally, we perform error using. And finally, we perform error
correction on our encoded cubits correction on our encoded cubits correction on our encoded cubits
repeatedly throughout the computation, repeatedly throughout the computation, repeatedly throughout the computation,
basically whenever we have a chance. And basically whenever we have a chance. And basically whenever we have a chance. And
it's critically important that we do it's critically important that we do it's critically important that we do
this. Intuitively speaking, as errors this. Intuitively speaking, as errors this. Intuitively speaking, as errors
take place, randomness or uncertainty take place, randomness or uncertainty take place, randomness or uncertainty
starts to build up in the state of our starts to build up in the state of our starts to build up in the state of our
cubits and we need to constantly work to cubits and we need to constantly work to cubits and we need to constantly work to
remove this randomness or entropy. And remove this randomness or entropy. And remove this randomness or entropy. And
this happens through error correction. this happens through error correction. this happens through error correction.
So you can kind of think of these error So you can kind of think of these error So you can kind of think of these error
correction steps as being like little correction steps as being like little correction steps as being like little
logical refrigerators or heat pumps that logical refrigerators or heat pumps that logical refrigerators or heat pumps that
allow the logical cubits to stay cold so allow the logical cubits to stay cold so allow the logical cubits to stay cold so
that the computation can work correctly. that the computation can work correctly. that the computation can work correctly.
Once all these choices have been made, Once all these choices have been made, Once all these choices have been made,
meaning the selection of all the gadgets meaning the selection of all the gadgets meaning the selection of all the gadgets
as well as the code and assuming a as well as the code and assuming a as well as the code and assuming a
particular noise model, there's a particular noise model, there's a particular noise model, there's a
fundamental question that we can ask fundamental question that we can ask fundamental question that we can ask
ourselves and that is, is this actually ourselves and that is, is this actually ourselves and that is, is this actually
helping. Are we making things better or helping. Are we making things better or helping. Are we making things better or
are we making things are we making things are we making things
worse? If there's too much noise, we worse? If there's too much noise, we worse? If there's too much noise, we
could very well be making things worse, could very well be making things worse, could very well be making things worse,
just like the 9Q shore code makes things just like the 9Q shore code makes things just like the 9Q shore code makes things
worse if the chance of an error is too worse if the chance of an error is too worse if the chance of an error is too
high. high. high.
But if the noise is below a certain But if the noise is below a certain But if the noise is below a certain
threshold, then all of this extra work threshold, then all of this extra work threshold, then all of this extra work
will get us somewhere. And as we'll will get us somewhere. And as we'll will get us somewhere. And as we'll
discuss toward the end of the lesson, discuss toward the end of the lesson, discuss toward the end of the lesson,
paths open up for further reduction of paths open up for further reduction of paths open up for further reduction of
the the the
error. In this part of the lesson, we'll error. In this part of the lesson, we'll error. In this part of the lesson, we'll
discuss error propagation in fall discuss error propagation in fall discuss error propagation in fall
tolerant quantum circuits having the tolerant quantum circuits having the tolerant quantum circuits having the
basic structure that I just basic structure that I just basic structure that I just
described. This turns out to be a described. This turns out to be a described. This turns out to be a
primary concern because if we don't primary concern because if we don't primary concern because if we don't
manage to control error propagation, manage to control error propagation, manage to control error propagation,
errors will quickly overwhelm our error errors will quickly overwhelm our error errors will quickly overwhelm our error
correction efforts. But if we're able to correction efforts. But if we're able to correction efforts. But if we're able to
control the propagation of errors, then control the propagation of errors, then control the propagation of errors, then
our code will stand a much better chance our code will stand a much better chance our code will stand a much better chance
of keeping up so that errors can be of keeping up so that errors can be of keeping up so that errors can be
corrected at a high enough rate to allow corrected at a high enough rate to allow corrected at a high enough rate to allow
our computations to our computations to our computations to
work. The starting point for this work. The starting point for this work. The starting point for this
discussion is the realization that two discussion is the realization that two discussion is the realization that two
cubit gates or multiple cubit gates more cubit gates or multiple cubit gates more cubit gates or multiple cubit gates more
generally can propagate errors even when generally can propagate errors even when generally can propagate errors even when
they function correctly. they function correctly. they function correctly.
For example, consider a C not gate. And For example, consider a C not gate. And For example, consider a C not gate. And
suppose that an X error occurs on the suppose that an X error occurs on the suppose that an X error occurs on the
control cubit prior to the C not gate control cubit prior to the C not gate control cubit prior to the C not gate
being being being
performed. Well, an X error functions performed. Well, an X error functions performed. Well, an X error functions
just like an X gate. And we find that just like an X gate. And we find that just like an X gate. And we find that
the circuit on the left is equivalent to the circuit on the left is equivalent to the circuit on the left is equivalent to
the one on the right where X errors the one on the right where X errors the one on the right where X errors
occur on both of the cubits after the C occur on both of the cubits after the C occur on both of the cubits after the C
knot. This is a propagation of errors knot. This is a propagation of errors knot. This is a propagation of errors
because the unfortunate location of the because the unfortunate location of the because the unfortunate location of the
X error on the left effectively turns it X error on the left effectively turns it X error on the left effectively turns it
into two X errors on the right. And into two X errors on the right. And into two X errors on the right. And
we'll never get around this. As long as we'll never get around this. As long as we'll never get around this. As long as
we use C not gates, there's always going we use C not gates, there's always going we use C not gates, there's always going
to be a chance that an X error occurs on to be a chance that an X error occurs on to be a chance that an X error occurs on
the control cubit right before it, which the control cubit right before it, which the control cubit right before it, which
then propagates to two X errors then propagates to two X errors then propagates to two X errors
after. But as we'll see, as long as we after. But as we'll see, as long as we after. But as we'll see, as long as we
take steps to limit the damage that this take steps to limit the damage that this take steps to limit the damage that this
causes, we'll be okay. And by the way, causes, we'll be okay. And by the way, causes, we'll be okay. And by the way,
the same phenomenon occurs when we have the same phenomenon occurs when we have the same phenomenon occurs when we have
a Z error on the target cubit just prior a Z error on the target cubit just prior a Z error on the target cubit just prior
to a C knot. And we discussed this to a C knot. And we discussed this to a C knot. And we discussed this
earlier and you can check these earlier and you can check these earlier and you can check these
equalities by simply performing the equalities by simply performing the equalities by simply performing the
multiplications or you can check them multiplications or you can check them multiplications or you can check them
using using using
kisskit. The upshot is that this will in kisskit. The upshot is that this will in kisskit. The upshot is that this will in
both cases cause correlated errors on both cases cause correlated errors on both cases cause correlated errors on
multiple cubits. And of course we multiple cubits. And of course we multiple cubits. And of course we
shouldn't forget that the C not gates shouldn't forget that the C not gates shouldn't forget that the C not gates
themselves could also be faulty which themselves could also be faulty which themselves could also be faulty which
can also cause correlated can also cause correlated can also cause correlated
errors. Adding to our concern, errors. Adding to our concern, errors. Adding to our concern,
subsequent two cubit gates might subsequent two cubit gates might subsequent two cubit gates might
propagate these errors even further. propagate these errors even further. propagate these errors even further.
For instance, if we have three C not For instance, if we have three C not For instance, if we have three C not
gates like this, just like we find when gates like this, just like we find when gates like this, just like we find when
we're measuring Xstabilizer generators, we're measuring Xstabilizer generators, we're measuring Xstabilizer generators,
for instance, then a single error can for instance, then a single error can for instance, then a single error can
quickly propagate to multiple cubits. quickly propagate to multiple cubits. quickly propagate to multiple cubits.
And if this happens within a single code And if this happens within a single code And if this happens within a single code
block, for instance, meaning a bunch of block, for instance, meaning a bunch of block, for instance, meaning a bunch of
cubits that together encode a logical cubits that together encode a logical cubits that together encode a logical
cubit, whatever error correcting code cubit, whatever error correcting code cubit, whatever error correcting code
we've chosen isn't going to stand a we've chosen isn't going to stand a we've chosen isn't going to stand a
chance because codes are designed to chance because codes are designed to chance because codes are designed to
correct lowweight errors, not highway correct lowweight errors, not highway correct lowweight errors, not highway
errors like this. errors like this. errors like this.
And so this is something that we have to And so this is something that we have to And so this is something that we have to
keep in mind front and center in fact as keep in mind front and center in fact as keep in mind front and center in fact as
we're thinking about the gadgets and the we're thinking about the gadgets and the we're thinking about the gadgets and the
error correction steps in fault tolerant error correction steps in fault tolerant error correction steps in fault tolerant
quantum quantum quantum
circuits. The simplest known way to circuits. The simplest known way to circuits. The simplest known way to
mitigate error propagation in fault mitigate error propagation in fault mitigate error propagation in fault
tolerant quantum circuits is to tolerant quantum circuits is to tolerant quantum circuits is to
implement gates transversally and I'll implement gates transversally and I'll implement gates transversally and I'll
explain what that means explain what that means explain what that means
momentarily. So let's keep the figure of momentarily. So let's keep the figure of momentarily. So let's keep the figure of
our fault tolerant cir implementation on our fault tolerant cir implementation on our fault tolerant cir implementation on
the screen just as a visual aid. the screen just as a visual aid. the screen just as a visual aid.
The idea here is that for some quantum The idea here is that for some quantum The idea here is that for some quantum
error correcting codes, certain gates error correcting codes, certain gates error correcting codes, certain gates
can be implemented transversally, which can be implemented transversally, which can be implemented transversally, which
means implementing them using gadgets means implementing them using gadgets means implementing them using gadgets
that are tensor products of operations that are tensor products of operations that are tensor products of operations
where each operation acts only on a where each operation acts only on a where each operation acts only on a
single cubit position within each code single cubit position within each code single cubit position within each code
block. This is easiest to explain block. This is easiest to explain block. This is easiest to explain
through some examples. The first and through some examples. The first and through some examples. The first and
arguably simplest example is transversal arguably simplest example is transversal arguably simplest example is transversal
implementations of polyge. implementations of polyge. implementations of polyge.
The idea is that we'd like to build The idea is that we'd like to build The idea is that we'd like to build
gadgets for each of the gates in our gadgets for each of the gates in our gadgets for each of the gates in our
circuit. And in all cases, those gadgets circuit. And in all cases, those gadgets circuit. And in all cases, those gadgets
operate on code blocks like we see for operate on code blocks like we see for operate on code blocks like we see for
the X and Z gates at the end of our the X and Z gates at the end of our the X and Z gates at the end of our
teleportation circuit. Of course, we teleportation circuit. Of course, we teleportation circuit. Of course, we
can't decode, apply a polygate, and then can't decode, apply a polygate, and then can't decode, apply a polygate, and then
re-encode because that would leave a re-encode because that would leave a re-encode because that would leave a
logical cubit unprotected and errors logical cubit unprotected and errors logical cubit unprotected and errors
might propagate within the code block as might propagate within the code block as might propagate within the code block as
a result of re-encoding. a result of re-encoding. a result of re-encoding.
Fortunately, we don't need to do that as Fortunately, we don't need to do that as Fortunately, we don't need to do that as
long as we use a stabilizer code because long as we use a stabilizer code because long as we use a stabilizer code because
every stabilizer code allows for poly every stabilizer code allows for poly every stabilizer code allows for poly
operations to be applied directly to our operations to be applied directly to our operations to be applied directly to our
logical cubits. And in fact, we can do logical cubits. And in fact, we can do logical cubits. And in fact, we can do
that with a tensor product of poly gates that with a tensor product of poly gates that with a tensor product of poly gates
on the physical cubits used for the on the physical cubits used for the on the physical cubits used for the
encoding. encoding. encoding.
For example, if we consider the 3x3 For example, if we consider the 3x3 For example, if we consider the 3x3
surface code that came up at the end of surface code that came up at the end of surface code that came up at the end of
the previous lesson, then we can the previous lesson, then we can the previous lesson, then we can
implement X and Z gates on the logical implement X and Z gates on the logical implement X and Z gates on the logical
cubit encoded by this code by tensoring cubit encoded by this code by tensoring cubit encoded by this code by tensoring
together X and Z gates along with together X and Z gates along with together X and Z gates along with
identity gates in the patterns that are identity gates in the patterns that are identity gates in the patterns that are
suggested suggested suggested
here. This is a transversal here. This is a transversal here. This is a transversal
implementation of these gates because implementation of these gates because implementation of these gates because
every one of these individual gate every one of these individual gate every one of these individual gate
operations acts only on a single cubit operations acts only on a single cubit operations acts only on a single cubit
position within each code block. position within each code block. position within each code block.
We're only using single cubic gates We're only using single cubic gates We're only using single cubic gates
here, so it's kind of trivial in a way, here, so it's kind of trivial in a way, here, so it's kind of trivial in a way,
but in any case, gadgets such as this but in any case, gadgets such as this but in any case, gadgets such as this
won't cause errors to won't cause errors to won't cause errors to
propagate. Now, any one of the propagate. Now, any one of the propagate. Now, any one of the
individual gates in a gadget such as individual gates in a gadget such as individual gates in a gadget such as
this could fail, and that will introduce this could fail, and that will introduce this could fail, and that will introduce
errors, but will always perform error errors, but will always perform error errors, but will always perform error
correction immediately after every correction immediately after every correction immediately after every
gadget. And as long as the error rate is gadget. And as long as the error rate is gadget. And as long as the error rate is
low enough, the error correcting code low enough, the error correcting code low enough, the error correcting code
will correct those errors. will correct those errors. will correct those errors.
And like I said, this is always possible And like I said, this is always possible And like I said, this is always possible
for polygates for every stabilizer code. for polygates for every stabilizer code. for polygates for every stabilizer code.
Although you do have to identify the Although you do have to identify the Although you do have to identify the
right pattern of polygates in the right pattern of polygates in the right pattern of polygates in the
gadgets to properly implement whatever gadgets to properly implement whatever gadgets to properly implement whatever
logical poly operation you're going logical poly operation you're going logical poly operation you're going
for. For another example which does for. For another example which does for. For another example which does
involve two cubic gates. This time we involve two cubic gates. This time we involve two cubic gates. This time we
can consider transversal implementations can consider transversal implementations can consider transversal implementations
of C not gates. of C not gates. of C not gates.
In particular, for every CSS code, we In particular, for every CSS code, we In particular, for every CSS code, we
can always implement a C not gate can always implement a C not gate can always implement a C not gate
between two logical cubits by simply between two logical cubits by simply between two logical cubits by simply
performing C not gates on corresponding performing C not gates on corresponding performing C not gates on corresponding
pairs of physical pairs of physical pairs of physical
cubits. And this satisfies the condition cubits. And this satisfies the condition cubits. And this satisfies the condition
of being transversal because each of of being transversal because each of of being transversal because each of
these individual C not gates acts only these individual C not gates acts only these individual C not gates acts only
on a single cubit position within each on a single cubit position within each on a single cubit position within each
code block. For example, the first C not code block. For example, the first C not code block. For example, the first C not
gate acts only on the topmost cubit gate acts only on the topmost cubit gate acts only on the topmost cubit
within each code block. the second C not within each code block. the second C not within each code block. the second C not
gates acts only on the second from top gates acts only on the second from top gates acts only on the second from top
cubit and so cubit and so cubit and so
on. In this case, we could have a on. In this case, we could have a on. In this case, we could have a
propagation of errors because C not propagation of errors because C not propagation of errors because C not
gates propagate errors and there's no gates propagate errors and there's no gates propagate errors and there's no
way to prevent that. But the key is that way to prevent that. But the key is that way to prevent that. But the key is that
the error propagation never happens the error propagation never happens the error propagation never happens
within a single code block. For example, within a single code block. For example, within a single code block. For example,
an X error could occur on the top cubit an X error could occur on the top cubit an X error could occur on the top cubit
of the top code block right before the of the top code block right before the of the top code block right before the
gadget is performed and the first C not gadget is performed and the first C not gadget is performed and the first C not
within the gadget will propagate that within the gadget will propagate that within the gadget will propagate that
error to the top cubit in the lower error to the top cubit in the lower error to the top cubit in the lower
block. But the two resulting errors are block. But the two resulting errors are block. But the two resulting errors are
now in separate code blocks. So the now in separate code blocks. So the now in separate code blocks. So the
error correction steps that take place error correction steps that take place error correction steps that take place
after the gadget will be able to correct after the gadget will be able to correct after the gadget will be able to correct
those errors individually because those those errors individually because those those errors individually because those
errors only affect a single cubit within errors only affect a single cubit within errors only affect a single cubit within
each code block. each code block. each code block.
In contrast, if the error propagation In contrast, if the error propagation In contrast, if the error propagation
happened inside of the same code block, happened inside of the same code block, happened inside of the same code block,
then we might be in trouble because the then we might be in trouble because the then we might be in trouble because the
error propagation is then turning a error propagation is then turning a error propagation is then turning a
lowway error into a higher way error lowway error into a higher way error lowway error into a higher way error
that our code may not be able to handle. that our code may not be able to handle. that our code may not be able to handle.
This implementation doesn't work for This implementation doesn't work for This implementation doesn't work for
nonCSS codes, but as long as we use a nonCSS codes, but as long as we use a nonCSS codes, but as long as we use a
CSS code, we can always implement C not CSS code, we can always implement C not CSS code, we can always implement C not
gates transversely in this gates transversely in this gates transversely in this
way. And for one more example, every way. And for one more example, every way. And for one more example, every
Clifford gate can be implemented Clifford gate can be implemented Clifford gate can be implemented
transversely for the steam code. And in transversely for the steam code. And in transversely for the steam code. And in
fact for any color code which is one of fact for any color code which is one of fact for any color code which is one of
the really nice things about color the really nice things about color the really nice things about color
codes in particular a hatomar gate on a codes in particular a hatomar gate on a codes in particular a hatomar gate on a
logical cubit can be implemented by logical cubit can be implemented by logical cubit can be implemented by
performing hatamar gates individually on performing hatamar gates individually on performing hatamar gates individually on
each of the physical cubits and a each of the physical cubits and a each of the physical cubits and a
logical sgate can be implemented not by logical sgate can be implemented not by logical sgate can be implemented not by
an sgate on each cubit but rather by an an sgate on each cubit but rather by an an sgate on each cubit but rather by an
s dagger gate on each physical s dagger gate on each physical s dagger gate on each physical
cubit. So the upshot is that transversal cubit. So the upshot is that transversal cubit. So the upshot is that transversal
gate implementations are inherently gate implementations are inherently gate implementations are inherently
fault tolerant because they never fault tolerant because they never fault tolerant because they never
propagate errors within a single code propagate errors within a single code propagate errors within a single code
block which gives the error correction block which gives the error correction block which gives the error correction
steps a fighting chance to correct steps a fighting chance to correct steps a fighting chance to correct
whatever errors are introduced or whatever errors are introduced or whatever errors are introduced or
propagated by the gates used for these propagated by the gates used for these propagated by the gates used for these
gadgets. So that's great and it's a gadgets. So that's great and it's a gadgets. So that's great and it's a
really simple way to build fault really simple way to build fault really simple way to build fault
tolerant gadgets and there are different tolerant gadgets and there are different tolerant gadgets and there are different
codes that allow for transversal codes that allow for transversal codes that allow for transversal
implementations of different gates such implementations of different gates such implementations of different gates such
as T as T as T
gates but unfortunately it's impossible gates but unfortunately it's impossible gates but unfortunately it's impossible
for any non-trivial code to implement a for any non-trivial code to implement a for any non-trivial code to implement a
universal set of gates transversely universal set of gates transversely universal set of gates transversely
which is a fact known as the eastern which is a fact known as the eastern which is a fact known as the eastern
canel canel canel
theorem in more technical terms what it theorem in more technical terms what it theorem in more technical terms what it
says is that as long as our code has says is that as long as our code has says is that as long as our code has
distance at least two the set of gates distance at least two the set of gates distance at least two the set of gates
that can be implemented transverse that can be implemented transverse that can be implemented transverse
iversally will necessarily generate a iversally will necessarily generate a iversally will necessarily generate a
discrete set of operations up to a discrete set of operations up to a discrete set of operations up to a
global phase and therefore this set global phase and therefore this set global phase and therefore this set
isn't universal because universal gate isn't universal because universal gate isn't universal because universal gate
sets necessarily generate infinitely sets necessarily generate infinitely sets necessarily generate infinitely
many many many
operations. I won't explain the proof of operations. I won't explain the proof of operations. I won't explain the proof of
this theorem which makes use of some this theorem which makes use of some this theorem which makes use of some
basic facts about lee groupoups and lee basic facts about lee groupoups and lee basic facts about lee groupoups and lee
algebbras which are not prerequisites algebbras which are not prerequisites algebbras which are not prerequisites
for this series. But the basic idea is for this series. But the basic idea is for this series. But the basic idea is
that infinite families of transversal that infinite families of transversal that infinite families of transversal
operations can't possibly keep us inside operations can't possibly keep us inside operations can't possibly keep us inside
of the code space because minuscule of the code space because minuscule of the code space because minuscule
differences in transversal operations differences in transversal operations differences in transversal operations
are well approximated by lowweight poly are well approximated by lowweight poly are well approximated by lowweight poly
operations which the code detects as operations which the code detects as operations which the code detects as
errors. So we're very happy when we can errors. So we're very happy when we can errors. So we're very happy when we can
implement gates transversely. But for implement gates transversely. But for implement gates transversely. But for
whatever code we choose, we know that we whatever code we choose, we know that we whatever code we choose, we know that we
will always have some gates that we will always have some gates that we will always have some gates that we
can't implement this way. And so we need can't implement this way. And so we need can't implement this way. And so we need
to look for alternative gadgets. By the to look for alternative gadgets. By the to look for alternative gadgets. By the
eastern canal theorem, we know that it's eastern canal theorem, we know that it's eastern canal theorem, we know that it's
not possible to implement a universal not possible to implement a universal not possible to implement a universal
set of quantum gates transversely even set of quantum gates transversely even set of quantum gates transversely even
for a meager distance 2 quantum error for a meager distance 2 quantum error for a meager distance 2 quantum error
correcting code. So we need to find correcting code. So we need to find correcting code. So we need to find
other ways to implement gates fall other ways to implement gates fall other ways to implement gates fall
tolerantly at least for some tolerantly at least for some tolerantly at least for some
gates. One well-known way to do this is gates. One well-known way to do this is gates. One well-known way to do this is
through the use of so-called magic through the use of so-called magic through the use of so-called magic
states. states. states.
For the purposes of this discussion, For the purposes of this discussion, For the purposes of this discussion,
let's recall the S and T gates which are let's recall the S and T gates which are let's recall the S and T gates which are
single cubic gates defined as is shown single cubic gates defined as is shown single cubic gates defined as is shown
here on the screen. Let's also observe here on the screen. Let's also observe here on the screen. Let's also observe
that first S is a Clifford operation that first S is a Clifford operation that first S is a Clifford operation
which follows from our definition of which follows from our definition of which follows from our definition of
Clifford operations. But you can also Clifford operations. But you can also Clifford operations. But you can also
check that conjugating a polymatrix by S check that conjugating a polymatrix by S check that conjugating a polymatrix by S
always gives us plus or minus another always gives us plus or minus another always gives us plus or minus another
polyatrix. And second, T is not a polyatrix. And second, T is not a polyatrix. And second, T is not a
Clifford operation. And in fact, if we Clifford operation. And in fact, if we Clifford operation. And in fact, if we
combine T with Hadamard and Cot, both of combine T with Hadamard and Cot, both of combine T with Hadamard and Cot, both of
which are Clifford operations, we get a which are Clifford operations, we get a which are Clifford operations, we get a
universal set of quantum universal set of quantum universal set of quantum
gates. So we can't implement a T gate gates. So we can't implement a T gate gates. So we can't implement a T gate
using just Clifford operations and using just Clifford operations and using just Clifford operations and
standard basis measurements alone. But standard basis measurements alone. But standard basis measurements alone. But
we can in fact implement a Tate using we can in fact implement a Tate using we can in fact implement a Tate using
Clifford operations and standard basis Clifford operations and standard basis Clifford operations and standard basis
measurements if in addition we have a measurements if in addition we have a measurements if in addition we have a
copy of the state that's shown here on copy of the state that's shown here on copy of the state that's shown here on
the screen, which is the Tate applied to the screen, which is the Tate applied to the screen, which is the Tate applied to
a plus state. a plus state. a plus state.
This state is called a magic state, This state is called a magic state, This state is called a magic state,
although it's not unique in that regard. although it's not unique in that regard. although it's not unique in that regard.
The term magic state is also used to The term magic state is also used to The term magic state is also used to
refer to different states that play a refer to different states that play a refer to different states that play a
similar role to this one in variations similar role to this one in variations similar role to this one in variations
of this whole of this whole of this whole
scheme. For instance, this circuit shown scheme. For instance, this circuit shown scheme. For instance, this circuit shown
here does the job. And it's not too here does the job. And it's not too here does the job. And it's not too
difficult to check difficult to check difficult to check
that. Here's the initial state with the that. Here's the initial state with the that. Here's the initial state with the
top system on the right. And if we apply top system on the right. And if we apply top system on the right. And if we apply
the C not gate and simplify, we get the the C not gate and simplify, we get the the C not gate and simplify, we get the
state that's shown here. state that's shown here. state that's shown here.
So if we measure the bottom cubit and So if we measure the bottom cubit and So if we measure the bottom cubit and
get a zero which happens with get a zero which happens with get a zero which happens with
probability 1/2, we don't perform the probability 1/2, we don't perform the probability 1/2, we don't perform the
escape and the output is t applied to s. escape and the output is t applied to s. escape and the output is t applied to s.
And if we measure one which also happens And if we measure one which also happens And if we measure one which also happens
with probability 1/2 of course we do with probability 1/2 of course we do with probability 1/2 of course we do
apply the escape and the net result is apply the escape and the net result is apply the escape and the net result is
that the output is again t applied to that the output is again t applied to that the output is again t applied to
s. This isn't the only way to do this by s. This isn't the only way to do this by s. This isn't the only way to do this by
the way. There are other similar looking the way. There are other similar looking the way. There are other similar looking
circuits that accomplish the same task circuits that accomplish the same task circuits that accomplish the same task
but it is one way to do it. but it is one way to do it. but it is one way to do it.
This is a somewhat simplified example of This is a somewhat simplified example of This is a somewhat simplified example of
something known as quantum gate something known as quantum gate something known as quantum gate
teleportation, which I alluded to back teleportation, which I alluded to back teleportation, which I alluded to back
in lesson 4, where teleportation is used in lesson 4, where teleportation is used in lesson 4, where teleportation is used
not to send cubits, but to apply gates. not to send cubits, but to apply gates. not to send cubits, but to apply gates.
The connection to teleportation may not The connection to teleportation may not The connection to teleportation may not
be entirely clear here because, like I be entirely clear here because, like I be entirely clear here because, like I
said, this is a simplified version, said, this is a simplified version, said, this is a simplified version,
which we can get away with because t is which we can get away with because t is which we can get away with because t is
diagonal. But if you're interested in diagonal. But if you're interested in diagonal. But if you're interested in
learning more, the keywords are quantum learning more, the keywords are quantum learning more, the keywords are quantum
gate teleportation. And we could also do something similar And we could also do something similar
to implement an S gate where this time to implement an S gate where this time to implement an S gate where this time
the only gates we need are a C not gate the only gates we need are a C not gate the only gates we need are a C not gate
and a Zgate along with the measurement and a Zgate along with the measurement and a Zgate along with the measurement
which is helpful if we don't have a which is helpful if we don't have a which is helpful if we don't have a
transversal implementation of an S transversal implementation of an S transversal implementation of an S
gate. So that's interesting but it's not gate. So that's interesting but it's not gate. So that's interesting but it's not
entirely clear that this actually entirely clear that this actually entirely clear that this actually
helps. In the case of the Tgate helps. In the case of the Tgate helps. In the case of the Tgate
implementation, for instance, we're implementation, for instance, we're implementation, for instance, we're
managing to apply a TATE to an input managing to apply a TATE to an input managing to apply a TATE to an input
state. But to do that, we seem to be state. But to do that, we seem to be state. But to do that, we seem to be
applying a TATE to a plus state to get a applying a TATE to a plus state to get a applying a TATE to a plus state to get a
magic state. And the situation is magic state. And the situation is magic state. And the situation is
similar for the Sgate similar for the Sgate similar for the Sgate
implementation. The point is that we implementation. The point is that we implementation. The point is that we
don't necessarily need to first create a don't necessarily need to first create a don't necessarily need to first create a
plus state and then apply a Tate to get plus state and then apply a Tate to get plus state and then apply a Tate to get
a magic state. There are other possible a magic state. There are other possible a magic state. There are other possible
ways to get this state. And in ways to get this state. And in ways to get this state. And in
particular, we can do this without particular, we can do this without particular, we can do this without
having to worry about propagating errors having to worry about propagating errors having to worry about propagating errors
in whatever computation we're working on in whatever computation we're working on in whatever computation we're working on
because the magic states can be created separately. This allows for a fault separately. This allows for a fault
tolerant implementation of a Tate as tolerant implementation of a Tate as tolerant implementation of a Tate as
I'll now briefly describe. I'll now briefly describe. I'll now briefly describe.
First, we're not actually going to run First, we're not actually going to run First, we're not actually going to run
the simple two cubit circuit shown here, the simple two cubit circuit shown here, the simple two cubit circuit shown here,
but instead we'll run this circuit using but instead we'll run this circuit using but instead we'll run this circuit using
cubits encoded in whatever error cubits encoded in whatever error cubits encoded in whatever error
correcting code we're using for our correcting code we're using for our correcting code we're using for our
fault tolerant fault tolerant fault tolerant
implementation. Which means that the implementation. Which means that the implementation. Which means that the
unitary gates and the measurements are unitary gates and the measurements are unitary gates and the measurements are
done by fault tolerant done by fault tolerant done by fault tolerant
gadgets. And that means we also need our gadgets. And that means we also need our gadgets. And that means we also need our
magic states to be encoded as magic states to be encoded as magic states to be encoded as
well. So everything will be fault well. So everything will be fault well. So everything will be fault
tolerant as long as we can get our hands tolerant as long as we can get our hands tolerant as long as we can get our hands
on encoded magic states. We are assuming on encoded magic states. We are assuming on encoded magic states. We are assuming
here by the way that we already have a here by the way that we already have a here by the way that we already have a
fall tolerant gadget for an escape. If fall tolerant gadget for an escape. If fall tolerant gadget for an escape. If
we were using the steam code for we were using the steam code for we were using the steam code for
instance, we could simply implement the instance, we could simply implement the instance, we could simply implement the
escape transversely. And for codes for escape transversely. And for codes for escape transversely. And for codes for
which that doesn't work, one option is which that doesn't work, one option is which that doesn't work, one option is
to first use this entire scheme to to first use this entire scheme to to first use this entire scheme to
implement an escape fault tolerantly and implement an escape fault tolerantly and implement an escape fault tolerantly and
then use that implementation here. then use that implementation here. then use that implementation here.
Although that's not the only option. But Although that's not the only option. But Although that's not the only option. But
anyway, getting back to the anyway, getting back to the anyway, getting back to the
implementation, the important question implementation, the important question implementation, the important question
that remains is how do we get our hands that remains is how do we get our hands that remains is how do we get our hands
on an encoded magic state given that we on an encoded magic state given that we on an encoded magic state given that we
don't yet have a fault tolerant don't yet have a fault tolerant don't yet have a fault tolerant
implementation of a Tate to make that implementation of a Tate to make that implementation of a Tate to make that
simple? And the key idea is that we can simple? And the key idea is that we can simple? And the key idea is that we can
use a probabilistic process to create use a probabilistic process to create use a probabilistic process to create
encoded magic states. And if it doesn't encoded magic states. And if it doesn't encoded magic states. And if it doesn't
happen to work out, then that's fine. We happen to work out, then that's fine. We happen to work out, then that's fine. We
can just throw away any failed attempts can just throw away any failed attempts can just throw away any failed attempts
and try again. and try again. and try again.
And that doesn't disturb the computation And that doesn't disturb the computation And that doesn't disturb the computation
we're actually trying to perform because we're actually trying to perform because we're actually trying to perform because
the creation of magic states is done the creation of magic states is done the creation of magic states is done
separately. A typical way that this is separately. A typical way that this is separately. A typical way that this is
done is through a process known as magic done is through a process known as magic done is through a process known as magic
state distillation. And it works like state distillation. And it works like state distillation. And it works like
this. We have a certain circuit known as this. We have a certain circuit known as this. We have a certain circuit known as
a distiller. We input into this a distiller. We input into this a distiller. We input into this
distiller a collection of noisy magic distiller a collection of noisy magic distiller a collection of noisy magic
states and then measure all but one of states and then measure all but one of states and then measure all but one of
the outputs. And if we condition on all the outputs. And if we condition on all the outputs. And if we condition on all
of the measurement outcomes being zero, of the measurement outcomes being zero, of the measurement outcomes being zero,
then what happens is that what comes out then what happens is that what comes out then what happens is that what comes out
is a less noisy magic state, which can is a less noisy magic state, which can is a less noisy magic state, which can
then be one of the cubits input into then be one of the cubits input into then be one of the cubits input into
another distiller. And the process another distiller. And the process another distiller. And the process
continues until we either fail because continues until we either fail because continues until we either fail because
one of the measurement outcomes is non one of the measurement outcomes is non one of the measurement outcomes is non
zero and we try again or we have a magic zero and we try again or we have a magic zero and we try again or we have a magic
state of sufficiently high quality that state of sufficiently high quality that state of sufficiently high quality that
we can safely use it to implement a Tate we can safely use it to implement a Tate we can safely use it to implement a Tate
in the computation that we care about. in the computation that we care about. in the computation that we care about.
There are different known ways to build There are different known ways to build There are different known ways to build
a distiller like this, but they all work a distiller like this, but they all work a distiller like this, but they all work
in basically the same way, which is that in basically the same way, which is that in basically the same way, which is that
they're decoders for stabilizer codes, they're decoders for stabilizer codes, they're decoders for stabilizer codes,
meaning that they're inverses of meaning that they're inverses of meaning that they're inverses of
encoding circuits, which is nice because encoding circuits, which is nice because encoding circuits, which is nice because
it means that the distiller can be a it means that the distiller can be a it means that the distiller can be a
Clifford Clifford Clifford
operation. And finally, this operation. And finally, this operation. And finally, this
distillation process is actually distillation process is actually distillation process is actually
something that takes place at the something that takes place at the something that takes place at the
logical cubit level. So in reality, it's logical cubit level. So in reality, it's logical cubit level. So in reality, it's
all done using gadgets for the distiller all done using gadgets for the distiller all done using gadgets for the distiller
and the and the and the
measurements. This process does have to measurements. This process does have to measurements. This process does have to
start somewhere. But as long as the start somewhere. But as long as the start somewhere. But as long as the
noise rate isn't too high, it is noise rate isn't too high, it is noise rate isn't too high, it is
possible to create noisy but good enough possible to create noisy but good enough possible to create noisy but good enough
magic states to start it off. For magic states to start it off. For magic states to start it off. For
instance, by applying an actual noisy T instance, by applying an actual noisy T instance, by applying an actual noisy T
gate to a cubit in a plus state and then gate to a cubit in a plus state and then gate to a cubit in a plus state and then
encoding that encoding that encoding that
cubit. So that was a very high level cubit. So that was a very high level cubit. So that was a very high level
description, but hopefully I've given description, but hopefully I've given description, but hopefully I've given
you a rough idea of how this works. you a rough idea of how this works. you a rough idea of how this works.
It's quite a bit more complicated than It's quite a bit more complicated than It's quite a bit more complicated than
implementing gates transversely, but implementing gates transversely, but implementing gates transversely, but
nevertheless, it offers a way to nevertheless, it offers a way to nevertheless, it offers a way to
implement a universal set of quantum implement a universal set of quantum implement a universal set of quantum
gates fall tolerantly, whereas gates fall tolerantly, whereas gates fall tolerantly, whereas
transversal gate implementations alone transversal gate implementations alone transversal gate implementations alone
can't do that. It's sometimes claimed can't do that. It's sometimes claimed can't do that. It's sometimes claimed
that the overhead for using magic states that the overhead for using magic states that the overhead for using magic states
to implement gates fault tolerantly to implement gates fault tolerantly to implement gates fault tolerantly
along these lines would be very high along these lines would be very high along these lines would be very high
where the vast majority of the work goes where the vast majority of the work goes where the vast majority of the work goes
into distilling magic states and running into distilling magic states and running into distilling magic states and running
the actual computation is more of an the actual computation is more of an the actual computation is more of an
afterthought. But that's actually not so afterthought. But that's actually not so afterthought. But that's actually not so
clear and there have been detailed clear and there have been detailed clear and there have been detailed
analyses that reveal that this method analyses that reveal that this method analyses that reveal that this method
could be in fact practical. There are could be in fact practical. There are could be in fact practical. There are
several other known schemes for several other known schemes for several other known schemes for
implementing gates fall tolerantly. Code implementing gates fall tolerantly. Code implementing gates fall tolerantly. Code
deformation and code switching are names deformation and code switching are names deformation and code switching are names
associated with some of these schemes associated with some of these schemes associated with some of these schemes
and new ways continue to be developed and new ways continue to be developed and new ways continue to be developed
and refined but I won't describe those and refined but I won't describe those and refined but I won't describe those
methods in this methods in this methods in this
lesson. There is another important issue lesson. There is another important issue lesson. There is another important issue
concerning fault tolerant concerning fault tolerant concerning fault tolerant
implementations of quantum circuits and implementations of quantum circuits and implementations of quantum circuits and
it concerns the error correction steps it concerns the error correction steps it concerns the error correction steps
themselves. So this is not about themselves. So this is not about themselves. So this is not about
implementing gates. It's about doing the implementing gates. It's about doing the implementing gates. It's about doing the
actual error actual error actual error
correction. The problem is that if we correction. The problem is that if we correction. The problem is that if we
think about the simple circuits that we think about the simple circuits that we think about the simple circuits that we
first saw in lesson 14 for measuring first saw in lesson 14 for measuring first saw in lesson 14 for measuring
stabilizer generators based on phase stabilizer generators based on phase stabilizer generators based on phase
estimation, we find that they're not estimation, we find that they're not estimation, we find that they're not
themselves fault tolerant because they themselves fault tolerant because they themselves fault tolerant because they
could potentially cause errors to could potentially cause errors to could potentially cause errors to
propagate within a single code block. propagate within a single code block. propagate within a single code block.
This goes back to the idea that anything This goes back to the idea that anything This goes back to the idea that anything
involving quantum information is involving quantum information is involving quantum information is
susceptible to errors and that includes susceptible to errors and that includes susceptible to errors and that includes
the circuits that are themselves meant the circuits that are themselves meant the circuits that are themselves meant
to correct errors. This seems rather to correct errors. This seems rather to correct errors. This seems rather
problematic, but there are in fact problematic, but there are in fact problematic, but there are in fact
multiple solutions known to this multiple solutions known to this multiple solutions known to this
problem. I'll mention just a few of problem. I'll mention just a few of problem. I'll mention just a few of
them, although I won't explain any of them, although I won't explain any of them, although I won't explain any of
them in detail. And by the way, if them in detail. And by the way, if them in detail. And by the way, if
you're getting the idea that fault you're getting the idea that fault you're getting the idea that fault
tolerant quantum computing is tolerant quantum computing is tolerant quantum computing is
complicated and there are a lot of complicated and there are a lot of complicated and there are a lot of
details to worry about, then that's details to worry about, then that's details to worry about, then that's
certainly understandable because it's certainly understandable because it's certainly understandable because it's
true. There are a lot of moving parts true. There are a lot of moving parts true. There are a lot of moving parts
here and my intention with this lesson here and my intention with this lesson here and my intention with this lesson
is not to explain all of the details but is not to explain all of the details but is not to explain all of the details but
rather to give you just a basic rather to give you just a basic rather to give you just a basic
introduction to how it all introduction to how it all introduction to how it all
works. One method for fault tolerant works. One method for fault tolerant works. One method for fault tolerant
quantum error correction is known as quantum error correction is known as quantum error correction is known as
shore error correction named after Peter shore error correction named after Peter shore error correction named after Peter
Shore who first proposed this method. Shore who first proposed this method. Shore who first proposed this method.
The idea is to perform each syndrome The idea is to perform each syndrome The idea is to perform each syndrome
measurement using a so-called cat state, measurement using a so-called cat state, measurement using a so-called cat state,
which is the same thing as a GHC state which is the same thing as a GHC state which is the same thing as a GHC state
when we have three cubits, but it can be when we have three cubits, but it can be when we have three cubits, but it can be
generalized to any number of cubits that generalized to any number of cubits that generalized to any number of cubits that
matches the weight of the stabilizer matches the weight of the stabilizer matches the weight of the stabilizer
generator that we're intending to generator that we're intending to generator that we're intending to
measure. In particular, the circuit measure. In particular, the circuit measure. In particular, the circuit
shown here works for measuring a shown here works for measuring a shown here works for measuring a
stabilizer generator. And it can be stabilizer generator. And it can be stabilizer generator. And it can be
shown that this method doesn't cause shown that this method doesn't cause shown that this method doesn't cause
errors to propagate within a code block. errors to propagate within a code block. errors to propagate within a code block.
It necessitates creating the cat state It necessitates creating the cat state It necessitates creating the cat state
itself. And to make it work reliably in itself. And to make it work reliably in itself. And to make it work reliably in
the presence of errors and potentially the presence of errors and potentially the presence of errors and potentially
faulty gates, the method actually faulty gates, the method actually faulty gates, the method actually
requires that we repeatedly run circuits requires that we repeatedly run circuits requires that we repeatedly run circuits
like this and make inferences about like this and make inferences about like this and make inferences about
where different errors may have occurred where different errors may have occurred where different errors may have occurred
during the during the during the
process. An alternative method is known process. An alternative method is known process. An alternative method is known
as steam error correction which was as steam error correction which was as steam error correction which was
first proposed by Andrew Stein and this first proposed by Andrew Stein and this first proposed by Andrew Stein and this
method works differently and it only method works differently and it only method works differently and it only
works for CSS codes. In this method, we works for CSS codes. In this method, we works for CSS codes. In this method, we
don't actually perform the syndrome don't actually perform the syndrome don't actually perform the syndrome
measurements on the encoded quantum measurements on the encoded quantum measurements on the encoded quantum
states in the circuit that we're trying states in the circuit that we're trying states in the circuit that we're trying
to run, but instead we intentionally to run, but instead we intentionally to run, but instead we intentionally
propagate errors to a workspace system propagate errors to a workspace system propagate errors to a workspace system
and then measure that system and and then measure that system and and then measure that system and
classically detect errors. This figure classically detect errors. This figure classically detect errors. This figure
shown here shows how to do this for X shown here shows how to do this for X shown here shows how to do this for X
errors in which case the workspace errors in which case the workspace errors in which case the workspace
cubits are initially set to an encoding cubits are initially set to an encoding cubits are initially set to an encoding
of a plus of a plus of a plus
state. So this is basically a state. So this is basically a state. So this is basically a
transversal implementation of a C not transversal implementation of a C not transversal implementation of a C not
gate which should do nothing because the gate which should do nothing because the gate which should do nothing because the
target is in a plus state. But if there target is in a plus state. But if there target is in a plus state. But if there
were X errors in the top cubits, they'll were X errors in the top cubits, they'll were X errors in the top cubits, they'll
propagate down to the lower system and propagate down to the lower system and propagate down to the lower system and
they'll be detected. There's also a they'll be detected. There's also a they'll be detected. There's also a
similar method for detecting Z errors similar method for detecting Z errors similar method for detecting Z errors
and that isn't shown here. There's also and that isn't shown here. There's also and that isn't shown here. There's also
a related method known as canel error a related method known as canel error a related method known as canel error
correction first proposed by Manny Canil correction first proposed by Manny Canil correction first proposed by Manny Canil
and it's based on teleportation rather and it's based on teleportation rather and it's based on teleportation rather
than a transversal C not and that allows than a transversal C not and that allows than a transversal C not and that allows
it to work for arbitrary stabilizer it to work for arbitrary stabilizer it to work for arbitrary stabilizer
codes and not just CSS codes. codes and not just CSS codes. codes and not just CSS codes.
The last thing that I'll discuss in this The last thing that I'll discuss in this The last thing that I'll discuss in this
lesson is a very important theorem known lesson is a very important theorem known lesson is a very important theorem known
as the threshold as the threshold as the threshold
theorem. Here's a somewhat informal theorem. Here's a somewhat informal theorem. Here's a somewhat informal
statement of the statement of the statement of the
theorem. It says that if we have any theorem. It says that if we have any theorem. It says that if we have any
quantum circuit having n gates where n quantum circuit having n gates where n quantum circuit having n gates where n
can be as large as we like, then it's can be as large as we like, then it's can be as large as we like, then it's
possible to implement that circuit with possible to implement that circuit with possible to implement that circuit with
high accuracy using a noisy quantum high accuracy using a noisy quantum high accuracy using a noisy quantum
circuit provided that the level of noise circuit provided that the level of noise circuit provided that the level of noise
is below a certain threshold value is below a certain threshold value is below a certain threshold value
that's independent of n. And moreover, that's independent of n. And moreover, that's independent of n. And moreover,
it isn't too expensive to do this in the it isn't too expensive to do this in the it isn't too expensive to do this in the
sense that the size of the noisy circuit sense that the size of the noisy circuit sense that the size of the noisy circuit
required is on the order of n times some required is on the order of n times some required is on the order of n times some
constant power of the logarithm of constant power of the logarithm of constant power of the logarithm of
n. To state the theorem more formally n. To state the theorem more formally n. To state the theorem more formally
requires being specific about the noise requires being specific about the noise requires being specific about the noise
model, which I won't do in this lesson. model, which I won't do in this lesson. model, which I won't do in this lesson.
It can for instance be proved for the It can for instance be proved for the It can for instance be proved for the
independent stochastic noise model that independent stochastic noise model that independent stochastic noise model that
I mentioned earlier where errors occur I mentioned earlier where errors occur I mentioned earlier where errors occur
independently at each possible location independently at each possible location independently at each possible location
in the circuit with some probability in the circuit with some probability in the circuit with some probability
that's strictly smaller than this that's strictly smaller than this that's strictly smaller than this
threshold value. But it can also be threshold value. But it can also be threshold value. But it can also be
proved for more general noise models proved for more general noise models proved for more general noise models
where there can be correlations among where there can be correlations among where there can be correlations among
errors. Now, this is a theoretical errors. Now, this is a theoretical errors. Now, this is a theoretical
result and the typical way that it's result and the typical way that it's result and the typical way that it's
proved doesn't necessarily translate to proved doesn't necessarily translate to proved doesn't necessarily translate to
something that we would actually do in something that we would actually do in something that we would actually do in
practice, but it's nevertheless a very practice, but it's nevertheless a very practice, but it's nevertheless a very
important theorem in practical terms. important theorem in practical terms. important theorem in practical terms.
And the reason for this is that it And the reason for this is that it And the reason for this is that it
basically says that there's no basically says that there's no basically says that there's no
fundamental barrier to performing fundamental barrier to performing fundamental barrier to performing
quantum computations using noisy quantum computations using noisy quantum computations using noisy
components. components. components.
As long as the error rate for these As long as the error rate for these As long as the error rate for these
components is below this threshold components is below this threshold components is below this threshold
value, they can be used to build value, they can be used to build value, they can be used to build
reliable quantum circuits of arbitrary reliable quantum circuits of arbitrary reliable quantum circuits of arbitrary
size. Another way to say it is that if size. Another way to say it is that if size. Another way to say it is that if
the theorem wasn't true, it's hard to the theorem wasn't true, it's hard to the theorem wasn't true, it's hard to
imagine large scale quantum computing imagine large scale quantum computing imagine large scale quantum computing
ever becoming a ever becoming a ever becoming a
reality. There's a lot of technical reality. There's a lot of technical reality. There's a lot of technical
details involved in formal proofs of details involved in formal proofs of details involved in formal proofs of
this theorem, which I won't attempt to this theorem, which I won't attempt to this theorem, which I won't attempt to
communicate. But I will try to explain communicate. But I will try to explain communicate. But I will try to explain
the basic idea at a high level. And I'll the basic idea at a high level. And I'll the basic idea at a high level. And I'll
also try to loosely connect the basic also try to loosely connect the basic also try to loosely connect the basic
idea to what we might expect to happen idea to what we might expect to happen idea to what we might expect to happen
in in in
practice. So let's suppose that this is practice. So let's suppose that this is practice. So let's suppose that this is
our circuit. It's the same circuit that our circuit. It's the same circuit that our circuit. It's the same circuit that
we've been talking about for the entire we've been talking about for the entire we've been talking about for the entire
lesson. But again, we can imagine that lesson. But again, we can imagine that lesson. But again, we can imagine that
our circuit is significantly larger and our circuit is significantly larger and our circuit is significantly larger and
more complicated than this. We'd like to more complicated than this. We'd like to more complicated than this. We'd like to
implement this circuit, but all of our implement this circuit, but all of our implement this circuit, but all of our
components are noisy. And assuming we're components are noisy. And assuming we're components are noisy. And assuming we're
using the independent stochcastic noise using the independent stochcastic noise using the independent stochcastic noise
model just for simplicity, there's some model just for simplicity, there's some model just for simplicity, there's some
probability P with which errors occur probability P with which errors occur probability P with which errors occur
independently at each possible location. independently at each possible location. independently at each possible location.
If the circuit was significantly larger If the circuit was significantly larger If the circuit was significantly larger
than the reciprocal of this probability than the reciprocal of this probability than the reciprocal of this probability
P, then we could be pretty sure that a P, then we could be pretty sure that a P, then we could be pretty sure that a
direct implementation of it wouldn't direct implementation of it wouldn't direct implementation of it wouldn't
work correctly. So we go through all of work correctly. So we go through all of work correctly. So we go through all of
the business of implementing the circuit the business of implementing the circuit the business of implementing the circuit
fall tolerantly as I've discussed up to fall tolerantly as I've discussed up to fall tolerantly as I've discussed up to
this point in the lesson and we ask this point in the lesson and we ask this point in the lesson and we ask
ourselves the question that I raised ourselves the question that I raised ourselves the question that I raised
earlier which is are we making things earlier which is are we making things earlier which is are we making things
better or are we making things worse? better or are we making things worse? better or are we making things worse?
Well if the probability P that Well if the probability P that Well if the probability P that
represents the error probability for represents the error probability for represents the error probability for
each of our components is too large then each of our components is too large then each of our components is too large then
this isn't going to help just like the this isn't going to help just like the this isn't going to help just like the
9Q short code doesn't help if the error 9Q short code doesn't help if the error 9Q short code doesn't help if the error
probability is above 3 and a/4% or so. probability is above 3 and a/4% or so. probability is above 3 and a/4% or so.
In particular, we've now made the In particular, we've now made the In particular, we've now made the
circuit larger. So there are a lot more circuit larger. So there are a lot more circuit larger. So there are a lot more
locations where errors could strike. locations where errors could strike. locations where errors could strike.
However, if P is small enough, this is However, if P is small enough, this is However, if P is small enough, this is
going to help. And what we'll going to help. And what we'll going to help. And what we'll
effectively do is to reduce the error effectively do is to reduce the error effectively do is to reduce the error
probability for the logical computation probability for the logical computation probability for the logical computation
that we're that we're that we're
performing. So let's think about this in performing. So let's think about this in performing. So let's think about this in
just a little bit more detail. And to just a little bit more detail. And to just a little bit more detail. And to
see the main ideas in as simple a see the main ideas in as simple a see the main ideas in as simple a
setting as possible, let's suppose that setting as possible, let's suppose that setting as possible, let's suppose that
we use the 7 cubit steam code for error we use the 7 cubit steam code for error we use the 7 cubit steam code for error
correction, which allows for the correction, which allows for the correction, which allows for the
correction of up to one error per code correction of up to one error per code correction of up to one error per code
block. So in order for a logical error block. So in order for a logical error block. So in order for a logical error
to occur in the original circuit, we'll to occur in the original circuit, we'll to occur in the original circuit, we'll
have to have at least two errors falling have to have at least two errors falling have to have at least two errors falling
into the same code block for our fault into the same code block for our fault into the same code block for our fault
tolerant implementation. tolerant implementation. tolerant implementation.
Keeping in mind that there are a lot of Keeping in mind that there are a lot of Keeping in mind that there are a lot of
different ways to have two or more different ways to have two or more different ways to have two or more
errors in the same code block, it can be errors in the same code block, it can be errors in the same code block, it can be
argued that the probability of a logical argued that the probability of a logical argued that the probability of a logical
error at each location in the original error at each location in the original error at each location in the original
circuit is at most C * P ^ 2 where C is circuit is at most C * P ^ 2 where C is circuit is at most C * P ^ 2 where C is
a number that depends on the code and on a number that depends on the code and on a number that depends on the code and on
the gadgets that we use. But critically, the gadgets that we use. But critically, the gadgets that we use. But critically,
it does not depend on the size of the it does not depend on the size of the it does not depend on the size of the
original circuit. original circuit. original circuit.
So if P is smaller than the reciprocal So if P is smaller than the reciprocal So if P is smaller than the reciprocal
of C which is the number that we can of C which is the number that we can of C which is the number that we can
take as our threshold value then this take as our threshold value then this take as our threshold value then this
translates to a reduction in error. So translates to a reduction in error. So translates to a reduction in error. So
our hard work in implementing the our hard work in implementing the our hard work in implementing the
circuit fall tolerantly is paying circuit fall tolerantly is paying circuit fall tolerantly is paying
off. However this new error rate might off. However this new error rate might off. However this new error rate might
still be too high for the entire circuit still be too high for the entire circuit still be too high for the entire circuit
to work correctly. So we need to do to work correctly. So we need to do to work correctly. So we need to do
more. In practice a natural thing to do more. In practice a natural thing to do more. In practice a natural thing to do
at this point is to choose a better code at this point is to choose a better code at this point is to choose a better code
and better gadgets to drive the error and better gadgets to drive the error and better gadgets to drive the error
rate down to a point where the rate down to a point where the rate down to a point where the
implementation is likely to work. implementation is likely to work. implementation is likely to work.
Theoretically speaking, a simple way to Theoretically speaking, a simple way to Theoretically speaking, a simple way to
argue that this is possible is to argue that this is possible is to argue that this is possible is to
concatenate. In other words, we think concatenate. In other words, we think concatenate. In other words, we think
about the fault tolerant implementation about the fault tolerant implementation about the fault tolerant implementation
of the original circuit as a new logical of the original circuit as a new logical of the original circuit as a new logical
circuit. And then we implement it fault circuit. And then we implement it fault circuit. And then we implement it fault
tolerantly. And then we do this again tolerantly. And then we do this again tolerantly. And then we do this again
and again as many times as we need. And and again as many times as we need. And and again as many times as we need. And
it turns out that this reduces the error it turns out that this reduces the error it turns out that this reduces the error
very quickly. And that's how the very quickly. And that's how the very quickly. And that's how the
threshold theorem is typically proved on threshold theorem is typically proved on threshold theorem is typically proved on
paper. paper. paper.
I'll give you just a very rough idea for I'll give you just a very rough idea for I'll give you just a very rough idea for
how the error rate decreases leaving out how the error rate decreases leaving out how the error rate decreases leaving out
the many technical details required to the many technical details required to the many technical details required to
prove this prove this prove this
formally. We start with the error formally. We start with the error formally. We start with the error
probability P for our original probability P for our original probability P for our original
components. And if we implement the components. And if we implement the components. And if we implement the
circuit fall tolerantly, the error circuit fall tolerantly, the error circuit fall tolerantly, the error
decreases by a factor of C * decreases by a factor of C * decreases by a factor of C *
P. If we then treat our fault tolerant P. If we then treat our fault tolerant P. If we then treat our fault tolerant
implementation as a logical circuit and implementation as a logical circuit and implementation as a logical circuit and
implement it fault tolerantly, we get a implement it fault tolerantly, we get a implement it fault tolerantly, we get a
similar decrease in the error rate. But similar decrease in the error rate. But similar decrease in the error rate. But
if we think about the effective error if we think about the effective error if we think about the effective error
rate for the original circuit, we find rate for the original circuit, we find rate for the original circuit, we find
that it drops even further because now that it drops even further because now that it drops even further because now
we're replacing the error rate P by C * we're replacing the error rate P by C * we're replacing the error rate P by C *
P ^ 2 because that's the effective error P ^ 2 because that's the effective error P ^ 2 because that's the effective error
rate after the first level fault rate after the first level fault rate after the first level fault
tolerant implementation. Specifically, tolerant implementation. Specifically, tolerant implementation. Specifically,
after just a little bit of algebra, we after just a little bit of algebra, we after just a little bit of algebra, we
find that the error goes down to CP find that the error goes down to CP find that the error goes down to CP
cubed * P. And if we do it again, we now cubed * P. And if we do it again, we now cubed * P. And if we do it again, we now
have an error rate of CP ^ 7 * P. And if have an error rate of CP ^ 7 * P. And if have an error rate of CP ^ 7 * P. And if
we continue to do this for a total of K we continue to do this for a total of K we continue to do this for a total of K
levels, what we find is that the error levels, what we find is that the error levels, what we find is that the error
shrinks doubly exponentially in K. So we shrinks doubly exponentially in K. So we shrinks doubly exponentially in K. So we
don't need to do this very many times to don't need to do this very many times to don't need to do this very many times to
make the error rate extremely small. make the error rate extremely small. make the error rate extremely small.
Meanwhile, the circuits are growing in Meanwhile, the circuits are growing in Meanwhile, the circuits are growing in
size, but the size only increases singly size, but the size only increases singly size, but the size only increases singly
exponentially in K because with each exponentially in K because with each exponentially in K because with each
level of fault tolerance, the size of level of fault tolerance, the size of level of fault tolerance, the size of
our circuit grows by at most a factor our circuit grows by at most a factor our circuit grows by at most a factor
that's determined by the maximum size of that's determined by the maximum size of that's determined by the maximum size of
our gadgets. And when it's all put together gadgets. And when it's all put together
and an appropriate choice for the number and an appropriate choice for the number and an appropriate choice for the number
of levels of concatenation is made, we of levels of concatenation is made, we of levels of concatenation is made, we
obtain the threshold theorem. But like I obtain the threshold theorem. But like I obtain the threshold theorem. But like I
already suggested, there are a lot of already suggested, there are a lot of already suggested, there are a lot of
details required to make this formal. So details required to make this formal. So details required to make this formal. So
what is this threshold value in what is this threshold value in what is this threshold value in
reality? Well, it depends on the code reality? Well, it depends on the code reality? Well, it depends on the code
and on the gadgets we and on the gadgets we and on the gadgets we
use for the steam code along with magic use for the steam code along with magic use for the steam code along with magic
state distillation. It's minuscule and state distillation. It's minuscule and state distillation. It's minuscule and
probably unlikely to be achievable in probably unlikely to be achievable in probably unlikely to be achievable in
practice. But using surface codes and practice. But using surface codes and practice. But using surface codes and
state-of-the-art gadgets, it's been state-of-the-art gadgets, it's been state-of-the-art gadgets, it's been
estimated to be around 1%. Which isn't estimated to be around 1%. Which isn't estimated to be around 1%. Which isn't
bad. And as new codes and methods are bad. And as new codes and methods are bad. And as new codes and methods are
discovered, we can reasonably expect it discovered, we can reasonably expect it discovered, we can reasonably expect it
to increase while simultaneously the to increase while simultaneously the to increase while simultaneously the
level of noise in the components we level of noise in the components we level of noise in the components we
build will build will build will
decrease. Reaching the point at which decrease. Reaching the point at which decrease. Reaching the point at which
large scale quantum computations can be large scale quantum computations can be large scale quantum computations can be
implemented fault tolerantly will not be implemented fault tolerantly will not be implemented fault tolerantly will not be
easy and it will not happen overnight. easy and it will not happen overnight. easy and it will not happen overnight.
But this theorem together with the But this theorem together with the But this theorem together with the
advances in quantum codes and in quantum advances in quantum codes and in quantum advances in quantum codes and in quantum
hardware that we've witnessed to date hardware that we've witnessed to date hardware that we've witnessed to date
provide us with the optimism that we provide us with the optimism that we provide us with the optimism that we
need to continue to push forward to need to continue to push forward to need to continue to push forward to
reach this ultimate goal of building a reach this ultimate goal of building a reach this ultimate goal of building a
large-scale fall tolerant quantum large-scale fall tolerant quantum large-scale fall tolerant quantum
computer. And that is the end of the computer. And that is the end of the computer. And that is the end of the
lesson and the end of the series. I lesson and the end of the series. I lesson and the end of the series. I
sincerely hope you found value in the sincerely hope you found value in the sincerely hope you found value in the
series and if you have, please consider series and if you have, please consider series and if you have, please consider
recommending it to a colleague or recommending it to a colleague or recommending it to a colleague or
friend. friend. friend.
Also, be sure to check out IBM Quantum Also, be sure to check out IBM Quantum Also, be sure to check out IBM Quantum
Learning for additional courses and Learning for additional courses and Learning for additional courses and
tutorials, and subscribe to the Kiskuit tutorials, and subscribe to the Kiskuit tutorials, and subscribe to the Kiskuit
YouTube channel if you haven't already, YouTube channel if you haven't already, YouTube channel if you haven't already,
so you can stay uptodate with our future so you can stay uptodate with our future so you can stay uptodate with our future
content releases. Thank you for content releases. Thank you for content releases. Thank you for
watching.

## General measurements ｜ Understanding Quantum Information & Computation ｜ Lesson 11

welcome back to understanding Quantum welcome back to understanding Quantum
information and computation my name is information and computation my name is information and computation my name is
John wattrus and I'm the technical John wattrus and I'm the technical John wattrus and I'm the technical
director for education at IBM director for education at IBM director for education at IBM
Quantum this is the 11th lesson of the Quantum this is the 11th lesson of the Quantum this is the 11th lesson of the
series and it's the third lesson in the series and it's the third lesson in the series and it's the third lesson in the
third unit which is on the general third unit which is on the general third unit which is on the general
formulation of quantum formulation of quantum formulation of quantum
information this lesson is about information this lesson is about information this lesson is about
measurements which represent an measurements which represent an measurements which represent an
interface between Quantum and classical interface between Quantum and classical interface between Quantum and classical
information when a measurement is information when a measurement is information when a measurement is
performed on a system while it's in a performed on a system while it's in a performed on a system while it's in a
Quantum State classical information is Quantum State classical information is Quantum State classical information is
extracted revealing something about that extracted revealing something about that extracted revealing something about that
Quantum State and generally changing or Quantum State and generally changing or Quantum State and generally changing or
destroying that state in the destroying that state in the destroying that state in the
process in a simplified formulation of process in a simplified formulation of process in a simplified formulation of
quantum information which was discussed quantum information which was discussed quantum information which was discussed
in the first unit of the series we in the first unit of the series we in the first unit of the series we
typically restrict our attention to typically restrict our attention to typically restrict our attention to
projective measurements including the projective measurements including the projective measurements including the
simplest type of measurement standard simplest type of measurement standard simplest type of measurement standard
basis basis basis
measurements it is however possible to measurements it is however possible to measurements it is however possible to
generalize the notion of a measurement generalize the notion of a measurement generalize the notion of a measurement
and that turns out to be a pretty useful and that turns out to be a pretty useful and that turns out to be a pretty useful
thing to thing to thing to
do in this lesson we'll talk about do in this lesson we'll talk about do in this lesson we'll talk about
measurements in full generality measurements in full generality measurements in full generality
including how they're described in including how they're described in including how they're described in
mathematical terms and how they fit into mathematical terms and how they fit into mathematical terms and how they fit into
the general formulation of quantum the general formulation of quantum the general formulation of quantum
information toward the end of the lesson information toward the end of the lesson information toward the end of the lesson
we'll also take a look at a couple of we'll also take a look at a couple of we'll also take a look at a couple of
fundamental Notions connected with fundamental Notions connected with fundamental Notions connected with
measurements namely Quantum State measurements namely Quantum State measurements namely Quantum State
discrimination and Quantum State discrimination and Quantum State discrimination and Quantum State
tomography so let's get tomography so let's get tomography so let's get
started here is an overview of the started here is an overview of the started here is an overview of the
lesson as we often do we'll start with lesson as we often do we'll start with lesson as we often do we'll start with
the basics including a couple of the basics including a couple of the basics including a couple of
different but equivalent ways that we different but equivalent ways that we different but equivalent ways that we
can describe measurements in math can describe measurements in math can describe measurements in math
mathematical mathematical mathematical
terms in particular we can describe terms in particular we can describe terms in particular we can describe
measurements by collections of matrices measurements by collections of matrices measurements by collections of matrices
and we can also describe them as and we can also describe them as and we can also describe them as
channels which I alluded to in the channels which I alluded to in the channels which I alluded to in the
previous previous previous
lesson we'll also discuss partial lesson we'll also discuss partial lesson we'll also discuss partial
measurements or in other words measurements or in other words measurements or in other words
measurements that are performed on just measurements that are performed on just measurements that are performed on just
one part of a larger compound one part of a larger compound one part of a larger compound
system then we'll move on to neymark system then we'll move on to neymark system then we'll move on to neymark
theorem which is a fundamental fact theorem which is a fundamental fact theorem which is a fundamental fact
concerning measurements that basically concerning measurements that basically concerning measurements that basically
says that General measurements can says that General measurements can says that General measurements can
always be implemented in a simple way always be implemented in a simple way always be implemented in a simple way
that's reminiscent of Stein spring that's reminiscent of Stein spring that's reminiscent of Stein spring
representations of channels in short representations of channels in short representations of channels in short
General measurements can always be General measurements can always be General measurements can always be
implemented by first introducing an implemented by first introducing an implemented by first introducing an
initialized workspace system then initialized workspace system then initialized workspace system then
performing a unitary operation and performing a unitary operation and performing a unitary operation and
finally performing a standard basis finally performing a standard basis finally performing a standard basis
measurement on the workpace measurement on the workpace measurement on the workpace
system this turns out to be pretty easy system this turns out to be pretty easy system this turns out to be pretty easy
to prove and we'll see how that goes and to prove and we'll see how that goes and to prove and we'll see how that goes and
we'll also see how this connects to we'll also see how this connects to we'll also see how this connects to
so-called non-destructive measurements so-called non-destructive measurements so-called non-destructive measurements
which I'll which I'll which I'll
describe finally in the last part of the describe finally in the last part of the describe finally in the last part of the
lesson lesson lesson
we'll discuss Quantum State we'll discuss Quantum State we'll discuss Quantum State
discrimination and Quantum State discrimination and Quantum State discrimination and Quantum State
tomography we'll begin with mathematical tomography we'll begin with mathematical tomography we'll begin with mathematical
descriptions of descriptions of descriptions of
measurements measurements provide us measurements measurements provide us measurements measurements provide us
with an interface between Quantum and with an interface between Quantum and with an interface between Quantum and
classical classical classical
information when a measurement is information when a measurement is information when a measurement is
performed on a system while it's in some performed on a system while it's in some performed on a system while it's in some
Quantum State classical information Quantum State classical information Quantum State classical information
about that Quantum state is about that Quantum state is about that Quantum state is
extracted in addition the quantum state extracted in addition the quantum state extracted in addition the quantum state
is generally changed or possibly is generally changed or possibly is generally changed or possibly
destroyed completely destroyed completely destroyed completely
initially we'll focus our attention on initially we'll focus our attention on initially we'll focus our attention on
so-called destructive measurements where so-called destructive measurements where so-called destructive measurements where
a Quantum State goes in and just a Quantum State goes in and just a Quantum State goes in and just
classical information comes out so classical information comes out so classical information comes out so
there's no specification of the there's no specification of the there's no specification of the
postmeasurement quantum state of the postmeasurement quantum state of the postmeasurement quantum state of the
system that was system that was system that was
measured intuitively speaking we can measured intuitively speaking we can measured intuitively speaking we can
imagine that a destructive measurement imagine that a destructive measurement imagine that a destructive measurement
destroys whatever system was measured so destroys whatever system was measured so destroys whatever system was measured so
its state after the measurement takes its state after the measurement takes its state after the measurement takes
place isn't place isn't place isn't
specified this doesn't literally mean specified this doesn't literally mean specified this doesn't literally mean
that physical devices are necessarily that physical devices are necessarily that physical devices are necessarily
destroyed it's just a mathematical destroyed it's just a mathematical destroyed it's just a mathematical
description and it's a very useful one description and it's a very useful one description and it's a very useful one
because it helps to keep things simple because it helps to keep things simple because it helps to keep things simple
and to focus in on the essential and to focus in on the essential and to focus in on the essential
mathematical aspects of mathematical aspects of mathematical aspects of
measurements there are in particular two measurements there are in particular two measurements there are in particular two
equivalent ways to describe destructive equivalent ways to describe destructive equivalent ways to describe destructive
measurements which I mentioned during measurements which I mentioned during measurements which I mentioned during
the overview of the the overview of the the overview of the
lesson one way to describe a destructive lesson one way to describe a destructive lesson one way to describe a destructive
measurement is by a collection of measurement is by a collection of measurement is by a collection of
matrices where in particular we have one matrices where in particular we have one matrices where in particular we have one
Matrix for each possible measurement Matrix for each possible measurement Matrix for each possible measurement
outcome outcome outcome
a second way to describe a destructive a second way to describe a destructive a second way to describe a destructive
measurement is by a channel where the measurement is by a channel where the measurement is by a channel where the
input is the state of whatever system is input is the state of whatever system is input is the state of whatever system is
being measured and the output is always being measured and the output is always being measured and the output is always
a diagonal density Matrix which we can a diagonal density Matrix which we can a diagonal density Matrix which we can
interpret as a description of a interpret as a description of a interpret as a description of a
probability distribution over the probability distribution over the probability distribution over the
possible classical measurement possible classical measurement possible classical measurement
outcomes the two descriptions are outcomes the two descriptions are outcomes the two descriptions are
equivalent and that's pretty useful equivalent and that's pretty useful equivalent and that's pretty useful
because it allows us to pick whichever because it allows us to pick whichever because it allows us to pick whichever
description is more convenient in a description is more convenient in a description is more convenient in a
given given given
situation we'll also talk about situation we'll also talk about situation we'll also talk about
non-destructive measurements where in non-destructive measurements where in non-destructive measurements where in
addition to a classical measurement addition to a classical measurement addition to a classical measurement
outcome postmeasurement Quantum state of outcome postmeasurement Quantum state of outcome postmeasurement Quantum state of
the system that was measured is also specified non-destructive measurements specified non-destructive measurements
are important and they arise frequently are important and they arise frequently are important and they arise frequently
but there is a sense in which they're but there is a sense in which they're but there is a sense in which they're
somehow not as fundamental as somehow not as fundamental as somehow not as fundamental as
destructive measurements in mathematical destructive measurements in mathematical destructive measurements in mathematical
terms in particular as it turns out terms in particular as it turns out terms in particular as it turns out
non-destructive measurements can always non-destructive measurements can always non-destructive measurements can always
be described as compositions of channels be described as compositions of channels be described as compositions of channels
and destructive measurements and with and destructive measurements and with and destructive measurements and with
we'll see how that works a bit later in we'll see how that works a bit later in we'll see how that works a bit later in
the the the
lesson now let's take a look at how lesson now let's take a look at how lesson now let's take a look at how
measurements meaning destructive measurements meaning destructive measurements meaning destructive
measurements can be described as measurements can be described as measurements can be described as
collections of collections of collections of
matrices throughout this discussion matrices throughout this discussion matrices throughout this discussion
we'll suppose that X is a system that's we'll suppose that X is a system that's we'll suppose that X is a system that's
being measured and just for the sake of being measured and just for the sake of being measured and just for the sake of
Simplicity we'll make a couple of Simplicity we'll make a couple of Simplicity we'll make a couple of
assumptions first we'll assume that the assumptions first we'll assume that the assumptions first we'll assume that the
classical State set of X consists of the classical State set of X consists of the classical State set of X consists of the
integers from 0 to n minus one for some integers from 0 to n minus one for some integers from 0 to n minus one for some
positive integer n and the possible positive integer n and the possible positive integer n and the possible
outcomes of the measurement are the outcomes of the measurement are the outcomes of the measurement are the
integers from 0 to M minus one again for integers from 0 to M minus one again for integers from 0 to M minus one again for
some positive integer M there doesn't some positive integer M there doesn't some positive integer M there doesn't
need to be any relationship between the need to be any relationship between the need to be any relationship between the
numbers n and M they're both arbitrary numbers n and M they're both arbitrary numbers n and M they're both arbitrary
positive positive positive
integers and of course we could replace integers and of course we could replace integers and of course we could replace
these sets by arbitrary classical State these sets by arbitrary classical State these sets by arbitrary classical State
sets if we wanted to do sets if we wanted to do sets if we wanted to do
that first let's recall how projective that first let's recall how projective that first let's recall how projective
measurements work which we first talked measurements work which we first talked measurements work which we first talked
about back in the third lesson of the about back in the third lesson of the about back in the third lesson of the
series given that we're assuming that series given that we're assuming that series given that we're assuming that
the measurement outcome are the integers the measurement outcome are the integers the measurement outcome are the integers
from 0 to n minus one we describe a from 0 to n minus one we describe a from 0 to n minus one we describe a
projective measurement by a collection projective measurement by a collection projective measurement by a collection
of projections Pi 0 through Pi M minus of projections Pi 0 through Pi M minus of projections Pi 0 through Pi M minus
one that's sum to the identity Matrix one that's sum to the identity Matrix one that's sum to the identity Matrix
and these matrices all have rows and and these matrices all have rows and and these matrices all have rows and
columns corresponding to the classical columns corresponding to the classical columns corresponding to the classical
states of X or in other words these are states of X or in other words these are states of X or in other words these are
all n byn all n byn all n byn
matrices if x is in a Quantum State matrices if x is in a Quantum State matrices if x is in a Quantum State
that's described by a Quantum State that's described by a Quantum State that's described by a Quantum State
Vector s then each possible measurement Vector s then each possible measurement Vector s then each possible measurement
outcome a appears with probability equal outcome a appears with probability equal outcome a appears with probability equal
to the ukian norm squ of Pi a multiplied to the ukian norm squ of Pi a multiplied to the ukian norm squ of Pi a multiplied
to the vector to the vector to the vector
s the ukian norm squar of any Vector is s the ukian norm squar of any Vector is s the ukian norm squar of any Vector is
equal to the inner product of that equal to the inner product of that equal to the inner product of that
Vector with itself so we get the Vector with itself so we get the Vector with itself so we get the
expression that's shown on the expression that's shown on the expression that's shown on the
screen and we can simplify that just a screen and we can simplify that just a screen and we can simplify that just a
little bit by using the fact that Pi a little bit by using the fact that Pi a little bit by using the fact that Pi a
is a projection so multiplying it to is a projection so multiplying it to is a projection so multiplying it to
itself doesn't do itself doesn't do itself doesn't do
anything and finally we can rewrite the anything and finally we can rewrite the anything and finally we can rewrite the
expression we've obtained in a slightly expression we've obtained in a slightly expression we've obtained in a slightly
different way using the cyclic property different way using the cyclic property different way using the cyclic property
of the trace again as is shown on the of the trace again as is shown on the of the trace again as is shown on the
screen and if it's not clear why we want screen and if it's not clear why we want screen and if it's not clear why we want
to write the expression like this the to write the expression like this the to write the expression like this the
short answer is that we'd like to short answer is that we'd like to short answer is that we'd like to
understand how projective measurements understand how projective measurements understand how projective measurements
work for density matrices and in particular by linearity matrices and in particular by linearity
we have that if the state of X is we have that if the state of X is we have that if the state of X is
described by a density Matrix row rather described by a density Matrix row rather described by a density Matrix row rather
than a Quantum State Vector s then the than a Quantum State Vector s then the than a Quantum State Vector s then the
probability for the outcome a to appear probability for the outcome a to appear probability for the outcome a to appear
is given by the trace of the projection is given by the trace of the projection is given by the trace of the projection
Pi a times the density Matrix row that Pi a times the density Matrix row that Pi a times the density Matrix row that
has to be the case because as we've seen has to be the case because as we've seen has to be the case because as we've seen
we can always think about a density we can always think about a density we can always think about a density
Matrix as being a weighted average of Matrix as being a weighted average of Matrix as being a weighted average of
pure pure pure
States and if we average the States and if we average the States and if we average the
corresponding probabilities we get this corresponding probabilities we get this corresponding probabilities we get this
expression because the trace is a linear expression because the trace is a linear expression because the trace is a linear
function so that's the expression we're function so that's the expression we're function so that's the expression we're
going going going
for and we can clean things up just a for and we can clean things up just a for and we can clean things up just a
little bit to focus in on density little bit to focus in on density little bit to focus in on density
matrices the main point of doing this by matrices the main point of doing this by matrices the main point of doing this by
the way is to establish a point of the way is to establish a point of the way is to establish a point of
reference to what we've already learned reference to what we've already learned reference to what we've already learned
and to see that this description of and to see that this description of and to see that this description of
projective measurements is in fact a projective measurements is in fact a projective measurements is in fact a
special case of a more General special case of a more General special case of a more General
description of measurements that we're description of measurements that we're description of measurements that we're
about to about to about to
see so what I'll do now is to replace see so what I'll do now is to replace see so what I'll do now is to replace
this description of how projective this description of how projective this description of how projective
measurements work for density matrices measurements work for density matrices measurements work for density matrices
with the description for General with the description for General with the description for General
measurements so we can understand the relationship and here it relationship and here it
is a general measurement not necessarily is a general measurement not necessarily is a general measurement not necessarily
A projective measurement is described by A projective measurement is described by A projective measurement is described by
a collection of positive semi-definite a collection of positive semi-definite a collection of positive semi-definite
matrices p 0 through PM minus one that's matrices p 0 through PM minus one that's matrices p 0 through PM minus one that's
sum to the identity and if this sum to the identity and if this sum to the identity and if this
measurement is performed on X while it's measurement is performed on X while it's measurement is performed on X while it's
in a state described by a density Matrix in a state described by a density Matrix in a state described by a density Matrix
row then each outcome a appears with row then each outcome a appears with row then each outcome a appears with
probability equal to the trace of PA * probability equal to the trace of PA * probability equal to the trace of PA *
Row in short the only thing that's Row in short the only thing that's Row in short the only thing that's
changed is that we've relaxed the changed is that we've relaxed the changed is that we've relaxed the
condition on these condition on these condition on these
matrices in particular the matrices matrices in particular the matrices matrices in particular the matrices
don't need to be projection matrices don't need to be projection matrices don't need to be projection matrices
they just need to be positive they just need to be positive they just need to be positive
semi-definite and correspondingly I've semi-definite and correspondingly I've semi-definite and correspondingly I've
replaced the letter Pi with the letter p replaced the letter Pi with the letter p replaced the letter Pi with the letter p
because Pi is a conventional name for a because Pi is a conventional name for a because Pi is a conventional name for a
projection whereas I like to use the projection whereas I like to use the projection whereas I like to use the
letter P for positive semi-definite letter P for positive semi-definite letter P for positive semi-definite
matrices this is a generalization of the matrices this is a generalization of the matrices this is a generalization of the
description we had for projective description we had for projective description we had for projective
measurements by the way because measurements by the way because measurements by the way because
projection matrices are always positive projection matrices are always positive projection matrices are always positive
semi-definite and in fact they can semi-definite and in fact they can semi-definite and in fact they can
alternatively be defined as positive alternatively be defined as positive alternatively be defined as positive
semi-definite matrices whose igen values semi-definite matrices whose igen values semi-definite matrices whose igen values
can only be zero and one we can observe that this definition one we can observe that this definition
makes sense in that we always obtain a makes sense in that we always obtain a makes sense in that we always obtain a
probability Vector for the outcome probability Vector for the outcome probability Vector for the outcome
probabilities in the following probabilities in the following probabilities in the following
way first these numbers that I referred way first these numbers that I referred way first these numbers that I referred
to as outcome probabilities are always to as outcome probabilities are always to as outcome probabilities are always
non- negative real numbers that's non- negative real numbers that's non- negative real numbers that's
because both PA and row are positive because both PA and row are positive because both PA and row are positive
semi-definite and the trace of the semi-definite and the trace of the semi-definite and the trace of the
product of two positive semi-definite product of two positive semi-definite product of two positive semi-definite
matrices is always a non- negative real matrices is always a non- negative real matrices is always a non- negative real
number and I'll leave that for you to number and I'll leave that for you to number and I'll leave that for you to
think about why that think about why that think about why that
is and second the numbers have to sum to is and second the numbers have to sum to is and second the numbers have to sum to
one because the trace is linear the one because the trace is linear the one because the trace is linear the
matrices p 0 through PN minus1 are matrices p 0 through PN minus1 are matrices p 0 through PN minus1 are
required to sum to the identity and required to sum to the identity and required to sum to the identity and
density matricies have Trace equal to density matricies have Trace equal to density matricies have Trace equal to
one so regardless of what density Matrix one so regardless of what density Matrix one so regardless of what density Matrix
row we started with the outcome row we started with the outcome row we started with the outcome
probabilities will indeed always form a probabilities will indeed always form a probabilities will indeed always form a
probability probability probability
Vector now let's take a look at a few Vector now let's take a look at a few Vector now let's take a look at a few
examples I already mentioned that examples I already mentioned that examples I already mentioned that
projective measurements are a special projective measurements are a special projective measurements are a special
case of General measurements because case of General measurements because case of General measurements because
projections are always positive projections are always positive projections are always positive
semi-definite semi-definite semi-definite
but it's nice to start simple so let's but it's nice to start simple so let's but it's nice to start simple so let's
begin with a standard basis measurement begin with a standard basis measurement begin with a standard basis measurement
of a of a of a
cubit we can represent a standard basis cubit we can represent a standard basis cubit we can represent a standard basis
measurement of a cubit by two matrices p measurement of a cubit by two matrices p measurement of a cubit by two matrices p
0 and P1 defined as is shown on the 0 and P1 defined as is shown on the 0 and P1 defined as is shown on the
screen p 0 is z bra zero and P1 is k1 screen p 0 is z bra zero and P1 is k1 screen p 0 is z bra zero and P1 is k1
bra bra bra
one these are both positive one these are both positive one these are both positive
semi-definite matrices and indeed their semi-definite matrices and indeed their semi-definite matrices and indeed their
projections and they clearly sum to the projections and they clearly sum to the projections and they clearly sum to the
identity so together they describe a identity so together they describe a identity so together they describe a
measurement measurement measurement
if we apply this measurement to a cubit if we apply this measurement to a cubit if we apply this measurement to a cubit
in a state described by a density Matrix in a state described by a density Matrix in a state described by a density Matrix
row then the probability for the alcome row then the probability for the alcome row then the probability for the alcome
zero to appear is the trace of p 0 * row zero to appear is the trace of p 0 * row zero to appear is the trace of p 0 * row
and the probability for the outcome one and the probability for the outcome one and the probability for the outcome one
is the trace of P1 * row and by the is the trace of P1 * row and by the is the trace of P1 * row and by the
cyclic property of the trace we obtain cyclic property of the trace we obtain cyclic property of the trace we obtain
the two diagonal entries of row as we expect here's another example where this expect here's another example where this
time the matrices aren't projections time the matrices aren't projections time the matrices aren't projections
they are positive semidefinite though they are positive semidefinite though they are positive semidefinite though
and they do happen to be density and they do happen to be density and they do happen to be density
matrices but that's not important but matrices but that's not important but matrices but that's not important but
what is important is that they sum to what is important is that they sum to what is important is that they sum to
the identity so together they describe a the identity so together they describe a the identity so together they describe a
measurement we can see what happens when measurement we can see what happens when measurement we can see what happens when
this measurement is performed on a cubit this measurement is performed on a cubit this measurement is performed on a cubit
in the plus state for instance just to in the plus state for instance just to in the plus state for instance just to
see how it works for one example of a see how it works for one example of a see how it works for one example of a
state and a calculation of the state and a calculation of the state and a calculation of the
probabilities is shown on the probabilities is shown on the probabilities is shown on the
screen notice in particular that screen notice in particular that screen notice in particular that
although the plus state is a pure State although the plus state is a pure State although the plus state is a pure State
and we can represent that state by the and we can represent that state by the and we can represent that state by the
quantum State Vector cat Plus we do have quantum State Vector cat Plus we do have quantum State Vector cat Plus we do have
to plug the density Matrix to plug the density Matrix to plug the density Matrix
representation of that state which is C representation of that state which is C representation of that state which is C
plus BR plus into the formulas to get plus BR plus into the formulas to get plus BR plus into the formulas to get
the the the
probabilities and let me also remark probabilities and let me also remark probabilities and let me also remark
that because we're talking about a that because we're talking about a that because we're talking about a
destructive measurement there isn't any destructive measurement there isn't any destructive measurement there isn't any
specification of the postmeasurement specification of the postmeasurement specification of the postmeasurement
quantum state of this Cubit so all we quantum state of this Cubit so all we quantum state of this Cubit so all we
have is a specification of the outcome have is a specification of the outcome have is a specification of the outcome
probabilities here's a final example for probabilities here's a final example for probabilities here's a final example for
now and it's a pretty interesting now and it's a pretty interesting now and it's a pretty interesting
one once again it's a measurement of a one once again it's a measurement of a one once again it's a measurement of a
single Cubit and to describe it we'll single Cubit and to describe it we'll single Cubit and to describe it we'll
start by defining four quantum State start by defining four quantum State start by defining four quantum State
vectors known as the tetrahedral vectors known as the tetrahedral vectors known as the tetrahedral
States and the reason that they're States and the reason that they're States and the reason that they're
called the tetrahedral States is because called the tetrahedral States is because called the tetrahedral States is because
they correspond to the four vertices of they correspond to the four vertices of they correspond to the four vertices of
a regular tetrahedron inscribed within a regular tetrahedron inscribed within a regular tetrahedron inscribed within
the block sphere and to be clear when we the block sphere and to be clear when we the block sphere and to be clear when we
say that this is a regular tetrahedron say that this is a regular tetrahedron say that this is a regular tetrahedron
we mean a four-sided solid where each of we mean a four-sided solid where each of we mean a four-sided solid where each of
the faces is an equilateral triangle so the faces is an equilateral triangle so the faces is an equilateral triangle so
there's a perfect symmetry here every there's a perfect symmetry here every there's a perfect symmetry here every
Edge has the same length Edge has the same length Edge has the same length
and it's the same angle that's formed and it's the same angle that's formed and it's the same angle that's formed
between any two adjacent between any two adjacent between any two adjacent
edges and now we can define an edges and now we can define an edges and now we can define an
interesting measurement with four interesting measurement with four interesting measurement with four
possible outcomes as is shown on the possible outcomes as is shown on the possible outcomes as is shown on the
screen or in words we can take our screen or in words we can take our screen or in words we can take our
matrices to be the density Matrix matrices to be the density Matrix matrices to be the density Matrix
representations of these states divided representations of these states divided representations of these states divided
by by by
two now these are definitely positive two now these are definitely positive two now these are definitely positive
semi-definite matrices because their semi-definite matrices because their semi-definite matrices because their
density matrices divided by two and you density matrices divided by two and you density matrices divided by two and you
could go through the arithmetic to check could go through the arithmetic to check could go through the arithmetic to check
that they do in fact sum to the identity that they do in fact sum to the identity that they do in fact sum to the identity
but another way to see that is to think but another way to see that is to think but another way to see that is to think
about the about the about the
Symmetry if we average these four states Symmetry if we average these four states Symmetry if we average these four states
together we get the completely mixed together we get the completely mixed together we get the completely mixed
state which is at the center of the state which is at the center of the state which is at the center of the
block sphere but because we're dividing block sphere but because we're dividing block sphere but because we're dividing
each one by two rather than four we get each one by two rather than four we get each one by two rather than four we get
twice the completely mixed state which twice the completely mixed state which twice the completely mixed state which
is the is the is the
identity I won't say anything more about identity I won't say anything more about identity I won't say anything more about
this measurement for now except to this measurement for now except to this measurement for now except to
observe that it isn't a projective observe that it isn't a projective observe that it isn't a projective
measurement but it is a nice example and measurement but it is a nice example and measurement but it is a nice example and
we'll see it come up again later in the we'll see it come up again later in the we'll see it come up again later in the
lesson lesson lesson
another way to describe general another way to describe general another way to describe general
measurements is as channels as we'll now measurements is as channels as we'll now measurements is as channels as we'll now
discuss the idea here is that classical discuss the idea here is that classical discuss the idea here is that classical
probabilistic States can be represented probabilistic States can be represented probabilistic States can be represented
by diagonal density matrices and so by diagonal density matrices and so by diagonal density matrices and so
because measurement outcomes are because measurement outcomes are because measurement outcomes are
classical we can think about classical we can think about classical we can think about
measurements as being channels where the measurements as being channels where the measurements as being channels where the
output is always a diagonal density output is always a diagonal density output is always a diagonal density
Matrix that describes the measurement Matrix that describes the measurement Matrix that describes the measurement
outcome so let's take a look at this in outcome so let's take a look at this in outcome so let's take a look at this in
a little bit more detail if we think a little bit more detail if we think a little bit more detail if we think
abstractly about about a general abstractly about about a general abstractly about about a general
measurement we can describe that measurement we can describe that measurement we can describe that
measurement as a channel measurement as a channel measurement as a channel
fi the input system for the channel is fi the input system for the channel is fi the input system for the channel is
whatever system X is being measured and whatever system X is being measured and whatever system X is being measured and
the output system is a system y whose the output system is a system y whose the output system is a system y whose
classical States correspond to the classical States correspond to the classical States correspond to the
measurement outcomes which we're taking measurement outcomes which we're taking measurement outcomes which we're taking
to be the integers from 0 to n minus one to be the integers from 0 to n minus one to be the integers from 0 to n minus one
for some positive integer for some positive integer for some positive integer
M and because measurement outcomes are M and because measurement outcomes are M and because measurement outcomes are
classical we require that for every classical we require that for every classical we require that for every
input density Matrix Row for the system input density Matrix Row for the system input density Matrix Row for the system
X the output five row must be a diagonal X the output five row must be a diagonal X the output five row must be a diagonal
density density density
Matrix for example if we think about a Matrix for example if we think about a Matrix for example if we think about a
standard basis measurement of a cubit standard basis measurement of a cubit standard basis measurement of a cubit
and we think about that measurement as and we think about that measurement as and we think about that measurement as
being described by a channel we can being described by a channel we can being described by a channel we can
express it as we have here on the screen express it as we have here on the screen express it as we have here on the screen
if the input density Matrix is row then if the input density Matrix is row then if the input density Matrix is row then
we'll get the outcome zero with we'll get the outcome zero with we'll get the outcome zero with
probability equal to bra 0o row cat Zer probability equal to bra 0o row cat Zer probability equal to bra 0o row cat Zer
and we'll get the outcome one with and we'll get the outcome one with and we'll get the outcome one with
probability bra one row Kat one and so probability bra one row Kat one and so probability bra one row Kat one and so
if we express these classical if we express these classical if we express these classical
measurement outcomes as density matrices measurement outcomes as density matrices measurement outcomes as density matrices
we'll get this expression for the we'll get this expression for the we'll get this expression for the
corresponding Channel and it turns out corresponding Channel and it turns out corresponding Channel and it turns out
that it's none other than the completely that it's none other than the completely that it's none other than the completely
def phasing def phasing def phasing
Channel and that's consistent with the Channel and that's consistent with the Channel and that's consistent with the
idea that the completely def phasing idea that the completely def phasing idea that the completely def phasing
Channel represents an extreme form of Channel represents an extreme form of Channel represents an extreme form of
decoherence which can mean different decoherence which can mean different decoherence which can mean different
things but when I use that term I'm things but when I use that term I'm things but when I use that term I'm
talking about the tendency for Quantum talking about the tendency for Quantum talking about the tendency for Quantum
States to become classical because the States to become classical because the States to become classical because the
environment around whatever system we're environment around whatever system we're environment around whatever system we're
talking about is watching in some sense talking about is watching in some sense talking about is watching in some sense
or in other words it's measuring so measurement outcomes can be described so measurement outcomes can be described
as channels as I've just described but as channels as I've just described but as channels as I've just described but
this is not just a possibility it's a this is not just a possibility it's a this is not just a possibility it's a
characterization of General measurements characterization of General measurements characterization of General measurements
and what I mean by that is the and what I mean by that is the and what I mean by that is the
following of course there are channels following of course there are channels following of course there are channels
that don't represent measurements that don't represent measurements that don't represent measurements
because the output might not always be a because the output might not always be a because the output might not always be a
diagonal density Matrix but if it is the diagonal density Matrix but if it is the diagonal density Matrix but if it is the
case that the output of a channel is case that the output of a channel is case that the output of a channel is
always a diagonal density Matrix always a diagonal density Matrix always a diagonal density Matrix
regardless of what input State row we regardless of what input State row we regardless of what input State row we
choose then it is necessar possible to choose then it is necessar possible to choose then it is necessar possible to
express that channel in the form that's express that channel in the form that's express that channel in the form that's
shown on the screen for some choice of a shown on the screen for some choice of a shown on the screen for some choice of a
measurement meaning a collection of measurement meaning a collection of measurement meaning a collection of
positive semidefinite matrices that sum positive semidefinite matrices that sum positive semidefinite matrices that sum
to the to the to the
identity and that's nice for multiple identity and that's nice for multiple identity and that's nice for multiple
reasons including the fact that it reasons including the fact that it reasons including the fact that it
basically tells us that our definition basically tells us that our definition basically tells us that our definition
of measurements as collections of of measurements as collections of of measurements as collections of
matrices isn't missing matrices isn't missing matrices isn't missing
anything another way of saying that is anything another way of saying that is anything another way of saying that is
that this way of thinking about that this way of thinking about that this way of thinking about
measurements as channels is operational measurements as channels is operational measurements as channels is operational
or axiomatic perhaps in the sense that or axiomatic perhaps in the sense that or axiomatic perhaps in the sense that
it captures what a measurement should be it captures what a measurement should be it captures what a measurement should be
which is a physical process that which is a physical process that which is a physical process that
transforms Quantum information into transforms Quantum information into transforms Quantum information into
classical classical classical
information and it turns out that it's information and it turns out that it's information and it turns out that it's
completely equivalent to our first completely equivalent to our first completely equivalent to our first
description of measurements which is description of measurements which is description of measurements which is
purely purely purely
algebraic I won't go through the proof algebraic I won't go through the proof algebraic I won't go through the proof
of the equivalence in this video but it of the equivalence in this video but it of the equivalence in this video but it
isn't difficult and if you're interested isn't difficult and if you're interested isn't difficult and if you're interested
you can find the proof in the written you can find the proof in the written you can find the proof in the written
material for the lesson which is linked material for the lesson which is linked material for the lesson which is linked
in the video in the video in the video
description we've discussed what happens description we've discussed what happens description we've discussed what happens
when a general measurement is performed when a general measurement is performed when a general measurement is performed
on a system in a given state on a system in a given state on a system in a given state
at least for the case of destructive at least for the case of destructive at least for the case of destructive
measurements next we'll talk about measurements next we'll talk about measurements next we'll talk about
partial measurements meaning that just partial measurements meaning that just partial measurements meaning that just
one part of a compound system is one part of a compound system is one part of a compound system is
measured to be precise let's suppose measured to be precise let's suppose measured to be precise let's suppose
that we have a pair of systems XZ that that we have a pair of systems XZ that that we have a pair of systems XZ that
together are in a state described by a together are in a state described by a together are in a state described by a
density Matrix row and a measurement density Matrix row and a measurement density Matrix row and a measurement
described by a collection of positive described by a collection of positive described by a collection of positive
semi-definite matrices that sum to the semi-definite matrices that sum to the semi-definite matrices that sum to the
identity is performed on the system X identity is performed on the system X identity is performed on the system X
alone naturally this results in in a alone naturally this results in in a alone naturally this results in in a
measurement outcome and because we're measurement outcome and because we're measurement outcome and because we're
talking about destructive measurements talking about destructive measurements talking about destructive measurements
we can imagine that the process of we can imagine that the process of we can imagine that the process of
measurement destroys the system measurement destroys the system measurement destroys the system
X but we still have the system Z to X but we still have the system Z to X but we still have the system Z to
worry about and we need to understand worry about and we need to understand worry about and we need to understand
how the measurement of X affects the how the measurement of X affects the how the measurement of X affects the
state of state of state of
Z first let's talk about the outcome probabilities because we're talking probabilities because we're talking
about a measurement of the system X about a measurement of the system X about a measurement of the system X
alone the probabilities for different alone the probabilities for different alone the probabilities for different
outcomes to appear can only depend on outcomes to appear can only depend on outcomes to appear can only depend on
the state of X in isolation which is to the state of X in isolation which is to the state of X in isolation which is to
say that they can only depend on the say that they can only depend on the say that they can only depend on the
reduced state of reduced state of reduced state of
X in particular the probability for a X in particular the probability for a X in particular the probability for a
given outcome a to appear is given by given outcome a to appear is given by given outcome a to appear is given by
the same formula from before which is the same formula from before which is the same formula from before which is
the trace of PA times the reduced state the trace of PA times the reduced state the trace of PA times the reduced state
of X which we denote by Row of X which we denote by Row of X which we denote by Row
subx the state row subx is obtained from subx the state row subx is obtained from subx the state row subx is obtained from
row by tracing out Z so we can make that row by tracing out Z so we can make that row by tracing out Z so we can make that
substitution and although it might not substitution and although it might not substitution and although it might not
be immediately obvious the question we be immediately obvious the question we be immediately obvious the question we
get can alternatively be written as is get can alternatively be written as is get can alternatively be written as is
shown on the screen it's the trace of PA shown on the screen it's the trace of PA shown on the screen it's the trace of PA
tensored with the identity Matrix on the tensored with the identity Matrix on the tensored with the identity Matrix on the
system Z * system Z * system Z *
row now that's an equality that's row now that's an equality that's row now that's an equality that's
definitely worth taking some time to definitely worth taking some time to definitely worth taking some time to
verify it's intuitive we're not doing verify it's intuitive we're not doing verify it's intuitive we're not doing
anything at all to Z so it makes sense anything at all to Z so it makes sense anything at all to Z so it makes sense
that the identity on Z should show up that the identity on Z should show up that the identity on Z should show up
like this but it can also be verified like this but it can also be verified like this but it can also be verified
algebraically and I will leave that to algebraically and I will leave that to algebraically and I will leave that to
you along with the suggestion to First you along with the suggestion to First you along with the suggestion to First
consider the case that row is is a consider the case that row is is a consider the case that row is is a
tensor product of two matrices and then tensor product of two matrices and then tensor product of two matrices and then
use linearity to conclude that it must use linearity to conclude that it must use linearity to conclude that it must
work for all row and I'll also say that work for all row and I'll also say that work for all row and I'll also say that
it does not depend on the fact that PA it does not depend on the fact that PA it does not depend on the fact that PA
is positive semi-definite or that row is is positive semi-definite or that row is is positive semi-definite or that row is
a density Matrix this is an equality a density Matrix this is an equality a density Matrix this is an equality
that would work for any choice of that would work for any choice of that would work for any choice of
matrices in place of PA and row as long matrices in place of PA and row as long matrices in place of PA and row as long
as they have the right dimensions for as they have the right dimensions for as they have the right dimensions for
the expression to make sense and now let's see how the state of sense and now let's see how the state of
Z is affected by the Z is affected by the Z is affected by the
measurement and the natural approach in measurement and the natural approach in measurement and the natural approach in
my view is to think about the my view is to think about the my view is to think about the
measurement as a measurement as a measurement as a
channel here's the channel that we channel here's the channel that we channel here's the channel that we
associate with our measurement and we're associate with our measurement and we're associate with our measurement and we're
thinking about this as a channel from thinking about this as a channel from thinking about this as a channel from
the system X to a new system y whose the system X to a new system y whose the system X to a new system y whose
classical States correspond to the classical States correspond to the classical States correspond to the
measurement outcomes so in this formula measurement outcomes so in this formula measurement outcomes so in this formula
Sigma represents a state of the system X Sigma represents a state of the system X Sigma represents a state of the system X
alone if we apply this channel to the alone if we apply this channel to the alone if we apply this channel to the
system X while the pair XZ is in the system X while the pair XZ is in the system X while the pair XZ is in the
state row then we'll obtain the density state row then we'll obtain the density state row then we'll obtain the density
Matrix that we have right here again Matrix that we have right here again Matrix that we have right here again
take as much time as you need to verify take as much time as you need to verify take as much time as you need to verify
this and again my suggestion is to First this and again my suggestion is to First this and again my suggestion is to First
consider the case that row is a tensor consider the case that row is a tensor consider the case that row is a tensor
product of two matrices and then use product of two matrices and then use product of two matrices and then use
linearity now we saw this type of state linearity now we saw this type of state linearity now we saw this type of state
where the first system is classical and where the first system is classical and where the first system is classical and
the second system is quantum as an the second system is quantum as an the second system is quantum as an
example in the density Matrix example in the density Matrix example in the density Matrix
lesson each a occurs with some lesson each a occurs with some lesson each a occurs with some
probability and that's the trace of the probability and that's the trace of the probability and that's the trace of the
second tensor Factor so you can think second tensor Factor so you can think second tensor Factor so you can think
about these second tensor factors as about these second tensor factors as about these second tensor factors as
being density matrices multiplied by being density matrices multiplied by being density matrices multiplied by
probabilities so to obtain the state of probabilities so to obtain the state of probabilities so to obtain the state of
Z conditioned on obtaining a particular Z conditioned on obtaining a particular Z conditioned on obtaining a particular
measurement outcome a we simply divide measurement outcome a we simply divide measurement outcome a we simply divide
out the probability or in other words we out the probability or in other words we out the probability or in other words we
normalize the second tensor Factor by normalize the second tensor Factor by normalize the second tensor Factor by
dividing it by its own Trace so in summary if we perform a Trace so in summary if we perform a
measurement on X while the pair XZ is in measurement on X while the pair XZ is in measurement on X while the pair XZ is in
the state row this is what the state row this is what the state row this is what
happens first each outcome a appears happens first each outcome a appears happens first each outcome a appears
with probability equal to the trace of with probability equal to the trace of with probability equal to the trace of
PA tensored with the identity time PA tensored with the identity time PA tensored with the identity time
row and second conditioned on obtaining row and second conditioned on obtaining row and second conditioned on obtaining
a particular measurement outcome a the a particular measurement outcome a the a particular measurement outcome a the
state of Z is given by the expression state of Z is given by the expression state of Z is given by the expression
shown on the screen which is what we get shown on the screen which is what we get shown on the screen which is what we get
by normalizing The Matrix we just saw a by normalizing The Matrix we just saw a by normalizing The Matrix we just saw a
moment ago and that's the same thing as moment ago and that's the same thing as moment ago and that's the same thing as
dividing the matrix by the probability dividing the matrix by the probability dividing the matrix by the probability
for a to appear I should mention that for a to appear I should mention that for a to appear I should mention that
this only makes sense if the probability this only makes sense if the probability this only makes sense if the probability
is non zero because otherwise we're is non zero because otherwise we're is non zero because otherwise we're
dividing by zero but that's okay because dividing by zero but that's okay because dividing by zero but that's okay because
we don't need to concern ourselves with we don't need to concern ourselves with we don't need to concern ourselves with
things that don't happen in this part of things that don't happen in this part of things that don't happen in this part of
the lesson we'll discuss a simple but the lesson we'll discuss a simple but the lesson we'll discuss a simple but
fundamental theorem about measurements fundamental theorem about measurements fundamental theorem about measurements
known as Neymar's known as Neymar's known as Neymar's
theorem in short what it says is that theorem in short what it says is that theorem in short what it says is that
any measurement can be implemented as any measurement can be implemented as any measurement can be implemented as
the figure on the screen the figure on the screen the figure on the screen
suggests putting this in words a given suggests putting this in words a given suggests putting this in words a given
General measurement on a system X can be General measurement on a system X can be General measurement on a system X can be
implemented in the following way implemented in the following way implemented in the following way
we first introduce an initialized we first introduce an initialized we first introduce an initialized
workspace system y whose classical workspace system y whose classical workspace system y whose classical
states are the possible measurement states are the possible measurement states are the possible measurement
outcomes which we're assuming are outcomes which we're assuming are outcomes which we're assuming are
numbered 0 through n minus numbered 0 through n minus numbered 0 through n minus
one then a unitary operation U is one then a unitary operation U is one then a unitary operation U is
performed on the pair performed on the pair performed on the pair
YX and then a standard basis measurement YX and then a standard basis measurement YX and then a standard basis measurement
is performed on is performed on is performed on
y so everything is basically set in y so everything is basically set in y so everything is basically set in
stone here except for the choice of U stone here except for the choice of U stone here except for the choice of U
and what the theorem says is that it is and what the theorem says is that it is and what the theorem says is that it is
in fact possible to choose this unitary in fact possible to choose this unitary in fact possible to choose this unitary
operation U so that the probabilities operation U so that the probabilities operation U so that the probabilities
for the different outcomes always agree for the different outcomes always agree for the different outcomes always agree
with the given General with the given General with the given General
measurement now let's see how Neymar's measurement now let's see how Neymar's measurement now let's see how Neymar's
theorem is proved and it isn't difficult theorem is proved and it isn't difficult theorem is proved and it isn't difficult
we just need to make a good choice for we just need to make a good choice for we just need to make a good choice for
the unitary operation U and verify that the unitary operation U and verify that the unitary operation U and verify that
it it it
works but first we need a basic fact works but first we need a basic fact works but first we need a basic fact
from Matrix Theory which is that for any from Matrix Theory which is that for any from Matrix Theory which is that for any
positive semi-definite Matrix P there's positive semi-definite Matrix P there's positive semi-definite Matrix P there's
always a unique positive semi-definite always a unique positive semi-definite always a unique positive semi-definite
Matrix Q such that Q s is equal to Matrix Q such that Q s is equal to Matrix Q such that Q s is equal to
P in general there could be many P in general there could be many P in general there could be many
matrices that squared to give us P but matrices that squared to give us P but matrices that squared to give us P but
there's always just one of them that's there's always just one of them that's there's always just one of them that's
positive positive positive
semi-definite the notation we use to semi-definite the notation we use to semi-definite the notation we use to
denote this Matrix is the square root of denote this Matrix is the square root of denote this Matrix is the square root of
p and that's also how we refer to it in p and that's also how we refer to it in p and that's also how we refer to it in
words if you want to calculate the words if you want to calculate the words if you want to calculate the
square root of P for some positive square root of P for some positive square root of P for some positive
semi-definite Matrix P by the way you semi-definite Matrix P by the way you semi-definite Matrix P by the way you
can do that by first calculating a can do that by first calculating a can do that by first calculating a
spectral decomposition of p and then spectral decomposition of p and then spectral decomposition of p and then
replacing each of the igen values by its replacing each of the igen values by its replacing each of the igen values by its
square root and leaving the igen vectors square root and leaving the igen vectors square root and leaving the igen vectors
alone the igen values are all non- alone the igen values are all non- alone the igen values are all non-
negative so this makes sense and it's negative so this makes sense and it's negative so this makes sense and it's
pretty easy to see that the Matrix we pretty easy to see that the Matrix we pretty easy to see that the Matrix we
get is in fact positive semi-definite get is in fact positive semi-definite get is in fact positive semi-definite
and squares to and squares to and squares to
P the fact that it's Unique is a P the fact that it's Unique is a P the fact that it's Unique is a
different matter it's not difficult to different matter it's not difficult to different matter it's not difficult to
prove that using some basic facts about prove that using some basic facts about prove that using some basic facts about
matrices but it's kind of tangential to matrices but it's kind of tangential to matrices but it's kind of tangential to
the lesson and we'll just take that part the lesson and we'll just take that part the lesson and we'll just take that part
as being as being as being
given and now that we have that Concept given and now that we have that Concept given and now that we have that Concept
in hand it's pretty easy to spef ify a in hand it's pretty easy to spef ify a in hand it's pretty easy to spef ify a
unitary operation U that works any unitary operation U that works any unitary operation U that works any
unitary Matrix that matches the pattern unitary Matrix that matches the pattern unitary Matrix that matches the pattern
that's shown on the screen will in fact that's shown on the screen will in fact that's shown on the screen will in fact
work the first n columns assuming X has work the first n columns assuming X has work the first n columns assuming X has
n classical States like before are n classical States like before are n classical States like before are
obtained by stacking the square roots of obtained by stacking the square roots of obtained by stacking the square roots of
the matrices that describe the the matrices that describe the the matrices that describe the
measurement on top of one another and measurement on top of one another and measurement on top of one another and
the remaining columns can be whatever we the remaining columns can be whatever we the remaining columns can be whatever we
want it doesn't make any difference as want it doesn't make any difference as want it doesn't make any difference as
long as the entire Matrix is unitary and by the way if you're noticing a and by the way if you're noticing a
resemblance to the connection between resemblance to the connection between resemblance to the connection between
Krauss and Stein spring representations Krauss and Stein spring representations Krauss and Stein spring representations
of channels that's not coincidental and of channels that's not coincidental and of channels that's not coincidental and
in fact what we have here is essentially in fact what we have here is essentially in fact what we have here is essentially
a special case of that where our cross a special case of that where our cross a special case of that where our cross
matricies all happen to be positive matricies all happen to be positive matricies all happen to be positive
semi-definite so there are two things we semi-definite so there are two things we semi-definite so there are two things we
need to check the first is that this need to check the first is that this need to check the first is that this
choice of you actually works and the choice of you actually works and the choice of you actually works and the
second is that it's possible to choose second is that it's possible to choose second is that it's possible to choose
the unspecified columns in a way that the unspecified columns in a way that the unspecified columns in a way that
makes you makes you makes you
unitary so let's start with the first unitary so let's start with the first unitary so let's start with the first
one here's the state that we start with one here's the state that we start with one here's the state that we start with
the bottom system Y is initialized to the bottom system Y is initialized to the bottom system Y is initialized to
the zero State while the top system the zero State while the top system the zero State while the top system
starts in the state starts in the state starts in the state
row and then we perform U which means row and then we perform U which means row and then we perform U which means
that we conjugate by that we conjugate by that we conjugate by
U we can write this all out in terms of U we can write this all out in terms of U we can write this all out in terms of
block matrices as is shown on the screen block matrices as is shown on the screen block matrices as is shown on the screen
we have U on the left UD dagger on the we have U on the left UD dagger on the we have U on the left UD dagger on the
right and K zero bra zero tensored with right and K zero bra zero tensored with right and K zero bra zero tensored with
row in the middle which takes the form row in the middle which takes the form row in the middle which takes the form
that we have here where we have Row in that we have here where we have Row in that we have here where we have Row in
the upper left hand corner and zeros the upper left hand corner and zeros the upper left hand corner and zeros
everywhere everywhere everywhere
else and if you think about this product else and if you think about this product else and if you think about this product
for a moment it's evident that the for a moment it's evident that the for a moment it's evident that the
unspecified entries of you don't matter unspecified entries of you don't matter unspecified entries of you don't matter
at all because they're always getting at all because they're always getting at all because they're always getting
multiplied by zero entries of this multiplied by zero entries of this multiplied by zero entries of this
Matrix in the Matrix in the Matrix in the
middle and if we work through the middle and if we work through the middle and if we work through the
multiplication what we get is this multiplication what we get is this multiplication what we get is this
Matrix that's shown right here described Matrix that's shown right here described Matrix that's shown right here described
as a block Matrix or alternatively we as a block Matrix or alternatively we as a block Matrix or alternatively we
can express it using direct notation can express it using direct notation can express it using direct notation
like this that is in fact the more convenient this that is in fact the more convenient
form for us so let's focus in on this form for us so let's focus in on this form for us so let's focus in on this
expression and for the sake of expression and for the sake of expression and for the sake of
convenience let's give this Matrix which convenience let's give this Matrix which convenience let's give this Matrix which
is a density Matrix the name is a density Matrix the name is a density Matrix the name
Sigma the last step is the standard Sigma the last step is the standard Sigma the last step is the standard
basis measurement on y so let's basis measurement on y so let's basis measurement on y so let's
calculate the reduced state in y and calculate the reduced state in y and calculate the reduced state in y and
that's easy given the expression we have that's easy given the expression we have that's easy given the expression we have
we simply Trace out X which corresponds we simply Trace out X which corresponds we simply Trace out X which corresponds
to the second tensor to the second tensor to the second tensor
Factor so the probability for any given Factor so the probability for any given Factor so the probability for any given
outcome a to appear is the diagonal outcome a to appear is the diagonal outcome a to appear is the diagonal
entry corresponding to a we can write entry corresponding to a we can write entry corresponding to a we can write
that like that like that like
this and it equals the trace of the this and it equals the trace of the this and it equals the trace of the
square root of PA * row * the square square root of PA * row * the square square root of PA * row * the square
root of PA and by the cyclic property of root of PA and by the cyclic property of root of PA and by the cyclic property of
the trace we get the trace of PA * row the trace we get the trace of PA * row the trace we get the trace of PA * row
which is what we were hoping for so the which is what we were hoping for so the which is what we were hoping for so the
probabilities for the different outcomes probabilities for the different outcomes probabilities for the different outcomes
to appear are correct they always agree to appear are correct they always agree to appear are correct they always agree
with the original measurement that we with the original measurement that we with the original measurement that we
started started started
with the second thing we need to check with the second thing we need to check with the second thing we need to check
is that it's possible to fill in the is that it's possible to fill in the is that it's possible to fill in the
unspecified Columns of U in such a way unspecified Columns of U in such a way unspecified Columns of U in such a way
that it becomes that it becomes that it becomes
unitary this follows from the fact that unitary this follows from the fact that unitary this follows from the fact that
the first n columns which are the ones the first n columns which are the ones the first n columns which are the ones
that are formed by the square roots of that are formed by the square roots of that are formed by the square roots of
the matrices that describe the the matrices that describe the the matrices that describe the
measurement are orthonormal and that can measurement are orthonormal and that can measurement are orthonormal and that can
be checked pretty be checked pretty be checked pretty
directly the relevant details are here directly the relevant details are here directly the relevant details are here
on the screen so pause the video and on the screen so pause the video and on the screen so pause the video and
take a few moments to think about that take a few moments to think about that take a few moments to think about that
if you wish but as I suggested before if you wish but as I suggested before if you wish but as I suggested before
it's actually a special case of it's actually a special case of it's actually a special case of
something that we encountered in the something that we encountered in the something that we encountered in the
previous lesson and that is the previous lesson and that is the previous lesson and that is the
proof so far in the lesson we've kept proof so far in the lesson we've kept proof so far in the lesson we've kept
our focus on destructive measurements our focus on destructive measurements our focus on destructive measurements
meaning that there's no specification of meaning that there's no specification of meaning that there's no specification of
the postmeasurement quantum state of the the postmeasurement quantum state of the the postmeasurement quantum state of the
system that was system that was system that was
measured for a non-destructive measured for a non-destructive measured for a non-destructive
measurement on the other hand there's measurement on the other hand there's measurement on the other hand there's
not only a classical measurement outcome not only a classical measurement outcome not only a classical measurement outcome
but also a postmeasurement Quantum state but also a postmeasurement Quantum state but also a postmeasurement Quantum state
of the system that was measured and as of the system that was measured and as of the system that was measured and as
we'll see there's a natural connection we'll see there's a natural connection we'll see there's a natural connection
to Neymar's to Neymar's to Neymar's
theorem there are in fact different ways theorem there are in fact different ways theorem there are in fact different ways
to formulate non-destructive to formulate non-destructive to formulate non-destructive
measurements in mathematical terms but measurements in mathematical terms but measurements in mathematical terms but
we'll start with one formulation that we'll start with one formulation that we'll start with one formulation that
basically comes directly from Neymar's basically comes directly from Neymar's basically comes directly from Neymar's
theorem consider a general measurement theorem consider a general measurement theorem consider a general measurement
meaning a destructive one described by a meaning a destructive one described by a meaning a destructive one described by a
collection of positive semi-definite collection of positive semi-definite collection of positive semi-definite
matrices p 0 through PN minus one that's matrices p 0 through PN minus one that's matrices p 0 through PN minus one that's
sum to the identity we can then turn this identity we can then turn this
measurement into non-destructive measurement into non-destructive measurement into non-destructive
measurement simply by considering the measurement simply by considering the measurement simply by considering the
implementation of this measurement that implementation of this measurement that implementation of this measurement that
we discussed in the context of Neymar's we discussed in the context of Neymar's we discussed in the context of Neymar's
theorem we introduce a new system y theorem we introduce a new system y theorem we introduce a new system y
interact it with X through a unitary interact it with X through a unitary interact it with X through a unitary
operation U and then perform a standard operation U and then perform a standard operation U and then perform a standard
basis measurement on y so the system X basis measurement on y so the system X basis measurement on y so the system X
is still there and we can consider its is still there and we can consider its is still there and we can consider its
state and based on the analysis that we state and based on the analysis that we state and based on the analysis that we
went through we can determine that the went through we can determine that the went through we can determine that the
postmeasurement state of X conditioned postmeasurement state of X conditioned postmeasurement state of X conditioned
on getting a particular measurement on getting a particular measurement on getting a particular measurement
outcome a is given by a density Matrix outcome a is given by a density Matrix outcome a is given by a density Matrix
that looks like that looks like that looks like
this in words we conjugate by the square this in words we conjugate by the square this in words we conjugate by the square
root of the corresponding measurement root of the corresponding measurement root of the corresponding measurement
Matrix and then normalize another way to formulate normalize another way to formulate
non-destructive measurements which is non-destructive measurements which is non-destructive measurements which is
more General is in terms of cross more General is in terms of cross more General is in terms of cross
matrices of channels and this is the matrices of channels and this is the matrices of channels and this is the
definition of measurements you'll find definition of measurements you'll find definition of measurements you'll find
in the well-known textbook of Neil in in the well-known textbook of Neil in in the well-known textbook of Neil in
Trang for Trang for Trang for
instance suppose that m0 through m m instance suppose that m0 through m m instance suppose that m0 through m m
minus1 are square matrices that satisfy minus1 are square matrices that satisfy minus1 are square matrices that satisfy
the equation that's shown on the screen the equation that's shown on the screen the equation that's shown on the screen
which is to say that we could think which is to say that we could think which is to say that we could think
about these matrices as being cross about these matrices as being cross about these matrices as being cross
matrices that describe a matrices that describe a matrices that describe a
channel in fact these matrices do channel in fact these matrices do channel in fact these matrices do
describe a channel but they also describe a channel but they also describe a channel but they also
describe a non-destructive measurement describe a non-destructive measurement describe a non-destructive measurement
and the way that it works when a system and the way that it works when a system and the way that it works when a system
in a given State row is measured is in a given State row is measured is in a given State row is measured is
first the outcome probabilities are first the outcome probabilities are first the outcome probabilities are
given by the equation shown on the given by the equation shown on the given by the equation shown on the
screen each outcome appears with screen each outcome appears with screen each outcome appears with
probability equal to the trace of ma * probability equal to the trace of ma * probability equal to the trace of ma *
row * ma row * ma row * ma
dagger and conditioned on the alcome a dagger and conditioned on the alcome a dagger and conditioned on the alcome a
appearing the state of the measured appearing the state of the measured appearing the state of the measured
system looks like system looks like system looks like
this non-destructive measurements of this non-destructive measurements of this non-destructive measurements of
this sort can be implemented in a this sort can be implemented in a this sort can be implemented in a
similar way to what we had for Neymar's similar way to what we had for Neymar's similar way to what we had for Neymar's
theorem and I'll leave that for you to theorem and I'll leave that for you to theorem and I'll leave that for you to
think about if you think about if you think about if you
choose it is possible to generalize the choose it is possible to generalize the choose it is possible to generalize the
concept of a non-destructive measurement concept of a non-destructive measurement concept of a non-destructive measurement
even further than this for example even further than this for example even further than this for example
there's a concept of a so-called Quantum there's a concept of a so-called Quantum there's a concept of a so-called Quantum
instrument that's sometimes quite useful instrument that's sometimes quite useful instrument that's sometimes quite useful
but I won't cover that in this but I won't cover that in this but I won't cover that in this
video in the last part of the lesson video in the last part of the lesson video in the last part of the lesson
I'll briefly discuss two tasks that are I'll briefly discuss two tasks that are I'll briefly discuss two tasks that are
associated with measurements Quantum associated with measurements Quantum associated with measurements Quantum
State discrimination and Quantum State State discrimination and Quantum State State discrimination and Quantum State
tomography there's quite a lot to be tomography there's quite a lot to be tomography there's quite a lot to be
said about both of these Concepts and said about both of these Concepts and said about both of these Concepts and
we're really just going to be scratching we're really just going to be scratching we're really just going to be scratching
the the the
surface so let's start by clarifying surface so let's start by clarifying surface so let's start by clarifying
what these terms refer to and how they what these terms refer to and how they what these terms refer to and how they
differ differ differ
first is quantum State first is quantum State first is quantum State
discrimination the idea here is that we discrimination the idea here is that we discrimination the idea here is that we
have a known collection of quantum have a known collection of quantum have a known collection of quantum
states of some system along with states of some system along with states of some system along with
probabilities associated with these probabilities associated with these probabilities associated with these
states so to be precise let's assume states so to be precise let's assume states so to be precise let's assume
that row 0 through row M minus1 are that row 0 through row M minus1 are that row 0 through row M minus1 are
density matrices representing possible density matrices representing possible density matrices representing possible
Quantum states of a system X and p 0 Quantum states of a system X and p 0 Quantum states of a system X and p 0
through PN minus one are probabilities through PN minus one are probabilities through PN minus one are probabilities
associated with these states so if we associated with these states so if we associated with these states so if we
collect the probabilities into a single collect the probabilities into a single collect the probabilities into a single
Vector we get a ability Vector a Vector we get a ability Vector a Vector we get a ability Vector a
succinct way of saying this by the way succinct way of saying this by the way succinct way of saying this by the way
is that we have an ensemble of quantum is that we have an ensemble of quantum is that we have an ensemble of quantum
States and now an abstract way that we States and now an abstract way that we States and now an abstract way that we
can think about Quantum State can think about Quantum State can think about Quantum State
discrimination as a task or as a problem discrimination as a task or as a problem discrimination as a task or as a problem
is that first a number between Z and N is that first a number between Z and N is that first a number between Z and N
minus one is chosen randomly according minus one is chosen randomly according minus one is chosen randomly according
to the to the to the
probabilities the system X is then probabilities the system X is then probabilities the system X is then
prepared in the state corresponding to prepared in the state corresponding to prepared in the state corresponding to
the randomly selected the randomly selected the randomly selected
number and the goal is to determine by number and the goal is to determine by number and the goal is to determine by
means of a measurement of X which number means of a measurement of X which number means of a measurement of X which number
was chosen was chosen was chosen
so we have a finite number of so we have a finite number of so we have a finite number of
Alternatives along with a prior and the Alternatives along with a prior and the Alternatives along with a prior and the
goal is to determine which alternative goal is to determine which alternative goal is to determine which alternative
actually actually actually
happened for some choices of states and happened for some choices of states and happened for some choices of states and
probabilities this may be easy and for probabilities this may be easy and for probabilities this may be easy and for
others it may not be possible to do this others it may not be possible to do this others it may not be possible to do this
without some chance of making an error without some chance of making an error without some chance of making an error
and there are various specific questions and there are various specific questions and there are various specific questions
that we could ask such as what is the that we could ask such as what is the that we could ask such as what is the
minimum probability of making an error minimum probability of making an error minimum probability of making an error
and what does a measurement that and what does a measurement that and what does a measurement that
achieves this minimum probability of achieves this minimum probability of achieves this minimum probability of
error look error look error look
like Quantum State tomography on the like Quantum State tomography on the like Quantum State tomography on the
other hand is other hand is other hand is
different here we have an unknown different here we have an unknown different here we have an unknown
Quantum state of a system so there's Quantum state of a system so there's Quantum state of a system so there's
typically no prior or any information typically no prior or any information typically no prior or any information
about possible about possible about possible
Alternatives this time however it's not Alternatives this time however it's not Alternatives this time however it's not
a single copy of the state that's made a single copy of the state that's made a single copy of the state that's made
available but rather many independent available but rather many independent available but rather many independent
copies are made available which is to copies are made available which is to copies are made available which is to
say that for some possibly large number say that for some possibly large number say that for some possibly large number
n we have n identical systems X1 through n we have n identical systems X1 through n we have n identical systems X1 through
xn that are each independently prepared xn that are each independently prepared xn that are each independently prepared
in the state in the state in the state
row and this time the goal is to find an row and this time the goal is to find an row and this time the goal is to find an
approximation of this unknown Quantum approximation of this unknown Quantum approximation of this unknown Quantum
State as a density matrix by measuring State as a density matrix by measuring State as a density matrix by measuring
the the the
systems so in summary in Quantum State systems so in summary in Quantum State systems so in summary in Quantum State
discrimination we have a single copy of discrimination we have a single copy of discrimination we have a single copy of
the state and we're trying to the state and we're trying to the state and we're trying to
discriminate among a finite number of discriminate among a finite number of discriminate among a finite number of
known Alternatives and in Quantum State known Alternatives and in Quantum State known Alternatives and in Quantum State
demography we have many independent demography we have many independent demography we have many independent
copies of an unknown Quantum State and copies of an unknown Quantum State and copies of an unknown Quantum State and
the goal is to find a description of the goal is to find a description of the goal is to find a description of
that state or at least a decent that state or at least a decent that state or at least a decent
approximation of approximation of approximation of
it quantum State discrimination is it quantum State discrimination is it quantum State discrimination is
interesting because it abstracts a type interesting because it abstracts a type interesting because it abstracts a type
of situation that commonly arises in of situation that commonly arises in of situation that commonly arises in
computation and cryptography for computation and cryptography for computation and cryptography for
instance where one alternative out of a instance where one alternative out of a instance where one alternative out of a
finite set of possibilities has taken finite set of possibilities has taken finite set of possibilities has taken
place and we or somebody else is trying place and we or somebody else is trying place and we or somebody else is trying
to figure out which one it was by to figure out which one it was by to figure out which one it was by
measuring a Quantum measuring a Quantum measuring a Quantum
system the simplest non-trivial case is system the simplest non-trivial case is system the simplest non-trivial case is
where there are just two Alternatives where there are just two Alternatives where there are just two Alternatives
meaning that m is equal to two so the meaning that m is equal to two so the meaning that m is equal to two so the
task is to discriminate between a pair task is to discriminate between a pair task is to discriminate between a pair
of of of
States this case turns out to be States this case turns out to be States this case turns out to be
essentially a completely solid problem essentially a completely solid problem essentially a completely solid problem
assuming that the goal is to minimize assuming that the goal is to minimize assuming that the goal is to minimize
the chance of an incorrect the chance of an incorrect the chance of an incorrect
discrimination there's an optimal choice discrimination there's an optimal choice discrimination there's an optimal choice
for a measurement and it's often called for a measurement and it's often called for a measurement and it's often called
the hellstrom the hellstrom the hellstrom
measurement it's a projective measurement it's a projective measurement it's a projective
measurement in fact and it can be measurement in fact and it can be measurement in fact and it can be
described as described as described as
follows first we consider the weighted follows first we consider the weighted follows first we consider the weighted
difference between the two Alternatives difference between the two Alternatives difference between the two Alternatives
meaning p 0 * row 0 minus P1 * Row one meaning p 0 * row 0 minus P1 * Row one meaning p 0 * row 0 minus P1 * Row one
so we're multiplying the probability of so we're multiplying the probability of so we're multiplying the probability of
drawing each of these two states to the drawing each of these two states to the drawing each of these two states to the
states themselves and we're taking the states themselves and we're taking the states themselves and we're taking the
difference that's a herian matrix difference that's a herian matrix difference that's a herian matrix
because it equals its own conjugate because it equals its own conjugate because it equals its own conjugate
transpose so it has a spectral transpose so it has a spectral transpose so it has a spectral
decomposition and we know in particular decomposition and we know in particular decomposition and we know in particular
that the igen values are all real that the igen values are all real that the igen values are all real
numbers so let's assume that we have a numbers so let's assume that we have a numbers so let's assume that we have a
spectral decomposition written in the spectral decomposition written in the spectral decomposition written in the
form that's shown on the form that's shown on the form that's shown on the
screen what we do next is to collect the screen what we do next is to collect the screen what we do next is to collect the
igen values into two sets where all of igen values into two sets where all of igen values into two sets where all of
the positive IG values are in one set the positive IG values are in one set the positive IG values are in one set
and all of the negative IG values are in and all of the negative IG values are in and all of the negative IG values are in
the the the
other to be precise let's take s0 to be other to be precise let's take s0 to be other to be precise let's take s0 to be
the set of all the indices k for which the set of all the indices k for which the set of all the indices k for which
the corresponding igen value Lambda K is the corresponding igen value Lambda K is the corresponding igen value Lambda K is
non- negative and let S1 be the non- negative and let S1 be the non- negative and let S1 be the
remaining indices which are the ones remaining indices which are the ones remaining indices which are the ones
corresponding to negative IG corresponding to negative IG corresponding to negative IG
values it doesn't actually matter which values it doesn't actually matter which values it doesn't actually matter which
set contains the indices corresponding set contains the indices corresponding set contains the indices corresponding
to any zero I values here we're making to any zero I values here we're making to any zero I values here we're making
an arbitrary choice to include the minus an arbitrary choice to include the minus an arbitrary choice to include the minus
zero but it wouldn't make any difference zero but it wouldn't make any difference zero but it wouldn't make any difference
if some or all of them were included in if some or all of them were included in if some or all of them were included in
S1 instead all that really matters is S1 instead all that really matters is S1 instead all that really matters is
that we're separating the positive and that we're separating the positive and that we're separating the positive and
negative IG values and finally we Define two projections Pi and finally we Define two projections Pi
0 and Pi 1 as is shown on the screen in 0 and Pi 1 as is shown on the screen in 0 and Pi 1 as is shown on the screen in
words Pi 0 is the projection onto the words Pi 0 is the projection onto the words Pi 0 is the projection onto the
Subspace spend by the igen vectors Subspace spend by the igen vectors Subspace spend by the igen vectors
corresponding to non- negative IG values corresponding to non- negative IG values corresponding to non- negative IG values
and Pi 1 is projection onto the space and Pi 1 is projection onto the space and Pi 1 is projection onto the space
expanded by the igon vectors expanded by the igon vectors expanded by the igon vectors
corresponding to negative IG corresponding to negative IG corresponding to negative IG
values these are indeed projections and values these are indeed projections and values these are indeed projections and
if we add them together we get the if we add them together we get the if we add them together we get the
identity Matrix because the igen vectors identity Matrix because the igen vectors identity Matrix because the igen vectors
S 0 through S N minus one form an S 0 through S N minus one form an S 0 through S N minus one form an
orthonormal orthonormal orthonormal
basis if this measurement is performed basis if this measurement is performed basis if this measurement is performed
in the situation at hand meaning that in the situation at hand meaning that in the situation at hand meaning that
the state being measured is row 0 with the state being measured is row 0 with the state being measured is row 0 with
probability p 0 and Row one with probability p 0 and Row one with probability p 0 and Row one with
probability P1 then it turns out that probability P1 then it turns out that probability P1 then it turns out that
the probability of a correct the probability of a correct the probability of a correct
identification has a pretty simple identification has a pretty simple identification has a pretty simple
formula in terms of the igen values formula in terms of the igen values formula in terms of the igen values
Lambda 0 through Lambda n minus Lambda 0 through Lambda n minus Lambda 0 through Lambda n minus
one I won't go through the details of one I won't go through the details of one I won't go through the details of
the calculation in this video you can the calculation in this video you can the calculation in this video you can
consult the written content Linked In consult the written content Linked In consult the written content Linked In
the video description if you're the video description if you're the video description if you're
interested or see if you can convince interested or see if you can convince interested or see if you can convince
yourself that this formula is indeed yourself that this formula is indeed yourself that this formula is indeed
correct we can also Express this correct we can also Express this correct we can also Express this
probability using a matrix Norm commonly probability using a matrix Norm commonly probability using a matrix Norm commonly
called the trace Norm it's a similar called the trace Norm it's a similar called the trace Norm it's a similar
notation to the ukian norm for vectors notation to the ukian norm for vectors notation to the ukian norm for vectors
except that we have a subscript of a one except that we have a subscript of a one except that we have a subscript of a one
to indicate that it's the trace Norm the to indicate that it's the trace Norm the to indicate that it's the trace Norm the
trace Norm is very important and it's trace Norm is very important and it's trace Norm is very important and it's
very commonly encountered in Quantum very commonly encountered in Quantum very commonly encountered in Quantum
Computing but I won't say too much more Computing but I won't say too much more Computing but I won't say too much more
about it in this lesson one way to about it in this lesson one way to about it in this lesson one way to
define it for herian matrices is that define it for herian matrices is that define it for herian matrices is that
it's the sum of the absolute values of it's the sum of the absolute values of it's the sum of the absolute values of
the igen values and so you can think the igen values and so you can think the igen values and so you can think
about this second equality as following about this second equality as following about this second equality as following
directly from that directly from that directly from that
definition the trace arm can also be definition the trace arm can also be definition the trace arm can also be
defined in other ways that are more defined in other ways that are more defined in other ways that are more
General and we'll encounter the tracor General and we'll encounter the tracor General and we'll encounter the tracor
again in the next lesson this is in fact again in the next lesson this is in fact again in the next lesson this is in fact
the best that you can do this the best that you can do this the best that you can do this
measurement is optimal assuming your measurement is optimal assuming your measurement is optimal assuming your
goal is to maximize the chance of a goal is to maximize the chance of a goal is to maximize the chance of a
correct correct correct
guess that fact is sometimes called the guess that fact is sometimes called the guess that fact is sometimes called the
hellstrom Holo theorem and it's also hellstrom Holo theorem and it's also hellstrom Holo theorem and it's also
sometimes just called hellstrom's sometimes just called hellstrom's sometimes just called hellstrom's
theorem so you can't beat this theorem so you can't beat this theorem so you can't beat this
measurement in terms of correctness measurement in terms of correctness measurement in terms of correctness
probability and it's nice because probability and it's nice because probability and it's nice because
there's a simple recipe for the there's a simple recipe for the there's a simple recipe for the
measurement itself and an exact formula measurement itself and an exact formula measurement itself and an exact formula
for the correctness probability in terms for the correctness probability in terms for the correctness probability in terms
of the trace of the trace of the trace
Norm the trace Norm is often used as a Norm the trace Norm is often used as a Norm the trace Norm is often used as a
way of measuring the observable way of measuring the observable way of measuring the observable
difference between density matrices for difference between density matrices for difference between density matrices for
precisely this reason and when people precisely this reason and when people precisely this reason and when people
refer to the trace distance between refer to the trace distance between refer to the trace distance between
States they're typically referring to States they're typically referring to States they're typically referring to
one half times the trace Norm of the one half times the trace Norm of the one half times the trace Norm of the
difference between the two density difference between the two density difference between the two density
matrices which here corresponds to the matrices which here corresponds to the matrices which here corresponds to the
case where p 0 and P1 are both 1/2 which case where p 0 and P1 are both 1/2 which case where p 0 and P1 are both 1/2 which
is a natural case to is a natural case to is a natural case to
consider Quantum State discrimination consider Quantum State discrimination consider Quantum State discrimination
for two states is optimally solved by for two states is optimally solved by for two states is optimally solved by
the heal from measurement for which we the heal from measurement for which we the heal from measurement for which we
have a simple recipe and a simple have a simple recipe and a simple have a simple recipe and a simple
formula for the correctness formula for the correctness formula for the correctness
probability so what happens when we want probability so what happens when we want probability so what happens when we want
to discriminate among three or more to discriminate among three or more to discriminate among three or more
States in this case we don't have a States in this case we don't have a States in this case we don't have a
formula in general for an optimal choice formula in general for an optimal choice formula in general for an optimal choice
of a measurement or a simple expression of a measurement or a simple expression of a measurement or a simple expression
for the maximum probability of for the maximum probability of for the maximum probability of
correctness it is however possible to correctness it is however possible to correctness it is however possible to
find a good approximation of an optimal find a good approximation of an optimal find a good approximation of an optimal
measurement with the help of a computer measurement with the help of a computer measurement with the help of a computer
and in particular that can be done using and in particular that can be done using and in particular that can be done using
a really fascinating and Powerful a really fascinating and Powerful a really fascinating and Powerful
optimization technique known as optimization technique known as optimization technique known as
semi-definite programming but it's not semi-definite programming but it's not semi-definite programming but it's not
something that you can feasibly something that you can feasibly something that you can feasibly
Implement using a pencil and a piece of Implement using a pencil and a piece of Implement using a pencil and a piece of
paper for paper for paper for
instance on the other hand if somebody instance on the other hand if somebody instance on the other hand if somebody
gives you a description of a measurement gives you a description of a measurement gives you a description of a measurement
and they claim to you that this and they claim to you that this and they claim to you that this
measurement is optimal for a Quantum measurement is optimal for a Quantum measurement is optimal for a Quantum
State discrimination task then it State discrimination task then it State discrimination task then it
actually is very easy to either verify actually is very easy to either verify actually is very easy to either verify
or falsify that claim and that could be or falsify that claim and that could be or falsify that claim and that could be
done using the so-called Holo Yan done using the so-called Holo Yan done using the so-called Holo Yan
Kennedy LAX conditions I won't explain Kennedy LAX conditions I won't explain Kennedy LAX conditions I won't explain
what they are in this video but they are what they are in this video but they are what they are in this video but they are
simple conditions and it's pretty cool simple conditions and it's pretty cool simple conditions and it's pretty cool
that this is possible here's an example which possible here's an example which
connects back to the tetrahedral states connects back to the tetrahedral states connects back to the tetrahedral states
that we saw earlier in the that we saw earlier in the that we saw earlier in the
lesson suppose we're giving one of these lesson suppose we're giving one of these lesson suppose we're giving one of these
four states selected at random with four states selected at random with four states selected at random with
probability one quarter for each one and probability one quarter for each one and probability one quarter for each one and
our goal is to determine which state our goal is to determine which state our goal is to determine which state
that we're that we're that we're
given it's not surprising perhaps but given it's not surprising perhaps but given it's not surprising perhaps but
the best choice you can make for a the best choice you can make for a the best choice you can make for a
measurement to do this is the measurement to do this is the measurement to do this is the
tetrahedral tetrahedral tetrahedral
measurement you can't always do measurement you can't always do measurement you can't always do
something like this for an arbitrary something like this for an arbitrary something like this for an arbitrary
collection of pure States but because of collection of pure States but because of collection of pure States but because of
the symmetries involved here this does the symmetries involved here this does the symmetries involved here this does
work and we actually get a valid work and we actually get a valid work and we actually get a valid
measurement and through the Holo un measurement and through the Holo un measurement and through the Holo un
Kennedy LAX conditions that I mentioned Kennedy LAX conditions that I mentioned Kennedy LAX conditions that I mentioned
a few moments AG ago it turns out to be a few moments AG ago it turns out to be a few moments AG ago it turns out to be
quite straightforward to prove that this quite straightforward to prove that this quite straightforward to prove that this
is indeed an optimal choice of a is indeed an optimal choice of a is indeed an optimal choice of a
measurement specifically what happens measurement specifically what happens measurement specifically what happens
here is that the measurement is always here is that the measurement is always here is that the measurement is always
correct with probability 1/2 which isn't correct with probability 1/2 which isn't correct with probability 1/2 which isn't
bad because there are four Alternatives bad because there are four Alternatives bad because there are four Alternatives
so the probability of a correct guess is so the probability of a correct guess is so the probability of a correct guess is
twice as good as random guessing and it twice as good as random guessing and it twice as good as random guessing and it
happens to be the best that you can do happens to be the best that you can do happens to be the best that you can do
for these for these for these
states now I'll say just a little bit states now I'll say just a little bit states now I'll say just a little bit
more about Quantum State tomography more about Quantum State tomography more about Quantum State tomography
where we have many independent copies of where we have many independent copies of where we have many independent copies of
an unknown Quantum state an unknown Quantum state an unknown Quantum state
and the goal is to find an approximation and the goal is to find an approximation and the goal is to find an approximation
of the density Matrix that describes of the density Matrix that describes of the density Matrix that describes
that that that
state there are actually several state there are actually several state there are actually several
different variants of this problem that different variants of this problem that different variants of this problem that
depend on your assumptions and in depend on your assumptions and in depend on your assumptions and in
particular on what sorts of measurements particular on what sorts of measurements particular on what sorts of measurements
you you you
allow for instance we could demand that allow for instance we could demand that allow for instance we could demand that
each system is measured individually and each system is measured individually and each system is measured individually and
the results are combined somehow to the results are combined somehow to the results are combined somehow to
arrive at a description of the density arrive at a description of the density arrive at a description of the density
Matrix or we could allow all of the Matrix or we could allow all of the Matrix or we could allow all of the
systems to be measured together as a systems to be measured together as a systems to be measured together as a
single compound system which is much single compound system which is much single compound system which is much
better in theory but it's also much more better in theory but it's also much more better in theory but it's also much more
demanding from a technological demanding from a technological demanding from a technological
standpoint in addition to the choices standpoint in addition to the choices standpoint in addition to the choices
one makes for the measurements there's one makes for the measurements there's one makes for the measurements there's
also an important consideration which is also an important consideration which is also an important consideration which is
how to reconstruct a description of the how to reconstruct a description of the how to reconstruct a description of the
density Matrix from whatever data you density Matrix from whatever data you density Matrix from whatever data you
get from the get from the get from the
measurements I won't go into details measurements I won't go into details measurements I won't go into details
about this my point is simply to suggest about this my point is simply to suggest about this my point is simply to suggest
that Quantum State tomography is really that Quantum State tomography is really that Quantum State tomography is really
more of an umbrella concept having many more of an umbrella concept having many more of an umbrella concept having many
different variants as opposed to being a different variants as opposed to being a different variants as opposed to being a
single procedure single procedure single procedure
there's been a great deal of research there's been a great deal of research there's been a great deal of research
into this problem including some fairly into this problem including some fairly into this problem including some fairly
recent breakthrough results that connect recent breakthrough results that connect recent breakthrough results that connect
the number of copies n to the accuracy the number of copies n to the accuracy the number of copies n to the accuracy
of the approximation that can be of the approximation that can be of the approximation that can be
obtained and it continues to be an obtained and it continues to be an obtained and it continues to be an
interesting and active area of interesting and active area of interesting and active area of
research to finish off the lesson let's research to finish off the lesson let's research to finish off the lesson let's
take a look at Quantum State tomography take a look at Quantum State tomography take a look at Quantum State tomography
for cubits and how we can do for cubits and how we can do for cubits and how we can do
it so let's suppose that row is a it so let's suppose that row is a it so let's suppose that row is a
density Matrix representing a state of a density Matrix representing a state of a density Matrix representing a state of a
single Cubit and X1 through xn are single Cubit and X1 through xn are single Cubit and X1 through xn are
cubits that have all been independently cubits that have all been independently cubits that have all been independently
prepared in the state row and for the prepared in the state row and for the prepared in the state row and for the
sake of this discussion let's assume sake of this discussion let's assume sake of this discussion let's assume
that n is that n is that n is
large what we can do is to divide the large what we can do is to divide the large what we can do is to divide the
systems into three roughly equal piles systems into three roughly equal piles systems into three roughly equal piles
and for the first pile we'll perform a and for the first pile we'll perform a and for the first pile we'll perform a
measurement with respect to the plus measurement with respect to the plus measurement with respect to the plus
minus minus minus
basis for each outcome corresponding to basis for each outcome corresponding to basis for each outcome corresponding to
plus we'll score plus one and for each plus we'll score plus one and for each plus we'll score plus one and for each
outcome corresponding to minus we'll outcome corresponding to minus we'll outcome corresponding to minus we'll
score minus one a more succinct way to score minus one a more succinct way to score minus one a more succinct way to
describe this measure describe this measure describe this measure
is that it's a poly X measurement is that it's a poly X measurement is that it's a poly X measurement
because the plus and minus states are IG because the plus and minus states are IG because the plus and minus states are IG
vectors of the poly X Matrix and the vectors of the poly X Matrix and the vectors of the poly X Matrix and the
corresponding igen values are plus one corresponding igen values are plus one corresponding igen values are plus one
and minus one and in fact that's also a and minus one and in fact that's also a and minus one and in fact that's also a
much more common way to describe it and much more common way to describe it and much more common way to describe it and
indeed the expected value for the score indeed the expected value for the score indeed the expected value for the score
is equal to the trace of Sigma x * is equal to the trace of Sigma x * is equal to the trace of Sigma x *
row now we don't necessarily learn what row now we don't necessarily learn what row now we don't necessarily learn what
this expected value is but if we average this expected value is but if we average this expected value is but if we average
the scores we obtain over lots and lots the scores we obtain over lots and lots the scores we obtain over lots and lots
of Trials we're pretty likely to get a of Trials we're pretty likely to get a of Trials we're pretty likely to get a
decent approximation to the this decent approximation to the this decent approximation to the this
expected value by the law of large expected value by the law of large expected value by the law of large
numbers and there are some well-known numbers and there are some well-known numbers and there are some well-known
statistical bounds that we can throw at statistical bounds that we can throw at statistical bounds that we can throw at
this if we want to be more quantitative this if we want to be more quantitative this if we want to be more quantitative
about that about that about that
statement we do something similar with statement we do something similar with statement we do something similar with
Sigma y on the second third of the Sigma y on the second third of the Sigma y on the second third of the
systems the only thing that changes here systems the only thing that changes here systems the only thing that changes here
is that we use the plus I and minus I is that we use the plus I and minus I is that we use the plus I and minus I
states which are igen vectors of Sigma states which are igen vectors of Sigma states which are igen vectors of Sigma
Y and similarly for Sigma Z for which Y and similarly for Sigma Z for which Y and similarly for Sigma Z for which
the standard basis vectors are IG the standard basis vectors are IG the standard basis vectors are IG
vectors and then finally we can use the vectors and then finally we can use the vectors and then finally we can use the
formula that's shown on the screen which formula that's shown on the screen which formula that's shown on the screen which
tells us what the coefficients should be tells us what the coefficients should be tells us what the coefficients should be
if row is written as a linear if row is written as a linear if row is written as a linear
combination of poly combination of poly combination of poly
matricies and by substituting our matricies and by substituting our matricies and by substituting our
approximations for these coefficients approximations for these coefficients approximations for these coefficients
and possibly adjusting them a little bit and possibly adjusting them a little bit and possibly adjusting them a little bit
to make sure that we get a positive to make sure that we get a positive to make sure that we get a positive
semi-definite Matrix we get an semi-definite Matrix we get an semi-definite Matrix we get an
approximation to approximation to approximation to
row of course there are details that I row of course there are details that I row of course there are details that I
haven't gone into such as the accuracy haven't gone into such as the accuracy haven't gone into such as the accuracy
of the approximation and the degree of of the approximation and the degree of of the approximation and the degree of
confidence that we can hope to have in confidence that we can hope to have in confidence that we can hope to have in
it but hopefully that gives you a big it but hopefully that gives you a big it but hopefully that gives you a big
basic idea of one way that tomography basic idea of one way that tomography basic idea of one way that tomography
can be performed on a cubit it's also possible to do Cubit cubit it's also possible to do Cubit
tomography using the touch or hedral tomography using the touch or hedral tomography using the touch or hedral
measurement and in fact this turns out measurement and in fact this turns out measurement and in fact this turns out
to be quite simple and elegant at least to be quite simple and elegant at least to be quite simple and elegant at least
in mathematical in mathematical in mathematical
terms what we can do is to perform just terms what we can do is to perform just terms what we can do is to perform just
this one measurement alone on all of the this one measurement alone on all of the this one measurement alone on all of the
independent copies of our unknown independent copies of our unknown independent copies of our unknown
Quantum State and by using the Quantum State and by using the Quantum State and by using the
remarkable formula shown right here we remarkable formula shown right here we remarkable formula shown right here we
can come up with an approximation of our can come up with an approximation of our can come up with an approximation of our
state by estimating the trace of PK * state by estimating the trace of PK * state by estimating the trace of PK *
Row for each K by the fraction of times Row for each K by the fraction of times Row for each K by the fraction of times
the corresponding measurement outcome the corresponding measurement outcome the corresponding measurement outcome
appears again this only works because of appears again this only works because of appears again this only works because of
the perfect symmetries that we have the perfect symmetries that we have the perfect symmetries that we have
among these four states and there are in among these four states and there are in among these four states and there are in
fact some really fascinating unsolved fact some really fascinating unsolved fact some really fascinating unsolved
problems that relate to generalizations problems that relate to generalizations problems that relate to generalizations
of this approach to larger of this approach to larger of this approach to larger
systems and that's a good place to stop systems and that's a good place to stop systems and that's a good place to stop
the lesson thank you for watching and I the lesson thank you for watching and I the lesson thank you for watching and I
hope that you will join me for the next hope that you will join me for the next hope that you will join me for the next
lesson which is about purifications and lesson which is about purifications and lesson which is about purifications and
Fidelity goodbye for now

## Grover's Algorithm ｜ Understanding Quantum Information & Computation ｜ Lesson 08

welcome back to understanding Quantum welcome back to understanding Quantum
information and computation my name is information and computation my name is information and computation my name is
John watus and I'm the technical John watus and I'm the technical John watus and I'm the technical
director for education at IBM director for education at IBM director for education at IBM
Quantum this is the eth lesson of the Quantum this is the eth lesson of the Quantum this is the eth lesson of the
series and it's the last lesson in the series and it's the last lesson in the series and it's the last lesson in the
second unit of the series which is on second unit of the series which is on second unit of the series which is on
Quantum Quantum Quantum
algorithms we've talked about the query algorithms we've talked about the query algorithms we've talked about the query
model of computation including Quantum model of computation including Quantum model of computation including Quantum
algorithms that offer provable algorithms that offer provable algorithms that offer provable
advantages over classical algorithms advantages over classical algorithms advantages over classical algorithms
within that model we've discussed within that model we've discussed within that model we've discussed
foundational Concepts connected with foundational Concepts connected with foundational Concepts connected with
Quantum algorithms and how we can Quantum algorithms and how we can Quantum algorithms and how we can
measure computational cost and in the measure computational cost and in the measure computational cost and in the
previous lesson we discussed phase previous lesson we discussed phase previous lesson we discussed phase
estimation and Shores estimation and Shores estimation and Shores
algorithm in this lesson we'll discuss algorithm in this lesson we'll discuss algorithm in this lesson we'll discuss
Grover's algorithm which provides a Grover's algorithm which provides a Grover's algorithm which provides a
quadratic advantage of quantum over quadratic advantage of quantum over quadratic advantage of quantum over
classical algorithms for so-called classical algorithms for so-called classical algorithms for so-called
unstructured search problems which I'll unstructured search problems which I'll unstructured search problems which I'll
describe once we get into the describe once we get into the describe once we get into the
lesson these are all foundational topics lesson these are all foundational topics lesson these are all foundational topics
concerning Quantum concerning Quantum concerning Quantum
algorithms there are other Quantum algorithms there are other Quantum algorithms there are other Quantum
algorithms as well as general paradigms algorithms as well as general paradigms algorithms as well as general paradigms
that Unite different Quantum algorithms that Unite different Quantum algorithms that Unite different Quantum algorithms
and Quantum algorithmic techniques and and Quantum algorithmic techniques and and Quantum algorithmic techniques and
if you're interested I encourage you to if you're interested I encourage you to if you're interested I encourage you to
check out the quantum algorithm Zoo check out the quantum algorithm Zoo check out the quantum algorithm Zoo
which categorizes known Quantum which categorizes known Quantum which categorizes known Quantum
algorithms and includes many references algorithms and includes many references algorithms and includes many references
to original research papers that have to original research papers that have to original research papers that have
contributed to their contributed to their contributed to their
development for this series however development for this series however development for this series however
we're just going to stick with the we're just going to stick with the we're just going to stick with the
fundamentals and starting with the next fundamentals and starting with the next fundamentals and starting with the next
lesson we'll move on to a new topic but lesson we'll move on to a new topic but lesson we'll move on to a new topic but
first let's talk about Grover's first let's talk about Grover's first let's talk about Grover's
algorithm here is the lesson overview algorithm here is the lesson overview algorithm here is the lesson overview
we'll begin with a short discussion of we'll begin with a short discussion of we'll begin with a short discussion of
the computational problem that we'll be the computational problem that we'll be the computational problem that we'll be
addressing which is unstructured addressing which is unstructured addressing which is unstructured
search intuitively speaking you can search intuitively speaking you can search intuitively speaking you can
think about the unstructured search think about the unstructured search think about the unstructured search
problem as searching through a problem as searching through a problem as searching through a
collection of items that isn't sorted collection of items that isn't sorted collection of items that isn't sorted
and hasn't been ordered in a useful and hasn't been ordered in a useful and hasn't been ordered in a useful
way then we'll discuss Grover's way then we'll discuss Grover's way then we'll discuss Grover's
algorithm which is a Quantum algorithm algorithm which is a Quantum algorithm algorithm which is a Quantum algorithm
that offers a quadratic advantage over that offers a quadratic advantage over that offers a quadratic advantage over
classical algorithms and we'll get into classical algorithms and we'll get into classical algorithms and we'll get into
precisely what that means precisely what that means precisely what that means
shortly a very High Lev description of shortly a very High Lev description of shortly a very High Lev description of
how this algorithm works is that first how this algorithm works is that first how this algorithm works is that first
there's an initialization step and then there's an initialization step and then there's an initialization step and then
an operation that I'll refer to as the an operation that I'll refer to as the an operation that I'll refer to as the
grover operation is iterated some number grover operation is iterated some number grover operation is iterated some number
of times and then finally a measurement of times and then finally a measurement of times and then finally a measurement
takes place that reveals a candidate takes place that reveals a candidate takes place that reveals a candidate
solution to the search solution to the search solution to the search
problem I'll explain what the grover problem I'll explain what the grover problem I'll explain what the grover
operation is and how it can be operation is and how it can be operation is and how it can be
implemented on a quantum computer and implemented on a quantum computer and implemented on a quantum computer and
then we'll go through an analysis of then we'll go through an analysis of then we'll go through an analysis of
this operation to better understand what this operation to better understand what this operation to better understand what
it does and why it's useful for it does and why it's useful for it does and why it's useful for
searching the third part of the lesson is the third part of the lesson is
concerned with the number of iterations concerned with the number of iterations concerned with the number of iterations
in Grover's algorithm or in other words in Grover's algorithm or in other words in Grover's algorithm or in other words
the number of times the grover operation the number of times the grover operation the number of times the grover operation
should be applied presuming that we wish should be applied presuming that we wish should be applied presuming that we wish
to maximize the likelihood that the to maximize the likelihood that the to maximize the likelihood that the
algorithm succeeds in finding whatever algorithm succeeds in finding whatever algorithm succeeds in finding whatever
it is that we're searching for while it is that we're searching for while it is that we're searching for while
simultaneously minimizing its simultaneously minimizing its simultaneously minimizing its
cost we'll start with the unique search cost we'll start with the unique search cost we'll start with the unique search
problem which is a special case in which problem which is a special case in which problem which is a special case in which
there's exactly one solution to our there's exactly one solution to our there's exactly one solution to our
search problem and then we'll consider search problem and then we'll consider search problem and then we'll consider
consider what happens when there are consider what happens when there are consider what happens when there are
multiple multiple multiple
Solutions and finally I'll conclude the Solutions and finally I'll conclude the Solutions and finally I'll conclude the
lesson with a few brief remarks about lesson with a few brief remarks about lesson with a few brief remarks about
Grover's algorithm and its Grover's algorithm and its Grover's algorithm and its
generalizations let's begin with the generalizations let's begin with the generalizations let's begin with the
computational problem that we'll be computational problem that we'll be computational problem that we'll be
addressing throughout the addressing throughout the addressing throughout the
lesson first as we often do we're going lesson first as we often do we're going lesson first as we often do we're going
to let Sigma denote the binary alphabet to let Sigma denote the binary alphabet to let Sigma denote the binary alphabet
and we'll use this notation for the and we'll use this notation for the and we'll use this notation for the
entirety of the lesson so I won't keep entirety of the lesson so I won't keep entirety of the lesson so I won't keep
reminding reminding reminding
you you you
suppose that we have a function f that suppose that we have a function f that suppose that we have a function f that
Maps binary strings of some length n to Maps binary strings of some length n to Maps binary strings of some length n to
the binary the binary the binary
alphabet our assumption is that we can alphabet our assumption is that we can alphabet our assumption is that we can
easily evaluate this function for easily evaluate this function for easily evaluate this function for
example maybe we have a Boolean circuit example maybe we have a Boolean circuit example maybe we have a Boolean circuit
for this function whose size is for this function whose size is for this function whose size is
polinomial In polinomial In polinomial In
N now this doesn't mean that this is N now this doesn't mean that this is N now this doesn't mean that this is
necessarily a simple function in any necessarily a simple function in any necessarily a simple function in any
sense or that we have a particularly sense or that we have a particularly sense or that we have a particularly
good understanding of this good understanding of this good understanding of this
function for instance maybe we have a function for instance maybe we have a function for instance maybe we have a
rather complicated and messy Boolean rather complicated and messy Boolean rather complicated and messy Boolean
ccle circuit for computing F that allows ccle circuit for computing F that allows ccle circuit for computing F that allows
us to evaluate it on different input us to evaluate it on different input us to evaluate it on different input
strings but doesn't necessarily have a strings but doesn't necessarily have a strings but doesn't necessarily have a
nice form that's designed to make our nice form that's designed to make our nice form that's designed to make our
lives lives lives
easier our goal is to find a solution by easier our goal is to find a solution by easier our goal is to find a solution by
which we mean a binary string of length which we mean a binary string of length which we mean a binary string of length
n that causes F to evaluate to n that causes F to evaluate to n that causes F to evaluate to
one in general this is a very difficult one in general this is a very difficult one in general this is a very difficult
computational problem or at least we computational problem or at least we computational problem or at least we
suspect that is very difficult in suspect that is very difficult in suspect that is very difficult in
particular if the input is in fact the particular if the input is in fact the particular if the input is in fact the
description of a Boolean circuit for description of a Boolean circuit for description of a Boolean circuit for
Computing the function f then this is a Computing the function f then this is a Computing the function f then this is a
so-called NP complete problem one way so-called NP complete problem one way so-called NP complete problem one way
that we can formalize this problem is as that we can formalize this problem is as that we can formalize this problem is as
a query problem and indeed for the a query problem and indeed for the a query problem and indeed for the
purposes of this lesson it's reasonable purposes of this lesson it's reasonable purposes of this lesson it's reasonable
to think about this problem purely in to think about this problem purely in to think about this problem purely in
the context of the query model but when the context of the query model but when the context of the query model but when
we think about where or when this we think about where or when this we think about where or when this
problem might arise in practice it's problem might arise in practice it's problem might arise in practice it's
arguably more natural to think about f arguably more natural to think about f arguably more natural to think about f
as being computed by a Boolean as being computed by a Boolean as being computed by a Boolean
circuit as we saw in lesson six though circuit as we saw in lesson six though circuit as we saw in lesson six though
we can connect these two points of view we can connect these two points of view we can connect these two points of view
through the construction that we went through the construction that we went through the construction that we went
through for taking a Boolean circuit and through for taking a Boolean circuit and through for taking a Boolean circuit and
converting it into a Quantum circuit for converting it into a Quantum circuit for converting it into a Quantum circuit for
implementing a query operation or a implementing a query operation or a implementing a query operation or a
query query query
gate in any case conceptually speaking gate in any case conceptually speaking gate in any case conceptually speaking
the input to the problem is this the input to the problem is this the input to the problem is this
function f and we're looking for a function f and we're looking for a function f and we're looking for a
solution if there is one and if there solution if there is one and if there solution if there is one and if there
isn't a solution meaning that F turns isn't a solution meaning that F turns isn't a solution meaning that F turns
out to be the constant zero function out to be the constant zero function out to be the constant zero function
then the correct output is to report then the correct output is to report then the correct output is to report
that there are no that there are no that there are no
Solutions this is is an unstructured Solutions this is is an unstructured Solutions this is is an unstructured
search problem in the sense that there's search problem in the sense that there's search problem in the sense that there's
nothing special about F that makes it nothing special about F that makes it nothing special about F that makes it
easy to find a easy to find a easy to find a
solution it's not like we're searching solution it's not like we're searching solution it's not like we're searching
through data that's already been ordered through data that's already been ordered through data that's already been ordered
or that's been loaded into a data or that's been loaded into a data or that's been loaded into a data
structure of some sort that facilitates structure of some sort that facilitates structure of some sort that facilitates
searching and there's no promise on the searching and there's no promise on the searching and there's no promise on the
function f that might help function f that might help function f that might help
us one way to think about it is that us one way to think about it is that us one way to think about it is that
it's like searching for a particular it's like searching for a particular it's like searching for a particular
phone number in a phone book as opposed phone number in a phone book as opposed phone number in a phone book as opposed
to searching for a name which is much to searching for a name which is much to searching for a name which is much
easier presuming that the names have easier presuming that the names have easier presuming that the names have
been been been
alphabetized in Practical terms it's not alphabetized in Practical terms it's not alphabetized in Practical terms it's not
really about searching through data really about searching through data really about searching through data
which we tend to organize and store in a which we tend to organize and store in a which we tend to organize and store in a
manner that makes searching easy it's manner that makes searching easy it's manner that makes searching easy it's
more representative of searching for a more representative of searching for a more representative of searching for a
solution to a complicated equation for solution to a complicated equation for solution to a complicated equation for
instance or perhaps searching for the instance or perhaps searching for the instance or perhaps searching for the
combination that opens a combination that opens a combination that opens a
lock as an aside it is sensible to think lock as an aside it is sensible to think lock as an aside it is sensible to think
about this problem in the context of the about this problem in the context of the about this problem in the context of the
query model regardless of how we might query model regardless of how we might query model regardless of how we might
feel about the Practical relevance of feel about the Practical relevance of feel about the Practical relevance of
that model that model that model
of course there aren't oracles or of course there aren't oracles or of course there aren't oracles or
blackboxes in real life but until we blackboxes in real life but until we blackboxes in real life but until we
learn otherwise complicated and learn otherwise complicated and learn otherwise complicated and
mysterious Boolean circuits are kind of mysterious Boolean circuits are kind of mysterious Boolean circuits are kind of
like black boxes now let's think about algorithms boxes now let's think about algorithms
for solving this unstructured search for solving this unstructured search for solving this unstructured search
problem and we're going to be doing this problem and we're going to be doing this problem and we're going to be doing this
in the query model just for the sake of in the query model just for the sake of in the query model just for the sake of
Simplicity if we have a Boolean circuit Simplicity if we have a Boolean circuit Simplicity if we have a Boolean circuit
for our function rather than a black box for our function rather than a black box for our function rather than a black box
then we can still run query algorithms then we can still run query algorithms then we can still run query algorithms
on it but of course we can't rule out on it but of course we can't rule out on it but of course we can't rule out
the possibility that some new algorithm the possibility that some new algorithm the possibility that some new algorithm
is discovered for this problem that is discovered for this problem that is discovered for this problem that
isn't based on making queries but we isn't based on making queries but we isn't based on making queries but we
won't concern ourselves with that possibility first just a little bit of possibility first just a little bit of
notation the total number of input notation the total number of input notation the total number of input
strings to the function f is 2 to the strings to the function f is 2 to the strings to the function f is 2 to the
little n where little n is the length of little n where little n is the length of little n where little n is the length of
the input strings to F and Hereafter I'm the input strings to F and Hereafter I'm the input strings to F and Hereafter I'm
going to let Big N be this number 2 to going to let Big N be this number 2 to going to let Big N be this number 2 to
the little n it's pretty common that the little n it's pretty common that the little n it's pretty common that
people use capital letter like this to people use capital letter like this to people use capital letter like this to
denote two to the power of the lowercase denote two to the power of the lowercase denote two to the power of the lowercase
version of that letter we're going to be version of that letter we're going to be version of that letter we're going to be
talking about the number capital N quite talking about the number capital N quite talking about the number capital N quite
a lot so this is just going to be a lot so this is just going to be a lot so this is just going to be
convenient and it'll help to keep things convenient and it'll help to keep things convenient and it'll help to keep things
a little bit a little bit a little bit
neater and again this notation holds for neater and again this notation holds for neater and again this notation holds for
the entire lesson now of course we can solve the lesson now of course we can solve the
search Problem classically by simply search Problem classically by simply search Problem classically by simply
iterating through all of the possible iterating through all of the possible iterating through all of the possible
end bit strings in whatever order we end bit strings in whatever order we end bit strings in whatever order we
choose evaluating F on each one of choose evaluating F on each one of choose evaluating F on each one of
them if we find a solution at any point them if we find a solution at any point them if we find a solution at any point
we can stop but in the worst case we'll we can stop but in the worst case we'll we can stop but in the worst case we'll
end up evaluating F on every single end up evaluating F on every single end up evaluating F on every single
input string so this requires n meaning input string so this requires n meaning input string so this requires n meaning
capital N queries we could choose to measure the queries we could choose to measure the
cost of this computation in a different cost of this computation in a different cost of this computation in a different
way not just in terms of the number of way not just in terms of the number of way not just in terms of the number of
queries but the number of queries queries but the number of queries queries but the number of queries
represents the number of times we have represents the number of times we have represents the number of times we have
to evaluate the function f and it's to evaluate the function f and it's to evaluate the function f and it's
reasonable to expect that the reasonable to expect that the reasonable to expect that the
evaluations of f will dominate the cost evaluations of f will dominate the cost evaluations of f will dominate the cost
C of this computation this is in fact the best we computation this is in fact the best we
can do with a deterministic algorithm can do with a deterministic algorithm can do with a deterministic algorithm
and again this is in the context of the and again this is in the context of the and again this is in the context of the
query query query
model if we've queried every input model if we've queried every input model if we've queried every input
string except for one of them and F has string except for one of them and F has string except for one of them and F has
taken the value zero on all of them so taken the value zero on all of them so taken the value zero on all of them so
far how can we be sure that the last far how can we be sure that the last far how can we be sure that the last
string isn't string isn't string isn't
solution and the answer is that we can't solution and the answer is that we can't solution and the answer is that we can't
so we do need all n queries in the worst so we do need all n queries in the worst so we do need all n queries in the worst
case we could also use case we could also use case we could also use
Randomness that can offer some very Randomness that can offer some very Randomness that can offer some very
minor improvements such as reducing the minor improvements such as reducing the minor improvements such as reducing the
expected number of queries needed to expected number of queries needed to expected number of queries needed to
find a solution when one exists or we find a solution when one exists or we find a solution when one exists or we
could effectively trade correctness could effectively trade correctness could effectively trade correctness
probability for a reduction in queries probability for a reduction in queries probability for a reduction in queries
but however you slice it we'll still but however you slice it we'll still but however you slice it we'll still
need a number of queries that's linear need a number of queries that's linear need a number of queries that's linear
in N So classically speaking we need a in N So classically speaking we need a in N So classically speaking we need a
linear number of queries meaning linear linear number of queries meaning linear linear number of queries meaning linear
in capital N Grover's algorithm is a in capital N Grover's algorithm is a in capital N Grover's algorithm is a
Quantum algorithm for search that does Quantum algorithm for search that does Quantum algorithm for search that does
better it requires just bigo of the better it requires just bigo of the better it requires just bigo of the
square root of n square root of n square root of n
queries those queries happen in queries those queries happen in queries those queries happen in
superposition so it isn't Magic but it superposition so it isn't Magic but it superposition so it isn't Magic but it
is a significant improvement over what's is a significant improvement over what's is a significant improvement over what's
possible classically and this is what's possible classically and this is what's possible classically and this is what's
meant by a quadratic meant by a quadratic meant by a quadratic
Advantage can this quadratic advantage Advantage can this quadratic advantage Advantage can this quadratic advantage
of quantum over classical be turned into of quantum over classical be turned into of quantum over classical be turned into
an actual advantage in a practical an actual advantage in a practical an actual advantage in a practical
sense probably not anytime soon but in sense probably not anytime soon but in sense probably not anytime soon but in
the distant future who knows but even if the distant future who knows but even if the distant future who knows but even if
if Grover's algorithm doesn't ever lead if Grover's algorithm doesn't ever lead if Grover's algorithm doesn't ever lead
to a practical Advantage for searching to a practical Advantage for searching to a practical Advantage for searching
it is nevertheless an important part of it is nevertheless an important part of it is nevertheless an important part of
the foundation of quantum algorithms the foundation of quantum algorithms the foundation of quantum algorithms
that's well worth that's well worth that's well worth
understanding before we move on to understanding before we move on to understanding before we move on to
Grover's algorithm itself we need to Grover's algorithm itself we need to Grover's algorithm itself we need to
talk briefly about so-called phase query talk briefly about so-called phase query talk briefly about so-called phase query
Gates many Quantum algorithms in the Gates many Quantum algorithms in the Gates many Quantum algorithms in the
query model are naturally expressed query model are naturally expressed query model are naturally expressed
using phase query Gates and we'll see using phase query Gates and we'll see using phase query Gates and we'll see
that Grover's algorithm is a specific that Grover's algorithm is a specific that Grover's algorithm is a specific
example let's start with a quick view of example let's start with a quick view of example let's start with a quick view of
ordinary query gates for a function f of ordinary query gates for a function f of ordinary query gates for a function f of
the form that we've been talking about the form that we've been talking about the form that we've been talking about
the query gate UF is defined onst the query gate UF is defined onst the query gate UF is defined onst
standard basis States as is shown on the standard basis States as is shown on the standard basis States as is shown on the
screen and for superpositions of screen and for superpositions of screen and for superpositions of
standard basis States the action is standard basis States the action is standard basis States the action is
determined by determined by determined by
linearity and it's worth reminding linearity and it's worth reminding linearity and it's worth reminding
ourselves that from a Boolean circuit ourselves that from a Boolean circuit ourselves that from a Boolean circuit
for f we can in fact build a circuit for for f we can in fact build a circuit for for f we can in fact build a circuit for
this operation UF at a cost that's this operation UF at a cost that's this operation UF at a cost that's
linear in the size of the Boolean linear in the size of the Boolean linear in the size of the Boolean
circuit the phase query gate for f on circuit the phase query gate for f on circuit the phase query gate for f on
the the other hand which will denote by the the other hand which will denote by the the other hand which will denote by
the name ZF operates like the name ZF operates like the name ZF operates like
this in particular this gate operates on this in particular this gate operates on this in particular this gate operates on
little n cubits as opposed to little n little n cubits as opposed to little n little n cubits as opposed to little n
plus one cubits and it basically just plus one cubits and it basically just plus one cubits and it basically just
automatically kicks the function value automatically kicks the function value automatically kicks the function value
into the phase rather than xoring the into the phase rather than xoring the into the phase rather than xoring the
output value onto an additional Cubit output value onto an additional Cubit output value onto an additional Cubit
like we have for like we have for like we have for
UF so if we think about what happens UF so if we think about what happens UF so if we think about what happens
when we use phase Kickback on a UF gate when we use phase Kickback on a UF gate when we use phase Kickback on a UF gate
we're basically just implementing a ZF we're basically just implementing a ZF we're basically just implementing a ZF
G we need a minus State and it costs one G we need a minus State and it costs one G we need a minus State and it costs one
evaluation of UF but we do in fact get evaluation of UF but we do in fact get evaluation of UF but we do in fact get
the minus State back and we can use it the minus State back and we can use it the minus State back and we can use it
for the next one if we want to or we for the next one if we want to or we for the next one if we want to or we
could just throw it away so that's what could just throw it away so that's what could just throw it away so that's what
a phase query gate is now if we're just a phase query gate is now if we're just a phase query gate is now if we're just
given a phase query gate ZF and this is given a phase query gate ZF and this is given a phase query gate ZF and this is
the only way that we have access to the the only way that we have access to the the only way that we have access to the
function f then we can't actually function f then we can't actually function f then we can't actually
recover a UF gate but if we have a recover a UF gate but if we have a recover a UF gate but if we have a
controlled version of ZF then we can controlled version of ZF then we can controlled version of ZF then we can
build a UF gate and I'll leave that one build a UF gate and I'll leave that one build a UF gate and I'll leave that one
to you as an exercise in addition to the phase query exercise in addition to the phase query
gate for the function f we're also going gate for the function f we're also going gate for the function f we're also going
to need a phase query gate for the nbit to need a phase query gate for the nbit to need a phase query gate for the nbit
or function or in other words the or function or in other words the or function or in other words the
function that takes value zero when it's function that takes value zero when it's function that takes value zero when it's
given the l0o string and otherwise it given the l0o string and otherwise it given the l0o string and otherwise it
takes the value takes the value takes the value
one and explicitly the way that zor one and explicitly the way that zor one and explicitly the way that zor
works on standard basis States is shown works on standard basis States is shown works on standard basis States is shown
here on the here on the here on the
screen this gate doesn't have any depend screen this gate doesn't have any depend screen this gate doesn't have any depend
on the function f so it doesn't require on the function f so it doesn't require on the function f so it doesn't require
any queries and one way that we can any queries and one way that we can any queries and one way that we can
build a Quantum circuit that implements build a Quantum circuit that implements build a Quantum circuit that implements
this operation is by doing something this operation is by doing something this operation is by doing something
similar to what we did for the function similar to what we did for the function similar to what we did for the function
f except that we start with a Boolean f except that we start with a Boolean f except that we start with a Boolean
circuit that computes the or function on circuit that computes the or function on circuit that computes the or function on
n n n
Bits And now that we have these Gates Bits And now that we have these Gates Bits And now that we have these Gates
we're ready to move on to the next we're ready to move on to the next we're ready to move on to the next
section now we'll discuss Grover's section now we'll discuss Grover's section now we'll discuss Grover's
algorithm beginning with a description algorithm beginning with a description algorithm beginning with a description
of the of the of the
algorithm we start by initializing algorithm we start by initializing algorithm we start by initializing
little n cubits each to the plus state little n cubits each to the plus state little n cubits each to the plus state
or in other words we put these cubits or in other words we put these cubits or in other words we put these cubits
into an equal superposition of all into an equal superposition of all into an equal superposition of all
binary strings of length binary strings of length binary strings of length
n what we're going to do is effectively n what we're going to do is effectively n what we're going to do is effectively
to steer the state of these cubits to steer the state of these cubits to steer the state of these cubits
toward a solution so it's intuitive that toward a solution so it's intuitive that toward a solution so it's intuitive that
we start in this state because we don't we start in this state because we don't we start in this state because we don't
know anything about the function f yet know anything about the function f yet know anything about the function f yet
at this point every string is equally at this point every string is equally at this point every string is equally
likely to be a solution and that's likely to be a solution and that's likely to be a solution and that's
reflected by the fact that these cubits reflected by the fact that these cubits reflected by the fact that these cubits
are in an equal superp position of all are in an equal superp position of all are in an equal superp position of all
strings and then we iterate we strings and then we iterate we strings and then we iterate we
repeatedly apply the grover operation repeatedly apply the grover operation repeatedly apply the grover operation
which is really the heart of the which is really the heart of the which is really the heart of the
algorithm and I'll describe it in just a algorithm and I'll describe it in just a algorithm and I'll describe it in just a
moment we apply this operation a total moment we apply this operation a total moment we apply this operation a total
of T times where T is some non- negative of T times where T is some non- negative of T times where T is some non- negative
integer but this number isn't actually integer but this number isn't actually integer but this number isn't actually
specified by the algorithm itself we specified by the algorithm itself we specified by the algorithm itself we
have to choose T somehow and in the next have to choose T somehow and in the next have to choose T somehow and in the next
section we're going to talk about good section we're going to talk about good section we're going to talk about good
choices for T but for now we can think choices for T but for now we can think choices for T but for now we can think
about this number of iterations t as about this number of iterations t as about this number of iterations t as
being being being
arbitrary and Fin finally we measure in arbitrary and Fin finally we measure in arbitrary and Fin finally we measure in
the standard basis and that gives us a the standard basis and that gives us a the standard basis and that gives us a
candidate candidate candidate
solution there's no guarantee that it is solution there's no guarantee that it is solution there's no guarantee that it is
a solution but if we choose T well then a solution but if we choose T well then a solution but if we choose T well then
the probability to get a solution can be the probability to get a solution can be the probability to get a solution can be
made made made
large and here is the grover operation large and here is the grover operation large and here is the grover operation
itself which we've named itself which we've named itself which we've named
G it's a composition of operations all G it's a composition of operations all G it's a composition of operations all
of them on N cubits so this is an N of them on N cubits so this is an N of them on N cubits so this is an N
Cubit Cubit Cubit
operation first we have a ZF gate or in operation first we have a ZF gate or in operation first we have a ZF gate or in
other words a phase query gate for the other words a phase query gate for the other words a phase query gate for the
input function f followed by a layer of input function f followed by a layer of input function f followed by a layer of
hadamard gates and then we have zor hadamard gates and then we have zor hadamard gates and then we have zor
which is the phase query gate for the or which is the phase query gate for the or which is the phase query gate for the or
function and finally another layer of function and finally another layer of function and finally another layer of
hatar hatar hatar
gates so we see that one iteration in gates so we see that one iteration in gates so we see that one iteration in
step two can be implemented by a circuit step two can be implemented by a circuit step two can be implemented by a circuit
having the form shown here on the screen having the form shown here on the screen having the form shown here on the screen
and here we have four cubits by the way and here we have four cubits by the way and here we have four cubits by the way
but in general we can have any number of but in general we can have any number of but in general we can have any number of
cubits a key thing to note here is that cubits a key thing to note here is that cubits a key thing to note here is that
we need one query of f to implement G we we need one query of f to implement G we we need one query of f to implement G we
need one query to implement ZF and none need one query to implement ZF and none need one query to implement ZF and none
of the other operations have any of the other operations have any of the other operations have any
dependence on F so they don't require dependence on F so they don't require dependence on F so they don't require
any queries so this number T that any queries so this number T that any queries so this number T that
represents the number of times we apply represents the number of times we apply represents the number of times we apply
G in step two also represents the number G in step two also represents the number G in step two also represents the number
of queries we of queries we of queries we
need here's a typical way that we might need here's a typical way that we might need here's a typical way that we might
use Grover's algorithm but it's use Grover's algorithm but it's use Grover's algorithm but it's
certainly not the only way like I said certainly not the only way like I said certainly not the only way like I said
before the first thing that we need to before the first thing that we need to before the first thing that we need to
do is to choose T and we'll get into do is to choose T and we'll get into do is to choose T and we'll get into
that in the next section that in the next section that in the next section
for whatever T we choose we run Grover's for whatever T we choose we run Grover's for whatever T we choose we run Grover's
algorithm for T iterations and we get a algorithm for T iterations and we get a algorithm for T iterations and we get a
candidate solution candidate solution candidate solution
X there's no guarantee that X is really X there's no guarantee that X is really X there's no guarantee that X is really
a solution but we can spend one a solution but we can spend one a solution but we can spend one
additional query to additional query to additional query to
check if we have a solution then we can check if we have a solution then we can check if we have a solution then we can
output that solution and stop and output that solution and stop and output that solution and stop and
otherwise we can either try again otherwise we can either try again otherwise we can either try again
possibly with a different choice of T possibly with a different choice of T possibly with a different choice of T
but not necessarily or we can give up but not necessarily or we can give up but not necessarily or we can give up
and report that there aren't any and report that there aren't any and report that there aren't any
solutions like a I said before this is solutions like a I said before this is solutions like a I said before this is
not the only way the algorithm might be not the only way the algorithm might be not the only way the algorithm might be
used but this is a natural way of used but this is a natural way of used but this is a natural way of
applying it to the search applying it to the search applying it to the search
problem next we'll see what the grover problem next we'll see what the grover problem next we'll see what the grover
operation actually does to the state of operation actually does to the state of operation actually does to the state of
the N cubits that it acts the N cubits that it acts the N cubits that it acts
on let's think of our n cubits as on let's think of our n cubits as on let's think of our n cubits as
forming a single register which I'll forming a single register which I'll forming a single register which I'll
name Q just to pick a name Q just to pick a name Q just to pick a
name remember that what we do first in name remember that what we do first in name remember that what we do first in
Grover's algorithm is to put Q into a Grover's algorithm is to put Q into a Grover's algorithm is to put Q into a
uniform superposition of all nbit uniform superposition of all nbit uniform superposition of all nbit
strings and we do that by applying hatar strings and we do that by applying hatar strings and we do that by applying hatar
Gates to n cubits in the all zero State Gates to n cubits in the all zero State Gates to n cubits in the all zero State
we then iterate the grover operation G we then iterate the grover operation G we then iterate the grover operation G
and then and then and then
measure it's going to be helpful to measure it's going to be helpful to measure it's going to be helpful to
partition the set of all nbit strings partition the set of all nbit strings partition the set of all nbit strings
into two sets a z and into two sets a z and into two sets a z and
A1 a Zer contains all of the strings A1 a Zer contains all of the strings A1 a Zer contains all of the strings
that cause F to evaluate to zero and A1 that cause F to evaluate to zero and A1 that cause F to evaluate to zero and A1
contains all of the strings that cause F contains all of the strings that cause F contains all of the strings that cause F
to evaluate to to evaluate to to evaluate to
one another way to say that is that A1 one another way to say that is that A1 one another way to say that is that A1
contains all the solutions to our search contains all the solutions to our search contains all the solutions to our search
problem while a z contains all of the problem while a z contains all of the problem while a z contains all of the
non-solutions we'll be especially non-solutions we'll be especially non-solutions we'll be especially
interested in two Quantum States one is interested in two Quantum States one is interested in two Quantum States one is
the uniform superposition over all of the uniform superposition over all of the uniform superposition over all of
the non-solutions that's k a z and the the non-solutions that's k a z and the the non-solutions that's k a z and the
other is uniform over all the solutions other is uniform over all the solutions other is uniform over all the solutions
and that one is Kat A1 I should point and that one is Kat A1 I should point and that one is Kat A1 I should point
out that these definitions only make out that these definitions only make out that these definitions only make
sense when these sets are non-empty sense when these sets are non-empty sense when these sets are non-empty
because otherwise we're summing up zero because otherwise we're summing up zero because otherwise we're summing up zero
things and dividing by zero and as we go things and dividing by zero and as we go things and dividing by zero and as we go
through this analysis of Grover's through this analysis of Grover's through this analysis of Grover's
algorithm we will be making that algorithm we will be making that algorithm we will be making that
assumption assumption assumption
we can very easily figure out what we can very easily figure out what we can very easily figure out what
Grover's algorithm does when f is either Grover's algorithm does when f is either Grover's algorithm does when f is either
the constant zero function or the the constant zero function or the the constant zero function or the
constant one function by treating them constant one function by treating them constant one function by treating them
as special cases but I will leave that as special cases but I will leave that as special cases but I will leave that
for you to do on your own not for you to do on your own not for you to do on your own not
surprisingly if every string is a surprisingly if every string is a surprisingly if every string is a
solution we will indeed obtain a solution we will indeed obtain a solution we will indeed obtain a
solution when we measure and if there solution when we measure and if there solution when we measure and if there
aren't any solutions then we won't aren't any solutions then we won't aren't any solutions then we won't
obtain one because in that case there obtain one because in that case there obtain one because in that case there
aren't aren't aren't
any but for the sake of this analysis any but for the sake of this analysis any but for the sake of this analysis
we're going to assume that both of these we're going to assume that both of these we're going to assume that both of these
sets are nonm empty so that these sets are nonm empty so that these sets are nonm empty so that these
definitions of these two states make definitions of these two states make definitions of these two states make
sense by the way this is a pretty common sense by the way this is a pretty common sense by the way this is a pretty common
notation whenever we have a non-empty notation whenever we have a non-empty notation whenever we have a non-empty
finite set s we can write c s to mean finite set s we can write c s to mean finite set s we can write c s to mean
the uniform superposition over the the uniform superposition over the the uniform superposition over the
elements of that set just like we have elements of that set just like we have elements of that set just like we have
here so let's collect these definitions here so let's collect these definitions here so let's collect these definitions
like this and we'll see how they connect like this and we'll see how they connect like this and we'll see how they connect
to Grover's to Grover's to Grover's
algorithm the first step of Grower's algorithm the first step of Grower's algorithm the first step of Grower's
algorithm is the initialization step algorithm is the initialization step algorithm is the initialization step
which puts Q into a uni form which puts Q into a uni form which puts Q into a uni form
superposition over all nbit strings and superposition over all nbit strings and superposition over all nbit strings and
just to give this state an easily just to give this state an easily just to give this state an easily
recognizable name I'm going to call it recognizable name I'm going to call it recognizable name I'm going to call it
cat u u is short for cat u u is short for cat u u is short for
uniform it's important for the sake of uniform it's important for the sake of uniform it's important for the sake of
the analysis that we first note that the analysis that we first note that the analysis that we first note that
this uniform State Catu is contained in this uniform State Catu is contained in this uniform State Catu is contained in
the Subspace spanned by the two vectors the Subspace spanned by the two vectors the Subspace spanned by the two vectors
K a0 and K K a0 and K K a0 and K
A1 in particular if we take the linear A1 in particular if we take the linear A1 in particular if we take the linear
combination of these two states that's combination of these two states that's combination of these two states that's
shown here on the screen we'll get cat U now the key to the entire analysis is U now the key to the entire analysis is
the fact that the state of Q remains in the fact that the state of Q remains in the fact that the state of Q remains in
this two-dimensional Subspace no matter this two-dimensional Subspace no matter this two-dimensional Subspace no matter
how many times we apply the grover how many times we apply the grover how many times we apply the grover
operation so everything is going to boil operation so everything is going to boil operation so everything is going to boil
down to what happens in a down to what happens in a down to what happens in a
two-dimensional two-dimensional two-dimensional
Subspace now let's see what the grover Subspace now let's see what the grover Subspace now let's see what the grover
operation does and in particular what it operation does and in particular what it operation does and in particular what it
does to vectors in the two-dimensional does to vectors in the two-dimensional does to vectors in the two-dimensional
Subspace spanned by cat a0 and K Subspace spanned by cat a0 and K Subspace spanned by cat a0 and K
A1 it's helpful to imagine that g is A1 it's helpful to imagine that g is A1 it's helpful to imagine that g is
split into two parts one part is the ZF split into two parts one part is the ZF split into two parts one part is the ZF
gate and the other part is the zor gate gate and the other part is the zor gate gate and the other part is the zor gate
sandwiched between two layers of sandwiched between two layers of sandwiched between two layers of
hadamard hadamard hadamard
gates here's the definition of ZF from gates here's the definition of ZF from gates here's the definition of ZF from
before and it's pretty straightforward before and it's pretty straightforward before and it's pretty straightforward
from that definition to see that its from that definition to see that its from that definition to see that its
action on our two states the uniform action on our two states the uniform action on our two states the uniform
superpositions of non-solutions and superpositions of non-solutions and superpositions of non-solutions and
solutions is simply to put a minus sign solutions is simply to put a minus sign solutions is simply to put a minus sign
in front of K A1 well nothing happens to in front of K A1 well nothing happens to in front of K A1 well nothing happens to
K K K
a0 and that's all we really need to know a0 and that's all we really need to know a0 and that's all we really need to know
about ZF for the sake of this analysis for the second part which is analysis for the second part which is
the Z orgate sandwi between two layers the Z orgate sandwi between two layers the Z orgate sandwi between two layers
of hatar gates let's start by recalling of hatar gates let's start by recalling of hatar gates let's start by recalling
how zor is defined one way to write it how zor is defined one way to write it how zor is defined one way to write it
is as is shown on the screen this is the is as is shown on the screen this is the is as is shown on the screen this is the
same expression as before and another same expression as before and another same expression as before and another
way to express it using the direct way to express it using the direct way to express it using the direct
notation is like this notation is like this notation is like this
and you can verify this expression just and you can verify this expression just and you can verify this expression just
by thinking about how the operation on by thinking about how the operation on by thinking about how the operation on
the right hand side acts on standard the right hand side acts on standard the right hand side acts on standard
basis States in particular it does basis States in particular it does basis States in particular it does
nothing to the all zero State and every nothing to the all zero State and every nothing to the all zero State and every
other standard basis state gets a minus other standard basis state gets a minus other standard basis state gets a minus
sign which agrees with the definition of sign which agrees with the definition of sign which agrees with the definition of
Z Z Z
or so if we sandwich this operation or so if we sandwich this operation or so if we sandwich this operation
between two layers of hadamard gates between two layers of hadamard gates between two layers of hadamard gates
then we get a similar expression except then we get a similar expression except then we get a similar expression except
that in place of the all zero standard that in place of the all zero standard that in place of the all zero standard
basis State we now have our uniform sub basis State we now have our uniform sub basis State we now have our uniform sub
position k u and that's because H tensor position k u and that's because H tensor position k u and that's because H tensor
n transforms the L zero standard basis n transforms the L zero standard basis n transforms the L zero standard basis
state to k u and also H tensor n is its state to k u and also H tensor n is its state to k u and also H tensor n is its
own conjugate transpose so it works in a own conjugate transpose so it works in a own conjugate transpose so it works in a
similar way acting from the right on the similar way acting from the right on the similar way acting from the right on the
bra rather than the cat and now we're going to figure out cat and now we're going to figure out
what G does to our two vectors a0 and A1 what G does to our two vectors a0 and A1 what G does to our two vectors a0 and A1
using the facts that we've just using the facts that we've just using the facts that we've just
observed let's start with observed let's start with observed let's start with
a0 first let's sub itute the expression a0 first let's sub itute the expression a0 first let's sub itute the expression
we obtained for the Z orgate sandwiched we obtained for the Z orgate sandwiched we obtained for the Z orgate sandwiched
between two layers of hadamar Gates next between two layers of hadamar Gates next between two layers of hadamar Gates next
we can use the fact that ZF does nothing we can use the fact that ZF does nothing we can use the fact that ZF does nothing
to cat a z so we can get rid of the ZF to cat a z so we can get rid of the ZF to cat a z so we can get rid of the ZF
operation and now using the description operation and now using the description operation and now using the description
of cat U that we have up here along with of cat U that we have up here along with of cat U that we have up here along with
the fact that K a0 and K A1 are the fact that K a0 and K A1 are the fact that K a0 and K A1 are
orthogonal unit vectors we can compute orthogonal unit vectors we can compute orthogonal unit vectors we can compute
the product of this bra right here and the product of this bra right here and the product of this bra right here and
this cat and what we get is the square this cat and what we get is the square this cat and what we get is the square
root of the number of nons Solutions root of the number of nons Solutions root of the number of nons Solutions
divided by n we can use this fact to divided by n we can use this fact to divided by n we can use this fact to
evaluate what we get and by substituting evaluate what we get and by substituting evaluate what we get and by substituting
in our expression for cat U and in our expression for cat U and in our expression for cat U and
simplifying we get the final result simplifying we get the final result simplifying we get the final result
that's shown here on the that's shown here on the that's shown here on the
screen in particular notice that this screen in particular notice that this screen in particular notice that this
Vector is contained in the Vector is contained in the Vector is contained in the
two-dimensional Subspace span by K a0 two-dimensional Subspace span by K a0 two-dimensional Subspace span by K a0
and K A1 as I suggested before and and K A1 as I suggested before and and K A1 as I suggested before and
moreover the coefficients happen to be moreover the coefficients happen to be moreover the coefficients happen to be
real real real
numbers as always verify this at your numbers as always verify this at your numbers as always verify this at your
own pace if you choose to do that I tend own pace if you choose to do that I tend own pace if you choose to do that I tend
to go through these sorts of things to go through these sorts of things to go through these sorts of things
quickly in these videos with the quickly in these videos with the quickly in these videos with the
expectation that you'll pause the video expectation that you'll pause the video expectation that you'll pause the video
to give it some thought if you choose to to give it some thought if you choose to to give it some thought if you choose to
do that so let's clean this up so that do that so let's clean this up so that do that so let's clean this up so that
we can save the final result for we can save the final result for we can save the final result for
later and now we'll do something similar later and now we'll do something similar later and now we'll do something similar
for the vector k for the vector k for the vector k
A1 we can use the same expression of G A1 we can use the same expression of G A1 we can use the same expression of G
as as as
before and this time the ZF gate injects before and this time the ZF gate injects before and this time the ZF gate injects
a minus sign which here is reflected by a minus sign which here is reflected by a minus sign which here is reflected by
the fact that we've swapped the two the fact that we've swapped the two the fact that we've swapped the two
terms in the operation in terms in the operation in terms in the operation in
parenthesis we then proceed in a very parenthesis we then proceed in a very parenthesis we then proceed in a very
similar way and the final result is similar way and the final result is similar way and the final result is
similar except that the two coefficients similar except that the two coefficients similar except that the two coefficients
are swapped and we now have a minus sign are swapped and we now have a minus sign are swapped and we now have a minus sign
in front of one of them which is just in front of one of them which is just in front of one of them which is just
how the algebra works out and again you how the algebra works out and again you how the algebra works out and again you
can check this at your own pace notice can check this at your own pace notice can check this at your own pace notice
that once again we have a real linear that once again we have a real linear that once again we have a real linear
combination of our two vectors K a0 and combination of our two vectors K a0 and combination of our two vectors K a0 and
K A1 K A1 K A1
we'll clean this one up as well and we'll clean this one up as well and we'll clean this one up as well and
we'll put the end result together with we'll put the end result together with we'll put the end result together with
the other one and now we can see how G the other one and now we can see how G the other one and now we can see how G
acts on the Subspace spanned by our two acts on the Subspace spanned by our two acts on the Subspace spanned by our two
vectors one way to do that is to express vectors one way to do that is to express vectors one way to do that is to express
it in terms of the 2x2 matrix M that's it in terms of the 2x2 matrix M that's it in terms of the 2x2 matrix M that's
shown here on the screen and just to be shown here on the screen and just to be shown here on the screen and just to be
clear what we're doing here is we're clear what we're doing here is we're clear what we're doing here is we're
associating the first row and the First associating the first row and the First associating the first row and the First
Column with K a0 and the second row and Column with K a0 and the second row and Column with K a0 and the second row and
column with Kat column with Kat column with Kat
A1 so far in the series we we really A1 so far in the series we we really A1 so far in the series we we really
just used matrices to describe the way just used matrices to describe the way just used matrices to describe the way
that operations work on standard basis that operations work on standard basis that operations work on standard basis
States but it is common to use matrices States but it is common to use matrices States but it is common to use matrices
to describe the actions of linear maps to describe the actions of linear maps to describe the actions of linear maps
on different sets of vectors not just on different sets of vectors not just on different sets of vectors not just
standard basis States and here we're standard basis States and here we're standard basis States and here we're
talking about what the operation G does talking about what the operation G does talking about what the operation G does
to the vectors K a0 and K A1 we can say to the vectors K a0 and K A1 we can say to the vectors K a0 and K A1 we can say
more about what G does to vectors in the more about what G does to vectors in the more about what G does to vectors in the
span of K a0 and K A1 if we take a span of K a0 and K A1 if we take a span of K a0 and K A1 if we take a
closer look at this Matrix closer look at this Matrix closer look at this Matrix
M and in particular what we'll find is M and in particular what we'll find is M and in particular what we'll find is
that m is a rotation Matrix meaning that that m is a rotation Matrix meaning that that m is a rotation Matrix meaning that
its action on this two-dimensional space its action on this two-dimensional space its action on this two-dimensional space
can be described as a rotation by some can be described as a rotation by some can be described as a rotation by some
fixed fixed fixed
angle there are a couple of different angle there are a couple of different angle there are a couple of different
ways to see this and one is to observe ways to see this and one is to observe ways to see this and one is to observe
that this Matrix M can be expressed as that this Matrix M can be expressed as that this Matrix M can be expressed as
the square of a matrix that looks the square of a matrix that looks the square of a matrix that looks
somewhat somewhat somewhat
simpler that might be hard to spot if simpler that might be hard to spot if simpler that might be hard to spot if
we're just looking at M by itself but we're just looking at M by itself but we're just looking at M by itself but
this isn't something that we need to this isn't something that we need to this isn't something that we need to
discover on our own we just need to discover on our own we just need to discover on our own we just need to
verify it and we can do that by simply verify it and we can do that by simply verify it and we can do that by simply
squaring The Matrix on the squaring The Matrix on the squaring The Matrix on the
right another way to write the Matrix on right another way to write the Matrix on right another way to write the Matrix on
the right not including the square is the right not including the square is the right not including the square is
shown on the screen and this is the shown on the screen and this is the shown on the screen and this is the
typical way to describe a rotation by a typical way to describe a rotation by a typical way to describe a rotation by a
given angle Theta as a given angle Theta as a given angle Theta as a
matrix the precise angle by which we're matrix the precise angle by which we're matrix the precise angle by which we're
rotating happens to be the inverse sign rotating happens to be the inverse sign rotating happens to be the inverse sign
of the square root of the number of of the square root of the number of of the square root of the number of
solutions divided by n and by the way solutions divided by n and by the way solutions divided by n and by the way
because this is the first time that because this is the first time that because this is the first time that
we're seeing it let me point out that we're seeing it let me point out that we're seeing it let me point out that
this angle is very important it's going this angle is very important it's going this angle is very important it's going
to keep keep showing up as we continue to keep keep showing up as we continue to keep keep showing up as we continue
with this with this with this
analysis the argument of the inverse analysis the argument of the inverse analysis the argument of the inverse
sign is a number between 0 and 1 and sign is a number between 0 and 1 and sign is a number between 0 and 1 and
specifically the way that we're specifically the way that we're specifically the way that we're
interpreting the inverse sign here is interpreting the inverse sign here is interpreting the inverse sign here is
that it's the angle between 0 and Pi / 2 that it's the angle between 0 and Pi / 2 that it's the angle between 0 and Pi / 2
that gives us the argument when we take that gives us the argument when we take that gives us the argument when we take
the the the
sign so that's one way to think about sign so that's one way to think about sign so that's one way to think about
this Matrix that we squared to get this Matrix that we squared to get this Matrix that we squared to get
M and because squaring The Matrix is the M and because squaring The Matrix is the M and because squaring The Matrix is the
same thing as applying it twice we see same thing as applying it twice we see same thing as applying it twice we see
that M can alternatively be written as a that M can alternatively be written as a that M can alternatively be written as a
rotation not by an angle of theta but by rotation not by an angle of theta but by rotation not by an angle of theta but by
two * Theta because by squaring we two * Theta because by squaring we two * Theta because by squaring we
rotate by an angle of theta rotate by an angle of theta rotate by an angle of theta
twice a different way to verify all this twice a different way to verify all this twice a different way to verify all this
by the way in case it's helpful is to by the way in case it's helpful is to by the way in case it's helpful is to
start with this final expression of M start with this final expression of M start with this final expression of M
and use the double angle formulas from and use the double angle formulas from and use the double angle formulas from
trigonometry to obtain the original trigonometry to obtain the original trigonometry to obtain the original
expression of expression of expression of
M and this is the expression of M which M and this is the expression of M which M and this is the expression of M which
describes the action that g has on the describes the action that g has on the describes the action that g has on the
Subspace Spann by K a0 and K A1 that's Subspace Spann by K a0 and K A1 that's Subspace Spann by K a0 and K A1 that's
going to be most useful going going to be most useful going going to be most useful going
forward now returning to Grover's forward now returning to Grover's forward now returning to Grover's
algorithm itself after the algorithm itself after the algorithm itself after the
initialization step but before we initialization step but before we initialization step but before we
perform any applications of G the state perform any applications of G the state perform any applications of G the state
of our n Cubit register Q is Catu the of our n Cubit register Q is Catu the of our n Cubit register Q is Catu the
uniform superposition over all nbit uniform superposition over all nbit uniform superposition over all nbit
strings and here we have the same strings and here we have the same strings and here we have the same
expression of cat u in terms of K a0 and expression of cat u in terms of K a0 and expression of cat u in terms of K a0 and
K A1 as K A1 as K A1 as
before but now that we've identified before but now that we've identified before but now that we've identified
this angle Theta we see that that we this angle Theta we see that that we this angle Theta we see that that we
could alternatively write k u as cosine could alternatively write k u as cosine could alternatively write k u as cosine
Theta * k a 0 plus sin Theta * K Theta * k a 0 plus sin Theta * K Theta * k a 0 plus sin Theta * K
A1 that's clear for the coefficient of K A1 that's clear for the coefficient of K A1 that's clear for the coefficient of K
A1 and for the other coefficient we just A1 and for the other coefficient we just A1 and for the other coefficient we just
need to observe that the size of a0 plus need to observe that the size of a0 plus need to observe that the size of a0 plus
the size of A1 is n and then we can just the size of A1 is n and then we can just the size of A1 is n and then we can just
get it from some basic get it from some basic get it from some basic
trigonometry so that's the state we trigonometry so that's the state we trigonometry so that's the state we
start with and the first time that we start with and the first time that we start with and the first time that we
apply G we rotate by an angle of 2 * apply G we rotate by an angle of 2 * apply G we rotate by an angle of 2 *
Theta so the total angle becomes 3 * Theta so the total angle becomes 3 * Theta so the total angle becomes 3 *
Theta and if we apply G again the angle Theta and if we apply G again the angle Theta and if we apply G again the angle
increases to 5 Theta because we've increases to 5 Theta because we've increases to 5 Theta because we've
effectively rotated by an additional effectively rotated by an additional effectively rotated by an additional
angle of 2 Theta and so on so after T angle of 2 Theta and so on so after T angle of 2 Theta and so on so after T
applications of G we can represent the applications of G we can represent the applications of G we can represent the
state that we get as is shown on the state that we get as is shown on the state that we get as is shown on the
screen where the angle of our state is screen where the angle of our state is screen where the angle of our state is
equal to 2T + 1 * Theta that's 2 * Theta equal to 2T + 1 * Theta that's 2 * Theta equal to 2T + 1 * Theta that's 2 * Theta
for each application of G along with an for each application of G along with an for each application of G along with an
additional Theta because that's the additional Theta because that's the additional Theta because that's the
state we started in state we started in state we started in
we can also think about the action of we can also think about the action of we can also think about the action of
the grover operation in geometric terms the grover operation in geometric terms the grover operation in geometric terms
and this helps to shed some light on how and this helps to shed some light on how and this helps to shed some light on how
it works and why its effect is to rotate it works and why its effect is to rotate it works and why its effect is to rotate
the state of the register the state of the register the state of the register
Q here's the main idea the grover Q here's the main idea the grover Q here's the main idea the grover
operation is a composition of the two operation is a composition of the two operation is a composition of the two
parts that we've already identified one parts that we've already identified one parts that we've already identified one
is ZF and the other is z or send between is ZF and the other is z or send between is ZF and the other is z or send between
two layers of hadamar gates these two parts are gates these two parts are
reflections and when we compose two reflections and when we compose two reflections and when we compose two
Reflections we obtain a Reflections we obtain a Reflections we obtain a
rotation so let's see how this works in rotation so let's see how this works in rotation so let's see how this works in
more more more
detail first let's focus on ZF and we're detail first let's focus on ZF and we're detail first let's focus on ZF and we're
going to restrict our attention to the going to restrict our attention to the going to restrict our attention to the
same two-dimensional Subspace that we've same two-dimensional Subspace that we've same two-dimensional Subspace that we've
been talking about meaning the one been talking about meaning the one been talking about meaning the one
that's spanned by cat a0 and Kat that's spanned by cat a0 and Kat that's spanned by cat a0 and Kat
A1 as we already observed the action of A1 as we already observed the action of A1 as we already observed the action of
ZF on this space is quite simple it does ZF on this space is quite simple it does ZF on this space is quite simple it does
nothing to cat a z and it puts a minus nothing to cat a z and it puts a minus nothing to cat a z and it puts a minus
sign in front of K sign in front of K sign in front of K
A1 this is a reflection and specifically A1 this is a reflection and specifically A1 this is a reflection and specifically
it's a reflection about the line it's a reflection about the line it's a reflection about the line
parallel to K a0 which we'll call L1 parallel to K a0 which we'll call L1 parallel to K a0 which we'll call L1
just to give it a just to give it a just to give it a
name to see why it's a reflection it name to see why it's a reflection it name to see why it's a reflection it
helps to draw a helps to draw a helps to draw a
picture here we have our two vectors K picture here we have our two vectors K picture here we have our two vectors K
a0 and K A1 which are orthogonal unit a0 and K A1 which are orthogonal unit a0 and K A1 which are orthogonal unit
vectors and we can think about the vectors and we can think about the vectors and we can think about the
screen as consisting of all of the real screen as consisting of all of the real screen as consisting of all of the real
linear combinations of these two vectors linear combinations of these two vectors linear combinations of these two vectors
so the circle represents all of the unit so the circle represents all of the unit so the circle represents all of the unit
vectors for which the coefficients of vectors for which the coefficients of vectors for which the coefficients of
these two vectors are real these two vectors are real these two vectors are real
numbers here's the line L1 which is numbers here's the line L1 which is numbers here's the line L1 which is
parallel to K a0 and if we consider some parallel to K a0 and if we consider some parallel to K a0 and if we consider some
arbitrary Vector s on this circle then arbitrary Vector s on this circle then arbitrary Vector s on this circle then
the effect of applying ZF is to reflect the effect of applying ZF is to reflect the effect of applying ZF is to reflect
about this line as is Illustrated on the about this line as is Illustrated on the about this line as is Illustrated on the
screen simply because we do nothing to k screen simply because we do nothing to k screen simply because we do nothing to k
a z and we put a minus sign in front of a z and we put a minus sign in front of a z and we put a minus sign in front of
K K K
A1 in the special case that s is k a A1 in the special case that s is k a A1 in the special case that s is k a
zero for instance nothing happens when zero for instance nothing happens when zero for instance nothing happens when
we perform this reflection and when s is we perform this reflection and when s is we perform this reflection and when s is
K A1 the reflection takes us all the way K A1 the reflection takes us all the way K A1 the reflection takes us all the way
to negative K A1 and that's consistent to negative K A1 and that's consistent to negative K A1 and that's consistent
with the action of ZF so ZF is a with the action of ZF so ZF is a with the action of ZF so ZF is a
reflection that can be visualized as we reflection that can be visualized as we reflection that can be visualized as we
have here the second part of G which is z or here the second part of G which is z or
sandwich between two layers of hatar sandwich between two layers of hatar sandwich between two layers of hatar
gates can be expressed as 2 * K Bru gates can be expressed as 2 * K Bru gates can be expressed as 2 * K Bru
minus the identity Matrix as we've minus the identity Matrix as we've minus the identity Matrix as we've
already scene and again this is a already scene and again this is a already scene and again this is a
reflection this time about the line L2 reflection this time about the line L2 reflection this time about the line L2
parallel to k u rather than K a0 we can visualize this operation in a a0 we can visualize this operation in a
similar way to similar way to similar way to
ZF here's the vector k ZF here's the vector k ZF here's the vector k
hypothetically so this is the line L2 hypothetically so this is the line L2 hypothetically so this is the line L2
parallel to K and once again the action parallel to K and once again the action parallel to K and once again the action
on an arbitrary Vector S is to reflect on an arbitrary Vector S is to reflect on an arbitrary Vector S is to reflect
it about the line which is L2 in this it about the line which is L2 in this it about the line which is L2 in this
case and finally as I mentioned before and finally as I mentioned before
whenever we compose two Reflections we whenever we compose two Reflections we whenever we compose two Reflections we
obtain a rotation and specifically it's obtain a rotation and specifically it's obtain a rotation and specifically it's
a rotation by twice the angle between a rotation by twice the angle between a rotation by twice the angle between
the lines of reflection so let's draw a similar reflection so let's draw a similar
picture to what we had picture to what we had picture to what we had
before here are the lines of reflection before here are the lines of reflection before here are the lines of reflection
which have the angle Theta between which have the angle Theta between which have the angle Theta between
them and if we take some arbitrary them and if we take some arbitrary them and if we take some arbitrary
Vector Vector Vector
s reflected around the first line L1 and s reflected around the first line L1 and s reflected around the first line L1 and
then reflected about the second line L2 then reflected about the second line L2 then reflected about the second line L2
then the net effect will be to rotate then the net effect will be to rotate then the net effect will be to rotate
the vector s by an angle of twice Theta the vector s by an angle of twice Theta the vector s by an angle of twice Theta
and this is something that's true in and this is something that's true in and this is something that's true in
general and you can try it out on some general and you can try it out on some general and you can try it out on some
additional examples of your own to see additional examples of your own to see additional examples of your own to see
that this is indeed the way it that this is indeed the way it that this is indeed the way it
works anyway hopefully this helps you to works anyway hopefully this helps you to works anyway hopefully this helps you to
get some intuition as to why the grover get some intuition as to why the grover get some intuition as to why the grover
operation has the effect of rotating operation has the effect of rotating operation has the effect of rotating
vectors next we'll turn to the issue of vectors next we'll turn to the issue of vectors next we'll turn to the issue of
choosing the the number of iterations T choosing the the number of iterations T choosing the the number of iterations T
in Grover's in Grover's in Grover's
algorithm let's start by thinking about algorithm let's start by thinking about algorithm let's start by thinking about
how the analysis we've seen so far which how the analysis we've seen so far which how the analysis we've seen so far which
reveals what the grover operation does reveals what the grover operation does reveals what the grover operation does
when we apply it repeatedly to the when we apply it repeatedly to the when we apply it repeatedly to the
uniform superposition relates to the uniform superposition relates to the uniform superposition relates to the
task that we're hoping to solve which is task that we're hoping to solve which is task that we're hoping to solve which is
to find a solution to our search problem to find a solution to our search problem to find a solution to our search problem
or in other words any string contained or in other words any string contained or in other words any string contained
in the set in the set in the set
A1 in short we're basically going to set A1 in short we're basically going to set A1 in short we're basically going to set
a Target and then figure out how many a Target and then figure out how many a Target and then figure out how many
times to apply the grover operation to times to apply the grover operation to times to apply the grover operation to
get close to that get close to that get close to that
Target first let's imagine that we have Target first let's imagine that we have Target first let's imagine that we have
any state that's a linear combination of any state that's a linear combination of any state that's a linear combination of
our two vectors K a0 and K our two vectors K a0 and K our two vectors K a0 and K
A1 if we perform a standard basis A1 if we perform a standard basis A1 if we perform a standard basis
measurement on this state we're going to measurement on this state we're going to measurement on this state we're going to
get an outcome in the set A1 which is get an outcome in the set A1 which is get an outcome in the set A1 which is
the set of solutions to our search the set of solutions to our search the set of solutions to our search
problem with probability equal to the problem with probability equal to the problem with probability equal to the
absolute value squar of whatever absolute value squar of whatever absolute value squar of whatever
coefficient we have for K A1 and for the coefficient we have for K A1 and for the coefficient we have for K A1 and for the
vector that's shown on the screen that vector that's shown on the screen that vector that's shown on the screen that
coefficient is beta this is is intuitive coefficient is beta this is is intuitive coefficient is beta this is is intuitive
if we were to measure the state K a0 if we were to measure the state K a0 if we were to measure the state K a0
we'd get a random nons solution and if we'd get a random nons solution and if we'd get a random nons solution and if
we measured K A1 we'd get a random we measured K A1 we'd get a random we measured K A1 we'd get a random
solution so if we measure this state solution so if we measure this state solution so if we measure this state
right here we'll get a random right here we'll get a random right here we'll get a random
non-solution with probability absolute non-solution with probability absolute non-solution with probability absolute
value of Alpha squ and a random solution value of Alpha squ and a random solution value of Alpha squ and a random solution
with probability absolute value of beta with probability absolute value of beta with probability absolute value of beta
squ but we can also just take a moment squ but we can also just take a moment squ but we can also just take a moment
to work it out here's the State written to work it out here's the State written to work it out here's the State written
out so that we can see what the entries out so that we can see what the entries out so that we can see what the entries
of this Vector are for standard bis of this Vector are for standard bis of this Vector are for standard bis
States so if we measure these are the States so if we measure these are the States so if we measure these are the
probabilities to obtain each possible probabilities to obtain each possible probabilities to obtain each possible
outcome and to get the probability to outcome and to get the probability to outcome and to get the probability to
obtain a solution we sum the obtain a solution we sum the obtain a solution we sum the
probabilities for all of the possible probabilities for all of the possible probabilities for all of the possible
solutions which gives us the absolute solutions which gives us the absolute solutions which gives us the absolute
value of beta squar as value of beta squar as value of beta squar as
expected now we've already analyzed what expected now we've already analyzed what expected now we've already analyzed what
happens when we run Grover's algorithm happens when we run Grover's algorithm happens when we run Grover's algorithm
for T iterations we get the state that's for T iterations we get the state that's for T iterations we get the state that's
shown here on the shown here on the shown here on the
screen so screen so screen so
because the coefficient of K A1 is the S because the coefficient of K A1 is the S because the coefficient of K A1 is the S
of 2T + 1 * Theta where once again here of 2T + 1 * Theta where once again here of 2T + 1 * Theta where once again here
is this very important angle Theta we is this very important angle Theta we is this very important angle Theta we
will see a solution with probability will see a solution with probability will see a solution with probability
sin^2 of 2T +1 * sin^2 of 2T +1 * sin^2 of 2T +1 *
Theta we'd like to make that probability Theta we'd like to make that probability Theta we'd like to make that probability
large ideally as close to one as we large ideally as close to one as we large ideally as close to one as we
can so as we're rotating the state can so as we're rotating the state can so as we're rotating the state
vector by iterating G we'll think about vector by iterating G we'll think about vector by iterating G we'll think about
K A1 as the target state and we'll try K A1 as the target state and we'll try K A1 as the target state and we'll try
to pick T so that we land close to this to pick T so that we land close to this to pick T so that we land close to this
state and as we're doing this it's good state and as we're doing this it's good state and as we're doing this it's good
to keep in mind that we'd also like to to keep in mind that we'd also like to to keep in mind that we'd also like to
minimize T because that's the number of minimize T because that's the number of minimize T because that's the number of
queries we need to make so if we want to make this make so if we want to make this
probability close to one and minimize T probability close to one and minimize T probability close to one and minimize T
it Mak sense to try to make the angle of it Mak sense to try to make the angle of it Mak sense to try to make the angle of
our state Vector close to Pi / 2 because our state Vector close to Pi / 2 because our state Vector close to Pi / 2 because
that's the smallest angle that makes the that's the smallest angle that makes the that's the smallest angle that makes the
square of the sign large so it's square of the sign large so it's square of the sign large so it's
basically our first opportunity basically our first opportunity basically our first opportunity
to make the probability large as we're to make the probability large as we're to make the probability large as we're
rotating the state rotating the state rotating the state
Vector now we can't necessarily hit this Vector now we can't necessarily hit this Vector now we can't necessarily hit this
angle pi over 2 exactly because T has to angle pi over 2 exactly because T has to angle pi over 2 exactly because T has to
be an be an be an
integer but if we think about it integer but if we think about it integer but if we think about it
approximately and solve for t as if it approximately and solve for t as if it approximately and solve for t as if it
were inequality then we see that ideally were inequality then we see that ideally were inequality then we see that ideally
choosing T to be pi over 4 * Theta minus choosing T to be pi over 4 * Theta minus choosing T to be pi over 4 * Theta minus
12 makes 12 makes 12 makes
sense but of course T must be a non- sense but of course T must be a non- sense but of course T must be a non-
negative integer so what we'll do is to negative integer so what we'll do is to negative integer so what we'll do is to
take the closest integer to what we're take the closest integer to what we're take the closest integer to what we're
imagining is this ideal value which is imagining is this ideal value which is imagining is this ideal value which is
the floor of pi divided 4 * the floor of pi divided 4 * the floor of pi divided 4 *
Theta taking the floor by the way just Theta taking the floor by the way just Theta taking the floor by the way just
means that we throw away any fractional means that we throw away any fractional means that we throw away any fractional
part so in other words it's the largest part so in other words it's the largest part so in other words it's the largest
integer that's less than or equal to integer that's less than or equal to integer that's less than or equal to
whatever it is that we're taking the whatever it is that we're taking the whatever it is that we're taking the
floor floor floor
of according to this expression if the of according to this expression if the of according to this expression if the
ideal value happens to be halfway ideal value happens to be halfway ideal value happens to be halfway
between integers we'll round up but that between integers we'll round up but that between integers we'll round up but that
won't actually matter at all for for the won't actually matter at all for for the won't actually matter at all for for the
sake of this sake of this sake of this
lesson so going forward we're going to lesson so going forward we're going to lesson so going forward we're going to
be focusing on this particular choice of be focusing on this particular choice of be focusing on this particular choice of
T but there is one other important T but there is one other important T but there is one other important
consideration that we need to make which consideration that we need to make which consideration that we need to make which
is that this specification of T depends is that this specification of T depends is that this specification of T depends
on how many solutions there are because on how many solutions there are because on how many solutions there are because
the angle Theta depends on the number of solutions Hereafter I'm going to use the solutions Hereafter I'm going to use the
letter s to refer to the number of letter s to refer to the number of letter s to refer to the number of
solutions or in other words the size of solutions or in other words the size of solutions or in other words the size of
the set a one just for the sake of the set a one just for the sake of the set a one just for the sake of
convenience anyway the point is that our convenience anyway the point is that our convenience anyway the point is that our
expression for T the value that we're expression for T the value that we're expression for T the value that we're
hypothesizing should be good depends on hypothesizing should be good depends on hypothesizing should be good depends on
Theta and Theta depends on S which is Theta and Theta depends on S which is Theta and Theta depends on S which is
the size of the set the size of the set the size of the set
A1 this isn't a problem if we know in A1 this isn't a problem if we know in A1 this isn't a problem if we know in
advance how many solutions there are but advance how many solutions there are but advance how many solutions there are but
if we don't know how many solutions if we don't know how many solutions if we don't know how many solutions
there are in advance we don't know what there are in advance we don't know what there are in advance we don't know what
Theta is so we can't pick T like this Theta is so we can't pick T like this Theta is so we can't pick T like this
and we'll have to do something different and we'll have to do something different and we'll have to do something different
I'll come back to the situation where we I'll come back to the situation where we I'll come back to the situation where we
don't know s a little bit later but to don't know s a little bit later but to don't know s a little bit later but to
start let's focus on the situation in start let's focus on the situation in start let's focus on the situation in
which we do know how many solutions which we do know how many solutions which we do know how many solutions
there are beginning with unique search there are beginning with unique search there are beginning with unique search
where there's guaranteed to be a single where there's guaranteed to be a single where there's guaranteed to be a single
unique solution that we're looking for I mentioned the unique search for I mentioned the unique search
problem as an example of a query problem problem as an example of a query problem problem as an example of a query problem
back in lesson back in lesson back in lesson
five the input is a function f from five the input is a function f from five the input is a function f from
nbits to one bit there's a unique n bit nbits to one bit there's a unique n bit nbits to one bit there's a unique n bit
string that causes F to evaluate to one string that causes F to evaluate to one string that causes F to evaluate to one
and the goal is to Output that string so and the goal is to Output that string so and the goal is to Output that string so
this is equivalent to the unstructured this is equivalent to the unstructured this is equivalent to the unstructured
search problem we've been talking about search problem we've been talking about search problem we've been talking about
but with the promise that there's but with the promise that there's but with the promise that there's
exactly one solution and in fact this is exactly one solution and in fact this is exactly one solution and in fact this is
the only version of the search problem the only version of the search problem the only version of the search problem
that love Grover considered in his that love Grover considered in his that love Grover considered in his
original 1996 paper that introduced original 1996 paper that introduced original 1996 paper that introduced
Grover's Grover's Grover's
algorithm for Unique search we have that algorithm for Unique search we have that algorithm for Unique search we have that
s equals 1 and therefore this important s equals 1 and therefore this important s equals 1 and therefore this important
angle Theta is equal to the inverse sign angle Theta is equal to the inverse sign angle Theta is equal to the inverse sign
of the square root of 1 of the square root of 1 of the square root of 1
n when n is large this value is n when n is large this value is n when n is large this value is
approximately theare root of 1/ N and approximately theare root of 1/ N and approximately theare root of 1/ N and
that's because for smaller and smaller that's because for smaller and smaller that's because for smaller and smaller
angles the sign function starts to look angles the sign function starts to look angles the sign function starts to look
more and more like the identity more and more like the identity more and more like the identity
function if we substitute that function if we substitute that function if we substitute that
approximation into our selection for T approximation into our selection for T approximation into our selection for T
we see that t is approximately the floor we see that t is approximately the floor we see that t is approximately the floor
of pi over 4 * the &lt; TK of of pi over 4 * the &lt; TK of of pi over 4 * the &lt; TK of
n that's good because we're on track to n that's good because we're on track to n that's good because we're on track to
be using Big O of the square root of n be using Big O of the square root of n be using Big O of the square root of n
queries though at this point we've yet queries though at this point we've yet queries though at this point we've yet
to take a look at exactly how well the to take a look at exactly how well the to take a look at exactly how well the
algorithm performs let's take a look at an example performs let's take a look at an example
where capital N is 128 which is 2 ^ 7 so where capital N is 128 which is 2 ^ 7 so where capital N is 128 which is 2 ^ 7 so
our function f is from Seven bits to one our function f is from Seven bits to one our function f is from Seven bits to one
bit and we're assuming that there's a bit and we're assuming that there's a bit and we're assuming that there's a
unique solution but to figure out how unique solution but to figure out how unique solution but to figure out how
well the algorithm works we really only well the algorithm works we really only well the algorithm works we really only
need to concern ourselves with the angle need to concern ourselves with the angle need to concern ourselves with the angle
Theta which we can calculate it's Theta which we can calculate it's Theta which we can calculate it's
approximately 0.08 85 radians which is a approximately 0.08 85 radians which is a approximately 0.08 85 radians which is a
little bit more than little bit more than little bit more than
5° and if we plug that value into our 5° and if we plug that value into our 5° and if we plug that value into our
expression for T we get eight so we're expression for T we get eight so we're expression for T we get eight so we're
thinking about what happens when we thinking about what happens when we thinking about what happens when we
search for a unique solution out of 128 search for a unique solution out of 128 search for a unique solution out of 128
possibilities using eight possibilities using eight possibilities using eight
queries we can draw a picture to queries we can draw a picture to queries we can draw a picture to
illustrate how the state changes over illustrate how the state changes over illustrate how the state changes over
time as we perform iterations of the time as we perform iterations of the time as we perform iterations of the
grover grover grover
operation as before we have the states K operation as before we have the states K operation as before we have the states K
a0 and K A1 and in this particular case a0 and K A1 and in this particular case a0 and K A1 and in this particular case
meaning unique search we have that Kat meaning unique search we have that Kat meaning unique search we have that Kat
A1 is in fact the standard basis State A1 is in fact the standard basis State A1 is in fact the standard basis State
corresponding to the unique solution corresponding to the unique solution corresponding to the unique solution
whatever it happens to be well K a0 is whatever it happens to be well K a0 is whatever it happens to be well K a0 is
the uniform superposition over all the uniform superposition over all the uniform superposition over all
strings besides the unique strings besides the unique strings besides the unique
solution here's the uniform solution here's the uniform solution here's the uniform
superposition over all strings which we superposition over all strings which we superposition over all strings which we
can write as cosine Theta K a0 plus sin can write as cosine Theta K a0 plus sin can write as cosine Theta K a0 plus sin
Theta K A1 and it makes sense that K a0 Theta K A1 and it makes sense that K a0 Theta K A1 and it makes sense that K a0
and K U are close in this case because and K U are close in this case because and K U are close in this case because
as we just observed a moment ago K a0 is as we just observed a moment ago K a0 is as we just observed a moment ago K a0 is
almost the uniform superposition over almost the uniform superposition over almost the uniform superposition over
all strings there's only one string all strings there's only one string all strings there's only one string
missing and that's the unique missing and that's the unique missing and that's the unique
solution applying G again rotates by an solution applying G again rotates by an solution applying G again rotates by an
additional 2 Theta so this is where we additional 2 Theta so this is where we additional 2 Theta so this is where we
are after two are after two are after two
iterations here's three 4 5 6 7 and 8 iterations here's three 4 5 6 7 and 8 iterations here's three 4 5 6 7 and 8
and now we see that we're pretty close and now we see that we're pretty close and now we see that we're pretty close
to Kat A1 so if we stop and measure at to Kat A1 so if we stop and measure at to Kat A1 so if we stop and measure at
this point we'll be quite likely to get this point we'll be quite likely to get this point we'll be quite likely to get
the unique the unique the unique
solution and in fact in this case solution and in fact in this case solution and in fact in this case
there's only about a half a percent there's only about a half a percent there's only about a half a percent
chance that we don't get the unique chance that we don't get the unique chance that we don't get the unique
Solution on the other hand if we were to Solution on the other hand if we were to Solution on the other hand if we were to
continue applying the grover operation continue applying the grover operation continue applying the grover operation
more than the recommended eight times more than the recommended eight times more than the recommended eight times
then the probability to get a solution then the probability to get a solution then the probability to get a solution
would go back down and over time it would go back down and over time it would go back down and over time it
would oscillate as the state goes around would oscillate as the state goes around would oscillate as the state goes around
the the the
circle so it's important that we stop circle so it's important that we stop circle so it's important that we stop
the iterations at the right time if we the iterations at the right time if we the iterations at the right time if we
want to succeed with high probability let's think about about the probability let's think about about the
probability that our choice for T probability that our choice for T probability that our choice for T
reveals the unique solution more reveals the unique solution more reveals the unique solution more
generally for different values of generally for different values of generally for different values of
n we'll write pn1 to denote the n we'll write pn1 to denote the n we'll write pn1 to denote the
probability that Grover's algorithm probability that Grover's algorithm probability that Grover's algorithm
succeeds in finding a unique solution succeeds in finding a unique solution succeeds in finding a unique solution
for each possible value of n the one for each possible value of n the one for each possible value of n the one
means one solution and we'll extend this means one solution and we'll extend this means one solution and we'll extend this
notation to other numbers of solutions shortly here's a table of these shortly here's a table of these
probabilities for small powers of two probabilities for small powers of two probabilities for small powers of two
it's starts with Nal 2 and the it's starts with Nal 2 and the it's starts with Nal 2 and the
probability is 1 12 which isn't very probability is 1 12 which isn't very probability is 1 12 which isn't very
good but then for Nal 4 the probability good but then for Nal 4 the probability good but then for Nal 4 the probability
is exactly one so it actually works is exactly one so it actually works is exactly one so it actually works
perfectly in this case it's a great perfectly in this case it's a great perfectly in this case it's a great
exercise by the way to explicitly go exercise by the way to explicitly go exercise by the way to explicitly go
through how these two cases work through how these two cases work through how these two cases work
particularly the case that n is equal to particularly the case that n is equal to particularly the case that n is equal to
four in that case we're searching for four in that case we're searching for four in that case we're searching for
one solution out of four possibilities one solution out of four possibilities one solution out of four possibilities
we need just a single query and it works we need just a single query and it works we need just a single query and it works
perfectly as n get gets larger we do see perfectly as n get gets larger we do see perfectly as n get gets larger we do see
some fluctuations the probabilities go some fluctuations the probabilities go some fluctuations the probabilities go
up and down a little bit but they appear up and down a little bit but they appear up and down a little bit but they appear
to be getting closer to one and indeed to be getting closer to one and indeed to be getting closer to one and indeed
that is true it can be proved that is true it can be proved that is true it can be proved
analytically that the probability is analytically that the probability is analytically that the probability is
always at least 1 - 1 / n so that's good always at least 1 - 1 / n so that's good always at least 1 - 1 / n so that's good
the algorithm isn't perfect we're not the algorithm isn't perfect we're not the algorithm isn't perfect we're not
guaranteed to get a solution but it's guaranteed to get a solution but it's guaranteed to get a solution but it's
quite likely particularly for large quite likely particularly for large quite likely particularly for large
values of n as well as the very values of n as well as the very values of n as well as the very
interesting anomaly that happens when n interesting anomaly that happens when n interesting anomaly that happens when n
is equal to is equal to is equal to
4 4 4
I should also point out that we can I should also point out that we can I should also point out that we can
always run the entire algorithm always run the entire algorithm always run the entire algorithm
including the measurements as many times including the measurements as many times including the measurements as many times
as we choose and if any one of the runs as we choose and if any one of the runs as we choose and if any one of the runs
succeeds we'll get a solution so if succeeds we'll get a solution so if succeeds we'll get a solution so if
these probabilities aren't good enough these probabilities aren't good enough these probabilities aren't good enough
we can easily boost them by just running we can easily boost them by just running we can easily boost them by just running
the algorithm again in fact even if the algorithm again in fact even if the algorithm again in fact even if
these probabilities were all equal to these probabilities were all equal to these probabilities were all equal to
1/2 for instance we'd still have a 1/2 for instance we'd still have a 1/2 for instance we'd still have a
useful searching useful searching useful searching
algorithm now let's consider what algorithm now let's consider what algorithm now let's consider what
happens when there are multiple happens when there are multiple happens when there are multiple
Solutions we'll start with a similar Solutions we'll start with a similar Solutions we'll start with a similar
example example to the one before again example example to the one before again example example to the one before again
n is equal to 128 but this time the n is equal to 128 but this time the n is equal to 128 but this time the
number of solutions s is four rather number of solutions s is four rather number of solutions s is four rather
than than than
1 again we can calculate Theta and T and 1 again we can calculate Theta and T and 1 again we can calculate Theta and T and
we get that Theta is about we get that Theta is about we get that Theta is about
01777 or a bit more than 10° and T is 01777 or a bit more than 10° and T is 01777 or a bit more than 10° and T is
four this time so the angle is about four this time so the angle is about four this time so the angle is about
twice the value from the previous twice the value from the previous twice the value from the previous
example and T is half of what it example and T is half of what it example and T is half of what it
was we can draw a picture like was we can draw a picture like was we can draw a picture like
before here's where we before here's where we before here's where we
start here's one iteration two three and start here's one iteration two three and start here's one iteration two three and
four this time we're very close to cat four this time we're very close to cat four this time we're very close to cat
A1 and the probability to get a solution A1 and the probability to get a solution A1 and the probability to get a solution
this time turns out to be greater than 99.9% but if we didn't know any better 99.9% but if we didn't know any better
and we thought that there was only one and we thought that there was only one and we thought that there was only one
solution we would take T to B8 like solution we would take T to B8 like solution we would take T to B8 like
before we'd rotate almost all the way before we'd rotate almost all the way before we'd rotate almost all the way
around to negative K a0 and that would around to negative K a0 and that would around to negative K a0 and that would
lead to a very small probability of lead to a very small probability of lead to a very small probability of
success so that's a further example success so that's a further example success so that's a further example
showing that there are good choices of T showing that there are good choices of T showing that there are good choices of T
and bad choices of T and here we see and bad choices of T and here we see and bad choices of T and here we see
that a good choice for S equals 1 turns that a good choice for S equals 1 turns that a good choice for S equals 1 turns
out to be a very bad choice for S equal out to be a very bad choice for S equal out to be a very bad choice for S equal
4 but assuming that we do know s in 4 but assuming that we do know s in 4 but assuming that we do know s in
advance and we choose T according to our advance and we choose T according to our advance and we choose T according to our
specification then it turns out that specification then it turns out that specification then it turns out that
Grover's algorithm will find a solution Grover's algorithm will find a solution Grover's algorithm will find a solution
with probability at least 1 - x over n with probability at least 1 - x over n with probability at least 1 - x over n
and often it'll be much larger than and often it'll be much larger than and often it'll be much larger than
that it's also at least s Over N but that it's also at least s Over N but that it's also at least s Over N but
that's actually much less interesting that's actually much less interesting that's actually much less interesting
what's more interesting is that as the what's more interesting is that as the what's more interesting is that as the
fraction s Over N gets small the bound fraction s Over N gets small the bound fraction s Over N gets small the bound
on the probability of success gets on the probability of success gets on the probability of success gets
closer and closer to closer and closer to closer and closer to
one now as I've already remarked several one now as I've already remarked several one now as I've already remarked several
times the number T represents not only times the number T represents not only times the number T represents not only
the number of iterations of the grover the number of iterations of the grover the number of iterations of the grover
operation we perform but also the number operation we perform but also the number operation we perform but also the number
of queries we're of queries we're of queries we're
making so how does this number depend on making so how does this number depend on making so how does this number depend on
s and n first let's start with the fact that n first let's start with the fact that
the inverse sign of X is always at least the inverse sign of X is always at least the inverse sign of X is always at least
X provided that X is between Z and 1 X provided that X is between Z and 1 X provided that X is between Z and 1
that's pretty easy to show if you happen that's pretty easy to show if you happen that's pretty easy to show if you happen
to know the tailor series for the to know the tailor series for the to know the tailor series for the
inverse sign function but there are inverse sign function but there are inverse sign function but there are
other ways to reason it and we're just other ways to reason it and we're just other ways to reason it and we're just
going to take this as given because it's going to take this as given because it's going to take this as given because it's
true that means that Theta is at least true that means that Theta is at least true that means that Theta is at least
the square root of s/ the square root of s/ the square root of s/
n and consequently T is at most &lt; / 4 * n and consequently T is at most &lt; / 4 * n and consequently T is at most &lt; / 4 *
the &lt; TK of n / the &lt; TK of n / the &lt; TK of n /
s so that's actually even better than s so that's actually even better than s so that's actually even better than
Big O of root n the larger s is in Big O of root n the larger s is in Big O of root n the larger s is in
comparison to n the fewer queries we comparison to n the fewer queries we comparison to n the fewer queries we
need to make and in a way it's not need to make and in a way it's not need to make and in a way it's not
surprising that this fraction shows up surprising that this fraction shows up surprising that this fraction shows up
like this given that its reciprocal like this given that its reciprocal like this given that its reciprocal
defines Theta and at the end of the day defines Theta and at the end of the day defines Theta and at the end of the day
Theta is really all that matters for Theta is really all that matters for Theta is really all that matters for
this this this
analysis as we've just seen Grover's analysis as we've just seen Grover's analysis as we've just seen Grover's
algorithm Works quite well when we know algorithm Works quite well when we know algorithm Works quite well when we know
in advance how many solutions there are in advance how many solutions there are in advance how many solutions there are
but what if we don't know how many but what if we don't know how many but what if we don't know how many
solutions there solutions there solutions there
are there's one pretty simple approach are there's one pretty simple approach are there's one pretty simple approach
and that is to just choose the number of and that is to just choose the number of and that is to just choose the number of
iterations iterations iterations
randomly and specifically choosing T randomly and specifically choosing T randomly and specifically choosing T
uniformly at random between one and the uniformly at random between one and the uniformly at random between one and the
floor of pi over 4 * root n turns out to floor of pi over 4 * root n turns out to floor of pi over 4 * root n turns out to
be a pretty good way to do be a pretty good way to do be a pretty good way to do
this the upper end of this range is this the upper end of this range is this the upper end of this range is
roughly the number of queries we would roughly the number of queries we would roughly the number of queries we would
choose for Unique search and actually choose for Unique search and actually choose for Unique search and actually
it's exactly the right number in most it's exactly the right number in most it's exactly the right number in most
cases this is when the angle Theta is as cases this is when the angle Theta is as cases this is when the angle Theta is as
small as it can possibly be excluding small as it can possibly be excluding small as it can possibly be excluding
the trivial case when there aren't any the trivial case when there aren't any the trivial case when there aren't any
solutions the idea here is that instead solutions the idea here is that instead solutions the idea here is that instead
of carefully aiming our state Vector at of carefully aiming our state Vector at of carefully aiming our state Vector at
K A1 like we did when we knew how many K A1 like we did when we knew how many K A1 like we did when we knew how many
solutions there were we're basically solutions there were we're basically solutions there were we're basically
rotating by a random angle as if we're rotating by a random angle as if we're rotating by a random angle as if we're
just giving the state Vector a random just giving the state Vector a random just giving the state Vector a random
spin if there happens to to be just one spin if there happens to to be just one spin if there happens to to be just one
solution then our random spin could push solution then our random spin could push solution then our random spin could push
the state Vector close to K A1 but it the state Vector close to K A1 but it the state Vector close to K A1 but it
might also just go part way there and if might also just go part way there and if might also just go part way there and if
there happens to be many solutions then there happens to be many solutions then there happens to be many solutions then
our random spin could potentially send our random spin could potentially send our random spin could potentially send
the state Vector around the circle the state Vector around the circle the state Vector around the circle
multiple multiple multiple
times so we don't expect this to reveal times so we don't expect this to reveal times so we don't expect this to reveal
a solution with a probability close to a solution with a probability close to a solution with a probability close to
one but the chance that this leads to a one but the chance that this leads to a one but the chance that this leads to a
solution if there is one turns out to be solution if there is one turns out to be solution if there is one turns out to be
pretty good it's always at least 40% in pretty good it's always at least 40% in pretty good it's always at least 40% in
fact that's not obvious but it is the fact that's not obvious but it is the fact that's not obvious but it is the
case it certainly isn't shocking though case it certainly isn't shocking though case it certainly isn't shocking though
because a truly random angle would have because a truly random angle would have because a truly random angle would have
a 50% chance of finding a a 50% chance of finding a a 50% chance of finding a
solution anyway 40% is not bad and solution anyway 40% is not bad and solution anyway 40% is not bad and
remember that we can always boost the remember that we can always boost the remember that we can always boost the
probability of finding a solution by probability of finding a solution by probability of finding a solution by
repeating the process choosing T repeating the process choosing T repeating the process choosing T
independently each independently each independently each
time by doing this the chances of not time by doing this the chances of not time by doing this the chances of not
finding a solution when there is one finding a solution when there is one finding a solution when there is one
decreases exponentially in the number of decreases exponentially in the number of decreases exponentially in the number of
times we repeat so if we want a 99% % times we repeat so if we want a 99% % times we repeat so if we want a 99% %
chance to find a solution what we can do chance to find a solution what we can do chance to find a solution what we can do
is to run this simple approach 10 times is to run this simple approach 10 times is to run this simple approach 10 times
again with t chosen independently each again with t chosen independently each again with t chosen independently each
time naturally if we don't find a time naturally if we don't find a time naturally if we don't find a
solution we output no solution we output no solution we output no
Solutions if there aren't any solutions Solutions if there aren't any solutions Solutions if there aren't any solutions
we won't find one and so in that case we won't find one and so in that case we won't find one and so in that case
we'll always output the correct answer we'll always output the correct answer we'll always output the correct answer
which is no which is no which is no
Solutions if there are solutions we Solutions if there are solutions we Solutions if there are solutions we
might fail to find one and incorrectly might fail to find one and incorrectly might fail to find one and incorrectly
declare that there are no Solutions but declare that there are no Solutions but declare that there are no Solutions but
the probability of that happening can be the probability of that happening can be the probability of that happening can be
made very small we also have as I mentioned a few we also have as I mentioned a few
moments ago that this requires a number moments ago that this requires a number moments ago that this requires a number
of queries or evaluations of f that's of queries or evaluations of f that's of queries or evaluations of f that's
Big O of root Big O of root Big O of root
n so using Grover's algorithm we can n so using Grover's algorithm we can n so using Grover's algorithm we can
still solve the search problem with high still solve the search problem with high still solve the search problem with high
probability with Big O of root end probability with Big O of root end probability with Big O of root end
queries even if we don't know in advance queries even if we don't know in advance queries even if we don't know in advance
how many solutions there how many solutions there how many solutions there
are there's also a more sophisticated are there's also a more sophisticated are there's also a more sophisticated
approach that can lead to a reduction in approach that can lead to a reduction in approach that can lead to a reduction in
the number of queries as the number of the number of queries as the number of the number of queries as the number of
solutions grows I won't describe it in solutions grows I won't describe it in solutions grows I won't describe it in
detail though and instead I'll just detail though and instead I'll just detail though and instead I'll just
focus on the main focus on the main focus on the main
idea we start with capital T being equal idea we start with capital T being equal idea we start with capital T being equal
to one we then run Grover's algorithm to one we then run Grover's algorithm to one we then run Grover's algorithm
for a random number of iterations but for a random number of iterations but for a random number of iterations but
where the upper bound is T so the first where the upper bound is T so the first where the upper bound is T so the first
time we get to step two we're going to time we get to step two we're going to time we get to step two we're going to
run Grover's algorithm with one run Grover's algorithm with one run Grover's algorithm with one
iteration but T is going to be iteration but T is going to be iteration but T is going to be
growing if we find a solution then we growing if we find a solution then we growing if we find a solution then we
output that solution and we stop and if output that solution and we stop and if output that solution and we stop and if
we don't find a solution we make t we don't find a solution we make t we don't find a solution we make t
larger and try again larger and try again larger and try again
and at some point before T gets too and at some point before T gets too and at some point before T gets too
large we'll terminate the process and large we'll terminate the process and large we'll terminate the process and
we'll declare that there aren't any we'll declare that there aren't any we'll declare that there aren't any
solutions it's the same basic idea as we solutions it's the same basic idea as we solutions it's the same basic idea as we
have in the simple approach where we're have in the simple approach where we're have in the simple approach where we're
giving the state Vector random spins giving the state Vector random spins giving the state Vector random spins
essentially but here we're starting out essentially but here we're starting out essentially but here we're starting out
with lighter spins first where T is with lighter spins first where T is with lighter spins first where T is
small just in case there are a lot of small just in case there are a lot of small just in case there are a lot of
Solutions in this way we won't waste Solutions in this way we won't waste Solutions in this way we won't waste
queries by making the state go around queries by making the state go around queries by making the state go around
and around the circle by spinning too and around the circle by spinning too and around the circle by spinning too
hard but if it doesn't work meaning that hard but if it doesn't work meaning that hard but if it doesn't work meaning that
we don't find a solution we try again we don't find a solution we try again we don't find a solution we try again
but we spin a little bit harder or but we spin a little bit harder or but we spin a little bit harder or
potentially a little bit potentially a little bit potentially a little bit
harder to make this work the rate of harder to make this work the rate of harder to make this work the rate of
increase of T has to be chosen carefully increase of T has to be chosen carefully increase of T has to be chosen carefully
and one way to make it work is to and one way to make it work is to and one way to make it work is to
replace T by the ceiling of 54s time T replace T by the ceiling of 54s time T replace T by the ceiling of 54s time T
So T is being increased exponentially So T is being increased exponentially So T is being increased exponentially
and intuitively we can think about it as and intuitively we can think about it as and intuitively we can think about it as
spinning 25% harder each spinning 25% harder each spinning 25% harder each
time and it's important for the analysis time and it's important for the analysis time and it's important for the analysis
that t increases exponentially like that t increases exponentially like that t increases exponentially like
this why 54s you might ask well it's this why 54s you might ask well it's this why 54s you might ask well it's
just a number that works and there's just a number that works and there's just a number that works and there's
actually a range of values that work but actually a range of values that work but actually a range of values that work but
two doesn't work for instance doubling T two doesn't work for instance doubling T two doesn't work for instance doubling T
each time is too fast of an increase to each time is too fast of an increase to each time is too fast of an increase to
take full advantage of the benefits of take full advantage of the benefits of take full advantage of the benefits of
lighter lighter lighter
spins anyway the point of this more spins anyway the point of this more spins anyway the point of this more
sophisticated method is that it gives sophisticated method is that it gives sophisticated method is that it gives
the same ASM totic bound on the number the same ASM totic bound on the number the same ASM totic bound on the number
of queries we require as we had before of queries we require as we had before of queries we require as we had before
Big O of the square root of n / s but we Big O of the square root of n / s but we Big O of the square root of n / s but we
don't don't need to know in advance how don't don't need to know in advance how don't don't need to know in advance how
many solutions there are and by the way many solutions there are and by the way many solutions there are and by the way
we can stop this process when T gets to we can stop this process when T gets to we can stop this process when T gets to
root n so if there aren't any solutions root n so if there aren't any solutions root n so if there aren't any solutions
we'll reach that conclusion after bigo we'll reach that conclusion after bigo we'll reach that conclusion after bigo
of root end of root end of root end
queries and that is how we can choose queries and that is how we can choose queries and that is how we can choose
the number of iterations in Grover's the number of iterations in Grover's the number of iterations in Grover's
algorithm to conclude the lesson I'll algorithm to conclude the lesson I'll algorithm to conclude the lesson I'll
make just a few brief remarks about make just a few brief remarks about make just a few brief remarks about
Grover's Grover's Grover's
algorithm first within the query model algorithm first within the query model algorithm first within the query model
Grover's algorithm is ASM totically Grover's algorithm is ASM totically Grover's algorithm is ASM totically
optimal what what that means is that optimal what what that means is that optimal what what that means is that
it's not possible to come up with a it's not possible to come up with a it's not possible to come up with a
query algorithm for solving the search query algorithm for solving the search query algorithm for solving the search
problem or even the unique search problem or even the unique search problem or even the unique search
problem specifically that uses ASM problem specifically that uses ASM problem specifically that uses ASM
totically fewer than Big O of root end totically fewer than Big O of root end totically fewer than Big O of root end
queries in the worst case that's queries in the worst case that's queries in the worst case that's
something that can be proved rigorously something that can be proved rigorously something that can be proved rigorously
and there are in fact multiple known and there are in fact multiple known and there are in fact multiple known
proofs of this proofs of this proofs of this
fact interestingly this was known even fact interestingly this was known even fact interestingly this was known even
before Grover's algorithm was discovered before Grover's algorithm was discovered before Grover's algorithm was discovered
Grover's algorithm matched the already Grover's algorithm matched the already Grover's algorithm matched the already
known lower bound on the number of known lower bound on the number of known lower bound on the number of
queries required queries required queries required
from time to time people do come up with from time to time people do come up with from time to time people do come up with
approaches that they believe are ASM approaches that they believe are ASM approaches that they believe are ASM
totically Superior to Grover's algorithm totically Superior to Grover's algorithm totically Superior to Grover's algorithm
but there's always either a mistake or but there's always either a mistake or but there's always either a mistake or
an assumption about the function f and an assumption about the function f and an assumption about the function f and
how it can be accessed or some other how it can be accessed or some other how it can be accessed or some other
problematic issue that renders the claim problematic issue that renders the claim problematic issue that renders the claim
incorrect if you're working properly incorrect if you're working properly incorrect if you're working properly
within the query model you can't beat within the query model you can't beat within the query model you can't beat
Grover's algorithm for unstructured Grover's algorithm for unstructured Grover's algorithm for unstructured
search second Grover's algorithm is search second Grover's algorithm is search second Grover's algorithm is
quite broadly applicable quite broadly applicable quite broadly applicable
in the sense that the square root speed in the sense that the square root speed in the sense that the square root speed
up that Grover's algorithm offers for up that Grover's algorithm offers for up that Grover's algorithm offers for
the unstructured search problem can be the unstructured search problem can be the unstructured search problem can be
obtained in a variety of different obtained in a variety of different obtained in a variety of different
settings for example sometimes it's settings for example sometimes it's settings for example sometimes it's
possible to use Grover's algorithm on possible to use Grover's algorithm on possible to use Grover's algorithm on
top of another algorithm to get a top of another algorithm to get a top of another algorithm to get a
quadratic Improvement beyond the quadratic Improvement beyond the quadratic Improvement beyond the
performance of that performance of that performance of that
algorithm Grover's algorithm is also algorithm Grover's algorithm is also algorithm Grover's algorithm is also
quite commonly used as a sub routine quite commonly used as a sub routine quite commonly used as a sub routine
inside of other Quantum inside of other Quantum inside of other Quantum
algorithms as just one simple example we algorithms as just one simple example we algorithms as just one simple example we
can use Grover's algorithm to get a can use Grover's algorithm to get a can use Grover's algorithm to get a
quadratic advantage quadratic advantage quadratic advantage
over classical algorithms for the over classical algorithms for the over classical algorithms for the
minimum problem that I mentioned in minimum problem that I mentioned in minimum problem that I mentioned in
lesson lesson lesson
five and finally the technique used in five and finally the technique used in five and finally the technique used in
Grover's algorithm where we have two Grover's algorithm where we have two Grover's algorithm where we have two
Reflections that are composed and Reflections that are composed and Reflections that are composed and
iterated to rotate a Quantum State iterated to rotate a Quantum State iterated to rotate a Quantum State
Vector can be generalized in a pretty Vector can be generalized in a pretty Vector can be generalized in a pretty
Major Major Major
Way an example is a technique known as Way an example is a technique known as Way an example is a technique known as
amplitude amplification where a process amplitude amplification where a process amplitude amplification where a process
that generalizes Grover's algorithm can that generalizes Grover's algorithm can that generalizes Grover's algorithm can
be applied to another quantum algorithm be applied to another quantum algorithm be applied to another quantum algorithm
to essentially boost its success to essentially boost its success to essentially boost its success
probability quadratically faster than probability quadratically faster than probability quadratically faster than
would be possible classically and that would be possible classically and that would be possible classically and that
technique has broad applications in technique has broad applications in technique has broad applications in
Quantum Quantum Quantum
algorithms so it may be true that algorithms so it may be true that algorithms so it may be true that
Grover's algorithm isn't likely to lead Grover's algorithm isn't likely to lead Grover's algorithm isn't likely to lead
to a practical Quantum Advantage anytime to a practical Quantum Advantage anytime to a practical Quantum Advantage anytime
soon if ever but Grover's algorithm is soon if ever but Grover's algorithm is soon if ever but Grover's algorithm is
without a doubt a fundamentally without a doubt a fundamentally without a doubt a fundamentally
important and broadly applicable Quantum important and broadly applicable Quantum important and broadly applicable Quantum
algorithm and it's representative of a algorithm and it's representative of a algorithm and it's representative of a
more General technique that has many more General technique that has many more General technique that has many
uses and that's it for this lesson and uses and that's it for this lesson and uses and that's it for this lesson and
for this unit of the series which has for this unit of the series which has for this unit of the series which has
been on Quantum been on Quantum been on Quantum
algorithms we discussed the query model algorithms we discussed the query model algorithms we discussed the query model
and how we can connect it to a more and how we can connect it to a more and how we can connect it to a more
standard model of computation along with standard model of computation along with standard model of computation along with
some other foundational Concepts some other foundational Concepts some other foundational Concepts
concerning how we can measure concerning how we can measure concerning how we can measure
computational computational computational
cost we then covered phase estimation cost we then covered phase estimation cost we then covered phase estimation
and factoring and then finally Grover's and factoring and then finally Grover's and factoring and then finally Grover's
algorithm as I've already suggested algorithm as I've already suggested algorithm as I've already suggested
there's a lot more to learn about there's a lot more to learn about there's a lot more to learn about
Quantum algorithms but in this series Quantum algorithms but in this series Quantum algorithms but in this series
we'll move on to a different topic in we'll move on to a different topic in we'll move on to a different topic in
the next unit in particular we'll take a the next unit in particular we'll take a the next unit in particular we'll take a
look at the general formulation of look at the general formulation of look at the general formulation of
quantum information where Quantum states quantum information where Quantum states quantum information where Quantum states
are represented by matrices of a certain are represented by matrices of a certain are represented by matrices of a certain
sort known as density sort known as density sort known as density
matrices as we'll see this formulation matrices as we'll see this formulation matrices as we'll see this formulation
turns out to be very useful and indeed turns out to be very useful and indeed turns out to be very useful and indeed
essential for understanding some aspects essential for understanding some aspects essential for understanding some aspects
of quantum information and it also of quantum information and it also of quantum information and it also
happens to be quite beautiful from a happens to be quite beautiful from a happens to be quite beautiful from a
mathematical mathematical mathematical
Viewpoint so I hope you'll join me for Viewpoint so I hope you'll join me for Viewpoint so I hope you'll join me for
unit three and goodbye until

## Multiple Systems ｜ Understanding Quantum Information & Computation ｜ Lesson 02

- Welcome back to Understanding Quantum
Information and Computation. My name is John Watrous, and I'm the technical director
of Education at IBM Quantum. This is the second lesson of the series. If you haven't already
watched the first lesson, be sure to check it out. We'll be building directly on what we talked about
in the first lesson, so it's important to have an
understanding of that material going into this lesson. Also, be aware that the series
includes textbook content in addition to videos, and you can find a link
to the textbook content in the video description. This is lesson two of
unit one of the series. Unit one covers the basics
of quantum information. In the previous lesson, we talked about quantum information in the setting of single systems, meaning that we have one
system that stores information and we're describing how
it works in isolation. We spent a fair amount of
time on classical information, mainly to establish a point of reference and to highlight the
mathematical similarities between quantum and classical information. We also saw how quantum states can be represented by vectors, specifically vectors having
complex number entries and Euclidean norm equal to one, and we discussed standard
basis measurements and unitary operations. In this lesson, we're gonna expand our
view to multiple systems, each of which is capable of
storing quantum information. This is not simply a matter of saying that each individual system
works as we discussed last time, there is a sense in which the hole is greater than the sum of the parts, and that's also true of
classical information when probabilities are involved. Multiple systems can
interact with one another and they can be correlated
with one another for instance, and in the quantum setting, this includes the possibility that the systems are entangled, which is a term that you
may have heard about before. Entanglement is a very
important phenomenon in quantum information and we'll have a lot more to say about it as the series continues. There is a simple principle, however, that offers us quite a lot of traction for thinking about and
understanding quantum information for multiple systems, and that is that we can always
take two or more systems and put them together or just conceptually
think about them together and view them as if they form
a single compound system. And when we think about a
compound system like this, as if it's a single system, we recognize that it must be described in a way that's consistent
with everything we learned in the previous lesson on single systems. We'll see how this works
and how the description of quantum information
from multiple systems emerges pretty naturally from how it works for single systems based on this one principle. Here's an overview of the lesson and it happens to be identical to the previous lessons overview, but this time our focus
will be on multiple systems rather than single systems. Just like we did in the previous lesson and for exactly the same reasons, we're going to start with
classical information. Once again, we'll see
very close similarities between the way that quantum
and classical information work at a mathematical level. We won't typically focus so
much on classical information or start each lesson in the series with classical information like this. Subsequent lessons will tend to be focused much more exclusively on quantum information and computation. But here at the beginning
this is a sensible approach and it goes back to the idea that thinking about classical information can serve as a valuable source of guidance for understanding quantum information. We're also gonna discuss
the important concept of a tensor product in
this part of the lesson. Tensor products are critically important in quantum information, but they also arise in
the classical setting. Then we'll move on to quantum information and much like in the previous lesson we'll discuss quantum states,
standard basis measurements, and unitary operations this
time for multiple systems. We'll start by talking
about classical states of multiple systems. Recall from the previous
lesson that a classical state of a system is a
configuration of that system that can be recognized and
described unambiguously without any uncertainty or error. And in mathematical terms, when we refer to a classical state set, we just mean a finite and non-empty set. We'll begin our discussion
of multiple systems by supposing that we have
just two systems X and Y. X has some set of classical states which we'll call sigma just
like in the previous lesson, and y has some set of classical states that we'll call gamma. The set sigma and gamma
could be the same set or they could be different, but we're giving them different names, not only because they
could be different sets, but also because it'll help
us to keep things clear if we give them different names. We'll find that it's
pretty easy to generalize this entire discussion from two systems to any number of systems, but
by starting with two systems, we can focus on the fundamental ideas without getting bogged down in details. Now, imagine that we
put X and Y side by side with X in the left and Y on the right, and we view these two systems together as if they form a single compound system. to give this new system a name, we can write it as an
ordered pair like this, or we can just juxtapose
the two letters like this. These are both common and we
can pick whichever one we want or whichever one is more convenient. Either way, we'll read it as XY. A very natural question
to ask at this point is what are the classical states of this new compound system XY? And the answer to that question is that the classical state set of XY is the Cartesian product of
the two classical state sets, sigma and gamma, which you
can see defined right here. At a given moment, X has some classical state
which is an element of sigma, and Y has some classical state
which is an element of gamma, and we're simply thinking
about these two states together as if they form a single state, and that's precisely the notion that the Cartesian product captures. We have have two elements,
one from each of our sets, and we want to think about
these two elements together as if they form a single
element from a single set. To be clear, there is an assumption here which is that we know
which one of our systems is X and which one is Y, and there's no confusion between them. It's not important that X is physically on the left hand
side and Y is on the right, but rather that the two
systems are distinguishable and that we've decided
to order them this way just so that we can keep
straight which one is which. Here's a very simple example where sigma is the binary alphabet and gamma is the set
containing the four card suits. If X stores a binary value and Y stores one of the four card suits, then there are eight
possible classical states of the pair XY, which you
can see listed right here. For more than two systems, this generalizes in a straightforward way. In particular if X1 through Xn are systems having classical
state sets sigma one through sigma n respectively. Then if we imagine grouping
the n systems together and viewing them as one system, the classical state set is
given by the Cartesian product of the n classical state sets
sigma one through sigma n. As you can see right here on the screen. The Cartesian product
can be defined like this for any number of sets. For example, if we have three systems and they all share the binary alphabet as their classical state set, so in other words we have three bits, then there are eight
possible classical states of the three bits viewed
together as a single system and you can see them listed right here. Each system individually can
be in the state zero or one, and we range over all of
the possible combinations. An n-tuple of classical states can alternatively be written as a string where we drop the
parentheses and the comma like you see right here. And this is particularly sensible when we think about
classical states as symbols as in the very common example
of the binary alphabet that we keep coming back to. Sometimes it's very convenient to do this and it tends to make
our expressions simpler and less cluttered. The notion of a string is in fact formalized
in mathematical terms through carte and products, so this is really just
an alternative notation and not something conceptually different. As an example, let's say that we have
10 bits X1 through X10. That means that the classical state set of the 10 bits together,
which you might naturally think about as a 10 bit register
in a computer for example, is the tenfold Cartesian
product of the binary alphabet with a self. And if we write those states like strings, they look like you see right
here, there's 1,024 of them, so of course they're not all listed here. These three dots right here
are supposed to indicate that there's a pattern that you can deduce and hopefully you get the idea. Now there's a convention
that we're going to follow for ordering the the elements
of Cartesian product sets, and it's important to
clarify this for the sake of representing both probabilistic states and quantum states as vectors. The convention is that
we order the elements of Cartesian product
sets lexicographically, which basically just
means dictionary ordering. To be more specific, we start with the assumption that each of the individual sets that we're taking the Cartesian product of already has its own ordering and we think about a
given n-tuple or a string as if it's a word and we
order them alphabetically. Another way to say this is that the positions within
an n-tuple or a string decrease in significance
from left to right, just like when we write numbers using the digits zero through nine. Here's an example that's
meant to clarify this. If we take the Cartesian product between the sets containing
one, two, and three and the binary alphabet, we get a set containing six elements and here they're listed in the
order that's being described. All of the elements with a
one in the first position come before all of the elements with a two in the first position and they come before all of the elements with a three in the first position. And whenever two elements
agree in the first position, we fall back on the ordering
in the second position. So that's the ordering that
we use for Cartesian products. Notice that when we use this convention and we write n-tuple as strings, we observed some very familiar patterns. For example, the Cartesian product of the
binary alphabet with itself is ordered like you see right here, and you can think about these strings as being ordered numerically if you take these strings
as binary encodings of non-negative mixtures
and this generalizes to any string length. Moving on to probabilistic states, we already know from the previous lesson that a probabilistic state associates a probability with each classical
state of a given system. If we have a compound system where we've taken two or more systems and we're viewing them collectively as if they form a single
system like we just discussed, then a probabilistic state
of this compound system must assign probabilities to the elements of the Cartesian product
of the individual systems, classical state sets. That's because that is
the classical state set of the compound system. For instance, supposing
that X and Y are bits, we have this example of a
probabilistic state of XY. The probability that both
bits are zero is one half and the probability that both
bits are one is one half, and so the two bits never disagree. It's a valid probabilistic state of XY, and we can alternatively
write it as a vector like you see right here and here you can see that we
are following the convention that I described a moment ago for ordering these classical states. This is an example of
a probabilistic state where X and Y are correlated, and this is the sense in which the hole is greater than the sum of the parts. As I suggested before, it's not simply that we
have a probabilistic state of X and a probabilistic state of Y and we put them together, but rather we first put
the systems together and then we think about
probabilistic states of the compound system. To be more precise about
correlations among systems. We start by defining the
notion of independence, which is essentially the
absence of correlation. Here by the way, the term independence refers
to statistical independence as opposed to linear independence
of vectors for instance. It's fundamental notion in
probability and statistics. If we have two systems X and Y and we have some probabilistic
state of XY in mind, then we say that X and Y are independent. If the formula that you
see right here is true for every choice of classical
states a and b of X and Y. One way to interpret this
condition of independence is that it says that the probability for X to be in any one classical state and the probability for Y to be in some other classical state have absolutely nothing
to do with one another. So, learning what state
one of the systems is in doesn't affect the
probabilities associated with the states of the other system. If you imagine randomly choosing
some classical state of X according to some procedure and then randomly choosing
some classical state of Y according to some other procedure that isn't in any way
connected to the one for X, like say if one person flips a coin and another person in a
different room rolls a die, then this is the formula
that you would use to get the probabilities
for different combinations of states to appear. The point of the definition is that independence is
when the formula is valid. When we think about probabilistic states in terms of probability vectors, there's a simple formulation
of this condition specifically if we have a
probabilistic state of XY, which we can express as a
vector in the direct notation as you see right here. Then to say that X and Y are independent is equivalent to the existence
of two probability vectors, cat Phi and cat Psi which by the way we can
simply read as Phi and Psi for the sake of brevity representing probabilistic states of X and Y separately that satisfy this equality
that you see right here. For all choices of a and b, it's essentially a
rephrasing of the definition in terms of vectors. For example, here's a probabilistic
state of a pair of bits XY for which X and Y are independent. The condition that's
required for independence is true for the vectors
that you see right here. That is the probability
for each possible setting for the two bits is given by the product of the corresponding probabilities
for the individual bits. For instance, the probability for both
bits to take the value zero is equal to one sixth, which is equal to one
quarter times two thirds, and likewise for the
other three possibilities which can be checked one at a time. For the probabilistic state we saw earlier on the other hand where we
have two bits that always agree and take the values zero and
one with probability one half we don't have independence and that's why we say that
the bits are correlated. Correlation simply means
a lack of independence. A simple way to argue that the
two bits are not independent is to suppose that they are and to derive a contradiction specifically if the two
bits were independent, there would have to exist
numbers q0, q1, r0 and r1 representing probabilities
for the two individual bits so that these equations
right here are satisfied. These are the probabilities
that come from our vector and these are the
equations that must be true by the condition of independence, but if we take a look at this
second equality right here, we see that it demands that
q0 times r1 is equal to zero, but the only way that the
product of two real numbers can possibly be equal to zero is if at least one of those
numbers itself is equal to zero. This is sometimes called
the zero product property of the real numbers. Another way to say it is that the product of two non-zero real
numbers is always non-zero. So this equality right
here implies that either q0 is equal to zero or r1 is equal to zero, but from the first and the last equalities we see that neither of
those two alternatives can possibly be true. If q0 is equal to zero, then this product must equal zero, but it's not equal to zero
and if r1 is equal to zero, then this product must be equal to zero, but that's not true either. So we have a contradiction
and we conclude that X and Y are not independent so they're correlated. The notion of independence
that we just discussed and specifically is formulation in terms of probability
vectors is closely connected to an operation that can
be performed on vectors known as the tensor product. Tensor products can be
defined not only for vectors but also for matrices
and in fact the notion of a tensor product can be defined for many other sorts of
mathematical objects as well. They can be described
in a pretty abstract way that's both important and
useful in pure mathematics, but we can also define
tensor products for vectors in a very simple and concrete way, and that's the path that
we'll take in this lesson. If we have two column vectors Phi and Psi written in the forms that you see here where sigma and gamma are any
choice of classical state sets we like or indices if you prefer, then the tensor product of these vectors is the vector that you see right here. We range over all elements
of the Cartesian product of the two sets, sigma and gamma, and the entry of this
vector that corresponds to each pair ab, which is
written as a string right here, is equal to the product of
the corresponding entries of these two vectors. In other words, the ab entry of Phi tensor Psi is the product of the a entry
of Phi and the b entry of Psi, and that's the meaning of
what's written down here. In essence, this is the same operation that we just saw a few moments ago when we discussed independence and now we're just giving a name to it. Here's exactly the same example
that we saw a moment ago written more explicitly in
terms of the tensor product. Again, we have that one sixth is equal to one quarter times one third, one 12th is equal to one quarter
times one third and so on. Here, by the way are a
couple of alternative ways to write the tensor
product of two vectors. When we're using the direct notation and we write two cats next
to each other like this, it means the tensor product. So you can think about the tensor product as kind of being the default way to multiply to column vectors together. It wouldn't make any sense
to think about a product of column vectors like this in terms of matrix multiplication because the dimensions don't work right, so there's really no concern
about ambiguity here. We can also write the
tensor product like this inside of a cat and it means
exactly the same thing. It's the tensor product
of the two vectors. The first one is very common and in fact it's perhaps more common to write a tensor product like this without the tensor product symbol than it is to include the symbol. The second one is
definitely much less common, but it is sometimes quite helpful mainly as a way to eliminate the need
for additional parentheses in certain situations. We can also express
tensor products explicitly in terms of actual columns of
numbers as you see right here, if we take the tensor product of these two column vectors right here, then this is the vector that we obtain and you can see that the entries here are products of the entries
coming from these two vectors. The ordering of the entries
of the tensor product is consistent with the
definition from a moment ago provided that we follow the convention that I described earlier for ordering the elements
of Cartesian product sets. Following that convention is equivalent to thinking
about the indices of the first vector as
having higher significance than the indices of the second vector, so you can see that as
we go through the entries of the tensor product, we have all of the products involving the first entry of the
first vector on the left, which is alpha one. Then all of the products
involving the second entry of the vector on the left,
which is alpha two and so on. Here's an example where
the vector on the left has three entries and the vector on the
right has four entries so we can see more
clearly what's going on. We have three entries in the first vector, so we start by writing down three copies of the second vector and then we multiply each one of those copies
by the corresponding entry in the first vector like this. We bring the alphas inside like this, and here we're just
multiplying vectors by scalers and finally we remove the
parentheses from the copies and we form a single column
vector like you see right here. As an important aside notice that we have the following formulas for the tensor product of
standard basis vectors. If we take the tensor
product of cat a and cat b, which we can write with
the tensor product symbol or without, as you see here, then the result is simply cat ab. If we want to write ab as an ordered pair rather than a string, we can do that and we obtain
this expression right here, but it's a lot more common
to write this right here where we emit the parenthesis
but we leave the comma. There is a sense in which
this one with the parenthesis is perhaps more correct, but it's a pretty standard
convention in mathematics that parentheses are removed when they aren't necessary or helpful. You can think about them as being implicit in this one right here and it's just less cluttered
and nicer to look at. The tensor product has
an important property which is that it's bilinear and what that means is that it's linear in each of the two arguments provided that the other one is fixed. In a bit more detail it's
linear in the first argument which is expressed by these two equations that you see right here, the first one says that
if we add two vectors, Phi1 and Phi2 and then take
the tensor product with Psi, we get the same result if
we took the tensor product with Psi first and then we added. The second one says that if
we multiply some vector Phi by some scaler alpha and then we take the
tensor product with Psi, the result is the same as if we first took the tensor product of Phi and Psi and then we multiply the result by alpha. And we also have linearity
in the second argument which is the same except that the roles of the two arguments are swapped. This product of bilinear
is pretty straightforward to verify from the definition, but it's good to write it down like this because we'll often make
use of these formulas. Notice by the way that scaler float freely through tensor products in the sense that's expressed by these
qualities that you see right here. That is there's no difference
between multiplying the first argument by a
scaler alpha in this case multiplying the second
argument by that scaler or multiplying the entire
thing by that scaler. These equalities follow from
these two equations right here and because there's no
difference between these vectors, we can just write the
expression as you see right here with no parentheses because
there's no need for them. They're not adding any clarity
or removing any ambiguity, so we just remove them and
we get a cleaner expression. All of this generalizes
to three or more systems. We can define the tensor product
of of three or more vectors by generalizing the formula
for the tensor product of two vectors in the natural way, which you can see expressed right here. Each entry of the tensor product is given by the product of
the corresponding entries of the vectors that we're
taking the tensor product of. An equivalent way to
define the tensor product of three or more vectors
is to do it recursively ultimately falling back on the definition of the tensor product between two vectors. So if n is at least three, we can think about the
tensor product of n vectors Phi1 through Phi n as being the tensor product between whatever vector we get by
taking the tensor product of the first n minus one of
them with the last vector, and if you want to know
what the tensor product of the first n minus one vectors is, you either apply the formula
again in case n minus one is at least three or use the definition
from before if n minus one is equal to two. The two ways of thinking
about tensor products of three or more vectors are equivalent. This time when we have
three or more vectors that we're taking the tensor product of, we don't use the term bilinear, we instead use the term multiline,
but the idea is the same. The tensor product is linear
in each of the positions provided that the vectors in
all of the other positions are fixed. Moving on to measurements
of multiple systems in probabilistic states, we immediately obtain a specification for how measurements
work for multiple systems by simply thinking about them
collectively as single systems provided that all of the
systems are measured. As a simple example, let's suppose that we have two
bits X and Y and collectively they're in this probabilistic
state right here, which we saw before. Naturally, if we measure
both of these bits, we'll get the outcome 00 meaning a zero for the first measurement and a zero for the second measurement with probability one half and we'll get the outcome 1,1 with probability one half as well. That's quite straightforward. In essence, measuring all of the systems is equivalent to measuring
the entire compound system. If we don't measure all
of the systems however, but instead we just measure
some of the systems, then the situation isn't
quite so immediately clear restricting our attention once again to the case of just two
systems to see the essence of what's going on in as
simple as setting as possible. We can ask the following question, suppose that we have two systems, X and Y that are together as a pair in some particular probabilistic state. What happens if we measure
just X and we do nothing to Y? Well, we can answer this question using some basic probability theory. First, we know that the probability that the measurement
gives a particular outcome has to be given by this formula
that you see right here. We can think about the probabilities for each of the classical
states of the pair XY as if we did measure both and we sum over all the probabilities that are consistent with X being in whatever classical state
a we're interested in. So that's why the sum right here is just over the classical states b of Y. One way to think about this
formula is that the probability of seeing a particular classical
state of X if we measure it has to be consistent with the probability of seeing that state if
we had also measured Y, even though we didn't, that's very natural and intuitive. The probability for a measurement of X to yield a particular outcome can possibly depend on whether or not Y was also measured
irrespective of the outcome. So that tells us what
the probabilities are for the different possible
outcomes of the measurement, but what happens to Y or our
knowledge of Y as a result? The answer is that there still
could exist some uncertainty about the state of Y as a
result of having measured X and seeing a particular result. And that uncertainty is represented by this formula right here for the conditional probability that Y is in some particular
classical state b, given that X is in some classical state, whatever classical state a,
we happen to have measured. To be precise, the probability that Y is
in some classical state b, given that X is in the classical state a, is equal to the probability
that X is in a and Y is in b divided by the probability that X is in the classical state a. So if we measure X, we're going to see
different classical states with probabilities determined
by this formula right here and depending on what outcome we get, we may still have some uncertainty about the classical state of Y and
that uncertainty is reflected by this formula right here. There's actually a very easy
way to rephrase all of this and to perform calculations
for specific cases using the direct notation
and it goes like this. We start by writing down the
probabilistic state of XY as a vector like this, and
this is completely general. The entries are denoted by Pab and these could be any numbers so long as the entire vector
is a probability vector. We can express cat ab as a
tensor product like you see right here and then we can use the
bilinear of the tensor product and specifically the fact that it's linear in the second argument to express this vector
as you see right here. What we're doing is effectively isolating the standard basis vectors
on the left hand side and putting whatever we need
to on the right hand side to allow that because that's gonna reveal what happens when we
measure the first system X. Specifically the probability
to get a particular outcome a is the sum over all of the probabilities that appear in the
parenthesis next to cat a. And conditioned on getting
a particular outcome a, the probability vector that describes whatever uncertainty remains about Y is the vector that was in
the parentheses next to a. Except that we have to normalize it to make it a probability vector and that is is to say that
we have to divide that vector by the sum of the entries in that vector to form a probability vector. Here by the way, the sum is taken overall
c in gamma rather than b just to use a different variable name so that there won't be any confusion about the scope of the
variables if you will. The sums in the numerator
and the denominator should be considered as separate things and we can avoid any chance
of confusion about that by using different
variables for the two sums. Here's a quick example to
illustrate how this all works. Suppose XY is a pair of bits in this probabilistic state right here to understand what happens
when we measure the first bit and we do nothing to the second, we write the vector as you see right here where cat zero and cat
one in the first position are isolated just as we
described a few moments ago. The first possibility is
that the measurement outcome is zero and the probability
of that happening is the sum of the probabilities
inside of the parentheses that are tensored to cat zero. So we get one 12th plus one quarter, which is one third and
conditioned on the outcome zero. The probabilistic state of Y
becomes this vector normalized or equivalently divided by the probability that we just calculated, and so what we get is
this vector right here. The second possibility is that the measurement outcome is one and the calculation is
done in a similar way using the second term in the expression rather than the first term. And if you'd like to verify the details, go ahead as always and pause the video to give yourself a moment do that. You can perform the same
sorts of calculations to figure out what happens when
Y is measured rather than X. It's the same method up to a symmetry between the two systems. This time we write the
probability vector like this, isolating the standard basis vectors on the right like you see right here, but otherwise it works
in exactly the same way. You can also use this method
for more than two systems. It can get a little bit
messy just to write down all of the required expressions, but conceptually it's
exactly the same idea. Finally, for the portion of the lesson
on classical information, we'll take a look at operations
on probabilistic states of multiple systems. Following the same reasoning that we've now seen a couple of times. We conclude that probabilistic operations on multiple systems are just
like probabilistic operations on single systems represented
by stochastic matrices. But this time those matrices
have rows and columns that correspond to the Cartesian product of the individual system's,
classical state sets. That is we're simply
viewing a compound system as if it's a single system and falling back on the description for how probabilistic operations work for single systems from
the previous lesson. Here's an example of an operation called a controlled-NOT
operation on two bits X and Y. This is in fact a deterministic operation and the way that it works is
that if X is equal to one, then a NOT operation is performed on Y and otherwise if X is equal
to zero then nothing happens. The bit X is sometimes
called a control bit because it determines whether
or not the NOT operation is applied to Y and Y is called the target bit because it's the bit that gets acted upon by the NOT operation. You can also imagine
swapping the roles of X and Y so that Y becomes the control
bit and X becomes the target, but that's a different operation from the one that's described here because the operation isn't
symmetric between X and Y. They're both called
controlled-NOT operations, and we just need to specify
which one of the bits is the control bit and
which one is the target bit. This operation has the action
that you see right here on standard basis states and you can check that
action against the definition of the operation. And here is the matrix
representation of the operation. When we think about this operation, we have in mind that there
are these two separate bits and they play the roles that I just described with
X being the control bit and Y being the target bit, but there's really nothing
about this action here or this matrix representation that demands that we
think about these two bits as being separate bits. We could alternatively just
think about this operation as being a deterministic
operation on a system having four classical states that we've decided to name
0,0, 0,1, 1,0 and 1,1. Here's another example of
a probabilistic operation on two bits, which isn't a deterministic
operation this time. The operation sets Y to be equal to X with probability one half and otherwise it goes the other way and it sets X to be equal to Y. This operation doesn't have a special name it's just meant as an example. And here's the matrix
representation of this operation. You can see that it can be expressed as an average of two matrices, one that represents the
deterministic operation where Y is set to whatever
value is stored in X and the other that sets
X to be equal to Y. So here we see that we can take averages of stochastic matrices to obtain new ones just like we had in the previous lesson. Now, let's consider
the following question. Suppose that we have two
probabilistic operations each on its own system described as usual by stochastic
matrices and to be precise, let's suppose that M is
a probabilistic operation on a system X and N is a
probabilistic operation on a system Y. The question is if we
simultaneously perform M on X and N on Y, how do we ascribe
the effect that this has on the compound system XY? Thinking about about this
intuitively for a moment, if we simultaneously
perform the two operations on their own respective systems, then we are in fact
performing an operation on the compound system, so it should be described
by some stochastic matrix. The short answer to this
question is that the matrix is obtained by taking a tensor product, not between vectors like
we talked about earlier, but between matrices in this
case, the matrices M and N. So let's take a few moments
to discuss tensor products of matrices and then we'll
return to this question. Here's the definition
of the tensor product between matrices and
it's very much analogous to the tensor product between vectors. If we suppose that the two
matrices that we're working with are written in the forms that you see here so that the entries of M are
denoted alpha ab as a and b range over some classical state set sigma and the entries of N are
denoted by beta cd as c and d range over some classical state set gamma. Then the product of M and N is the matrix that you see right here. What we have is similar
to the tensor product between vectors. the rows and columns of M tensor N correspond to pairs of classical states which are written as strings right here, and the entries are products of the corresponding entries of M and N. Here's another way of writing it in terms of the individual
entries of these matrices. The ac bd entry of M tensor N is the product of the ab entry
of M and the cd entry of N. There's another way of
describing M tensor N, which is that it's the
unique matrix that transforms any vector of the form Phi tensor Psi into the tensor product of M applied to Phi with and applied to Psi, and that has to be true for every choice of these vectors Phi and Psi. One way to think about
this is that the tensor product of matrices gets along perfectly with the tensor product between vectors. This is an equation that you
would naturally hope is true and indeed it is. In terms of matrices that
are written out explicitly, we can define the tensor
product as you see here. There's a lot of dots in this equation and they're meant to
illustrate the pattern, which is the same pattern
that we had for vectors except now we have rows and columns as opposed to just column vectors. In case it's not clear, the
following example may help. Here we're talking about
the tensor product of two two by two matrices. We first write down four
copies of the second matrix, one for each entry of the first matrix. We multiply each of these
copies of the second matrix by the corresponding
entry of the first matrix, and then we collect it all into
a single four by four matrix as you see right here. We can also define the
tensor product of three or more matrices in an analogous way and perhaps the easiest
way to express this is as you see right here
where we have a formula for each entry of the tensor product, which is simply the product
of the corresponding entries. You can also define the tensor product of three or more matrices recursively in terms of the tensor
product of just two matrices just like we had for vectors. And one final note is
that the tensor product of matrices is multiplicative, which means that a
product of tensor products is equal to the tensor
product of the products. You can think about
this as a generalization of the formula from a few moments ago for the action of intensive
product of matrices on tensor product of vectors. And again, this is something that we
would certainly hope to be true and we can be glad that it is and we'll make use of this
fact whenever we need. And now we can return to
the question from before which asks how we describe
the effect of performing an operation M on a system X and an operation N on the system Y together as an operation
on the compound system XY. And the answer to that
question is that the action is described by the
tensor product of M and N. Tensor products represent independence this time between operations as opposed to what we saw
before for probability vectors. Here's a quick example. Suppose that we perform
the probabilistic operation described by this matrix on a bit X. This happens to be an operation
that we saw in lesson one and we simultaneously perform
a NOT operation on a bit Y. The combined operation on the pair XY is therefore represented by the tensor product of the matrices corresponding to these two operations. Here's the one on X and
here's the NOT operation on Y and we get this matrix
that you see right here. And you can see that indeed
this is a stochastic matrix and that will always be the case. The tensor product of any
two stochastic matrices or any number of stochastic
matrices for that matter will always be stochastic. It is a very common situation
that we encounter a situation in which one operation is
performed on one system and nothing is done to another system. In this situation, we follow exactly the same prescription with the understanding that doing nothing is represented by the identity matrix. For example, if we have two bits X and
Y and we reset the bit X to the zero state, which is the same thing as performing the constant
zero function on X and we do nothing to Y, then the resulting operation on the par XY is represented by this
tensor product right here. All of this generalizes
to any number of systems and we'll see some additional examples when we turn to the quantum setting, which works in exactly the same way except that we have unitary matrices rather than stochastic matrices, and that's enough about
classical information for now. Now we'll turn our attention
to quantum information and how it works for multiple systems. The pattern is the same as it
was for classical information. We can view multiple quantum systems as single compound systems and apply what we know
from the previous lesson. Again, tensor products
will play a key role and overall we obtain a description for how quantum information
works for multiple systems. That's very much analogous
and mathematically similar to what we just saw for
classical information. We'll start with quantum states. Quantum states of multiple systems are represented by column vectors having complex number entries
and Euclid arm equal to one just like quantum states
of single systems. This time the indices of
the quantum state vectors correspond to the Cartesian product of the classical state sets
of the individual systems because again, that is
the classical state set of the compound system. For example, if X and Y or Qubits, then the classical state
set for the par XY together is the Cartesian product of the
binary alphabet with itself, which we can think about as
the set up all binary strings of length two. So these vectors that you see here are all examples of quantum
state vectors of XY. If we want to, we can alternatively write
these vectors like this where we have a separate
cat for each Qubit where the tensor product
between them is implicit. Or we can also write the
tensor product symbol explicitly like this if you prefer. And another option is
to subscript the cats with the names of the systems
that they represent like this. And you can do this with
the tensor product symbol or without. That can sometimes be very helpful for keeping things straight. These are just different styles for expressing the same vectors and we can pick whichever one we want or whichever we prefer or
whichever makes the most sense for the situation at hand. When things get more
complicated for instance, we might decide that one of
these styles adds clarity and we're free to make
whichever choice we want. Tensor products of quantum state vectors are also quantum state vectors. For the case of two systems for instance, if Phi is a quantum state
vector of a system X and Psi is a quantum state
vector of a system Y, then the tensor product of Phi and Psi is a quantum state vector of XY. We call states like this product states and similar to what we saw in the classical probabilistic setting, they represent independence
between the systems X and Y. So for instance, if one person has a laboratory
in one part of the world and they create a system X and initialize it to the quorum state Phi and another person in
another part of the world independently initializes a system Y to the quantum state Psi, then if we choose to think about XY as forming a single system even though it's two parts or in different parts of the world, then the compound system XY is
in the state Phi tensor Psi. This all generalizes to
more than two systems in the natural way. If we have quantum
states Psi1 through PsiN of separate systems Xn through Xn, then the tensor product of these n vectors represents a quantum state, which is again called a product state of the compound system X1 through Xn. Here's an example of a
quantum state of two Qubits. Although it might not
look like it immediately. It is an example of a product state because we can write
it as a tensor product as you see right here. So it's an example of a
product state regardless of how it's written, what's relevant is whether
or not it's possible to write it as a tensor product. This quantum state, however, which is again a state of two
Qubits is not a product state. It's similar in spirit to
the example that we saw in the probabilistic setting of two perfectly correlated random bits and the same argument reveals that it can't be written
as a tensor product. So let's move this over here and let's take a look at the argument in a little bit more detail. Let's imagine that we
could write this state as a tensor product of
two vectors Phi and Psi. The entry corresponding
to the classical state 0,1 is zero for this vector, which we can see just by looking at it. This would imply however, if the vector was a tensor
product of two vectors that the zero entry of Phi
times the one entry of Psi would be equal to zero, but that means that either
the zero entry of Phi or the one entry of Psi is equal to zero or they both could be equal to zero, but that contradicts the fact
that these two products here must both be non-zero. They're equal to one
of our squared of two. The argument is in fact pretty
much identical to what we had in the probabilistic setting. There's nothing uniquely
quantum about this at all. In any case, what we find is that the original state is not a product state. Notice that it's not important
that these two numbers here are one over squared of two. All that matters is that
both values are non-zero. This particular quantum state
is a very interesting one. It's an example of an
entangled state of two Qubits. We'll talk more about
entanglement later in the series. Entanglement can be complicated, particularly when we're
talking about quantum states in the general formulation
of quantum information, but here in the simplified description of quantum information entanglement is equivalent to a lack of independence. So for a quantum state vector,
if it's not a product state, it's an entangled state
and this particular state is commonly viewed as
the archetypal example of an entangled quantum state. And in fact, we sometimes
think about this state as representing one unit of entanglement, particularly when we're
thinking about entanglement as a resource that can be
used to do interesting things. The quantum state that
we just talked about is one of the four so-called bell states, which you can see here
along with the names that are commonly used for these states, Phi plus, Phi minus,
Psi plus and Psi minus. They're all entangled
and they do form a basis for the space of all
vectors representing states of two Qubits. So these four vectors together
are called the Bell basis. It's a very commonly
encountered collection of quantum states named
in honor of John Bell, who was a physicist who made
very important contributions to the study of entanglement in the 1960s. Here are a couple of well known examples of quantum states of three Qubits. The first one is called a GHZ state. It's kind of like the Phi
plus state that we just saw except for three Qubits instead of two. And the second example is called a W state and it looks like you see here, so it's uniform over
the three binary strings of length three that have
exactly one bit set to one. Neither of these states is a product state and they both have interesting properties and arise pretty commonly as examples, so they're good to know about
and we will see them again. Next we'll talk about
measurements of systems in quantum states. Following the same reasoning
that we've been following this far in the lesson, which is to think about multiple systems as if they're single systems. We immediately arrive at a description for how measurements of
multiple quantum systems work provided that all of the
systems are measured. That is if we have some
quantum state vector Psi that represents a quantum
state of an n-tuple systems X1 through Xn and we measure
the entire compound system, which is equivalent to measuring every one of the individual systems, then each possible n-tuple
of classical states will be the outcome of the measurement with probability equal to
the absolute value squared of the entry of Psi that
corresponds to that n-tuple. For example, if we have
a pair of systems XY in this quantum state right here, then if we measure both systems, then we'll get the outcome zero hearts meaning zero for the measurement of X and hearts for the measurement of Y with probability nine 25ths and we'll get the outcome one
speeds with probability 16/25. That's really quite straightforward. We're essentially just ignoring the fact that these are two separate systems and we're viewing them as if
they're one single system, but this raises the
following natural question, which we already considered
in the classical setting. Suppose that we have two systems X and Y that are collectively
in some quantum state and we don't measure both, but
we only measure one system. Let's say that we measure the system X and we do nothing to Y. Of course, we can also ask what happens if we measure Y and not
X and more generally what happens if we have
three or more systems and we measure some proper subset of them. But once we have an
answer to this question, we'll have answers to
those questions as well. We know that in general
a quantum state vector of the pair XY takes the
form that you see right here, assuming as usual that sigma and gamma are the classical state sets
of X and Y respectively. The entries of this vector are
denoted by alpha ab ranging over all the possible choices of a and b, and they can be any complex number that cause this vector
to be a unit vector. Now we already know that if
both systems are measured, then we get the probabilities for the different possible outcomes by taking the absolute value squared of the corresponding entry. And so if just X is measured, then the probability to
obtain a particular outcome a is given by this expression, we simply some over all of
the possible outcomes b, that could be obtained
if Y was also measured even though it wasn't. The idea is the same as in
the probabilistic setting. The probability to get
a particular outcome when X is measured can't
possibly depend upon whether or not Y was also measured. So this tells us what
the probabilities are for the different possible outcomes. In addition, there will be some
change in the state of Y as a result of having
performed this measurement on X and having obtained a particular outcome. So let's take a look at how this works. Similar to what we did in
the probabilistic setting, we can express the
quantum state vector Psi as you see right here where Phi a is this vector for each choice of a classical state a of the system X, we're using the bilinear
of the tensor product just like before. Only now we've chosen to name
these different vectors Phi a, just for convenience. We already observed that the probability that the measurement yields
each possible outcome a is as you see right here, but now we can see that this probability is equal to the Euclid norm
squared of the vector Phi a, and now as a result of the measurement of X giving a particular outcome a, we have that the quantum state
of the compound system XY becomes this state that
you see right here. Just like we had for single systems, the state collapses in essence, but it doesn't collapse
entirely in general, it only collapses as far as it needs to in order to be consistent
with the measurement of X yielding the outcome a. The quantum state of Y becomes Phi a normalized meaning that we
divide by its Euclid norm so that it becomes a unit vector. This is analogous to what happened in the
probabilistic setting where instead of dividing
by the Euclid norm, we divided by the sum
of the entries of Phi a in order to get a probability vector. But here because we're
talking about quantum states, we divide by the Euclid norm
to get a quantum state vector. Let's take a look at a couple of examples to see how this works
for a specific state. Here's the state that we'll consider. We're assuming that X and Y are Qubits and they're in this particular state Psi. And let's suppose that X is measured. We begin by writing the state's
Psi as you see right here, effectively isolating the
standard basis factors of X and tensing them with whatever we need to in order to get the original state. Just like in the general
description from a moment ago. The probability to obtain the outcome zero is the Euclid norm squared
of this vector right here. The thing that we're tensing to cat zero in our expression of the vector Psi. And so the probability is three quarters and conditioned on
getting the outcome zero. The quantum state of
the pair XY becomes this where we normalize the
vector tensor to cat zero, and that's equivalent to
dividing by the square root of the probability that
we just calculated. We can then simplify
and this is what we get. The probability to get the
measurement outcome one is calculated in a similar way. We take the Euclid arm squared
of this vector right here. The thing that's tensed to cat
one in our expression of Psi and the resulting
probability is one quarter, which it has to be of course, because our probabilities
have to sum to one. In this case, the quantum state of XY becomes this where again we're normalizing by dividing by the Euclid norm so that we obtain this quantum
state vector as a result. The same method is used
in a symmetric way, if Y is measured instead of X. Here's the original quantum state vector, and this time because Y
is measured instead of X, we express the vector Psi
as you see right here, where this time we isolate the
standard basis vectors of Y rather than X tensing
with whatever we need to to get an expression of Psi. Going through the calculation, which you can check by
pausing the video if you like. We see that the probability
to get the outcome zero is Phi8 this time, and here's
the quantum state vector that results conditioned
upon getting that result. And the probability to get the
outcome one is three eights, in which case this is
our quantum state vector. You can use exactly the same method in case you have more than two systems and any proper subset
of them are measured. And there's an example
in the textbook content for this lesson that you can
check out if you're interested. In short, we can always divide the systems into two separate collections, the ones that are measured
and the ones that aren't. And once we've done that, we fall back on the method
method that I've just described, and that's how measurements for multiple quantum systems work. And now finally, we move on to unitary
operations on multiple systems. Using the same basic idea that multiple systems can be viewed as single compound systems. We conclude what you see
written here on the screen, quantum operations on compound systems are represented by unitary matrices just like we have for single systems and specifically the rows and columns of those unitary matrices correspond to the Cartesian product of the classical state sets of whatever collection of
systems we're talking about. For example, if we have a system X
whose classical states are one, two, and three and Y is a Qubit, then the matrix that you see right here is an example of a unitary matrix that represents an
operation on the pair XY. The details of how we
order Cartesian products that we talked about before don't really matter for
the sake of this example, it's just a six by six matrix
that happens to be unitary. There's nothing special about it, although it is built out of a
couple of important matrices. The point is that you could
choose any six by six unitary matrix and it would represent
a unitary operation on XY. If we apply a collection of
unitary operations independently each on its own system, then the action of them all together on the compound system is
given by the tensor product. And here on the screen you can see a more precise
statement of that fact. If our unitary operations are described by the unitary matrices U1 through Un one for each of our systems, then the combined action
on the compound system is given by the tensor
product of U1 up to Un. Once again, similar to what we had in the probabilistic setting, an important example is that
we apply some unitary operation U to a system X and we do
nothing to some other system Y. The operation on XY that we get is U tensored with the identity matrix. The identity matrix
describes doing nothing to Y. Sometimes it's helpful to write explicitly that this identity matrix
corresponds to the system Y, just to make the formulas more clear. And one option is to
give the identity matrix a subscript like this that indicates the system that it acts upon. When it's clear enough, though we usually don't
bother with a subscript, but when things get more complicated, it can be very helpful. And we can also consider
the symmetric case where some unitary
operation V is applied to Y and nothing is done to X, and this time the resulting
unitary operation on XY, is represented by this
tensor product here. And once again, we can put a subscript on the
identity matrix if it helps. Here are a couple of specific examples. If X and Y are Qubits and we
perform a Hadamard operation on X and we do nothing to Y, we get the matrix that you see right here. If on the other hand, we perform the Hadamard operation on Y and we do nothing to X, the ordering of the tensor
product is reversed. And so we get the matrix
that you see right here. Not every unitary operation
on a compound system can be expressed as a tensor product. If we have a tensor product,
then we have independence. And there are some pretty
fundamental examples of operations that certainly
don't act independently on multiple systems. For example, if X and Y have
the same classical state set, which could be any
classical state set sigma, then we can think about the
swap operation on the pair XY. This operation doesn't change the ordering of the two systems, we still have X in the
left and Y on the right, but rather it represents
swapping the contents of the two systems. Here you can see that expressed
in the form of an equation where we have a tensor
product of two quantum states. Another way to write it is as
a matrix as we see right here. The way you can think
about this description is that it tells you what happens for standard basis vectors. And once we know what happens
for standard basis vectors, we know what happens for
arbitrary quantum states by linearity, which in fact is also true for
this expression right here. So this particular operation right here happens to be a deterministic operation as well as a unitary operation. If X and Y are Qubits, for instance, we get this matrix
representation right here. Here's an interesting connection
between the swap operation and the four Bell states, which are shown right
here for convenience. The swap operation leaves the
first three of them alone, but it puts a minus sign in
front of the Psi minus state, so there's something
special about that one. You can check these equations directly by looking at these vectors and considering what happens
for the standard basis states swapping the two classical states for the first three Bell
states doesn't do anything we get the same state we started with. In the fourth case however, the minus sign effectively
moves from one term to the other when we perform the swap of the classical states, and that's equivalent to multiplying the vector by minus one. So performing the swap operation somehow kicks out a minus sign
from this fourth Bell state. We'll have more to say about
these equations later on, but now let's move on to
another type of operation, which is a controlled operation. We're going to suppose that X is a Qubit and Y is any system you want. If U as a unitary operation on Y, then a controlled U operation on XY is the operation that's
expressed right here. First, using the direct notation and then using what's
called a block matrix you just imagined filling
in this bigger matrix with the smaller ones where these zeros really represent matrices
filled with zeros. So each of these four blocks represents a matrix having a number
of rows and columns that agrees with the number
of classical states of Y. The idea of a controlled operation, which we already encountered
in the classical setting when we discussed a
controlled NOT operation, is that if X is set to zero,
then nothing happens to Y. But if X is set to one,
then U is applied to Y. So X is called the control Qubit, and Y is called the target system, which in general doesn't
have to be a Qubit. For a specific example,
which I just mentioned, we have a controlled NOT operation. If the first qubit is set
to zero, nothing happens. And if the first qubit is set to one, then a NOT operation or equivalently, a sigma X operation is
applied to the second Qubit. And here is it's matrix representation. We're talking about two Qubits here, so we could equally well
decide that the second Qubit is the control and the
first is the target. To describe that operation, we just swap the ordering
in the tensor product, and the result is the matrix
that you see right here. If instead of a controlled-NOT operation, we consider a controlled sigma
Z or controlled Z operation. We obtain this matrix
representation right here. And for this one, it doesn't actually matter
which is the control and which is the target. We get the same result either way. We can go further and we can consider a controlled swap operation. And here we're talking
about swapping two Qubits, so we end up with three Qubits when we include the controlled Qubit. Each matrix representation
looks like this. It's pretty special operation and it's known as a Fredkin operation, or more commonly a Fredkin gate. When we think about it
as a gate in a circuit, we'll discuss circuits in the next lesson. The last example is a controlled,
controlled-NOT operation, which again is an
operation on three Qubits. And here's its matrix representation. This operation is much better
known as a Toffli operation or a Toffoli gate. We'll discuss it further
and we'll see why both it and the Fredkin can operation are very important
operations in a later lesson. And that is the end of the second lesson. In this lesson, we discussed how quantum information works for multiple systems. I hope you'll join me for
the third lesson soon, which is on quantum circuits
protocols and games. Bye until then.

## Phase Estimation and Factoring ｜ Understanding Quantum Information & Computation ｜ Lesson 07

- Welcome back to Understanding Quantum Information and Computation. My name is John Watrous and I'm the Technical Director for
Education at IBM Quantum. This is the seventh lesson of the series and in this lesson we'll discuss the phase estimation problem and how we can solve it
with a quantum computer. We'll then apply phase estimation to the problem of integer factorization, which we discussed in the previous lesson. What we'll obtain is Shor's algorithm, which allows integers to be
factorized at polynomial cost or in polynomial time if we prefer to think in terms of the time required rather than the number of
elementary operations we need. This is the crown jewel of
quantum algorithms in some sense, and while factorized large integers may have limited utility for most people, it remains one of the most
compelling pieces of evidence that we have for the hypothesis that quantum computers will
offer striking advantages over classical computers. Here's an overview of the lesson. We'll start with a definition of the phase estimation problem itself, and then we'll discuss a method for solving this problem
with a quantum computer. We'll take it one step at a time, beginning with what you can
think of as a kind of a warmup. In the phase estimation problem, the goal is to approximate
a certain real number and in this warmup part
we'll see how we can get a very low precision
approximation to this number. This isn't going to be
good enough for factoring when we apply our solution to
the phase estimation problem to integer factorization, we're actually going to need a pretty high precision approximation, but it's nevertheless useful to begin with this sort of a warmup just to get a sense for
how the whole thing works. We'll then discuss the issue of how we can get more precision, and this will lead very
naturally to an operation known as the Quantum Fourier transform. We'll see exactly what this operation is and how it can be implemented
with a quantum circuit. And once we have it, we'll be able to apply
it to phase estimation to come up with a general procedure for solving the problem
with very high precision. Then in the last part of the lesson, we'll see how a solution to
the phase estimation problem allows us to factor integers efficiently. And specifically what we'll get is Shor's algorithm for
integer factorization, or at least something that's equivalent to Shor's algorithm. The way that this works is that we basically give
ourselves a stepping stone, which is yet another problem called the order finding problem. And in particular we'll see how we can solve the order finding problem using the phase estimation procedure and then we'll see how
we can factor integers by solving the order finding problem. We're going to begin with a special case of a theorem called the Spectral theorem, and specifically we're going to focus on what the theorem tells
us for unitary matrices. The spectral theorem is
a very important theorem from linear algebra and
functional analysis. It tells us something interesting about how certain linear mappings can be expressed in a certain form, it's often called a
spectral decomposition, that's useful for many reasons. And like I said, we're going to focus on what this theorem tells
us for unitary matrices. The spectral theorem is
more general than this. It doesn't work for all matrices, but it does work for a class of matrices called normal matrices, which include all unitary matrices as well as some matrices
that aren't unitary. But just to keep things simple, we'll restrict our attention
to unitary matrices for now because that's all we need for
the purposes of this lesson. Here's a statement of the spectral theorem just for unitary matrices. Suppose that U is an
N by N unitary matrix. What the theorem says
is that there must exist an orthonormal basis of
vectors, psi one through psi N, along with N complex numbers
Lambda one through Lambda N, all lying on the complex unit circle so that U can be expressed
as we have in this equation. This is called a spectral
decomposition of U, as I've already mentioned, and it can be very convenient to know that U can be written in this way. It can sometimes be convenient to have these vectors and these numbers when we're working with
a particular unitary matrix U, and it can also be very
useful simply to know that such an expression is always possible as a way of proving certain things about unitary matrices for instance. In this statement of the theorem, each of these numbers,
Lambda one through lambda N, is written in a particular way as E to the power two pi I times theta where each of the thetas
theta one through theta N is a real number, and
we'll come back to this way of writing each of these
numbers in just a little bit. An important thing to notice
about these vectors and numbers is that each vector psi
K must be an eigenvector of the matrix U with
corresponding eigenvalue lambda K. Which means that if we
multiply psi K by U, what we get is lambda K times psi K. So what U does to each of these vectors is simply to multiply it by the corresponding number lambda K. In general for some arbitrary vector, this won't be how U works, but eigenvectors are special. And what the theorem tells us is that there is in fact
an ortho normal basis of these eigenvectors. This equation down here by the way, follows from the expression that we have for U from the theorem, together with the fact that
the vectors are orthonormal. If you multiply the
expression of U to any psi K and use the orthonormality of the vectors, you'll get a sum in which
all of the terms are zero except for one of them, and the one term that we're
left with looks like this. Now we can define the
phase estimation problem. The problem is a little
bit unusual as we'll see. We're given two things. First, we're given the description of a unitary quantum circuit, that acts on N qubits
for some choice of N, and when we say that this
is a unitary circuit, we just mean that all of
the gates are unitary, so there are no measurements
in this circuit. Second, we're given an N Qubit state psi, and that's the unusual part. We actually have a quantum
state input for this problem, and it's a single copy of this state. For all the other computational problems we've talked about in this series, the input has been classical, either in the form of a string
of bits or as a black box. And that's not surprising because the computational
problems that we encounter in our daily lives tend to be classical, 'cause those are the
problems we care most about. The way that you can
think about this problem is that it's something that appears or is likely to appear as a sub problem. And the phase estimation procedure we'll eventually come up with is something that we
generally use as a sub-routine inside of some larger computation, presumably one for a classical
problem that we care about. Now, this is going to be a promise problem and the state psi isn't
just any old quantum state. We're promised that psi is an eigenvector of whatever unitary operation U it is that the given circuit describes. And the goal is to find or
approximate the eigenvalue that corresponds to the eigenvector psi. Let's state the problem a
little bit more precisely. The input consists of a unitary
quantum circuit on N qubits, which can be encoded as a string of bits using whatever method we prefer for encoding quantum circuit descriptions, along with an N Qubit state psi. We can call the circuit whatever we want, but what really matters in terms of the statement of this problem is the unitary operation itself that's performed when the circuit is run, and that unitary operation
is described by the Matrix U. But we're not given U as a matrix, we're just given the description of a quantum circuit that implements U. The promise, as I've already stated, is that psi is an eigenvector of U. So if we ran our circuit on psi, we'd get psi coming out multiplied by whatever the eigenvalue is, which would be a global
phase in that situation. And finally, the output of the problem is an approximation to the eigenvalue corresponding to psi, and specifically we're looking for an approximation to the real number theta, between zero and one, including
zero, but not including one that satisfies the equation
that we have on the screen. To explain this way of
parameterizing the eigenvalue corresponding to pis, and also to clarify what we mean by an approximation, let's draw a picture of
the complex unit circle. We can put the numbers one, I,
negative one, and negative I in their usual places and we can consider some arbitrary point on the
circle parameterized by theta as we have in the problem statement. This is the point we
get if we start at one and rotate by an angle of
two times pi times theta, so the angle isn't
theta, it's two pi theta, and another way of thinking about it is that if we start at one and we travel a distance of two pi theta on the circle itself, we'll get to our point, E to
the two times pi times theta. Every point in the complex unit circle can be represented in this way
for a unique choice of theta. And that's because we go
all the way around circle as theta goes from zero to one, and if we went all the
way to theta equals one, we'd be back where we
started at theta equals zero, so that's why we include zero, but we don't include one. Just to make sure that each
point on the complex unit circle corresponds to a unique value of theta in the range that we're considering. There are various ways
that we could imagine approximating a particular value of theta, but the way that we'll do it is to approximate theta with a fraction, where the denominator
is two to the power M for some choice of a positive integer M, and the numerator is an integer y between zero and two to the M minus one. You can think about this as an mbit approximation to theta if you wish, and we could write this
number in binary notation where we have a binary point followed by the binary
representation of Y. I say binary point by the
way rather than decimal point because decimal means base 10 and I'm talking about base two
here, but it's the same idea. But in any case we can
also just think about it as a fraction like it
appears on the screen. And to be clear, when we think about
approximations in this context, it should be understood
that values of theta near zero and values of theta near one are in fact close together. So for example, if theta is equal to 0.99 and we approximate this value
of theta by the value zero, then that's a pretty good approximation in the sense that it's
within one of our hundred of the actual value, when
we wrap around the circle and go back to zero when we get to one. The term modular one is sometimes used to describe the situation, where we're essentially
equating theta equals one with theta equals zero. The point is that we really care about the location of the point
E to the two pi I theta on the unit circle, and thinking about this
way of approximating theta makes sense when we have that aim in mind. The problem statement itself,
as it appears on the screen, isn't specific about how good the approximation to theta needs to be, but that's something that we can fine tune depending upon our needs. In some situations we might be satisfied with a low precision
approximation to theta, and in other situations we might demand a very high precision approximation, and the problem statement itself
can be refined accordingly. Next we'll see how we can solve
the phase estimation problem with a quantum computer and
we'll start with a warmup. This won't be our final
solution to the problem. What we'll do is to make use of the phase kickback phenomenon to try to learn something
about the eigenvalue corresponding to a given eigenvector. Recall that in the
phase estimation problem we're given the description
of a unitary quantum circuit for some operation U, and what we do first is to
use this circuit description to create a new circuit that implements a controlled U operation like
is pictured on the screen. This is pretty simple to do. What we can do is to add a control Qubit, which is pictured on
the top in this diagram and use this new control
Qubit as a control for every single gate that
appears in our circuit. So all of the gates are applied if this control Qubit is set to one, and none of them is applied if the control Qubit is set to zero, That means that we're going
to need a controlled version of every gate that
appears in the circuit U, but we don't need to think
about these new controlled gates as single gates if we don't want to. They could be implemented
as small circuits made out of gates from whatever gates set we've chosen to use. And once we have a
controlled version of U, we can consider running this circuit, where we first put the control Qubit into a superposition of zero and one, apply the controlled U gate to the eigenvector side that we're given, and then apply a Hadamard gate
to the control and measure. We'll see why it makes sense to do this as we go through the analysis, but even before we get to the details, we can immediately see a similarity with Deutsche's algorithm for instance, where we're applying a
couple of Hadamard gates and measuring as a way
of effectively probing what it is that some given
operation is doing here. It's a controlled unitary
rather than a query gate, but there are very similar ideas at play. So let's see what the circuit does. The initial state is the eigenvector psi, tensored with the zero
state for the control Qubit, applying the first Hadamard gate transforms the state of the
top Qubit to a plus state and we can expand the entire
state as is shown right here. We then apply the controlled U operation and what that does is to apply U to psi when the control Qubit is set to one, but not when it's set to zero. So we get the state pi two
that's written on the screen. We can simplify the state
using the fact that psi is an eigenvector of U with eigenvalue e to the two pi I theta. So we get that eigenvalue multiplying the second
term, but not the first. And we can simplify using the bilinearity
of the tensor product. So what we have here is the
phase kickback phenomenon, but this time it's happening for some arbitrary eigenvector psi of some arbitrary unitary operation U, s opposed to a not operation being applied to a minus
state more specifically. So that's the state after the controlled U operation is applied. By the way, it's worth
taking a moment to notice that the state of the bottom end Qubits is still the eigenvector psi. Which is great because that
gives us the opportunity to use it again if we choose. Then the second Hadamard gate is performed and here is the state that we get. And I haven't shown all the
details of the derivation here, but if you apply the
Hadamard gate and simplify, this is what you should get. So that's the state just
prior to the measurement. So we can focus on this state to see what we can learn
about theta by measuring. And if we measure the top
Qubit or the rightmost Qubit, these are the probabilities that we get for the two outcomes. And here have used a
couple of basic formulas that relate sins and cosines
to complex exponentials to express these probabilities in terms of sins and cosines. Specifically, the probability to get zero is the cosine squared of pi times theta and the probability to get one is the sin squared of the same angle. And this makes sense
because the sin squared plus the cosine squared of
any angle is equal to one. So our probabilities
sum to one as we expect. And here is a plot of these probabilities as a function of theta. To be clear, this is a plot of
the probabilities themselves as opposed to a plot depicting a single probability
distribution for instance. For example, if theta
is equal to one half, then the probability that
we get the outcome one which is drawn in purple is equal to one while the probability
to get the outcome zero, which is drawn in blue, is zero. On the other hand, if theta is equal to one
quarter or three quarters, then we have that each of the two outcomes is equally likely. And of course for all choices of theta, the sum of the probabilities is equal to one as we've already observed. So we don't learn exactly what
theta is from this procedure, but it does give us some
information about theta. And if we imagine a situation in which we're promised
that theta is equal to zero or theta is equal to one half, this procedure will tell us exactly which one of the
two possibilities it is. So that's a simple way that we can use the phase kickback phenomenon to learn something about the eigenvalue. So how can we learn more
about the eigenvalue? Well, this is a situation in which we can let
ignorance be our guide. Because there really aren't
too many things that we can do. We don't know much of anything
about the eigenvector sin, and we also don't know
much about the operation U. We have a circuit for U, but for all we know it's
a horrible, confusing mess that we won't be able to make
any sense of by examining it. But one natural option is to run the circuit multiple times. In particular, we could try to do exactly the same thing that we just did, except that instead of applying just a single controlled U operation, we apply that operation twice. Or we could apply it any number of times, but let's start with twice
just to see what happens. Well, it's actually pretty
simple what happens. By applying the controlled
U operation twice, we're effectively just
squaring the eigenvalue because it'll get kicked into
the phase of the top Qubit two times rather than just once. And so when we think about
the outcome probabilities as a function of theta, we're essentially just
doubling the frequency. For each possible choice of theta, the outcome probabilities
are the same as they would be if we doubled theta and then
took the fractional part to get a number between zero and one. For example, if theta
is equal to one half, then the eigenvalue is negative one. So by applying U twice we're effectively squaring the eigenvalue and the result is the same as it would be for theta equals zero. Which is that the probability
for the outcome zero, pictured in blue, is equal to one, and the probability for the outcome one, pictured in purple, is equal to zero. This is very simple, but we can in fact get
some additional information about theta by doing this. For example, if we know in advance that either theta is equal to zero or theta equals one quarter, then this circuit will reveal to us which of the two possibilities it is. On the other hand, we can no longer distinguish the case that theta is equal to zero from the case that theta equals one half. So this procedure isn't strictly better than the one from before when
we had a single control U gate but by also running this circuit we can gain additional information. It's not immediately clear
how the additional information we can get by iterating the controlled U operation multiple times can be reconciled with
the information we get by performing it just once, and in fact there are multiple
ways we could proceed. For instance, there's a technique called iterative phase estimation, which involves running circuits like the ones that we've seen, along with some minor variations on them, to come up with an approximation to theta. But I'm gonna describe
what could be called a more traditional approach, where we use a single quantum circuit that has multiple control qubits that control different
numbers of iterations of the unitary operation U. First I'll describe how this
works for two control qubits and then a bit later we'll generalize it to more than two control qubits. Here's a quantum circuit
that uses two control qubits. The top control Qubit
controls a single U operation while the second from top
Qubit controls two copies of U. So what we're doing is combining the two circuits that we saw previously, in a way that makes use of the single copy of the eigenvector psi that we have. You'll notice however
that the Hadamard gates after the controlled U
operations have been removed and there are no measurements yet. What we're going to do is to
see how this circuit works and then we'll figure
out what we should do with the top two qubits in order to learn as much as possible about the value theta that
we're trying to approximate. After the Hadamard gates performed, we had the state that's
shown on the screen. And this is going to be a convenient way of writing the state to
see what happens next. A zero and A one are bits, and if we think about the top two qubits as being a two Qubit register, we're thinking about A zero, which corresponds to the top Qubit as being the lesser significant bit, and A one which corresponds
to the lower control Qubit as being the more significant bit. After the first controlled
U operation is performed, we obtain this state. The classical state corresponding to the top control Qubit is a zero, so the result of the phase kickback can be expressed in this way. Then after the second and
third controlled U operations are performed, we obtain this state. The idea is the same as for the first one, except this time we're applying the control U operation twice. So we multiply A one by two in
the exponent to reflect this. And now we can simplify this expression by thinking about the bits
A one and A zero together as an integer between zero and
three using binary notation. That is when we think
about the two bit string, A one A zero, as the binary
encoding of an integer, what we get is two
times A one plus A zero. And the understanding is
that when we write Cat X, we're talking about X written as a binary string of length
too using binary notation. So now let's focus on
these two control qubits to figure out what we
can learn about theta from them at this point. The remaining N qubits are
still in the same state psi that we were originally given, which is really a wonderful
thing about this approach because like I said before, we can use that state again if we want to gain even more
information about theta. But for now, let's just think about what we can learn from
these two control qubits. To help us to figure out how to proceed, let's imagine that we're promised that theta is equal to Y over four for some integer y between zero and three. In general, theta may not
be one of these four values, but this is a natural case to consider. We saw in the original single control Qubit version of the procedure that if we were promised that theta is either zero or one half the circuit would reveal which one it was. And we also noticed that in the version where we applied the
controlled U operation twice, we could perfectly discriminate the case that theta equals zero from the case that theta
equals one quarter. So we might hope that with our new circuit we can discriminate these four cases. In essence, what we're
doing is that we're using the notion of a promise problem
to make the problem easier, so that we can see how to proceed. And that's not a bad idea, in general. If you have a problem and you
don't know how to solve it, try making it easier. If you can solve the easier problem, the insight you gain might be helpful in solving the more general problem. And if you can't solve the easier problem, you're probably not ready
for the more general problem. So keep on making it easier
until you can solve it. Anyway, getting back to our problem, let's define a two Qubit state for each of the possible values of Y. In other words, phi Y is equal to the state
shown on the top of the screen when theta is chosen
specifically to be Y over four. And here are those four
states written out explicitly. So just to reiterate, if
theta is equal to Y over four for Y being zero, one, two, or
three and we run our circuit, we're going to be left with
one of these four states, whichever one corresponds
to the right value of Y. And we just wanna figure
out which one it is. And the question is, is this possible? And the answer to that question is yes, because these four states are orthogonal. If you compute the inner product
between any pair of them, you'll get zero. And because they're orthogonal, they can be perfectly discriminated by a projective measurement. And in particular we can
express such a measurement by defining one projection for each vector like we have here. This always works, anytime
we have an orthonormal basis, and you can check this
against the definitions we saw in lesson three
for this specific case if you're so inclined. So we conclude that it is
possible to accomplish our task, which is to learn the value of Y from the state that's shown
on the top of the screen. And if we'd like to do that
with a quantum circuit, we can proceed as I'll now explain. First we can define a unitary matrix V whose columns are the four
vectors listed in order. That matrix will have the action described on the bottom of the screen,
on the standard basis states, and we can write down the matrix
to see what it looks like. Because V maps each standard basis state, kappa Y to the vector phi Y, we have that by applying V dagger, or in other words V inverse to phi Y will transform back the other way to the corresponding standard basis state. So what we can do is to apply V dagger to whatever state we
obtained from our circuit. And then if we perform a
standard basis measurement, we'll get the number Y
that we're looking for, expressed as a string
of length two in binary. Before we go any further, let's talk a bit more
about the unitary matrix V. This is a special matrix and
you may have seen it before. It's the matrix associated
with a linear operation on four dimensional vectors known as the discrete Fourier transform. To be more precise, the discrete Fourier transform can be defined for any
positive integer dimension, and we'll take a look at it
in general in a few moments. And this is what it
looks like specifically when the dimension is equal to four. We can think about the
discrete Fourier transform in different ways. We can think about it purely abstractly as a linear mapping on vectors that can be represented by a matrix. And we can also think about
it as a computational problem where we're given a vector, and we want to compute the result of applying this linear mapping. That turns out to be an incredibly important
computational problem, and there's a highly efficient algorithm for performing that computation known as the fasts Fourier transform. In fact, this is such an important problem in signal processing for example, that the fasts Fourier
transform is considered by many to be one of the most important and consequential
algorithms ever discovered. We can also think about the
discrete Fourier transform as a unitary quantum operation that can be applied to
a quantum state vector. And when we think about it in that way, we call it the quantum Fourier
transform, or QFT for short. Mathematically speaking, it's essentially the same thing as the discrete Fourier transform, but we call it the
quantum Fourier transform just to clarify that it's an operation being applied to a quantum state vector. So let's change the name of our matrix, from V to QFT four to reflect the fact that the operation that we've come up with is in fact the quantum Fourier transform. So here's the complete circuit for learning the value of Y when we're promised that
theta equals Y over four for Y being zero, one, two, or three. And just to make sure it's clear, we're applying the inverse of
the quantum Fourier transform, or in other words QFT four dagger to map one of the four possible states back to the standard basis state, cat Y, so that we can recover Y by measuring. So that's the circuit we
get from thinking about the promised version of the
phase estimation problem, where we're promised that theta is zero, one quarter, one
half, or three quarters, but there's nothing that prevents us from running this circuit for
an arbitrary value of theta. And if we do that, here
are the probabilities for the four possible
outcomes that we obtain. Each of the four possible
values for Y has its own color, and the plot tells us
what the probabilities are for each one as a function of theta. So for instance, if theta
is equal to three quarters, we'll get the outcome
three with probability one, well the probabilities for the other three outcomes are all zero as we already know from thinking about the promise version of the problem. If theta is close to three quarters but not exactly three quarters, there will be some non-zero probability associated with the other outcomes, but we'll still get the outcome
three with high probability. So even though theta may
not be equal to Y over four, we can still think
about the outcome we get from the measurements divided by four as being a guess or an
approximation for theta. The guess may be wrong, but nevertheless it is
providing us with information. And one thing that we
can learn from the plot is that best approximations always have the highest
probabilities associated with them. When I say best approximation, I mean whatever value of Y over four is closest to the true value of theta, in the modular one sense. The worst possible case is that theta is exactly halfway between
two values of Y over four. For example, if theta is equal
to three eighths or 0.375, in this case we see
that the approximations one quarter and two quarters
or one half are equally good. So in this particular
case there are actually two best approximations rather than one. And we're equally likely to get the outcome one
and the outcome two. The other two outcomes zero and three, which don't give very good
approximations to theta when we divide by four, actually do appear with some probability which appears to be just
under 10% in both cases. So this procedure isn't perfect, but it does provide us with information. And it's noteworthy that
regardless of theta, we're always guaranteed to draw either the best approximation or one of the two best approximations when there's a tie, with probability that
appears to be just above 40%. And the closer that theta
is to the approximation, the higher the probability to see the corresponding value of Y. And that's a pretty
significant improvement over using a single control Qubit. This is in fact the phase
estimation procedure when we choose to use two control qubits. And what we'll do next is to generalize this
procedure in a natural way, where we don't stop with
just two control qubits but we use however many
control qubits we choose. And what we'll see is that by
generalizing the procedure, by adding more control qubits, we'll be able to get more and more accurate approximations of theta. And that will be the general
phase estimation procedure. In order to generalize the
phase estimation procedure from two control qubits
to however many we choose, we're going to need to understand how the quantum Fourier transform is defined for other dimensions. And we're also gonna need to discuss how it can be implemented
with a quantum circuit. Here's the definition of the
quantum Fourier transform when the dimension is an
arbitrary positive integer N. Here we're thinking about this operation as being an operation on a system whose classical states are
zero up to N minus one. And here the matrix that
describes this operation has been expressed using
the direct notation. We can also describe it in terms of its action
on standard basis states, just to connect it with
what we already saw when N is equal to four. So here's what it looks
like when N is equal to one, which isn't a very interesting case. Here it is for N is equal to two, and you'll notice what we get in this case is a Hadamard operation. Here's the quantum Fourier
transform for N equals three. For N equals four, which
we've already seen. And for one last example, here it is when N is equal to eight. Just like we had before
for the N equals four case, These are the matrices that we associate with the discrete Fourier transform for each of the dimensions. But we call it the
quantum Fourier transform when we want to think about
it as a quantum operation. Sometimes by the way, the leading factor of one over the square root of N is not included when we think about the discrete Fourier
transform as a matrix, but we need that factor here to make sure that we get a unitary matrix. There's also sometimes a minus sign in each of the exponents, but not always. It's basically just a personal preference that doesn't change the fundamental
nature of the transform. But in any case, we'll define the quantum Fourier transform as it's written here on the screen. There's a very useful shorthand
notation that's common and we'll make use of it
a bit later in the lesson. And that is to define a
complex number omega N, like we have here. Specifically as e to the two pi I over N, which can also be written
using signs and cosines as we have right here. Sometimes we call this
number an N through of unity because when we raise it
to the power N, we get one. Here's a figure that shows a few examples. Omega one is equal to one, omega two is equal to negative one, and omega four is equal to I. In general to get omega N, we can imagine slicing up the unit circle into N equal size pieces like it's a pizza being shared by N people and omega N is at the
corner of the first piece, assuming that the other corner
of that piece is at one. So in particular as N
gets larger and larger, the pieces will get smaller and smaller, and omega N will get
closer and closer to one. And now if we use that notation we can express the
quantum Fourier transform in a slightly cleaner way. Sometimes this notation
helps to keep things simple as we will see happening
a bit later in the lesson. There's an efficient quantum
circuit implementation of the quantum Fourier transform
when N is a power of two, which we'll now take a look at. Although we tend define the
quantum Fourier transform for any dimension N,
we're only gonna need it in the case that N is a power of two, in order to generalize the
phase estimation procedure. The implementation will make
use of controlled phase gauge which are depicted as we
have here on the screen. They look like two
controls connected together along with a number which
is alpha in this case. And the action of this gate is described by the matrix that's shown here. These are literally
controlled phase gates, meaning that we have a control Qubit that controls a phase gate. We saw phase gates way back in lesson one. But for phase gates specifically, it doesn't actually matter
which Qubit is the control and which is the target. And we saw that in lesson two, for the specific case of a Z gate. And now that we have that notion, we can move on to how the
quantum Fourier transform can be implemented. The implementation is recursive in nature and perhaps the easiest way
to describe it in simple terms is to give an example that
can easily be generalized. Here's how we can do it
when N is equal to 32. So there are five qubits. To be clear, we're associating the
standard basis states of these five qubits with the integers between zero and 31 using binary notation. The first thing we do is to perform the quantum Fourier
transform for N equals 16 to all of the qubits except for
the least significant Qubit, which is the one on the top. And that's the recursive
part of the construction. So to implement this operation, we have to follow the same
pattern but with one fewer Qubit. And in the base case we
have just a single Qubit for which the quantum Fourier transform is just a Hadamard gate. Once we apply the QFT on all of the qubits except for the top one, we apply a bunch of controlled phase gates using the phases that are
indicated in the diagram. We then apply a Hadamard
gate to the top Qubit, and shuffle the qubits around a little bit using some swap gates. These swap gates have the effect of cyclically permuting the qubits which is needed to make
the construction work. We can follow exactly the same pattern for other choices of N
provided they're powers of two, where we always start the
controlled phase gates with alpha being pi over
two for the bottom Qubit and dividing by two each time
moving left in the diagram to get the subsequent phases. Now it's not at all obvious
that this construction works. And I'm not gonna explain
why it works in this video. If you're interested in seeing
the details for why it works, you can find them in the
written content for the lesson. I will say however that
what's being exploited here is the basic structure of
discrete Fourier transforms and in fact it's exactly
the same structure that allows the fast
Fourier transform to work. Another way to say this is that if we look at the fast Fourier transform, we can think about it in
terms of quantum circuits. And this is the circuit that we get. So how many gates do we need in total for this implementation? To figure that out, let's let SM be the number of gates that
we need when we have M qubits. When M is equal to one, we just need a single gate because the QFT is just a Hadamard operation in this case. If M is at least two, then these
are the gates that we need. First we need SM minus one gates for the quantum Fourier
transform on M minus one qubits, whatever that number happens to be. We also need M minus one
controlled phase gates, M minus one swap gates, and one additional Hadamard gate. So here's an expression
of SM in both cases when M is equal to one and
when M is at least two. This is an example of what's called a recurrence relation and
they show up very commonly in the analysis of algorithms. This particular one
happens to be very simple and it has a so-called
closed form expression, which we can obtain by summing. To be precise, the value of SM is the sum of the first M
consecutive odd integers, which equals M squared, as you may have either encountered before or maybe you just noticed that
at some point in your life. And it happens to be pretty easy to prove this formula is true by induction on M in
case you're interested in a formal proof. So that's it. We can implement the
quantum Fourier transform on M qubits using M squared gates. Just a couple of very
quick additional remarks. First we don't actually
need all of the swap gates that I've described. It is possible to effectively
push them all out to the end as long as we adjust which qubits the controlled phase gates
act on appropriately. And if we do that we'll only
need M over two swap gates in total rounding down. We'll still have a quadratic cost, but the number of gates can be reduced in this way nevertheless. And one final remark is that we can actually make the cost even less if we're willing to settle for
a pretty good approximation. For example, when M gets large, a lot of the controlled phase gates are pretty close to
the identity operation, and eliminating the
ones that are very close can save us quite a lot on gates while still giving us a
pretty good approximation. It is also possible to approximate the quantum Fourier transform
with very shallow circuits, meaning ones with small depth, in case that's something
that we wish to do. And now that we know what the
quantum Fourier transform is for different dimensions, as well as how it can be implemented w hen the dimension is a power of two, we can go back to the
phase estimation procedure and see how it can be generalized where we use some arbitrary
number M of control qubits. Here's what the procedure looks like as a quantum circuit for
an arbitrary choice of M. We have M control qubits on top, which we assume are all
initialized to the zero state. We apply a Hadamard
gate to each one of them and then each of them is
used as a control Qubit to apply some power of
the unitary operation U, where we double the number
of times U is applied for each control Qubit. Here in this diagram I've indicated this by including the powers
on U inside of the boxes rather than showing the controlled U gate being applied that many times, as a way of both making
the diagram more clear and saving on space so
that it fits on the screen. Now the first thing that we need to notice
about this procedure is that it might become very expensive. In particular we're
doubling the number of times each U operation gets applied
for each control Qubit. And if the way that we do this is simply to iterate a controlled U gate however many times we need, we're going to get a huge circuit. It'll be exponentially
large in the number M. That's a major limitation in general, and in particular it limits our ability to get precise approximations
of eigenvalues. In the specific case of factoring though, which we'll get to shortly, it turns out that we're very fortunate and we can play a trick that allows us to avoid this exponential blow up in cost. But in general for arbitrary
unitary operations U, this won't be possible and
we'll pay a steep price for increasing the
number of control qubits. I'm not going to go
through a detailed analysis of the circuit in this video and instead I'll just mention that there is a fairly simple
expression for the state just prior to the measurements which is shown here on the screen. From that state we can get an expression for the probabilities
of the various outcomes, and it happens to look like this. And once we have this expression,
we can reason about it, and really this expression
is all we need to carry on. Like I said, I'm not gonna get into
the details in this video, but you can find them in the
written content for the lesson and it turns out that it's
actually not that complicated, so check it out if you're so inclined. If we do go through the analysis, what we find is that the situation is not all that different
from the M equals two case, except that we get more precision. In particular, we can think
first about the probability to get the very best or one of
the two best approximations, which we can express like we have here. You can ignore this subscript of a one on the absolute value if you like. This is just a symbol that we
can use to remind ourselves that we're talking about
approximations on the circle where we think about theta equals zero and theta equals one as being the same. The point here is that
we're thinking about whatever choice of Y makes Y over two to the power M as close as possible to theta. And once again, just like we
had for the M equals two case, we're guaranteed to get this outcome Y with probability at least 40%. In fact, the probability is
at least four over pi squared, which is a little bit more than 40%. 10 is not a bad
approximation to pi squared when you're in a hurry. And that's actually pretty remarkable. What we're talking about is getting the very best
approximation to theta as a fraction Y over two to the power M where Y is an integer. And we're getting the
very best approximation with probability greater than 40%. We can also think about
approximations that are worse. Specifically let's imagine
that Y isn't quite so good, and more specifically let's suppose that there's a better
approximation to theta of the same form that lies between theta and Y over two to the M, which can be expressed as
is written on the screen. In such a case the
probability to get this Y will be relatively small, at most 25%. It'll actually be lower than that, but 25% is what we get from
a pretty simple analysis, but it's good enough. As an example. Here's the plot we get when we use three control
qubits rather than two. This time I'm only
plotting the probabilities for the outcomes three, four, and five because I don't want the
figure to be too messy. When theta is equal to
one half for instance, we're guaranteed to get the outcome four. And so we take our
approximation to be four eighths or one half because two to
the power three is eight. And in this case we're exactly right. And as long as theta is closer to one half than it is to say three
eighths for instance, we get the outcome four with a probability higher
than the outcome three. And with probability,
again larger than 40%. Here's the plot when M is equal to four. And the curves actually
look pretty much identical, except that now we've zoomed
in by a factor of two. So now the outcomes that are plotted include seven, eight, and nine and we're taking the approximation to be one of these numbers
divided by 16 rather than eight. And I'll show one more plot where we increase to M equals five. And again we're gonna be
zooming in by a factor of two. And here it is. Once again the curves look
pretty much identical, but we're effectively
doubling our precision. And as before we obtained the very best approximation to theta, with probability greater than 40%. That's pretty amazing. But if 40% is not good enough
as a measure of confidence, what we can do is to run the procedure a small number of times, and
take the mode of the outcomes, meaning the one that
appears most frequently. And if we do that, we'll be extremely likely to end up with a very good approximation to theta. Remember by the way that every time we run the
phase estimation procedure we have that the eigenvector
comes out unchanged. So it is available for us to run the procedure
multiple times like this. In short, we're gathering
statistics about theta, and the point is that by doing this we can get a very precise
approximation to theta with very high confidence, and we don't need that many iterations of the procedure to do this. And that is how the phase
estimation procedure works. In the last part of the lesson, we'll apply the technique
of phase estimation to the integer factorization problem. As I mentioned at the start of the video, the way this works is that we consider an intermediate problem known as the order finding problem. We'll see how we can solve
the order finding problem using phase estimation, and then we'll briefly discuss how solving the order finding problem allows us to factor integers efficiently. The second part is completely classical and I'll only summarize the
basic idea in this video. But let's begin with the
order finding problem. First, let's get some
handy notation in place. For every positive integer N, we define ZN to be the integers
zero through N minus one. So for instance, Z one has a
single element which is zero, Z two has elements zero and one, z three has elements zero,
one and two, and so on. So ZN is a set. But we can also think about
it as more than a set. We can think about arithmetic operations like addition and multiplication on ZN by defining them modular N. For example, let's suppose
N is equal to seven. We have that three and five
are elements of Z seven and if we multiply them together we get 15 which leaves a remainder of
one after dividing by seven. Sometimes we express this by writing that three times five is
congruent to one module seven, like is written here. But we can also simply write that three times five equals one. Of course three times five
isn't normally equal to one, it's equal to 15. But when it's clear that
we're working in Z seven meaning that multiplication
is defined modular seven, we can simply write that
three times five equals one. Now among the elements
of ZN for any positive N, some of them will have
GCD equal to one with N and some won't. And the elements that do have GCD equal to one with N are special. and we write ZN star to refer to the set of just these elements. So for example, here's Z 21 star. among the elements of Z 21, which there are 21 of course, it turns out that 12 of them have GcD equals of one
with 21 and the rest don't. In particular anytime that
we have an element of Z 21 that's divisible by either
three or seven, we skip it. There are various reasons
why this set is special, and one of the reasons is that anytime we have an element of ZN star and we consider the positive
integer powers of that number, we will always eventually
hit the number one, where just like before we're talking about multiplication modular N. the smallest positive integer power that gives us one is called
the order of that element. For example, think about the element A equals four in Z 21 star. four times four is 16, which is not equal to one modular of 21, but if we multiply by four one more time, we get 64, and 64 modular
21 is equal to one. So the order of four modular 21 is three. Getting to one like this always happens for all of the elements of ZN star. And by the way, it never
happens for elements of ZN that aren't in ZN star. So having GCD equal to
one with N like this is both necessary and
sufficient for this to happen. And just to finish up the
example of N equals 21, here are the smallest
positive integer powers for which this works for
each element of Z 21 star, and they can each be checked one by one. And now we can state the
order finding problem, which is simply to find the order for a given element in ZN star. To be precise, the input is
two positive integer A and N for which the GCD is equal to one, and the output is the
smallest positive integer R such that A to the power R is
congruent to one modular N, or equivalently the
smallest positive integer R for which A to the R is equal to one. Assuming we're doing arithmetic in ZN. This is believed to be a
computationally difficult problem, no efficient classical algorithm
is known for solving it. And as we'll discuss the problem is at least as hard as
integer factorization in the sense that if we have an algorithm for solving order finding, we would be able to use that algorithm to solve the integer
factorization problem efficiently. In fact, it goes the other way as well, meaning that if you happen to have an algorithm for factoring, you can use it to solve
the order finding problem. So the two problems are
in some sense equivalent in terms of their
computational difficulty. Now we'll see how we can solve
the order finding problem on a quantum computer
using phase estimation. To make a connection between the problems, let's imagine that we have a system whose classical state set is AN. For any choice of an element A in ZN star, we'll define an operation
MA like we have here. In words what MA does is to multiply by A, and here the multiplication is modular N. And for the remainder of this lesson, you should always think about
expressions inside of cats as being computed modular N, just so we can keep things simple and we don't have to keep on
writing mod N over and over. This is a unitary operation, but that's only true
because A is in ZN star. If the GCD of A and N isn't one, this operation won't be unitary. Here's an example. If N is equal to 15 and A is equal to two, which is in Z 15 star, then
this is the action of M two. And if you examine this
example for a few moments, you'll see that what's happening is that the elements of ZN are just getting permuted by this action. So it's both deterministic and unitary, which is to say that as a matrix, MA is a permutation matrix. And here's the main idea. The eigenvalues of MA are
very closely connected with the order of A. And by using phase estimation to approximate these
eigenvalues closely enough we can determine the order of A. So what exactly are the eigenvectors and the eigenvalues of MA? Well, we're not going to need all of them, we only need some of them. Let's start off with an easy example. Here's an eigenvector of MA, we've named it size zero, and it's given by the
sum of cat one, cat a, cat A squared, and so on up to cat A to the power R minus one, all divided by the square root of R to make it a unit vector. Here R is the order of A. And I'll reserve the
letter R for the order of A for the rest of the lesson. So whenever you see R, that's the order of A for
whatever A we're talking about. The eigenvalue associated
with this eigenvector is one. And we can verify that by
applying MA to this vector to see what we get. MA multiplies by A, so
we just need to increase each of the exponents by
one and this is what we get. But the order of A is R, so A to the power R equals one. And we can make that substitution. But that's the same vector
that we started with. All we've done is to
shuffle the cuts around but we're taking the sum so
this doesn't change anything. So MA times psi zero is equal to psi zero. So it's an eigenvector of
MA with eigenvalue one. That's a very simple
example of an eigenvector and to identify more of them, let's recall the same notation that we had earlier in the lesson. Omega R is the complex
number on the unit circle we get by taking E to the
power of two times pi times I divided by R. Last time we had an N rather than an R, but it still works the same
way for R in place of N. We need this number because
it's going to be appearing in the other eigenvectors
of MA that we'll identify. And now that we've recalled that notation, we can observe that this
vector named psi one is also an eigenvector of MA. It's similar to psi zero, we have exactly the same cats and we're summing them up and
dividing by the square of R. But this time we have various phases multiplying each of the cats. Omega R to the negative one for A, omega R to the negative two for A squared, and so on up to omega R to
the power negative R minus one for A to the power R minus one. So the power of omega R
is always negative one times whatever power of A
appears inside of the cat. I won't go through it in detail, but it is shown on the screen in case you'd like to re-watch the video and pause as needed. But if we do the multiplication and we simplify again using the fact that A to the power R is equal to one as well as the very important fact that omega to the power R is equal to one, what we'll find is that we
again have an eigenvector of MA. This time as we do the simplification, we'll need to pull out a factor of omega R to see that indeed we have an eogenvector and that vector of omega R represents the eigenvalue associated with this eigenvector. So that's a second eigenvector
eigenvalue pair of NA. We can identify additional eigenvectors using exactly the same reasoning, And in fact, if we simply replace omega R with omega R to the power J for each J between zero and R minus one, we'll always get an eigenvector, and the associated eigenvalue will be omega R to the power J. So this actually includes
psi zero and pis one, which we already saw
as well as R minus two, other eigenvector, eigenvalue pairs. They all work in basically the same way, and to verify them, we just need to know that A to the power R equals one as well as omega R to
the power R equals one. There are other eigenvectors of MA like cat zero for instance, but they aren't going to be useful to us and we don't need to worry about them. We only need to worry about psi zero through psi R minus one. Let's focus on psi one for a moment, just to try to understand why it is that we care about these eigenvectors and what they can tell
us about the order R, which is what we're trying to find. Supposing that we're given
the eigenvector psi one, we can attempt to learn
R in the following way. First, we perform phase
estimation on this eigenvector using a quantum circuit that
implements the operation MA. What we get is an approximation to theta, which is one over R in this case. And second because we're looking for R and we have an
approximation to one over R, the natural thing to do is
to compute the reciprocal and to round it off to
the nearest integer. We have to round like this
because R has to be an integer. But we don't have one of our R exactly, we just have an approximation to it. But if the approximation is good enough, we'll get R when we round,
which is what we're looking for. And now the question is how much precision do we need to correctly determine R by computing the reciprocal and rounding? Or in other words, how accurate does our
approximation need to be to make this all work? Well, it turns out that we do need a pretty accurate approximation. For example, we can't
afford to confuse one over R with one over R plus one, or one over R minus one for instance. In those two fractions, one over R plus one and
one over R minus one are pretty close to one
over R when R is large. But although we do need a
pretty accurate approximation, we don't need a ridiculously
accurate approximation, and it can be shown that
if our approximation satisfies the inequality that
we have here on the screen, then rounding off will
give us the right answer. I won't try to argue that in this video, but it is shown in the written
material for the lesson in case you're interested in the details. And if that's the accuracy
that we're going for, then it suffices to use a
number of control qubits in the phase estimation procedure that's equal to two times
the length of N plus one. Where as we had in the previous lessons, the length of N is the number of bits that we need to express N in binary. By the way, if we're content
to get the best approximation with probability at least 40% and we don't try to
increase our confidence like I described earlier, then we don't actually need the plus one. But in any case, we have that the number
of control qubits we need is linear in the length of N
written as a binary string. We've seen that if we have
the eigenvector psi one, we can use phase estimation to find R, provided we use enough accuracy. Unfortunately it's not so easy to get our hands on this eigenvector. So let's ask a different question. Suppose we don't have psi one, but rather we have psi
for a random choice of J. Can we still figure out what R is? Or at the very least can we
figure out something about R? And the answer is yes, we can figure out something
about R, at least on average. What we can do is to try
to figure out J over R using a very similar method
to the one that we just saw, but with a key difference. First, we'll do exactly the
same thing we did before, which is to run the phase
estimation procedure on the eigenvector psi J. And what we'll get is an
approximation to J over R, as opposed to one over R,
which is what we had before. But now we have a small problem, because computing the
reciprocal won't give us R, it will give us R over J. And by the way, we're not
assuming that we know what J is, we're simply given psi J for a random J, and we're not told what J is. So what can we do? Well, it turns out that there's a well-known
classical algorithm that can help us, called the
continued fraction algorithm. And what this algorithm can do is to take our approximation
Y over two to the M and tell us which fraction U over V, written in lowest terms, and where U and V are both
integers smaller than N is closest to our approximation. It's a truly fascinating algorithm, and if you're interested in it, you should definitely check it out. It's very closely connected to Euclid's algorithm for
computing GCDs in fact. But I won't say anything
more about it in this video. The point of this is that if we have a good
approximation to some fraction for which the numerator and
denominator aren't too big, the algorithm can recover the fraction, and that's what we're doing here. And now we're left with exactly
the same question as before, which is how much precision do we need? And it turns out that
exactly the same bound that we had before works in this case. And so choosing M in exactly
the same way as before works here as well. Now this isn't necessarily
going to tell us R. What we'll learn if we use
enough precision, is J over R, but that will be a
fraction in lowest terms. So we could get unlucky in the sense that this random choice of J could have common factors with R, and they'll be hidden from us. In the worst case for instance, we could get J equals zero and we will learn absolutely nothing. But that is literally the only
case where we learn nothing. In general, we'll at least
get a non-trivial factor of R, and if we're able to draw random samples and get psi J for different choices of J, we can in fact recover
R with high probability by computing the least common multiple of all of the denominators
that we observe. And this actually doesn't
require very many samples at all. The bottom line is that some choices of J will hide factors of R, but random choices of J won't be able to hide the
factors of R for long. So that is the idea. It remains to consider the implementation and in particular to figure
out how much it costs. The essence of the idea, to compute the order of a
given element A and ZN star is to apply the phase estimation procedure to the operation MA. Let's figure out what
the cost of doing this is as a function of the length of capital N, which I'll denote by little n. Here's a circuit for phase estimation. We're going to need little n qubits on the bottom part of the circuit to encode elements of ZN in binary. So we have N control qubits on top and little n qubits on the bottom, which is nice because that matches with the names that we
used earlier in the lesson. In order to count the
number of gates that we need for the entire circuit, let's focus first on the cost of the controlled unitary operations. If we use the techniques
from the previous lesson, lesson six, we can implement MA using big O of little n squared gates using efficient classical algorithms for arithmetic modular N. I'll skip the details in this video, but like several other
things I've talked about, you can read more about it in the written material for the lesson. It's not enough though
to implement just MA. We also need to implement MA squared, MA to the fourth power, to
the eighth power, and so on all the way up to MA to a power that's exponential in M. And we know we're going to need to take M to be linear in little n
to get enough precision. This is a big concern,
and I raised it earlier. If we were to implement powers
of MA by simply iterating MA, we'd be in trouble because all these iterations of MA
would incur exponential cost. Fortunately, we can play a trick to avoid this blow up in cost. What we can do is as follows, for each power K that we need. First, we compute A to
the power K, modular N. This is the modular exponentiation problem that I mentioned in lesson six. And it can be solved efficiently. And this is something that we can do with a classical computer. This computation doesn't
need to be performed by a quantum circuit at all. So let's let B be the result. So B is an element of ZN star. And second, in place of MA to the power K, we just use a circuit for MB. And that's it. B is just some element of ZN star, So the cost to implement MB
is big O of little n squared. In a nutshell, what we're doing is exponentating in ZN rather than exponentating
a quantum circuit. But the actual operation we
get is the same either way. It's just multiplication by A, K times. This is kind of a miracle in fact. And we wouldn't be able to
solve the order finding problem or effect integers efficiently
with a quantum computer if it wasn't for this. And now that we know that each of the controlled
unitary operations, including the powers, can be implemented at cost big O events squared, we can count the total
number of gates we need. We have N Hadamard gates, and because M is equal
to big O of little n, the cost is big O of little n. We have M controlled unitary operations that each have cost big O of N squared. So the total cost here
is big O of N cubed. And finally, the quantum Fourier transform has cost big O of n squared. So the total cost is big O of N cubed. In particular, we see that even with our
module exponentiation trick, the cost of the controlled
unitary operations dominates. But nevertheless, the cost
is polynomial in little n. There are also some classical
computations required, like computing the modular exponentiations and running the continued
fraction algorithm, but those computations
also have polynomial cost. And in fact, they can be done in big O of N cubed operations as well. There's only one remaining issue, and that is that we need to get our hands on one of these eigenvectors. How can we do this? The solution is actually very simple. We don't run the circuit
on an eigenvector. We just run it on the state one, meaning the binary
encoding of the number one, which we're thinking
about as an element of ZN. So let's make that
replacement in the diagram. The reason this works is because of this equation right here. The state cat one is in
fact a uniform superposition of the R eigenvectors that
we've been talking about. And that's not immediately obvious, but if you calculate the
sum, you should get cat one. And if we run the circuit on this state, the output that we'll get
will be exactly the same as if we'd randomly chosen J and run the procedure on
the eigenvector psi J. Again, that's not obvious, but an analysis reveals it to be true. And finally, that is it. We have an efficient quantum
algorithm for order finding. There's one very short part
of the lesson remaining and a concerns the relationship between integer factorization
and order finding. I mentioned that if we can solve the order finding problem efficiently, then we can also factor
integers efficiently. And I want to very briefly give you the main idea for how this works. It's completely classical. This part actually has nothing
to do with quantum computing. Here's a method for finding
a non-trivial factor of a given positive integer N. And by the way, this method only works when N is an odd number, and it's not a prime
number raised to a power. Those two cases when N
is even or a prime power can be handled separately. There are efficient classical methods for both detecting and
handling those two cases. This method is probabilistic. It might fail, but the
probability of failure will be at most a half, and we can simply run it
multiple times independently to reduce the probability
of error exponentially fast. First, we choose A between
two and N minus one at random, uniformly. Then we compute the GCD of A and N. And if the GCD happens
to be larger than one, then we've just been incredibly lucky. The GCD will be a factor of N and we can just output it and stop. Otherwise, the algorithm continues on and we now know that A is in ZN star. And now we compute the order of A, which is where we need a quantum computer. Or maybe we have a different
way to compute the order. The algorithm doesn't really
care where it comes from, but the only way that we know
how to do this efficiently is with a quantum computer. If the order of R is even, then we compute the GCD of
A to the power R over two minus one with N. And if that GCD is larger than one, then again we have a factor of N, so we output it and we stop. Otherwise the algorithm has failed. And in that case, we
can go back to the start and choose a different A and hope for a better
outcome the next time around. And that's it. And by the way, once
we found a factor of N, we can recurs on both that factor and divide it by that factor until we have a prime factorization of N. So why should this work? Well, there are two main points. The first is that because A to the power R is congruent to one modular N, we have that N divides A
to the power R minus one. Those two statements
are actually equivalent. This is the definition of mod N that you'll find in number theory books. Second, if R is even, then this equation that
we have here is true. This is the well-known formula
for a difference of squares. In fact, the equation is true for all R, but we need R over two to
be an integer in step four. And so if we put the two facts together, we see that every prime factor of N must divide one of the two
expressions in parentheses, because that's the only way
that N can divide the product. Now, it could be that every
single prime factor of N divides the first
expression in parentheses and not the second, but for our random A, that's not very likely. And so when we compute
the GCD in step four, we're likely to get the product of some of the prime
factors of N, but not all. To analyze this carefully
requires some number theory, but it is pretty basic number theory. So if you find number
theory to be interesting and you'd like to learn more about it, try working through this analysis. Working through things like this is an excellent way to learn more, but I will leave that to you. And that's the end of the lesson. In this lesson, we covered
the phase estimation problem, how we can solve it efficiently
on a quantum computer, and how we can apply the solution to obtain a polynomial
cost quantum algorithm for integer factorization. I hope you'll join me for the next lesson, which is on Grover's
quantum searching algorithm. Goodbye until then.

## Purifications and Fidelity ｜ Understanding Quantum Information & Computation ｜ Lesson 12

welcome back to understanding Quantum welcome back to understanding Quantum
information and computation my name is information and computation my name is information and computation my name is
John matus and I'm the technical John matus and I'm the technical John matus and I'm the technical
director for education at IBM director for education at IBM director for education at IBM
Quantum this is the 12th lesson in the Quantum this is the 12th lesson in the Quantum this is the 12th lesson in the
series and it's the last lesson in the series and it's the last lesson in the series and it's the last lesson in the
third unit which is on the general third unit which is on the general third unit which is on the general
formulation of quantum formulation of quantum formulation of quantum
information this lesson is centered information this lesson is centered information this lesson is centered
around a fundamental notion in Quantum around a fundamental notion in Quantum around a fundamental notion in Quantum
information which is that of a information which is that of a information which is that of a
purification of a Quantum purification of a Quantum purification of a Quantum
State a purification of a Quantum State State a purification of a Quantum State State a purification of a Quantum State
represented by some density Matrix is a represented by some density Matrix is a represented by some density Matrix is a
pure state of a larger compound system pure state of a larger compound system pure state of a larger compound system
that leaves us with the original state that leaves us with the original state that leaves us with the original state
when the rest of the compound system is when the rest of the compound system is when the rest of the compound system is
discarded or traced discarded or traced discarded or traced
out every density Matrix has a out every density Matrix has a out every density Matrix has a
purification and we can learn a lot purification and we can learn a lot purification and we can learn a lot
about Quantum States and about Quantum about Quantum States and about Quantum about Quantum States and about Quantum
information in general by studying information in general by studying information in general by studying
purifications at an intuitive level purifications at an intuitive level purifications at an intuitive level
Quantum State vectors which represent Quantum State vectors which represent Quantum State vectors which represent
pure states are simpler mathematical pure states are simpler mathematical pure states are simpler mathematical
objects than density objects than density objects than density
matrices and by thinking about matrices and by thinking about matrices and by thinking about
purifications of States we're able toew purifications of States we're able toew purifications of States we're able toew
arbitrary Quantum states of systems as arbitrary Quantum states of systems as arbitrary Quantum states of systems as
being just parts of something larger but being just parts of something larger but being just parts of something larger but
simpler we'll also talk about the simpler we'll also talk about the simpler we'll also talk about the
Fidelity between two Quantum states Fidelity between two Quantum states Fidelity between two Quantum states
which is a value that quantifies their which is a value that quantifies their which is a value that quantifies their
similarity or overlap and it's often similarity or overlap and it's often similarity or overlap and it's often
used as a way of measuring how good a used as a way of measuring how good a used as a way of measuring how good a
particular state is as compared with particular state is as compared with particular state is as compared with
some ideal some ideal some ideal
state in particular we'll discuss the state in particular we'll discuss the state in particular we'll discuss the
definition of fidelity along with some definition of fidelity along with some definition of fidelity along with some
of its basic properties and we'll see of its basic properties and we'll see of its basic properties and we'll see
how Fidelity connects with purifications how Fidelity connects with purifications how Fidelity connects with purifications
here's an overview of the lesson we'll here's an overview of the lesson we'll here's an overview of the lesson we'll
start with purifications including the start with purifications including the start with purifications including the
definition and a simple proof of their definition and a simple proof of their definition and a simple proof of their
existence which tells us how to find one existence which tells us how to find one existence which tells us how to find one
if we need if we need if we need
one then we'll discuss something known one then we'll discuss something known one then we'll discuss something known
as the Schmid decomposition of a Quantum as the Schmid decomposition of a Quantum as the Schmid decomposition of a Quantum
State Vector of two systems and we'll State Vector of two systems and we'll State Vector of two systems and we'll
see how Schmid decompositions relate to see how Schmid decompositions relate to see how Schmid decompositions relate to
purifications then we'll discuss a purifications then we'll discuss a purifications then we'll discuss a
critically important relationship critically important relationship critically important relationship
between any two purifications of the between any two purifications of the between any two purifications of the
same state which is known as the unitary same state which is known as the unitary same state which is known as the unitary
equivalence of purifications equivalence of purifications equivalence of purifications
and finally we'll go through a few and finally we'll go through a few and finally we'll go through a few
examples we then turn to Fidelity examples we then turn to Fidelity examples we then turn to Fidelity
including its definition and some of its including its definition and some of its including its definition and some of its
basic properties we'll talk about the basic properties we'll talk about the basic properties we'll talk about the
gentle measurement Lemma which is a very gentle measurement Lemma which is a very gentle measurement Lemma which is a very
handy fact that ties Fidelity back to handy fact that ties Fidelity back to handy fact that ties Fidelity back to
measurements and finally to conclude the measurements and finally to conclude the measurements and finally to conclude the
lesson we'll turn toan's theorem which lesson we'll turn toan's theorem which lesson we'll turn toan's theorem which
establishes a fundamental connection establishes a fundamental connection establishes a fundamental connection
between Fidelity and between Fidelity and between Fidelity and
purifications let's start with the purifications let's start with the purifications let's start with the
definition of a definition of a definition of a
purification a purification of a state purification a purification of a state purification a purification of a state
which assume is represented by a density which assume is represented by a density which assume is represented by a density
Matrix is a pure state of a larger Matrix is a pure state of a larger Matrix is a pure state of a larger
compound system that leaves the original compound system that leaves the original compound system that leaves the original
state that we started with when the rest state that we started with when the rest state that we started with when the rest
of the compound system is traced of the compound system is traced of the compound system is traced
out we can express this more precisely out we can express this more precisely out we can express this more precisely
in mathematical terms as in mathematical terms as in mathematical terms as
follows if we have a system X that's in follows if we have a system X that's in follows if we have a system X that's in
some State row meaning that row is a some State row meaning that row is a some State row meaning that row is a
density Matrix representing a state of X density Matrix representing a state of X density Matrix representing a state of X
and S is a Quantum State Vector of a and S is a Quantum State Vector of a and S is a Quantum State Vector of a
pair of systems XY such that tracing out pair of systems XY such that tracing out pair of systems XY such that tracing out
y from the pure State C side bro plus I y from the pure State C side bro plus I y from the pure State C side bro plus I
leaves us with row then we say that s is leaves us with row then we say that s is leaves us with row then we say that s is
a purification of a purification of a purification of
row naturally the names don't matter we row naturally the names don't matter we row naturally the names don't matter we
can name our systems and our states can name our systems and our states can name our systems and our states
whatever we want and The Ordering of the whatever we want and The Ordering of the whatever we want and The Ordering of the
systems also doesn't matter for example systems also doesn't matter for example systems also doesn't matter for example
if the system y was on the left rather if the system y was on the left rather if the system y was on the left rather
than on the right but everything else than on the right but everything else than on the right but everything else
was the same we'd still refer to SAI as was the same we'd still refer to SAI as was the same we'd still refer to SAI as
being a purification of row but we'll being a purification of row but we'll being a purification of row but we'll
mainly stick to this setup for the mainly stick to this setup for the mainly stick to this setup for the
remainder of the lesson just for the remainder of the lesson just for the remainder of the lesson just for the
sake of sake of sake of
Simplicity and the first thing to say Simplicity and the first thing to say Simplicity and the first thing to say
about definition is that purifications about definition is that purifications about definition is that purifications
always exist for every density Matrix always exist for every density Matrix always exist for every density Matrix
row and to be more precise every density row and to be more precise every density row and to be more precise every density
Matrix row has a purification like the Matrix row has a purification like the Matrix row has a purification like the
definition describes provided that the definition describes provided that the definition describes provided that the
system y is at least as large as X which system y is at least as large as X which system y is at least as large as X which
is to say that y has at least as many is to say that y has at least as many is to say that y has at least as many
classical States as classical States as classical States as
X the notion of a purification and the X the notion of a purification and the X the notion of a purification and the
fact that purifications always exist fact that purifications always exist fact that purifications always exist
turns out to be critically important and turns out to be critically important and turns out to be critically important and
it's also part of what makes Quantum it's also part of what makes Quantum it's also part of what makes Quantum
information beautiful and hopefully I'll information beautiful and hopefully I'll information beautiful and hopefully I'll
be able to convince of that fact by the be able to convince of that fact by the be able to convince of that fact by the
end of the end of the end of the
lesson now let's prove that every lesson now let's prove that every lesson now let's prove that every
density metrix has a purification and in density metrix has a purification and in density metrix has a purification and in
the process we'll see how we can get our the process we'll see how we can get our the process we'll see how we can get our
hands on a purification when we need hands on a purification when we need hands on a purification when we need
one just like we had in the definition one just like we had in the definition one just like we had in the definition
of a purification let's suppose that X of a purification let's suppose that X of a purification let's suppose that X
is a system and row is a density Matrix is a system and row is a density Matrix is a system and row is a density Matrix
that represents a state of that represents a state of that represents a state of
X now consider any expression of row as X now consider any expression of row as X now consider any expression of row as
a convex combination of pure States as a convex combination of pure States as a convex combination of pure States as
we have written on the screen in words we have written on the screen in words we have written on the screen in words
for some choice of of a positive integer for some choice of of a positive integer for some choice of of a positive integer
n we have a probability Vector p 0 n we have a probability Vector p 0 n we have a probability Vector p 0
through PN minus1 as well as Quantum through PN minus1 as well as Quantum through PN minus1 as well as Quantum
State vectors 5 0 through 5 nus one and State vectors 5 0 through 5 nus one and State vectors 5 0 through 5 nus one and
by averaging the corresponding pure by averaging the corresponding pure by averaging the corresponding pure
States weighted by the probabilities we States weighted by the probabilities we States weighted by the probabilities we
obtain obtain obtain
Row for instance we could use the Row for instance we could use the Row for instance we could use the
spectral theorem to obtain an expression spectral theorem to obtain an expression spectral theorem to obtain an expression
of row like this and if we did that we of row like this and if we did that we of row like this and if we did that we
could take n to be the number of could take n to be the number of could take n to be the number of
classical states of X in fact we could classical states of X in fact we could classical states of X in fact we could
take n to be the rank of row which is take n to be the rank of row which is take n to be the rank of row which is
equal to the number of non-zero won equal to the number of non-zero won equal to the number of non-zero won
values of row counting each one a number values of row counting each one a number values of row counting each one a number
of times equal to its multiplicity and of times equal to its multiplicity and of times equal to its multiplicity and
that's never larger than the number of that's never larger than the number of that's never larger than the number of
classical states of X but it could be classical states of X but it could be classical states of X but it could be
smaller on the other hand this doesn't smaller on the other hand this doesn't smaller on the other hand this doesn't
have to be a spectral decomposition of have to be a spectral decomposition of have to be a spectral decomposition of
row and we don't need these Quantum row and we don't need these Quantum row and we don't need these Quantum
State vectors to be orthogonal for State vectors to be orthogonal for State vectors to be orthogonal for
instance any expression of row like this instance any expression of row like this instance any expression of row like this
is is is
fine and here's a purification of row fine and here's a purification of row fine and here's a purification of row
the sum ranges over the same values as the sum ranges over the same values as the sum ranges over the same values as
in our expression of row and inside of in our expression of row and inside of in our expression of row and inside of
the sum we take the square roots of the the sum we take the square roots of the the sum we take the square roots of the
probabilities multiplied to F A tensored probabilities multiplied to F A tensored probabilities multiplied to F A tensored
with k a here by the way we're assuming with k a here by the way we're assuming with k a here by the way we're assuming
that 0 through n minus one are classical that 0 through n minus one are classical that 0 through n minus one are classical
states of the second system y so it states of the second system y so it states of the second system y so it
makes sense that s is a Quantum State makes sense that s is a Quantum State makes sense that s is a Quantum State
Vector of the pair XY like in the Vector of the pair XY like in the Vector of the pair XY like in the
definition it's not actually important definition it's not actually important definition it's not actually important
here that we use standard basis states here that we use standard basis states here that we use standard basis states
of Y in this expression you could of Y in this expression you could of Y in this expression you could
substitute any orthonormal collection of substitute any orthonormal collection of substitute any orthonormal collection of
states in place of K 0 through Kat n states in place of K 0 through Kat n states in place of K 0 through Kat n
minus one if you wanted to minus one if you wanted to minus one if you wanted to
and now we can check that this is indeed and now we can check that this is indeed and now we can check that this is indeed
a purification by Computing the partial a purification by Computing the partial a purification by Computing the partial
Trace over Trace over Trace over
y here's an expression of the state that y here's an expression of the state that y here's an expression of the state that
we get by performing the partial trace we get by performing the partial trace we get by performing the partial trace
the trace of k a bra B is either one or the trace of k a bra B is either one or the trace of k a bra B is either one or
zero depending upon whether or not a is zero depending upon whether or not a is zero depending upon whether or not a is
equal to B and so the expression equal to B and so the expression equal to B and so the expression
simplifies to our original expression of simplifies to our original expression of simplifies to our original expression of
row and it's a simple as that so it's row and it's a simple as that so it's row and it's a simple as that so it's
very easy to get our hands on a very easy to get our hands on a very easy to get our hands on a
purification as long as we have enough purification as long as we have enough purification as long as we have enough
classical states of Y or alternatively a classical states of Y or alternatively a classical states of Y or alternatively a
large enough orthonormal set to use in large enough orthonormal set to use in large enough orthonormal set to use in
place of the standard Bas of States cat place of the standard Bas of States cat place of the standard Bas of States cat
0 through cat n minus 0 through cat n minus 0 through cat n minus
one and by the way if the number of one and by the way if the number of one and by the way if the number of
classical states of Y is smaller than classical states of Y is smaller than classical states of Y is smaller than
the rank of row then there won't be a the rank of row then there won't be a the rank of row then there won't be a
purification and I'll leave that for you purification and I'll leave that for you purification and I'll leave that for you
to ponder if you choose to do that here's an example let's consider that here's an example let's consider
the density Matrix that we saw several the density Matrix that we saw several the density Matrix that we saw several
times in the lesson on density matrices times in the lesson on density matrices times in the lesson on density matrices
here we have an expression of this here we have an expression of this here we have an expression of this
density Matrix as a convex combination density Matrix as a convex combination density Matrix as a convex combination
of pure States it's not a spectral of pure States it's not a spectral of pure States it's not a spectral
decomposition but we don't decomposition but we don't decomposition but we don't
care and with almost no effort at all we care and with almost no effort at all we care and with almost no effort at all we
can write down a purification of the can write down a purification of the can write down a purification of the
state by following the state by following the state by following the
recipe that's not the only purification recipe that's not the only purification recipe that's not the only purification
of this density Matrix purifications of this density Matrix purifications of this density Matrix purifications
aren't unique but that's okay the aren't unique but that's okay the aren't unique but that's okay the
definition doesn't say anything about definition doesn't say anything about definition doesn't say anything about
uniqueness and in fact there's a very uniqueness and in fact there's a very uniqueness and in fact there's a very
interesting relationship among different interesting relationship among different interesting relationship among different
purifications of the same state but purifications of the same state but purifications of the same state but
we'll discuss that a little bit later in we'll discuss that a little bit later in we'll discuss that a little bit later in
the the the
lesson next we'll consider Schmid DEC lesson next we'll consider Schmid DEC lesson next we'll consider Schmid DEC
compositions which are expressions of compositions which are expressions of compositions which are expressions of
bipartite quantum State vectors in a bipartite quantum State vectors in a bipartite quantum State vectors in a
certain form that I'll describe certain form that I'll describe certain form that I'll describe
momentarily they're named after aard momentarily they're named after aard momentarily they're named after aard
Schmidt the same Schmid known for the Schmidt the same Schmid known for the Schmidt the same Schmid known for the
Graham Schmid orthogonalization Graham Schmid orthogonalization Graham Schmid orthogonalization
procedure but in the case of Schmid procedure but in the case of Schmid procedure but in the case of Schmid
decompositions they probably shouldn't decompositions they probably shouldn't decompositions they probably shouldn't
be named after Schmidt it's actually be named after Schmidt it's actually be named after Schmidt it's actually
mathematically equivalent to something mathematically equivalent to something mathematically equivalent to something
called the singular value decomposition called the singular value decomposition called the singular value decomposition
for matrices which is something that was for matrices which is something that was for matrices which is something that was
actually rediscovered multiple times actually rediscovered multiple times actually rediscovered multiple times
but that does happen but that does happen but that does happen
sometimes but anyway here's what we mean sometimes but anyway here's what we mean sometimes but anyway here's what we mean
when we refer to a Schmid when we refer to a Schmid when we refer to a Schmid
decomposition for an arbitrary choice of decomposition for an arbitrary choice of decomposition for an arbitrary choice of
a Quantum State Vector s for a pair of a Quantum State Vector s for a pair of a Quantum State Vector s for a pair of
systems XY it's always possible to write systems XY it's always possible to write systems XY it's always possible to write
s in a special form that form is as we s in a special form that form is as we s in a special form that form is as we
have here on the screen where R is some have here on the screen where R is some have here on the screen where R is some
positive integer p 0 through p r minus positive integer p 0 through p r minus positive integer p 0 through p r minus
one are positive real numbers that one are positive real numbers that one are positive real numbers that
together form a probability Vector x0 together form a probability Vector x0 together form a probability Vector x0
through XR minus1 are unit vectors through XR minus1 are unit vectors through XR minus1 are unit vectors
corresponding to the system X and y0 corresponding to the system X and y0 corresponding to the system X and y0
through y rus1 are unit vectors through y rus1 are unit vectors through y rus1 are unit vectors
corresponding to the system corresponding to the system corresponding to the system
y now that might not look particularly y now that might not look particularly y now that might not look particularly
special until I tell you that both of special until I tell you that both of special until I tell you that both of
the sets x0 through XR minus one and y0 the sets x0 through XR minus one and y0 the sets x0 through XR minus one and y0
through y Rus one must be through y Rus one must be through y Rus one must be
orthonormal so why is that orthonormal so why is that orthonormal so why is that
special well if we were to pick any special well if we were to pick any special well if we were to pick any
basis for one of the two bases let's say basis for one of the two bases let's say basis for one of the two bases let's say
the X vectors which correspond to the the X vectors which correspond to the the X vectors which correspond to the
system X then we could always express s system X then we could always express s system X then we could always express s
in this form and that would force us to in this form and that would force us to in this form and that would force us to
select the Y vectors in a particular way select the Y vectors in a particular way select the Y vectors in a particular way
in fact we do this pretty routinely like in fact we do this pretty routinely like in fact we do this pretty routinely like
we do with a standard basis when we're we do with a standard basis when we're we do with a standard basis when we're
setting ourselves up to analyze a setting ourselves up to analyze a setting ourselves up to analyze a
standard basis standard basis standard basis
measurement but in general even if we measurement but in general even if we measurement but in general even if we
choose an orthonormal basis for the X choose an orthonormal basis for the X choose an orthonormal basis for the X
vectors the Y vectors we get aren't vectors the Y vectors we get aren't vectors the Y vectors we get aren't
necessarily going to be necessarily going to be necessarily going to be
orthonormal similarly if you picked an orthonormal similarly if you picked an orthonormal similarly if you picked an
orthonormal basis for the Y vectors and orthonormal basis for the Y vectors and orthonormal basis for the Y vectors and
you tried to express s in form again you you tried to express s in form again you you tried to express s in form again you
could but again the X vectors wouldn't could but again the X vectors wouldn't could but again the X vectors wouldn't
necessarily be necessarily be necessarily be
orthonormal and finally if you did pick orthonormal and finally if you did pick orthonormal and finally if you did pick
orthonormal vectors for both systems and orthonormal vectors for both systems and orthonormal vectors for both systems and
you tried to express s in a form like you tried to express s in a form like you tried to express s in a form like
this it wouldn't necessarily work this it wouldn't necessarily work this it wouldn't necessarily work
because you might find yourself in need because you might find yourself in need because you might find yourself in need
of some cross terms but there's only one of some cross terms but there's only one of some cross terms but there's only one
index in this sum which is a so the form index in this sum which is a so the form index in this sum which is a so the form
is special but it is always possible to is special but it is always possible to is special but it is always possible to
write a given unit Vector s like this write a given unit Vector s like this write a given unit Vector s like this
and here's how you can do it and here's how you can do it and here's how you can do it
first compute a spectral decomposition first compute a spectral decomposition first compute a spectral decomposition
for the reduced state of either of the for the reduced state of either of the for the reduced state of either of the
two systems it doesn't matter which but two systems it doesn't matter which but two systems it doesn't matter which but
for the sake of describing how this is for the sake of describing how this is for the sake of describing how this is
done let's look at the reduced state of done let's look at the reduced state of done let's look at the reduced state of
X so we're tracing out y so that means X so we're tracing out y so that means X so we're tracing out y so that means
that p 0 through p r minus one are igen that p 0 through p r minus one are igen that p 0 through p r minus one are igen
values of this reduced State row and x0 values of this reduced State row and x0 values of this reduced State row and x0
through XR minus one are unit IG vectors through XR minus one are unit IG vectors through XR minus one are unit IG vectors
corresponding to these IG values and by corresponding to these IG values and by corresponding to these IG values and by
the way the understanding here is that the way the understanding here is that the way the understanding here is that
we're ignoring the terms for which which we're ignoring the terms for which which we're ignoring the terms for which which
the IG values are zero because those the IG values are zero because those the IG values are zero because those
terms don't contribute anything to the terms don't contribute anything to the terms don't contribute anything to the
sum and that is to say that p 0 through sum and that is to say that p 0 through sum and that is to say that p 0 through
p r minus one are all positive real p r minus one are all positive real p r minus one are all positive real
numbers and R is the number of non-zero numbers and R is the number of non-zero numbers and R is the number of non-zero
igen values counting multiplicities igen values counting multiplicities igen values counting multiplicities
which is equal to the rank of which is equal to the rank of which is equal to the rank of
row so that gives us p 0 through p r row so that gives us p 0 through p r row so that gives us p 0 through p r
minus one as well as x0 through XR minus minus one as well as x0 through XR minus minus one as well as x0 through XR minus
one and all that's left is to find the one and all that's left is to find the one and all that's left is to find the
vectors y0 through y Rus vectors y0 through y Rus vectors y0 through y Rus
one there's only one choice that can one there's only one choice that can one there's only one choice that can
possibly work those vectors are uniquely possibly work those vectors are uniquely possibly work those vectors are uniquely
determined at this point and they can determined at this point and they can determined at this point and they can
easily be found using the simple formula easily be found using the simple formula easily be found using the simple formula
that's shown on the that's shown on the that's shown on the
screen and what's amazing here is that screen and what's amazing here is that screen and what's amazing here is that
if we Define these vectors in this way if we Define these vectors in this way if we Define these vectors in this way
then they are guaranteed to be then they are guaranteed to be then they are guaranteed to be
orthonormal and that gives us a Schmid orthonormal and that gives us a Schmid orthonormal and that gives us a Schmid
decomposition I won't argue that in this decomposition I won't argue that in this decomposition I won't argue that in this
video but it isn't too difficult and you video but it isn't too difficult and you video but it isn't too difficult and you
can consult the written material for the can consult the written material for the can consult the written material for the
lesson in case you're interested check lesson in case you're interested check lesson in case you're interested check
the video description for a link as the video description for a link as the video description for a link as
usual usual usual
as an example let's consider this as an example let's consider this as an example let's consider this
Quantum State Vector which we obtained Quantum State Vector which we obtained Quantum State Vector which we obtained
as a purification of the density Matrix as a purification of the density Matrix as a purification of the density Matrix
example that we keep coming back example that we keep coming back example that we keep coming back
to it isn't a Schmid decomposition to it isn't a Schmid decomposition to it isn't a Schmid decomposition
because the zero and the plus States because the zero and the plus States because the zero and the plus States
aren't orthonormal but we can find one aren't orthonormal but we can find one aren't orthonormal but we can find one
using the procedure that I just using the procedure that I just using the procedure that I just
described the first step is to compute a described the first step is to compute a described the first step is to compute a
spectral decomposition for the redu spectral decomposition for the redu spectral decomposition for the redu
state of X which is that same example of state of X which is that same example of state of X which is that same example of
a density Matrix and here's a spectral a density Matrix and here's a spectral a density Matrix and here's a spectral
decomposition I of it which we saw back decomposition I of it which we saw back decomposition I of it which we saw back
in lesson in lesson in lesson
9 so we choose p 0 P1 x0 and X1 as is 9 so we choose p 0 P1 x0 and X1 as is 9 so we choose p 0 P1 x0 and X1 as is
shown on the screen p 0 and P1 are the shown on the screen p 0 and P1 are the shown on the screen p 0 and P1 are the
igen values of row and x0 and X1 are igen values of row and x0 and X1 are igen values of row and x0 and X1 are
corresponding unit igen corresponding unit igen corresponding unit igen
vectors and so it remains to compute y0 vectors and so it remains to compute y0 vectors and so it remains to compute y0
and y1 and if we do this we get cat Plus and y1 and if we do this we get cat Plus and y1 and if we do this we get cat Plus
for y0 and -1 * Kat minus for for y0 and -1 * Kat minus for for y0 and -1 * Kat minus for
y1 that's not immediate you have to y1 that's not immediate you have to y1 that's not immediate you have to
actually do the calculation to see it actually do the calculation to see it actually do the calculation to see it
but that is what we get and as expected but that is what we get and as expected but that is what we get and as expected
those two vectors are orthonormal as those two vectors are orthonormal as those two vectors are orthonormal as
they will always be if you've done the they will always be if you've done the they will always be if you've done the
calculations correctly and that gives us a Schmid DEC correctly and that gives us a Schmid DEC
composition Schmid decompositions are composition Schmid decompositions are composition Schmid decompositions are
very handy in many situations and very handy in many situations and very handy in many situations and
shortly we'll see what they tell us shortly we'll see what they tell us shortly we'll see what they tell us
about about about
purifications purifications aren't purifications purifications aren't purifications purifications aren't
unique a given Quantum State represented unique a given Quantum State represented unique a given Quantum State represented
by a density Ma by a density Ma by a density Ma
will in fact have infinitely many will in fact have infinitely many will in fact have infinitely many
purifications there is however a close purifications there is however a close purifications there is however a close
relationship that always holds between relationship that always holds between relationship that always holds between
any two purifications of the same state any two purifications of the same state any two purifications of the same state
and it's sometimes called the unitary and it's sometimes called the unitary and it's sometimes called the unitary
equivalence of equivalence of equivalence of
purifications here's how it purifications here's how it purifications here's how it
works let row be a density Matrix works let row be a density Matrix works let row be a density Matrix
representing a Quantum state of a system representing a Quantum state of a system representing a Quantum state of a system
X and suppose that we have any two X and suppose that we have any two X and suppose that we have any two
purifications of that state that are purifications of that state that are purifications of that state that are
pure states of the same compound pure states of the same compound pure states of the same compound
system so so for the sake of system so so for the sake of system so so for the sake of
concreteness let's suppose that s and fi concreteness let's suppose that s and fi concreteness let's suppose that s and fi
are quantum State vectors of a pair of are quantum State vectors of a pair of are quantum State vectors of a pair of
systems XY that both give us row when systems XY that both give us row when systems XY that both give us row when
the system Y is traced the system Y is traced the system Y is traced
out what the unitary equivalence of out what the unitary equivalence of out what the unitary equivalence of
purification says is that there must be purification says is that there must be purification says is that there must be
a unitary operation U that acts only on a unitary operation U that acts only on a unitary operation U that acts only on
the system y that transforms SII into the system y that transforms SII into the system y that transforms SII into
fi in other words although these are fi in other words although these are fi in other words although these are
arbitrary purifications of row and we arbitrary purifications of row and we arbitrary purifications of row and we
don't assume anything about them except don't assume anything about them except don't assume anything about them except
that they both purify row and they're that they both purify row and they're that they both purify row and they're
both states of the same compound system both states of the same compound system both states of the same compound system
XY they have to be equivalent in some XY they have to be equivalent in some XY they have to be equivalent in some
sense up to a unitary operation on y sense up to a unitary operation on y sense up to a unitary operation on y
alone we'll see a few examples that alone we'll see a few examples that alone we'll see a few examples that
illustrate why this is interesting and illustrate why this is interesting and illustrate why this is interesting and
what implications it has but first let's what implications it has but first let's what implications it has but first let's
see why it's see why it's see why it's
true let's begin with a spectral true let's begin with a spectral true let's begin with a spectral
decomposition of row which is the decomposition of row which is the decomposition of row which is the
density Matrix that both SII and fi density Matrix that both SII and fi density Matrix that both SII and fi
purify and just like we did when we purify and just like we did when we purify and just like we did when we
discussed the Schmid decomposition we're discussed the Schmid decomposition we're discussed the Schmid decomposition we're
going to assume that we've thrown away going to assume that we've thrown away going to assume that we've thrown away
all of the terms corresponding to zero all of the terms corresponding to zero all of the terms corresponding to zero
igen values so p 0 through p r minus1 igen values so p 0 through p r minus1 igen values so p 0 through p r minus1
are all positive real numbers x0 through are all positive real numbers x0 through are all positive real numbers x0 through
XR minus one are orthonormal igen XR minus one are orthonormal igen XR minus one are orthonormal igen
vectors of row and R happens to be the vectors of row and R happens to be the vectors of row and R happens to be the
rank of row and by the way just as an rank of row and by the way just as an rank of row and by the way just as an
aside notice how often we're reaching aside notice how often we're reaching aside notice how often we're reaching
for the spectral theorem at this point for the spectral theorem at this point for the spectral theorem at this point
it's an incredibly handy tool and in it's an incredibly handy tool and in it's an incredibly handy tool and in
fact we'd be lost without fact we'd be lost without fact we'd be lost without
it anyway now that we have a spectral it anyway now that we have a spectral it anyway now that we have a spectral
composition row we can compute Schmid composition row we can compute Schmid composition row we can compute Schmid
decompositions of our two vectors just decompositions of our two vectors just decompositions of our two vectors just
like before and they're going to be like before and they're going to be like before and they're going to be
almost exactly the same the coefficients almost exactly the same the coefficients almost exactly the same the coefficients
and the X vectors are the same because and the X vectors are the same because and the X vectors are the same because
they come from the spectral they come from the spectral they come from the spectral
decomposition of the reduced state which decomposition of the reduced state which decomposition of the reduced state which
is the same for both vectors the only is the same for both vectors the only is the same for both vectors the only
difference is that we may get different difference is that we may get different difference is that we may get different
vectors for the second system for the vectors for the second system for the vectors for the second system for the
first Vector which is s we'll get one first Vector which is s we'll get one first Vector which is s we'll get one
set of vectors y0 through y Rus one and set of vectors y0 through y Rus one and set of vectors y0 through y Rus one and
for the second Vector which is f we'll for the second Vector which is f we'll for the second Vector which is f we'll
get a POS possibly different collection get a POS possibly different collection get a POS possibly different collection
which we can call z0 through zrus which we can call z0 through zrus which we can call z0 through zrus
one so we now have schmitti compositions one so we now have schmitti compositions one so we now have schmitti compositions
for our two vectors which look very for our two vectors which look very for our two vectors which look very
similar except that the vectors for the similar except that the vectors for the similar except that the vectors for the
second system may be different but these second system may be different but these second system may be different but these
are Schmitty compositions so these two are Schmitty compositions so these two are Schmitty compositions so these two
collections are both collections are both collections are both
orthonormal and so we can simply choose orthonormal and so we can simply choose orthonormal and so we can simply choose
a unitary Matrix U that sends ya to Za a unitary Matrix U that sends ya to Za a unitary Matrix U that sends ya to Za
for each for each for each
a you can always do that unitary a you can always do that unitary a you can always do that unitary
operations are in fact equivalent to operations are in fact equivalent to operations are in fact equivalent to
linear maps that transform one linear maps that transform one linear maps that transform one
orthonormal basis into another and if orthonormal basis into another and if orthonormal basis into another and if
these two sets don't happen to span the these two sets don't happen to span the these two sets don't happen to span the
entire space corresponding to Y we can entire space corresponding to Y we can entire space corresponding to Y we can
always extend them both to orthonormal always extend them both to orthonormal always extend them both to orthonormal
Bases using coincidentally the gr Schmid Bases using coincidentally the gr Schmid Bases using coincidentally the gr Schmid
orthogonalization orthogonalization orthogonalization
procedure and that is how the unitary procedure and that is how the unitary procedure and that is how the unitary
equivalence of purifications is equivalence of purifications is equivalence of purifications is
proved in lesson four of the series we proved in lesson four of the series we proved in lesson four of the series we
discussed the superdense coding protocol discussed the superdense coding protocol discussed the superdense coding protocol
which allows a sender to transmit two which allows a sender to transmit two which allows a sender to transmit two
classical bits to a receiver by sending classical bits to a receiver by sending classical bits to a receiver by sending
one cubit at the cost of one ebit of one cubit at the cost of one ebit of one cubit at the cost of one ebit of
shared shared shared
entanglement so I'm assuming that you're entanglement so I'm assuming that you're entanglement so I'm assuming that you're
already familiar with that protocol and already familiar with that protocol and already familiar with that protocol and
the point of this example is to briefly the point of this example is to briefly the point of this example is to briefly
explain how it connects to the unitary explain how it connects to the unitary explain how it connects to the unitary
equivalence of equivalence of equivalence of
purifications remember that the way that purifications remember that the way that purifications remember that the way that
the protocol works is that Alice and Bob the protocol works is that Alice and Bob the protocol works is that Alice and Bob
share an ebit or in other words a five share an ebit or in other words a five share an ebit or in other words a five
plus Bell plus Bell plus Bell
State Alice chooses to transform that State Alice chooses to transform that State Alice chooses to transform that
Bell State into any of the Four Bell Bell State into any of the Four Bell Bell State into any of the Four Bell
States depending on the two bits that States depending on the two bits that States depending on the two bits that
she wants to send and then she sends her she wants to send and then she sends her she wants to send and then she sends her
Cubit to Bob and he measures to see Cubit to Bob and he measures to see Cubit to Bob and he measures to see
which Bell State he which Bell State he which Bell State he
has notice that all four bell States has notice that all four bell States has notice that all four bell States
give us the same reduced state for Bob's give us the same reduced state for Bob's give us the same reduced state for Bob's
Cubit which is the completely mixed Cubit which is the completely mixed Cubit which is the completely mixed
state and in fact that's already enough state and in fact that's already enough state and in fact that's already enough
to conclude that super dense cating is to conclude that super dense cating is to conclude that super dense cating is
possible all four bell States purify the possible all four bell States purify the possible all four bell States purify the
completely mixed state so by the unitary completely mixed state so by the unitary completely mixed state so by the unitary
equivalence of purifications we know equivalence of purifications we know equivalence of purifications we know
that it must be possible for Alice to that it must be possible for Alice to that it must be possible for Alice to
transform the five plus Bell State into transform the five plus Bell State into transform the five plus Bell State into
of the others by applying some unitary of the others by applying some unitary of the others by applying some unitary
operation to her operation to her operation to her
Cubit of course it's not difficult to Cubit of course it's not difficult to Cubit of course it's not difficult to
come up with unitary operations that come up with unitary operations that come up with unitary operations that
actually work and they're very simple actually work and they're very simple actually work and they're very simple
but it's interesting nevertheless that but it's interesting nevertheless that but it's interesting nevertheless that
we can reach the conclusion that this is we can reach the conclusion that this is we can reach the conclusion that this is
possible without even thinking about the possible without even thinking about the possible without even thinking about the
unitary operations unitary operations unitary operations
themselves and moreover we can conclude themselves and moreover we can conclude themselves and moreover we can conclude
that generalizations of superdense that generalizations of superdense that generalizations of superdense
coding to larger systems are possible coding to larger systems are possible coding to larger systems are possible
provided that we have some way of provided that we have some way of provided that we have some way of
generalizing the Bell states to larger generalizing the Bell states to larger generalizing the Bell states to larger
systems so that they all give the systems so that they all give the systems so that they all give the
completely mix State as Bob's reduce completely mix State as Bob's reduce completely mix State as Bob's reduce
State and there are in fact different State and there are in fact different State and there are in fact different
known ways to do known ways to do known ways to do
that a different setting to which the that a different setting to which the that a different setting to which the
unitary equivalence of purifications has unitary equivalence of purifications has unitary equivalence of purifications has
implications is quantum implications is quantum implications is quantum
cryptography and one of its most cryptography and one of its most cryptography and one of its most
important implications is that it rules important implications is that it rules important implications is that it rules
out the existence of an unconditionally out the existence of an unconditionally out the existence of an unconditionally
secure Quantum protocol for a secure Quantum protocol for a secure Quantum protocol for a
cryptographic primitive known as bit cryptographic primitive known as bit cryptographic primitive known as bit
commitment so what is bit commitment so what is bit commitment so what is bit
commitment bit commitment is an example commitment bit commitment is an example commitment bit commitment is an example
of a cryptographic ative by which we of a cryptographic ative by which we of a cryptographic ative by which we
mean a low-level task or procedure that mean a low-level task or procedure that mean a low-level task or procedure that
provides some basic functionality which provides some basic functionality which provides some basic functionality which
can then be used as a building block to can then be used as a building block to can then be used as a building block to
do more interesting things like creating do more interesting things like creating do more interesting things like creating
digital signature schemes or building digital signature schemes or building digital signature schemes or building
zero knowledge protocols for zero knowledge protocols for zero knowledge protocols for
instance in the case of bit commitment instance in the case of bit commitment instance in the case of bit commitment
the basic functionality is that one the basic functionality is that one the basic functionality is that one
player Alice is able to commit to a player Alice is able to commit to a player Alice is able to commit to a
binary value of her choice to a second binary value of her choice to a second binary value of her choice to a second
player Bob but her choice remains hidden player Bob but her choice remains hidden player Bob but her choice remains hidden
until she chooses to reveal it until she chooses to reveal it until she chooses to reveal it
we require two specific properties from we require two specific properties from we require two specific properties from
a protocol that implements bit a protocol that implements bit a protocol that implements bit
commitment it must be binding which commitment it must be binding which commitment it must be binding which
means that Alice can't change her mind means that Alice can't change her mind means that Alice can't change her mind
once she's committed to a bit and it once she's committed to a bit and it once she's committed to a bit and it
must be concealing which means that Bob must be concealing which means that Bob must be concealing which means that Bob
can't determine Alice's bit until she can't determine Alice's bit until she can't determine Alice's bit until she
chooses to reveal chooses to reveal chooses to reveal
it so it would be pretty nice to be able it so it would be pretty nice to be able it so it would be pretty nice to be able
to implement this primitive using to implement this primitive using to implement this primitive using
Quantum information because it's a Quantum information because it's a Quantum information because it's a
useful useful useful
primitive but without any additional primitive but without any additional primitive but without any additional
assumptions it turns out to be assumptions it turns out to be assumptions it turns out to be
impossible possible to do it perfectly impossible possible to do it perfectly impossible possible to do it perfectly
with Quantum information or even close with Quantum information or even close with Quantum information or even close
to perfectly and the unitary equivalence to perfectly and the unitary equivalence to perfectly and the unitary equivalence
of purifications reveals of purifications reveals of purifications reveals
that the idea at a very high level is that the idea at a very high level is that the idea at a very high level is
that if there were a Quantum protocol that if there were a Quantum protocol that if there were a Quantum protocol
for bit commitment then there would have for bit commitment then there would have for bit commitment then there would have
to be a so-called purified version of to be a so-called purified version of to be a so-called purified version of
that protocol where we can imagine that that protocol where we can imagine that that protocol where we can imagine that
Alice has a system a bob has a system B Alice has a system a bob has a system B Alice has a system a bob has a system B
and after the commitment is made but and after the commitment is made but and after the commitment is made but
before Alice reveals the pair AB is in before Alice reveals the pair AB is in before Alice reveals the pair AB is in
one of two pure States size zero or I one of two pure States size zero or I one of two pure States size zero or I
one depending on the bit that Alice one depending on the bit that Alice one depending on the bit that Alice
commits if the protocol is perfectly commits if the protocol is perfectly commits if the protocol is perfectly
concealing then it must be that Bob concealing then it must be that Bob concealing then it must be that Bob
can't tell the difference between the can't tell the difference between the can't tell the difference between the
two commitments so his reduceed state two commitments so his reduceed state two commitments so his reduceed state
must be the same for the two must be the same for the two must be the same for the two
possibilities however that implies by possibilities however that implies by possibilities however that implies by
the unitary equivalence of purifications the unitary equivalence of purifications the unitary equivalence of purifications
that Alice can transform either one into that Alice can transform either one into that Alice can transform either one into
the other by applying some unitary the other by applying some unitary the other by applying some unitary
operation to a alone so the protocol operation to a alone so the protocol operation to a alone so the protocol
completely failed to be completely failed to be completely failed to be
binding there are details in there that binding there are details in there that binding there are details in there that
I've swept under the rug but the heart I've swept under the rug but the heart I've swept under the rug but the heart
of the matter is that the unitary of the matter is that the unitary of the matter is that the unitary
equivalence of purifications tells us equivalence of purifications tells us equivalence of purifications tells us
that this primitive is not something that this primitive is not something that this primitive is not something
that we can reliably Implement using that we can reliably Implement using that we can reliably Implement using
Quantum information without additional Quantum information without additional Quantum information without additional
assumptions of some assumptions of some assumptions of some
kind the last example of an implication kind the last example of an implication kind the last example of an implication
of the unitary equivalence of of the unitary equivalence of of the unitary equivalence of
purifications is a theorem known as the purifications is a theorem known as the purifications is a theorem known as the
Houston Joseph wooders theorem or hjw Houston Joseph wooders theorem or hjw Houston Joseph wooders theorem or hjw
theorem for theorem for theorem for
short let's start with a statement of short let's start with a statement of short let's start with a statement of
the theorem itself and this is a the theorem itself and this is a the theorem itself and this is a
slightly simplified version of it just slightly simplified version of it just slightly simplified version of it just
to avoid some technicalities so that we to avoid some technicalities so that we to avoid some technicalities so that we
can focus on the main can focus on the main can focus on the main
ideas suppose that X and Y are systems ideas suppose that X and Y are systems ideas suppose that X and Y are systems
and F is any Quantum State Vector and F is any Quantum State Vector and F is any Quantum State Vector
representing a pure state of the pair representing a pure state of the pair representing a pure state of the pair
XY and now suppose that we think about XY and now suppose that we think about XY and now suppose that we think about
the reduced state of the system X alone the reduced state of the system X alone the reduced state of the system X alone
and we come up with any way that we and we come up with any way that we and we come up with any way that we
choose to express this reduced State as choose to express this reduced State as choose to express this reduced State as
a convex combination of pure States as a convex combination of pure States as a convex combination of pure States as
is shown on the screen is shown on the screen is shown on the screen
so for instance the number capital N in so for instance the number capital N in so for instance the number capital N in
this expression which is the number of this expression which is the number of this expression which is the number of
pure States we're averaging over could pure States we're averaging over could pure States we're averaging over could
be arbitrarily large possibly much be arbitrarily large possibly much be arbitrarily large possibly much
larger than the dimension of the space larger than the dimension of the space larger than the dimension of the space
corresponding to X or to Y for that corresponding to X or to Y for that corresponding to X or to Y for that
matter and what the theorem tells us is matter and what the theorem tells us is matter and what the theorem tells us is
that there must exist a measurement on that there must exist a measurement on that there must exist a measurement on
the system y having n outcomes such that the system y having n outcomes such that the system y having n outcomes such that
the following happens when the system Y the following happens when the system Y the following happens when the system Y
is is is
measured first each measurement outcome measured first each measurement outcome measured first each measurement outcome
appears with probability exactly equal appears with probability exactly equal appears with probability exactly equal
to the corresponding probability in our to the corresponding probability in our to the corresponding probability in our
expression of the reduced state of X as expression of the reduced state of X as expression of the reduced state of X as
a convex combination of pure a convex combination of pure a convex combination of pure
States and second conditioned on getting States and second conditioned on getting States and second conditioned on getting
a particular measurement outcome the a particular measurement outcome the a particular measurement outcome the
state of X becomes the corresponding state of X becomes the corresponding state of X becomes the corresponding
pure state from the convex pure state from the convex pure state from the convex
combination so why is that combination so why is that combination so why is that
interesting well it means that for any interesting well it means that for any interesting well it means that for any
way that we choose to think about the way that we choose to think about the way that we choose to think about the
reduced state of X as a convex reduced state of X as a convex reduced state of X as a convex
combination of pure States combination of pure States combination of pure States
there's some measurement of Y that there's some measurement of Y that there's some measurement of Y that
effectively makes this way of thinking effectively makes this way of thinking effectively makes this way of thinking
about the reduced state of x a about the reduced state of x a about the reduced state of x a
reality and what's remarkable is that reality and what's remarkable is that reality and what's remarkable is that
this is possible even when the state f this is possible even when the state f this is possible even when the state f
is chosen first and the convex is chosen first and the convex is chosen first and the convex
combination is chosen combination is chosen combination is chosen
later as long as it's an expression of later as long as it's an expression of later as long as it's an expression of
the reduced state of X there will be a the reduced state of X there will be a the reduced state of X there will be a
measurement of Y that works exactly like measurement of Y that works exactly like measurement of Y that works exactly like
this so let's see how we can prove this so let's see how we can prove this so let's see how we can prove
this we start with two different this we start with two different this we start with two different
expressions for the reduced of X which expressions for the reduced of X which expressions for the reduced of X which
will give the name Row for will give the name Row for will give the name Row for
convenience next we introduce a new convenience next we introduce a new convenience next we introduce a new
system Z whose classical states are the system Z whose classical states are the system Z whose classical states are the
possible measurement outcomes or possible measurement outcomes or possible measurement outcomes or
equivalently the indices in our convex equivalently the indices in our convex equivalently the indices in our convex
combination and now we can observe that combination and now we can observe that combination and now we can observe that
the following two Quantum State vectors the following two Quantum State vectors the following two Quantum State vectors
gamma 0 and Gamma 1 which are both state gamma 0 and Gamma 1 which are both state gamma 0 and Gamma 1 which are both state
vectors for the compound system XYZ are vectors for the compound system XYZ are vectors for the compound system XYZ are
both purifications of row and just to be both purifications of row and just to be both purifications of row and just to be
clear when we're thinking about these clear when we're thinking about these clear when we're thinking about these
vectors as being ifications of row we're vectors as being ifications of row we're vectors as being ifications of row we're
thinking about the systems Y and Z thinking about the systems Y and Z thinking about the systems Y and Z
together as a single compound system together as a single compound system together as a single compound system
that gets traced out to leave that gets traced out to leave that gets traced out to leave
Row the first Vector gamma 0er is Row the first Vector gamma 0er is Row the first Vector gamma 0er is
essentially the same thing as the essentially the same thing as the essentially the same thing as the
original pure State 5 except that we're original pure State 5 except that we're original pure State 5 except that we're
initializing the new system Z to the initializing the new system Z to the initializing the new system Z to the
zero zero zero
State it's clear that if we Trace out Y State it's clear that if we Trace out Y State it's clear that if we Trace out Y
and Z we'll be left with row so gamma and Z we'll be left with row so gamma and Z we'll be left with row so gamma
0er is a purification of 0er is a purification of 0er is a purification of
row the second Vector Gamma 1 is row the second Vector Gamma 1 is row the second Vector Gamma 1 is
basically the vector we get by purifying basically the vector we get by purifying basically the vector we get by purifying
row based on the convex combination just row based on the convex combination just row based on the convex combination just
like I described earlier in the like I described earlier in the like I described earlier in the
lesson this time the system Y is lesson this time the system Y is lesson this time the system Y is
initialized and I'm assuming just for initialized and I'm assuming just for initialized and I'm assuming just for
Simplicity that zero is a classical Simplicity that zero is a classical Simplicity that zero is a classical
state of Y but you could substitute any state of Y but you could substitute any state of Y but you could substitute any
standard basis state or in fact any pure standard basis state or in fact any pure standard basis state or in fact any pure
state of Y into this expression and it state of Y into this expression and it state of Y into this expression and it
wouldn't make any wouldn't make any wouldn't make any
difference in any case tracing out Y and difference in any case tracing out Y and difference in any case tracing out Y and
Z again leaves us with row so the bottom Z again leaves us with row so the bottom Z again leaves us with row so the bottom
line is that these are both line is that these are both line is that these are both
purifications of row and therefore by purifications of row and therefore by purifications of row and therefore by
the unitary equivalence of purifications the unitary equivalence of purifications the unitary equivalence of purifications
there's a unitary operation U on the there's a unitary operation U on the there's a unitary operation U on the
pair YZ that transforms gamma 0 into pair YZ that transforms gamma 0 into pair YZ that transforms gamma 0 into
Gamma Gamma Gamma
1 and that's all we need to come up with 1 and that's all we need to come up with 1 and that's all we need to come up with
a description of a measurement that does a description of a measurement that does a description of a measurement that does
what the theorem what the theorem what the theorem
says a simple way to describe it is with says a simple way to describe it is with says a simple way to describe it is with
a circuit diagram like is shown here on a circuit diagram like is shown here on a circuit diagram like is shown here on
the the the
screen in words we start out with the screen in words we start out with the screen in words we start out with the
state F we introduce Z initialized to state F we introduce Z initialized to state F we introduce Z initialized to
the zero State perform U and then the zero State perform U and then the zero State perform U and then
measure the top system which is Z with a measure the top system which is Z with a measure the top system which is Z with a
standard basis standard basis standard basis
measurement effectively what this does measurement effectively what this does measurement effectively what this does
is to transform fi into Gamma One and is to transform fi into Gamma One and is to transform fi into Gamma One and
just by eyeballing the description of just by eyeballing the description of just by eyeballing the description of
Gamma 1 we can see that this does what Gamma 1 we can see that this does what Gamma 1 we can see that this does what
the theorem says which is that we get the theorem says which is that we get the theorem says which is that we get
each outcome a with probability PA and each outcome a with probability PA and each outcome a with probability PA and
conditioned on getting a the system X is conditioned on getting a the system X is conditioned on getting a the system X is
left in the state sa left in the state sa left in the state sa
a and just to be clear the measurement a and just to be clear the measurement a and just to be clear the measurement
itself that the theorem implies the itself that the theorem implies the itself that the theorem implies the
existence of is implemented by existence of is implemented by existence of is implemented by
everything in the dotted rectangle in everything in the dotted rectangle in everything in the dotted rectangle in
the figure and if you want to know how the figure and if you want to know how the
measurement can be described as a measurement can be described as a measurement can be described as a
collection of matrices here's an collection of matrices here's an collection of matrices here's an
expression of expression of expression of
them I won't explain this part and it's them I won't explain this part and it's them I won't explain this part and it's
not really important for the sake of the not really important for the sake of the not really important for the sake of the
example but that is an expression of the example but that is an expression of the example but that is an expression of the
measurement matrices in case you're measurement matrices in case you're measurement matrices in case you're
interested next we'll discuss Fidelity interested next we'll discuss Fidelity interested next we'll discuss Fidelity
which is a measure of how similar two which is a measure of how similar two which is a measure of how similar two
Quantum states are or alternatively how Quantum states are or alternatively how Quantum states are or alternatively how
much they overlap much they overlap much they overlap
we'll start with the definition which we'll start with the definition which we'll start with the definition which
might look a little bit mysterious at might look a little bit mysterious at might look a little bit mysterious at
first glance but we'll unravel it and by first glance but we'll unravel it and by first glance but we'll unravel it and by
the end of the discussion I hope that I the end of the discussion I hope that I the end of the discussion I hope that I
will have convinced you that it's will have convinced you that it's will have convinced you that it's
actually a much nicer function than it actually a much nicer function than it actually a much nicer function than it
might seem at first might seem at first might seem at first
glance and here's the glance and here's the glance and here's the
definition the Fidelity between two definition the Fidelity between two definition the Fidelity between two
density matrices row and sigma is equal density matrices row and sigma is equal density matrices row and sigma is equal
to the trace of the square root of the to the trace of the square root of the to the trace of the square root of the
square root of row * Sigma time the square root of row * Sigma time the square root of row * Sigma time the
square root of square root of square root of
row we'll talk about row we'll talk about row we'll talk about
it first let's just try to make sense of it first let's just try to make sense of it first let's just try to make sense of
the definition the definition the definition
itself we have three square roots and itself we have three square roots and itself we have three square roots and
they all refer to the square root of a they all refer to the square root of a they all refer to the square root of a
positive semi-definite Matrix which as positive semi-definite Matrix which as positive semi-definite Matrix which as
we discussed in the previous lesson is we discussed in the previous lesson is we discussed in the previous lesson is
the unique positive semi-definite Matrix the unique positive semi-definite Matrix the unique positive semi-definite Matrix
that squares to give us whatever Matrix that squares to give us whatever Matrix that squares to give us whatever Matrix
we're taking the square root of because we're taking the square root of because we're taking the square root of because
row is a density Matrix it must be row is a density Matrix it must be row is a density Matrix it must be
positive semi-definite so we can take positive semi-definite so we can take positive semi-definite so we can take
the root of the root of the root of
it and if we think about theot of row * it and if we think about theot of row * it and if we think about theot of row *
Sigma theot of row for a moment we see Sigma theot of row for a moment we see Sigma theot of row for a moment we see
that it's also positive semi-definite that it's also positive semi-definite that it's also positive semi-definite
and in particular we can write it as and in particular we can write it as and in particular we can write it as
mger * M for M being theare root of mger * M for M being theare root of mger * M for M being theare root of
Sigma time the sare root of Sigma time the sare root of Sigma time the sare root of
row and therefore we can take the square row and therefore we can take the square row and therefore we can take the square
root of it in particular we can compute root of it in particular we can compute root of it in particular we can compute
a spectral decomposition of the root of a spectral decomposition of the root of a spectral decomposition of the root of
row * Sigma * theun of row row * Sigma * theun of row row * Sigma * theun of row
and then to take the square root we just and then to take the square root we just and then to take the square root we just
need to take the square roots of the IG need to take the square roots of the IG need to take the square roots of the IG
values and leave the IG vectors values and leave the IG vectors values and leave the IG vectors
alone and finally we take the trace of alone and finally we take the trace of alone and finally we take the trace of
this Matrix and because the trace equals this Matrix and because the trace equals this Matrix and because the trace equals
the sum of the IG values we obtain the the sum of the IG values we obtain the the sum of the IG values we obtain the
expression that we have right expression that we have right expression that we have right
here so that's probably something that here so that's probably something that here so that's probably something that
you don't want to compute by hand in you don't want to compute by hand in you don't want to compute by hand in
general and this explanation might not general and this explanation might not general and this explanation might not
offer very much insight into the offer very much insight into the offer very much insight into the
Fidelity or why we care about it but Fidelity or why we care about it but Fidelity or why we care about it but
hopefully it clarifies the technical hopefully it clarifies the technical hopefully it clarifies the technical
meaning of the meaning of the meaning of the
formula by the way just as an aside formula by the way just as an aside formula by the way just as an aside
sometimes people Define the Fidelity to sometimes people Define the Fidelity to sometimes people Define the Fidelity to
be the square of the function that I've be the square of the function that I've be the square of the function that I've
defined right here and they refer to defined right here and they refer to defined right here and they refer to
this function as the root this function as the root this function as the root
Fidelity you'll see it defined both ways Fidelity you'll see it defined both ways Fidelity you'll see it defined both ways
which certainly has the potential to which certainly has the potential to which certainly has the potential to
cause confusion but it usually doesn't cause confusion but it usually doesn't cause confusion but it usually doesn't
because people are generally pretty because people are generally pretty because people are generally pretty
careful about making clear which careful about making clear which careful about making clear which
definition they're using and you should definition they're using and you should definition they're using and you should
be sure to do the same be sure to do the same be sure to do the same
thing to my eye there really isn't a thing to my eye there really isn't a thing to my eye there really isn't a
right or a wrong definition I Define it right or a wrong definition I Define it right or a wrong definition I Define it
like this simply because I got used to like this simply because I got used to like this simply because I got used to
doing it this way and I do find that doing it this way and I do find that doing it this way and I do find that
it's a little bit more convenient I it's a little bit more convenient I it's a little bit more convenient I
guess because I'd rather Square guess because I'd rather Square guess because I'd rather Square
something then take a square root later something then take a square root later something then take a square root later
on when the need arises but of course on when the need arises but of course on when the need arises but of course
you're free to make your own you're free to make your own you're free to make your own
choice but anyway let me get back to the choice but anyway let me get back to the choice but anyway let me get back to the
Fidelity as it's defined Fidelity as it's defined Fidelity as it's defined
here there are other equivalent ways to here there are other equivalent ways to here there are other equivalent ways to
define the Fidelity and it's actually a define the Fidelity and it's actually a define the Fidelity and it's actually a
pretty remarkable function in that pretty remarkable function in that pretty remarkable function in that
respect and it can actually be respect and it can actually be respect and it can actually be
characterized in some pretty interesting ways here's a pretty straightforward way ways here's a pretty straightforward way
to rewrite the definition in terms of to rewrite the definition in terms of to rewrite the definition in terms of
the trace Norm which we saw toward the the trace Norm which we saw toward the the trace Norm which we saw toward the
end of the previous end of the previous end of the previous
lesson I didn't go into too much detail lesson I didn't go into too much detail lesson I didn't go into too much detail
about the trace norm and in particular I about the trace norm and in particular I about the trace norm and in particular I
didn't even Define it for non-h herian didn't even Define it for non-h herian didn't even Define it for non-h herian
matrices but it does have a simple matrices but it does have a simple matrices but it does have a simple
enough definition enough definition enough definition
one way to define the trace Norm of an one way to define the trace Norm of an one way to define the trace Norm of an
arbitrary Matrix m is that it's the arbitrary Matrix m is that it's the arbitrary Matrix m is that it's the
trace of the square root of M * m dagger trace of the square root of M * m dagger trace of the square root of M * m dagger
or equivalently the trace of the sare or equivalently the trace of the sare or equivalently the trace of the sare
root of M dagger time M you get the same root of M dagger time M you get the same root of M dagger time M you get the same
value either way you do it and we can value either way you do it and we can value either way you do it and we can
use that definition to simplify the use that definition to simplify the use that definition to simplify the
definition of the Fidelity as we have definition of the Fidelity as we have definition of the Fidelity as we have
down here on the bottom of the screen down here on the bottom of the screen down here on the bottom of the screen
and that's nice because it reveals that and that's nice because it reveals that and that's nice because it reveals that
it's symmetric in the two arguments we can also Express the trace arguments we can also Express the trace
Norm of a square Matrix matx M as the Norm of a square Matrix matx M as the Norm of a square Matrix matx M as the
maximum of the absolute value of the maximum of the absolute value of the maximum of the absolute value of the
trace of M * U ranging over all unary trace of M * U ranging over all unary trace of M * U ranging over all unary
matrices U which is not obvious but it matrices U which is not obvious but it matrices U which is not obvious but it
is true and that gives us another is true and that gives us another is true and that gives us another
expression for the expression for the expression for the
Fidelity so here we have three Fidelity so here we have three Fidelity so here we have three
equivalent definitions for the Fidelity equivalent definitions for the Fidelity equivalent definitions for the Fidelity
and that's just the beginning there are and that's just the beginning there are and that's just the beginning there are
several other equivalent definitions and several other equivalent definitions and several other equivalent definitions and
we'll see a particularly important one we'll see a particularly important one we'll see a particularly important one
which comes from man's theorem toward which comes from man's theorem toward which comes from man's theorem toward
the end of the lesson the end of the lesson the end of the lesson
okay so that's fine but what does it all okay so that's fine but what does it all okay so that's fine but what does it all
mean at an intuitive level and why mean at an intuitive level and why mean at an intuitive level and why
should we care about this should we care about this should we care about this
function to answer these questions it's function to answer these questions it's function to answer these questions it's
helpful to consider the special case in helpful to consider the special case in helpful to consider the special case in
which at least one of the two density which at least one of the two density which at least one of the two density
matrices represents a pure matrices represents a pure matrices represents a pure
state if both states are pure then the state if both states are pure then the state if both states are pure then the
Fidelity is simply the absolute value of Fidelity is simply the absolute value of Fidelity is simply the absolute value of
the inner product between the two the inner product between the two the inner product between the two
Quantum State vectors and in some ways Quantum State vectors and in some ways Quantum State vectors and in some ways
this is a principal motivation for the this is a principal motivation for the this is a principal motivation for the
Fidelity Fidelity Fidelity
itself the absolute value of the inner itself the absolute value of the inner itself the absolute value of the inner
product between two Quantum State product between two Quantum State product between two Quantum State
vectors is always a number between zero vectors is always a number between zero vectors is always a number between zero
and one and it gives us a natural way of and one and it gives us a natural way of and one and it gives us a natural way of
measuring how much overlap there is measuring how much overlap there is measuring how much overlap there is
between two Quantum State vectors and between two Quantum State vectors and between two Quantum State vectors and
you can think about the Fidelity as you can think about the Fidelity as you can think about the Fidelity as
being a useful and well- behaved way to being a useful and well- behaved way to being a useful and well- behaved way to
extend this way of measuring overlap extend this way of measuring overlap extend this way of measuring overlap
from Pure states to density from Pure states to density from Pure states to density
matrices if one of the states is pure matrices if one of the states is pure matrices if one of the states is pure
and the other one isn't necessarily pure and the other one isn't necessarily pure and the other one isn't necessarily pure
then again we have a simple expression then again we have a simple expression then again we have a simple expression
for the Fidelity which can be pretty for the Fidelity which can be pretty for the Fidelity which can be pretty
handy and that expression is shown right handy and that expression is shown right handy and that expression is shown right
here the Fidelity has many interesting here the Fidelity has many interesting here the Fidelity has many interesting
properties and that's one of the reasons properties and that's one of the reasons properties and that's one of the reasons
why people use why people use why people use
it it has a definition that isn't really it it has a definition that isn't really it it has a definition that isn't really
all that nice but it's a very useful all that nice but it's a very useful all that nice but it's a very useful
function never less and it's often very function never less and it's often very function never less and it's often very
easy to use in part because of its many easy to use in part because of its many easy to use in part because of its many
amazing amazing amazing
properties so let's take a look at some properties so let's take a look at some properties so let's take a look at some
of them starting with the observation of them starting with the observation of them starting with the observation
that the Fidelity between two density that the Fidelity between two density that the Fidelity between two density
matrices is always a number between 0 matrices is always a number between 0 matrices is always a number between 0
and and and
one it's zero if and only if the states one it's zero if and only if the states one it's zero if and only if the states
are orthogonal meaning that the images are orthogonal meaning that the images are orthogonal meaning that the images
are orthogonal subspaces or equivalently are orthogonal subspaces or equivalently are orthogonal subspaces or equivalently
that the product of the two matrices is that the product of the two matrices is that the product of the two matrices is
zero and the Fidelity is one if and only zero and the Fidelity is one if and only zero and the Fidelity is one if and only
if the two density matrices are if the two density matrices are if the two density matrices are
equal every other case is somewhere in equal every other case is somewhere in equal every other case is somewhere in
between but intuitively speaking the between but intuitively speaking the between but intuitively speaking the
closer the Fidelity is to one the more closer the Fidelity is to one the more closer the Fidelity is to one the more
similar the states are or alternatively similar the states are or alternatively similar the states are or alternatively
the more they the more they the more they
overlap the Fidelity is also symmetric I overlap the Fidelity is also symmetric I overlap the Fidelity is also symmetric I
mentioned that a few moments ago it's mentioned that a few moments ago it's mentioned that a few moments ago it's
pretty clear from the expression that we pretty clear from the expression that we pretty clear from the expression that we
saw for the Fidelity in terms of the saw for the Fidelity in terms of the saw for the Fidelity in terms of the
trace Norm but it's an important thing trace Norm but it's an important thing trace Norm but it's an important thing
to recognize to recognize to recognize
nevertheless next the Fidelity is nevertheless next the Fidelity is nevertheless next the Fidelity is
multiplicative for product States multiplicative for product States multiplicative for product States
meaning that the Fidelity between two meaning that the Fidelity between two meaning that the Fidelity between two
product States is always equal to the product States is always equal to the product States is always equal to the
product of the fidelities between the product of the fidelities between the product of the fidelities between the
individual individual individual
states here's a really useful property states here's a really useful property states here's a really useful property
if we have any two density matrices of if we have any two density matrices of if we have any two density matrices of
the same size the same size the same size
along with any channel that takes these along with any channel that takes these along with any channel that takes these
density matrices as input the Fidelity density matrices as input the Fidelity density matrices as input the Fidelity
can only become larger when that channel can only become larger when that channel can only become larger when that channel
is applied to both is applied to both is applied to both
States so in other words in terms of the States so in other words in terms of the States so in other words in terms of the
Fidelity channels can only make States Fidelity channels can only make States Fidelity channels can only make States
more similar to one another they can't more similar to one another they can't more similar to one another they can't
make them less similar and finally for this list anyway similar and finally for this list anyway
there's a pretty close relationship there's a pretty close relationship there's a pretty close relationship
between the Fidelity and the trace between the Fidelity and the trace between the Fidelity and the trace
distance between states which is defined distance between states which is defined distance between states which is defined
as half of the trace Norm of the as half of the trace Norm of the as half of the trace Norm of the
difference between the two density difference between the two density difference between the two density
matrices the trace distance between two matrices the trace distance between two matrices the trace distance between two
states connects strongly to how well we states connects strongly to how well we states connects strongly to how well we
can tell them apart which we discussed can tell them apart which we discussed can tell them apart which we discussed
in the previous lesson in the context of in the previous lesson in the context of in the previous lesson in the context of
State discrimination and the hellstrom State discrimination and the hellstrom State discrimination and the hellstrom
Holo theorem while the Fidelity Holo theorem while the Fidelity Holo theorem while the Fidelity
quantifies how much two states quantifies how much two states quantifies how much two states
overlap so although they're not exactly overlap so although they're not exactly overlap so although they're not exactly
the same thing it's not shocking that the same thing it's not shocking that the same thing it's not shocking that
there is a relationship between these there is a relationship between these there is a relationship between these
two quantities but in any case these two quantities but in any case these two quantities but in any case these
inequalities which are typically called inequalities which are typically called inequalities which are typically called
the fuk Vander graph inequalities make the fuk Vander graph inequalities make the fuk Vander graph inequalities make
this relationship mathematically precise this relationship mathematically precise this relationship mathematically precise
and it's pretty important that there's and it's pretty important that there's and it's pretty important that there's
absolutely no dependence here on the absolutely no dependence here on the absolutely no dependence here on the
size of the system we're talking about we can see this relationship a bit about we can see this relationship a bit
more clearly with the help of a more clearly with the help of a more clearly with the help of a
figure if we have two states and we figure if we have two states and we figure if we have two states and we
locate the trace distance between them locate the trace distance between them locate the trace distance between them
on the x-axis and the Fidelity in the on the x-axis and the Fidelity in the on the x-axis and the Fidelity in the
y-axis then the corresponding vertical y-axis then the corresponding vertical y-axis then the corresponding vertical
and horizontal lines must intersect and horizontal lines must intersect and horizontal lines must intersect
somewhere with within this gray somewhere with within this gray somewhere with within this gray
region the most interesting region of region the most interesting region of region the most interesting region of
the plot by the way at least practically the plot by the way at least practically the plot by the way at least practically
speaking is the upper leftand corner of speaking is the upper leftand corner of speaking is the upper leftand corner of
the gray region in particular when the the gray region in particular when the the gray region in particular when the
trace distance is very small the trace distance is very small the trace distance is very small the
Fidelity must be very close to one and Fidelity must be very close to one and Fidelity must be very close to one and
vice vice vice
versa these inequalities get used very versa these inequalities get used very versa these inequalities get used very
frequently and part of the reason for frequently and part of the reason for frequently and part of the reason for
that is that we often care more about that is that we often care more about that is that we often care more about
the trace distance ultimately because it the trace distance ultimately because it the trace distance ultimately because it
has a natural interpretation connecting has a natural interpretation connecting has a natural interpretation connecting
with Quantum State discrimination with Quantum State discrimination with Quantum State discrimination
but the Fidelity has so many amazing but the Fidelity has so many amazing but the Fidelity has so many amazing
properties that it's generally the properties that it's generally the properties that it's generally the
easier one to work with and as long as easier one to work with and as long as easier one to work with and as long as
we're in the upper left hand corner of we're in the upper left hand corner of we're in the upper left hand corner of
the gray region which is often where we the gray region which is often where we the gray region which is often where we
find ourselves we don't lose too much by find ourselves we don't lose too much by find ourselves we don't lose too much by
converting back and forth between the converting back and forth between the converting back and forth between the
Fidelity and the trace distance so those Fidelity and the trace distance so those Fidelity and the trace distance so those
are some of the properties of the are some of the properties of the are some of the properties of the
Fidelity there's lots more but it's a Fidelity there's lots more but it's a Fidelity there's lots more but it's a
good start and hopefully the list is good start and hopefully the list is good start and hopefully the list is
helpful for beginning to develop some helpful for beginning to develop some helpful for beginning to develop some
intuition about the intuition about the intuition about the
Fidelity here's a nice example of how we Fidelity here's a nice example of how we Fidelity here's a nice example of how we
can connect the Fidelity to can connect the Fidelity to can connect the Fidelity to
non-destructive measurements it's a very non-destructive measurements it's a very non-destructive measurements it's a very
handy fact known as The Gentle handy fact known as The Gentle handy fact known as The Gentle
measurement measurement measurement
Lemma suppose that we have a system x a Lemma suppose that we have a system x a Lemma suppose that we have a system x a
state of X represented by a density state of X represented by a density state of X represented by a density
Matrix row and a measurement on system X Matrix row and a measurement on system X Matrix row and a measurement on system X
described by a collection of positive described by a collection of positive described by a collection of positive
semi-definite semi-definite semi-definite
matrices let's suppose that one of the matrices let's suppose that one of the matrices let's suppose that one of the
measurement outcomes is very likely to measurement outcomes is very likely to measurement outcomes is very likely to
appear if we were to perform the appear if we were to perform the appear if we were to perform the
measurement on the state measurement on the state measurement on the state
row just for Simplicity let's suppose row just for Simplicity let's suppose row just for Simplicity let's suppose
that it's the outcome Z Z that's very that it's the outcome Z Z that's very that it's the outcome Z Z that's very
likely and in particular let's assume likely and in particular let's assume likely and in particular let's assume
that this outcome appears with that this outcome appears with that this outcome appears with
probability greater than 1us Epsilon for probability greater than 1us Epsilon for probability greater than 1us Epsilon for
some small value of Epsilon that we some small value of Epsilon that we some small value of Epsilon that we
don't actually need to specify Epsilon don't actually need to specify Epsilon don't actually need to specify Epsilon
is just some small positive real is just some small positive real is just some small positive real
number now we've described our number now we've described our number now we've described our
measurement as if it were a destructive measurement as if it were a destructive measurement as if it were a destructive
measurement but we do know from the measurement but we do know from the measurement but we do know from the
previous lesson that we could implement previous lesson that we could implement previous lesson that we could implement
this measurement in a non-destructive this measurement in a non-destructive this measurement in a non-destructive
way and specifically we could do it as way and specifically we could do it as way and specifically we could do it as
we saw in our discussion of Neymar's we saw in our discussion of Neymar's we saw in our discussion of Neymar's
theorem and that works like theorem and that works like theorem and that works like
this each outcome a appears with this each outcome a appears with this each outcome a appears with
probability equal to the trace of PA * probability equal to the trace of PA * probability equal to the trace of PA *
row and conditioned on getting that row and conditioned on getting that row and conditioned on getting that
outcome the state of X changes in outcome the state of X changes in outcome the state of X changes in
particular in the case that we get the particular in the case that we get the particular in the case that we get the
likely outcome zero the state becomes likely outcome zero the state becomes likely outcome zero the state becomes
the squ otk of p 0 * row * theun of p 0 the squ otk of p 0 * row * theun of p 0 the squ otk of p 0 * row * theun of p 0
all normalized so that we get a density all normalized so that we get a density all normalized so that we get a density
Matrix and what the gentle measurement Matrix and what the gentle measurement Matrix and what the gentle measurement
limit tells us is that in this event limit tells us is that in this event limit tells us is that in this event
meaning that we do indeed get the likely meaning that we do indeed get the likely meaning that we do indeed get the likely
measurement outcome the state we obtain measurement outcome the state we obtain measurement outcome the state we obtain
will not have changed very much from the will not have changed very much from the will not have changed very much from the
original and in particular the Fidelity original and in particular the Fidelity original and in particular the Fidelity
squared will necessarily be larger than squared will necessarily be larger than squared will necessarily be larger than
1 minus 1 minus 1 minus
Epsilon and this is great because this Epsilon and this is great because this Epsilon and this is great because this
is completely General we haven't said is completely General we haven't said is completely General we haven't said
anything at all about the measurement or anything at all about the measurement or anything at all about the measurement or
the state except that one of the the state except that one of the the state except that one of the
outcomes is very outcomes is very outcomes is very
likely so the Lemma can be applied in likely so the Lemma can be applied in likely so the Lemma can be applied in
many different settings where we'd like many different settings where we'd like many different settings where we'd like
to verify something about a Quantum to verify something about a Quantum to verify something about a Quantum
State without disturbing it very State without disturbing it very State without disturbing it very
much so let's see how we can prove much so let's see how we can prove much so let's see how we can prove
it and for the most part it's actually a it and for the most part it's actually a it and for the most part it's actually a
pretty simple matter of calculating the pretty simple matter of calculating the pretty simple matter of calculating the
Fidelity between the pre and Fidelity between the pre and Fidelity between the pre and
postmeasurement postmeasurement postmeasurement
States we can do this by using the States we can do this by using the States we can do this by using the
original definition and ordinarily that original definition and ordinarily that original definition and ordinarily that
might leave us with a messy formula but might leave us with a messy formula but might leave us with a messy formula but
in this case we luck out because the in this case we luck out because the in this case we luck out because the
thing inside of the outer square root is thing inside of the outer square root is thing inside of the outer square root is
actually the square of a matrix that actually the square of a matrix that actually the square of a matrix that
happens to be positive happens to be positive happens to be positive
semi-definite which makes Computing the semi-definite which makes Computing the semi-definite which makes Computing the
square square square
trivial we can now use the cyclic trivial we can now use the cyclic trivial we can now use the cyclic
property of the trace along with the property of the trace along with the property of the trace along with the
linearity of the trace to simplify our linearity of the trace to simplify our linearity of the trace to simplify our
expression as it shown on the expression as it shown on the expression as it shown on the
screen notice by the way that these are screen notice by the way that these are screen notice by the way that these are
all equalities so we haven't all equalities so we haven't all equalities so we haven't
approximated anything at this point approximated anything at this point approximated anything at this point
we're really just we're really just we're really just
simplifying but now we do need an simplifying but now we do need an simplifying but now we do need an
inequality because we'd like to get rid inequality because we'd like to get rid inequality because we'd like to get rid
of that square root in the numerator and of that square root in the numerator and of that square root in the numerator and
we can indeed do that if we're willing we can indeed do that if we're willing we can indeed do that if we're willing
to settle for a lower bound and if to settle for a lower bound and if to settle for a lower bound and if
you're interested in the details for you're interested in the details for you're interested in the details for
this part you can get the inequality this part you can get the inequality this part you can get the inequality
pretty easily by considering a spectral pretty easily by considering a spectral pretty easily by considering a spectral
decomposition of p decomposition of p decomposition of p
0 the IG values of p 0 have to be 0 the IG values of p 0 have to be 0 the IG values of p 0 have to be
between 0 and one that's always true for between 0 and one that's always true for between 0 and one that's always true for
matrices that describe matrices that describe matrices that describe
measurements and once we know that the measurements and once we know that the measurements and once we know that the
inequality follows pretty directly from inequality follows pretty directly from inequality follows pretty directly from
the fact that taking the square root of the fact that taking the square root of the fact that taking the square root of
a number between 0o and one can only a number between 0o and one can only a number between 0o and one can only
make it larger another way to say that make it larger another way to say that make it larger another way to say that
is that squaring a number between Z and is that squaring a number between Z and is that squaring a number between Z and
one can only make it one can only make it one can only make it
smaller we can now simplify just a smaller we can now simplify just a smaller we can now simplify just a
little bit more and we we get the square little bit more and we we get the square little bit more and we we get the square
root of the trace of p 0 * root of the trace of p 0 * root of the trace of p 0 *
row finally squaring both sides and row finally squaring both sides and row finally squaring both sides and
plugging in our bound on the probability plugging in our bound on the probability plugging in our bound on the probability
gives us what we gives us what we gives us what we
want so the formula for the Fidelity want so the formula for the Fidelity want so the formula for the Fidelity
isn't the nicest or easiest to use isn't the nicest or easiest to use isn't the nicest or easiest to use
formula in the world but here it formula in the world but here it formula in the world but here it
actually works pretty actually works pretty actually works pretty
nicely to conclude the lesson we'll take nicely to conclude the lesson we'll take nicely to conclude the lesson we'll take
a look at alman's theorem which is a a look at alman's theorem which is a a look at alman's theorem which is a
fundamental fact about the Fidelity that fundamental fact about the Fidelity that fundamental fact about the Fidelity that
connects it with the notion of a connects it with the notion of a connects it with the notion of a
purification purification purification
what the theorem says in simple terms is what the theorem says in simple terms is what the theorem says in simple terms is
that the Fidelity between any two that the Fidelity between any two that the Fidelity between any two
Quantum States is equal to the maximum Quantum States is equal to the maximum Quantum States is equal to the maximum
inner product in absolute value between inner product in absolute value between inner product in absolute value between
two purifications of those two purifications of those two purifications of those
States in a bit more detail suppose that States in a bit more detail suppose that States in a bit more detail suppose that
row and sigma are density matrices for row and sigma are density matrices for row and sigma are density matrices for
some system X and suppose that Y is a some system X and suppose that Y is a some system X and suppose that Y is a
system that has at least as many system that has at least as many system that has at least as many
classical States as X and therefore classical States as X and therefore classical States as X and therefore
we're guaranteed that both of these we're guaranteed that both of these we're guaranteed that both of these
states have purifications that are states have purifications that are states have purifications that are
quantum State vectors of XY quantum State vectors of XY quantum State vectors of XY
we can then Express the implication of we can then Express the implication of we can then Express the implication of
the theorem in the form of the equation the theorem in the form of the equation the theorem in the form of the equation
shown on the screen the Fidelity between shown on the screen the Fidelity between shown on the screen the Fidelity between
row and sigma is equal to the maximum row and sigma is equal to the maximum row and sigma is equal to the maximum
absolute value of the inner product absolute value of the inner product absolute value of the inner product
between F and SII ranging over all fi between F and SII ranging over all fi between F and SII ranging over all fi
and S that purify row and sigma and S that purify row and sigma and S that purify row and sigma
respectively so let's clean this up and respectively so let's clean this up and respectively so let's clean this up and
work with the precise mathematical work with the precise mathematical work with the precise mathematical
statement going statement going statement going
forward what we'll do now is to prove forward what we'll do now is to prove forward what we'll do now is to prove
the theorem but first it's worth taking the theorem but first it's worth taking the theorem but first it's worth taking
a moment to appreciate a moment to appreciate a moment to appreciate
that it offers us a pretty intuitive way that it offers us a pretty intuitive way that it offers us a pretty intuitive way
of thinking about the of thinking about the of thinking about the
Fidelity it also happens to be quite Fidelity it also happens to be quite Fidelity it also happens to be quite
useful in its own right for instance to useful in its own right for instance to useful in its own right for instance to
prove even more interesting things about prove even more interesting things about prove even more interesting things about
the Fidelity to prove it we'll start with Fidelity to prove it we'll start with
spectral decompositions of our density spectral decompositions of our density spectral decompositions of our density
matrices no surprises matrices no surprises matrices no surprises
there once we have spectral there once we have spectral there once we have spectral
decompositions it's easy to come up with decompositions it's easy to come up with decompositions it's easy to come up with
two specific purifications of these two specific purifications of these two specific purifications of these
states but here we're actually going to states but here we're actually going to states but here we're actually going to
play a play a play a
trick notice that in both cases we don't trick notice that in both cases we don't trick notice that in both cases we don't
have a standard basis VOR as the second have a standard basis VOR as the second have a standard basis VOR as the second
tensor Factor like we did earlier in the tensor Factor like we did earlier in the tensor Factor like we did earlier in the
lesson but rather we have the same lesson but rather we have the same lesson but rather we have the same
Vector as in the first tensor Factor Vector as in the first tensor Factor Vector as in the first tensor Factor
except that it has a bar over it and except that it has a bar over it and except that it has a bar over it and
that means the entry-wise complex that means the entry-wise complex that means the entry-wise complex
conjugate of these vectors so in other conjugate of these vectors so in other conjugate of these vectors so in other
words we're basically just taking a words we're basically just taking a words we're basically just taking a
second copy of the igen vectors but we second copy of the igen vectors but we second copy of the igen vectors but we
take the complex conjugate of each entry take the complex conjugate of each entry take the complex conjugate of each entry
and we'll see why we do that in a few and we'll see why we do that in a few and we'll see why we do that in a few
moments moments moments
but it's not at all obvious that we but it's not at all obvious that we but it's not at all obvious that we
should do this and that's why we call it should do this and that's why we call it should do this and that's why we call it
a a a
trick in any case these are trick in any case these are trick in any case these are
purifications of row and sigma purifications of row and sigma purifications of row and sigma
respectively and that's true by virtue respectively and that's true by virtue respectively and that's true by virtue
of the fact that our igen vectors are of the fact that our igen vectors are of the fact that our igen vectors are
orthonormal and therefore so are their orthonormal and therefore so are their orthonormal and therefore so are their
entry-wise complex entry-wise complex entry-wise complex
conjugates and now we can apply the conjugates and now we can apply the conjugates and now we can apply the
unitary equivalence of purifications unitary equivalence of purifications unitary equivalence of purifications
which says that every purification of which says that every purification of which says that every purification of
our two states can be obtained by our two states can be obtained by our two states can be obtained by
applying a unitary operation to the applying a unitary operation to the applying a unitary operation to the
system that gets traced out for the system that gets traced out for the system that gets traced out for the
specific choices of purifications that specific choices of purifications that specific choices of purifications that
we just we just we just
made the unitary operations don't have made the unitary operations don't have made the unitary operations don't have
to be the same for the two purifications to be the same for the two purifications to be the same for the two purifications
of course so here we're using the letter of course so here we're using the letter of course so here we're using the letter
U for the first one and the letter V for U for the first one and the letter V for U for the first one and the letter V for
the second the second the second
one so the point is that every one so the point is that every one so the point is that every
purification of our two states can be purification of our two states can be purification of our two states can be
written as we have on the screen for written as we have on the screen for written as we have on the screen for
some choice of unitary operations u and v and now we can take the expression v and now we can take the expression
from the theorem and from the theorem and from the theorem and
simplify if we plug in our expressions simplify if we plug in our expressions simplify if we plug in our expressions
for F and S and simplify a little bit we for F and S and simplify a little bit we for F and S and simplify a little bit we
get the expression that's shown here on get the expression that's shown here on get the expression that's shown here on
the the the
screen now there are a couple of steps screen now there are a couple of steps screen now there are a couple of steps
in there and I haven't shown the details in there and I haven't shown the details in there and I haven't shown the details
so you know what to do if you want to do so you know what to do if you want to do so you know what to do if you want to do
it but let me mention that the reason it but let me mention that the reason it but let me mention that the reason
why we chose the purifications as we did why we chose the purifications as we did why we chose the purifications as we did
was to allow this inner product to was to allow this inner product to was to allow this inner product to
simplify in exactly this way and in simplify in exactly this way and in simplify in exactly this way and in
particular we put the complex conjugates particular we put the complex conjugates particular we put the complex conjugates
in there so we'd effectively be able to in there so we'd effectively be able to in there so we'd effectively be able to
flip around the inner products in the flip around the inner products in the flip around the inner products in the
second tensor factor to get the specific second tensor factor to get the specific second tensor factor to get the specific
form that we have right form that we have right form that we have right
here we're almost there now we can use here we're almost there now we can use here we're almost there now we can use
the cyclic property of the trace along the cyclic property of the trace along the cyclic property of the trace along
with the spectral decompositions we with the spectral decompositions we with the spectral decompositions we
started with to get this started with to get this started with to get this
expression and at this point we can expression and at this point we can expression and at this point we can
employ one of the alternative ways of employ one of the alternative ways of employ one of the alternative ways of
expressing the Fidelity using the trace expressing the Fidelity using the trace expressing the Fidelity using the trace
norm and we're norm and we're norm and we're
done certainly there are details in done certainly there are details in done certainly there are details in
there that I glossed over very quickly there that I glossed over very quickly there that I glossed over very quickly
and it's the kind of proof that you and it's the kind of proof that you and it's the kind of proof that you
really do need to work through yourself really do need to work through yourself really do need to work through yourself
to fully grasp but hopefully this gives to fully grasp but hopefully this gives to fully grasp but hopefully this gives
you the basic you the basic you the basic
idea and as usual you can find more idea and as usual you can find more idea and as usual you can find more
details in the written content for the details in the written content for the details in the written content for the
lesson which is linked in the video lesson which is linked in the video lesson which is linked in the video
description and that's the end of the description and that's the end of the description and that's the end of the
lesson which has been about lesson which has been about lesson which has been about
purifications Fidelity and the purifications Fidelity and the purifications Fidelity and the
connections between them and it's also connections between them and it's also connections between them and it's also
the end of the unit which has been about the end of the unit which has been about the end of the unit which has been about
the general formulation of Quantum the general formulation of Quantum the general formulation of Quantum
information I hope you will join me for information I hope you will join me for information I hope you will join me for
the fourth and final unit of the series the fourth and final unit of the series the fourth and final unit of the series
which is on Quantum error which is on Quantum error which is on Quantum error
correction thanks for watching

## Quantum Algorithmic Foundations ｜ Understanding Quantum Information & Computation ｜ Lesson 06

- Welcome back to Understanding Quantum
Information and Computation. My name is John Watrous and
I'm the technical director for IBM Quantum Education. This is the sixth lesson of the series and it's the second lesson in
the second unit of the series, which is on quantum algorithms. In the previous lesson, we discussed the query
model of computation, and we saw that in that model, quantum algorithms can
provide a striking advantage over classical algorithms, including an exponential advantage in the case of Simon's problem. Unfortunately, the query
model doesn't really have any direct or immediate
practical relevance. The notion of a black box is a very useful
abstraction as it turns out, but we have some work to
do if we're going to apply the insights that we've
gained from the query model to a more standard computational setting where inputs are given as strings of bits rather than as oracles or black boxes. The main purpose of this lesson is to establish a foundation
from which we can do that. Here's an overview of the lesson. For much of the lesson, we're going to be focusing
on computational cost, or in other words, how difficult different
computational tasks are and how we can measure that difficulty. Roughly speaking, this translates to how long we have to wait
for computations to finish and ultimately which
computational problems we can solve and which
ones are beyond our reach. We're going to start with
a couple of examples. Integer factorization and computing greatest common
divisors or GCDs for short. These are very good examples for illustrating the
underlying principles, and they also both happen
to be highly relevant to Shor's algorithm, which we'll
discuss in the next lesson, so we'll effectively
get double the mileage out of these examples. Then we'll discuss the notion of computational costs more generally and how we can measure it. There are many different
ways that this can be done, but we'll focus pretty
narrowly on how we can do this with circuit models of computation, specifically Boolean circuits
and quantum circuits. The focus here will be on algorithms as opposed to computer
hardware, for instance, and we'll be more concerned with how the costs of
running an algorithm scale as the specific problem instances it's run on grow in size rather
than on how many seconds, minutes, or hours some
particular computation requires. The idea that motivates this point of view is that algorithms have
fundamental importance. To this day, we're
still running algorithms that were developed a
very, very long time ago and very naturally, they will be deployed against larger and
larger problem instances using faster and more reliable hardware as technology develops. And the way that they scale tells us a great deal
about how useful they are and how useful they'll continue to be. In the last section of the lesson, we'll turn to a critically important task, which is running classical computations on quantum computers. The reason this is important is not because we hope to replace classical computers
with quantum computers, which seems extremely unlikely to happen anytime soon if it ever happens, but rather because it opens up many interesting possibilities
for quantum algorithms. Specifically, once we know how to run classical computations
on quantum computers, they become available to quantum
algorithms as subroutines so we can effectively
leverage everything we know about classical algorithms in the pursuit of quantum computational advantages. The classical computers that we have today are truly marvels of technology. They're incredibly fast, but they're not so fast that no computational problem
is beyond their reach. Some computational problems
are so inherently difficult that although we do have
algorithms to solve them, no computer on the planet Earth today is fast enough to run these
algorithms to completion on even moderately sized inputs within the lifetime of a human or even within the lifetime
of the Earth itself. To explain further, let's take a look at the
integer factorization problem. The problem statement is very simple. The input to the problem is an integer N that's at least two, and the output is the
prime factorization of N, and what that means is a list of the prime number factors of N together with the powers that these prime numbers must be raised in order to get N by
multiplying them all together. Of course, we can always multiply the prime factors together in any order, but aside from their ordering, prime factorizations are always unique for every choice of N. That fact is called the
fundamental theorem of arithmetic, and if you're interested
in learning more about it, it's something that you can find at the very beginning of
many books on number theory. As a very simple example, here's the prime factorization
of the number 12. It's two to the power two
times three to the power one. Not surprisingly, a
computer can tell you this in the blink of an eye by simply searching
iteratively for prime factors. Here's another example. I won't try to read this number. It's about 3.4 quattuordecillion, and I have to admit that I find it strange that there's even such a name, but anyway, it has 46 decimal digits and here you can see
its prime factorization. A computer can also tell you this. I factored this number using the factoring method that's included in the symbolic mathematics
library for Python, and it took a few seconds, which is not a particularly long time, but there was a noticeable delay. And certainly the factor N method doesn't find this prime factorization by naively searching for prime factors. It actually uses a hybrid
of different algorithms including one called the
colored row algorithm, which is an algorithm for factoring that's more sophisticated than naively searching iteratively
for prime factors, and although that might seem
like a pretty large number, as an input to a computational problem, it isn't really all that large at all. It's only 46 characters. Here's one more example. It's a number with 309 decimal digits. If we write it in binary notation, it has 1,024 bits and that's
why it's called RSA1024. The prime factorization of this number is not currently known. This number comes from the
RSA factoring challenge, which was run by RSA
Laboratories from 1991 to 2007. They offered cash prizes
for the prime factorizations of various numbers of increasing
size, including this one, which was worth $100,000 US, but that prize was never collected. The security of the RSA
public key crypto system is based on the difficulty
of integer factorization. If you can factor, you
can break the system, and so the purpose of the challenge was basically to track the
state of the art in factoring. The bottom line is that
factoring this number is beyond our current abilities. As far as we know, the
problem itself is too complex for classical computers
to solve in practice. The largest RSA challenge number to have been factored so far, by the way, has 250 decimal digits. It might be a little bit hard
to read this on the screen, but here's the prime
factorization of RSA250. It was done in 2020 using an algorithm known as the number field sieve, and it was a collective effort involving tens of thousands
of computers around the world. Now, let's consider a different
computational problem, which is the problem of computing the greatest common
divisor, or GCD for short, of two non-negative integers. Here's a statement of the problem. The input consists of two
numbers this time, N and M, and we're gonna assume that
they aren't both equal to zero. The goal is to compute the
GCD of these two numbers, which is the largest integer D that evenly divides both N and M, and if you think about the
prime factorizations of N and M, what you'll find is that the GCD is the product of all the prime numbers that divide both N and M raised to the minimum of the powers that appear in the two
prime factorizations. So it's related to the problem
of integer factorization, but it turns out to be much, much easier for computers to solve. We can compute GCDs for numbers with many thousands of digits
in the blink of an eye. It's kind of pointless for me to try to show you examples on the screen because we could fill the
entire screen with digits and we'd still be describing easily solved instances of this problem. But if you're interested, follow the link in the video description to the textbook page for this lesson and you'll find code cells for the sorts of computations
that I'm talking about, including ones that compute
GCDs of large numbers, and you can try them out for yourself. The question is why is this possible to compute GCDs so easily? And the answer to that question is that we have efficient
algorithms to compute GCDs. Among them is Euclid's algorithm, which was discovered over 2,000 years ago, but there are also other
efficient GCD algorithms. And of course we're talking about classical algorithms here, so that leads to another question. Could there be an efficient
classical algorithm for integer factorization
that would allow us to factor numbers like RSA1024
in the blink of an eye? And the answer is yes. Absolutely there could be an efficient classical
algorithm for factoring and currently we don't know any way to rule out that possibility. All that we can say is that
we haven't found one yet. For all the classical
factoring algorithms we know, we're still essentially
searching for factors and that puts numbers like
RSA1024 out of our reach. Efficient GCD algorithms,
on the other hand, just don't work that way. They don't search for a
greatest common divisor. They effectively just build
it with a mechanical procedure that's roughly comparable in cost to multiplying the same
two numbers together, and I'll say a little bit more about this later in the lesson. Of course, this is all building
up to Shor's algorithm, which we'll see in the next lesson, which is an efficient quantum algorithm for computing prime factorizations, but we still have some ground
to cover before we get there. Next we're going to talk
about a mathematical framework through which we can measure the computational costs
of different algorithms, including ones we've already talked about. To be clear, we're going to be focusing very narrowly on circuits, which is just a small piece
of a much broader subject. Let's start with a very high level view of an ordinary computation
from the previous lesson, meaning in particular that we're not thinking
about query problems, we're thinking about
problems where the input is given explicitly as a binary string. Like I said in the previous lesson, we could of course allow
other symbols if we chose to, but let's just keep things simple and stick with binary strings for both the input and the output. The computation itself could be modeled or described in a variety of ways such as with the Turing
machine model of computation, Boolean circuits, quantum circuits, or programs written in
a language like Python or any other language of your choice. Like I said a moment ago, we're gonna be focusing on
circuits, Boolean and quantum. First, let's spend a little bit more time talking about the input and the output, which we're assuming are binary strings. Using binary strings, we can encode many different sorts of
mathematical objects, such as numbers, vectors,
matrices, graphs, descriptions of molecules, for instance, as well as lists of all of these objects and other ones as well. As a very simple example, if we wanna encode non-negative
integers as binary strings, we can use binary notation. Here in this table, you'll see the first 13
non-negative integers along with their binary
representations or encodings. Here in the third column, you can see the length of
each of these encodings, which is just the total number of bits you need for the encoding, and here's a simple formula that tells you exactly how many bits
are needed in general for any non-negative integer N. Notice in particular that
the length of the encoding is generally much smaller
than the number itself. It's logarithmic in the number. RSA1024 for instance, has an encoding with length 1,024, but the number itself is of
course much larger than that. This encoding scheme works
for non-negative integers, but if we want to handle
arbitrary integers, including negative ones, we
can simply tack on a sign bit. We can also allow any number of leading zeros in these encodings if we want the encodings
to fill out a block of bits or a register of a fixed size as long as it's big enough. Sometimes that's very convenient. This isn't the only way to encode integers as strings of bits. There are other ways that you could do it, but this is a pretty
standard way to do it. For other sorts of objects that might form the inputs or the outputs to the computational
problems we're interested in, we can come up with
other encoding schemes. For example, think about how
you might encode a vector or a graph as a binary string or think about how you might encode two binary strings into
a single binary string using just the symbol zero and one without a space or any other symbol to separate the two strings. There are certainly ways to
do these things, and in fact, we have a lot of freedom
as to exactly how we do it. Often there won't be a single standard or universally agreed upon way to encode whatever thing we're talking
about as a binary string, but that's okay, we can just pick one or we can invent a new one if we prefer. From a purely formal viewpoint, we're thinking about computations as essentially mechanical transformations on strings of bits and the specifics of whatever
encoding schemes we use are certainly important at some level. But nevertheless, when we're designing and
analyzing algorithms, we usually don't worry all that much about the specifics of
the encoding schemes. If you ask two different people to come up with a way to encode, say, molecules of a certain
type as binary strings, you might expect them to come up with different ways to do it, but if these were reasonable people who knew what they were doing, you could also reasonably expect that it would be a pretty
simple computational process to translate back and forth between the two encoding schemes, most likely requiring a very
straightforward computation that would be quite easy compared to the actual computation
we're interested in. The bottom line is that the details concerning the encoding schemes we use are often quite secondary and our focus is on the
algorithms themselves. None of this is going to be an issue for this lesson, by the way. We're mainly gonna be focusing on problems that concern integers, which we'll assume are encoded in binary, but it's something to be aware of. In general, for whatever sort of problem we're talking about, the input length refers
to the number of bits we need to encode the input, and we view the input length as being the size of the instance of whatever problem we're thinking about. Now, let's consider
the computation itself, which is represented by the
blue rectangle in this figure. The way that we'll measure
the cost of this computation is to count the number of elementary operations that it requires. Intuitively speaking, an elementary operation is one involving a small fixed
number of bits or qubits that can be performed quickly and easily like competing the and of
two bits, for instance. At a formal level,
there are different ways to define what an elementary operation is, depending upon what
computational model we're using. For example, an elementary operation could be one step of a Turing machine if we were using that model. For the purposes of this lesson, we're gonna be focusing on circuit models, as I've already stated, and specifically quantum
and Boolean circuits. When we're working with circuits, it's typical that we think about each gate as representing an elementary operation. We've seen several different quantum gates thus far in the series,
including X, Y and Z gates, Hadamard gates, S and T gates, controlled NOT gates, swap gates, Toffoli gates, and Fredkin gates. We also talked about query gates in the context of the query model, but we're not going to be working within the query model in this lesson, so we won't worry about
query gates for now. We also saw that any unitary operation on any number of qubits
can be viewed as a gate if we want to do that. For the purposes of measuring cost though, that's not going to be very helpful, so what we're going to do is we're simply going
to pick a set of gates and we'll view that set as being our set of elementary operations. First, we'll include the
single qubit unitary gates that you see here,
namely X, Y and Z gates, Hadamard gates and S and T gates, as well as their inverses
or conjugate transposes. We don't actually need all of these gates. As soon as we have Hadamard and T gates, we can implement the rest of them, but just for the sake of convenience, we're gonna include all of them. We'll also include controlled NOT gates, and these will be the only
multiple qubit gates in our set. And finally we'll include single-qubit standard basis measurements so that we can actually get some classical information
coming out of our circuits. If we focus just on the unitary gates and forget about the
measurements for a moment, we obtain what's called
a universal gate set. What that means is that
any unitary operation on any number of qubits can
either be implemented exactly or arbitrarily closely
with a quantum circuit built out of just these gates. In other words, this is a
good enough set of gates that we can build any unitary
operation out of them. Not necessarily exactly, but we can get as good an
approximation as we want. That might require a lot of gates, and it's actually not too hard to reason that it must require a
lot of gates sometimes, but nevertheless, it's always possible. It's not a simple matter to prove that this is a universal gate set though, and we won't discuss how
that's done in the series, but it is a pretty well known fact and there are in fact different
ways that it can be proved. There are also other gate sets that are sometimes used
instead of this one, such as Toffoli gates, Hadamard gates, and S gates in addition to measurements, but this is a pretty standard choice. So in summary, these are
the quantum operations that we'll view as being elementary. For Boolean circuits,
we're going to consider these four gates to be
elementary operations: AND gates, OR gates, NOT gates and FANOUT. FANOUT, by the way, is not
always counted as being a gate, but it's going to be
important for us to do this as we will see later in the lesson. Similar to what we had for
our standard quantum gate set, this set is a little bit redundant. You don't need both AND and OR. You can create either one using the other with three NOT gates, but it doesn't hurt to include both and it's just convenient. As you may already know, this is a universal gate set
for deterministic computations, meaning that any function
from bits to bits can be implemented with a Boolean circuit composed of gates from this collection. Once again, just like we
had for quantum circuits, this doesn't mean that every function can be implemented efficiently
by a Boolean circuit, and indeed most functions
can only be implemented with very large circuits
for the simple reason that there are lots and lots of functions and they can't all have small circuits. There just aren't enough
small circuits to go around. Universality is easier
to prove in this case than in the quantum case as it turns out. So if you don't already
know how to prove it, give it some thought and
see if you can figure it out if you're so inclined. Now that we've decided on the
gates we're going to allow and we're going to consider
to be elementary operations, we can think about how many of them are required to perform
various computations. To do that, we'll make
use of this definition for the size of a circuit. This definition works for both
Boolean and quantum circuits and it's very simple. The size of a circuit is the
total number of gates it has. For example, if we return
to the same example we've seen several times, which is our Boolean circuit for computing the exclusive OR of two bits, we see that it has size seven
because there's seven gates, two FANOUT gates, two NOT gates, two AND gates and one OR gate. If we decided not to count
FANOUT gates, which is common, it would have size equal to five, but we are going to count FANOUT gates, so its size is seven. The size of a circuit
corresponds to the number of elementary operations
we need to perform. So if we were to perform
those operations sequentially and the size would tell
us how long it would take, so you can think about a circuit size as representing sequential running time. This is going to be how we
measure computational cost for the purposes of this lesson. There is another property
of circuits though that can also be viewed as the amount of time
required to run a circuit, and that's the circuit's depth. To be precise, the depth
is the maximum number of gates encountered on any path from an input wire to an output wire, which is equivalent to the number of layers of gates we need
to create the circuit, where a layer refers to a bunch of gates that could be performed simultaneously because they don't have
any wires in common. For example, our circuit
from before has depth four, and we can see directly from the diagram that there are four layers. The FANOUT gates can be
performed simultaneously, the NOT gates can be
performed simultaneously and the AND gates can be
performed simultaneously as well. So we can think about circuit depth as representing parallel running time. Circuit depth is certainly an interesting and important notion and there are some very
sophisticated techniques that can be used to reduce the depth required for some computations, sometimes quite dramatically, in fact, but it's an advanced topic and we're not gonna have too much more to say about it in this series. So once again, we'll
measure computational cost in terms of circuit
size or in other words, the number of elementary operations required to perform a computation. At the beginning of the lesson, we talked about the integer
factorization problem and we saw that the time required for an algorithm to solve this problem can increase as inputs grow in length, and this is typically
the issue we focus on when we're analyzing algorithms. That is, we're interested in how the costs of running algorithms scale as inputs get larger and larger. Of course, this sort of scaling isn't the only thing that matters. For example, we may simply be interested in finding the factors
of RSA1024 and that's it. So in a case like that, we'd be talking about a
single problem instance where nothing is growing. But when we're thinking about algorithms, the way that their costs
scale matters a great deal and it tells us a lot
about the algorithms. This point of view is
consistent with the idea that algorithms are fundamental, and as I talked about before, as time goes on and technology advances, they will be deployed against
larger and larger inputs. But notice that while inputs to computational problems
can grow in size, circuits don't grow. Each circuit has a fixed size. And for that reason, if we want to describe an
algorithm using circuits, we're actually going to need a family of circuits to do that, and by a family of circuits, we mean a collection of circuits that gets larger and larger so that we can accommodate inputs as they get larger and larger. For example, let's consider a classical deterministic algorithm for the integer factorization problem, maybe one based on trial division, but the specifics don't really matter for the sake of this example. We could in principle
describe such an algorithm using a family of Boolean circuits where if we want to factor a number whose binary representation has N-bits, we grab the circuit CN from our family and we input the number
that we want to factor into the input bits of that circuit. Sometimes we might choose to parameterize these
circuits in different ways, so it might not always be the case that the Nth circuit has N input bits, but that's a pretty typical way to do it. Naturally, as the input
lengths get longer, we would expect the circuits
in this family to get larger reflecting the fact that
factoring four bit numbers is a lot easier than factoring 1,024 bit numbers, for instance. And so, with that in mind, we measure the cost of such
an algorithm by a function, which tells us how many
elementary operations we need for each possible input length, and we can do this for both classical and quantum algorithms. As an example, let's consider the problem of integer addition, which is certainly much
simpler than factoring or even competing GCDs. Just to keep things simple, let's suppose that both N and
M are non-negative integers and that they're represented by binary strings having the same length. How might we build Boolean circuits that add these two numbers together? Well, let's start with an algorithm, the standard algorithm for addition, which is the base two analog of the method that I presume we all learned for adding numbers together with a
pencil and a piece of paper, so we start with the
least significant bits, add them together, and that gives us the least
significant bit of the result along with a carry bit
that we have to consider when we move to the next position. And we continue moving through the bits, generating output bits
along with carry bits until we get to the end. This method can be
expressed pretty directly in terms of Boolean circuits. One way to think about it is to start off with a small circuit for adding two single bit integers together. That'll give us a number
between zero and two, where the lesser significant bit is the XOR of the input bits and the more significant bit, or the carry bit if you prefer, is the AND of the two input bits. Sometimes we refer to a circuit that performs these
two operations together as a half adder. That works fine when there
are just two input bits, but if we want to add together integers with more than just one bit each, we're going to need to deal
with the carry bits somehow. So what we can do is to cascade
two half adders together and take the OR of the
carry bits they produce to create what's called a full adder, which is basically a Boolean circuit for adding three single
bit numbers together. And once we have a full adder, we can add numbers together
simply by cascading, proceeding from the least significant bit to the most significant bit. We're basically just implementing the standard addition algorithm using the full adders to
deal with the carry bits. So what's the cost of this computation? Well, we need seven elementary
gates for each XOR gate, which means 10 gates for a half adder and therefore 21 gates for a full adder, which is two half adders plus an OR gate. So at the end of the day, if our input numbers both have AND bits, we need a half adder for
the least significant bit and a full adder for every other bit, and the end result is 21 times N minus one plus 10 or 21 N minus 11 gates in total. If we decided for some reason that we didn't want to
count the FANOUT gates and we included the XOR
gates in our gate set, we'd need five N minus
three gates in total. It's nice to know exactly how many gates or elementary operations are required to perform different computations, such as we just saw with integer addition. And of course, if we actually want to implement these computations, we need to know these details. But if we always go into this much detail, especially for more
complicated algorithms, we'll be very quickly buried in details that in many cases have
secondary importance. One way to simplify matters that's very common in the
analysis of algorithms is to make use of big-O notation, which allows us to express
and compare rates of growth in a pretty simple way
that's very convenient. And here you can see the definition that underlies this notation. Suppose that we have
two functions G and H, which we can think of as taking non-negative integer
values for simplicity, although that's not actually required. Maybe G and H represent costs of different computations, for instance. We then say that G is big-O of H, which is traditionally written
with an equal sign like this. If G of N is at most some
positive constant C times H of N, so long as N is large enough. So this is about what
happens when N gets large. Formally, what large enough means is that there's some choice of N zero so that the inequality is true whenever N is at least N zero. The key here is that these
numbers C and N zero are fixed. They can be any numbers at
all, but whatever they are, they can't change as N grows. If you've never seen this
kind of definition before, it might take some getting used to and you can find resources
that will explain this concept in a lot more detail than
I'm able to in this lesson. Usually what we do is to choose
the function H to be simple in order to express something about how quickly the function G grows. For example, the function whose value is 17 times N cubed minus 257 times N squared plus
65,537 is big-O of N cubed, and you can fit this
example to the definition by picking C and N zero appropriately. For example, if we take C to be 18, then we'll have that
the expression is indeed at most 18 times N cubed
as long as N is big enough. Tying this notation back to the
example of integer addition, what we can now say is that there exists a family of Boolean circuits for adding non-negative integers together such that the size of the N circuit, meaning the one that works
for inputs of length N is big-O of N. The actual size of CN
according to our analysis was 21 times N minus 11,
but that's big-O of N. Now it's not always a smart thing to do to throw away details like this, but here it makes sense because it reveals the essential nature of the standard addition algorithm, which is that its cost scales linearly in the size of the numbers
that we're adding together, and this expression is not sensitive to some of the low level
details of our model, such as precisely which gates we consider to be elementary operations. For instance, if we decided that XOR gates should be considered as
elementary operations or that FANOUT gates shouldn't
count as gates or both, we'd still have that integer addition can be computed at cost big-O of N. Here's another example of
a computational problem, multiplication of integers. Similar to addition, we can ask what the cost
of this computation is. If we consider the standard
multiplication algorithm, meaning the base two analog of the method for multiplying numbers
taught in elementary schools, which is basically just
shifting and adding when we're working with numbers in binary, we find that there are Boolean circuits having size big-O of N squared for multiplying N-bit integers. I won't go through this one in detail. Of course the circuits are a
little bit more complicated than the ones for addition, but if you think about the way that the standard
multiplication algorithm works, it shouldn't come as a surprise. In essence, we need to
do roughly N additions, each of which has linear cost, and so we end up with
quadratic cost overall. This can be generalized. If one of the numbers has N bits and the other one has N bits, then we need circuits of
size big-O of N times M. Here, by the way, we are actually using an extension of the big-O notation where we have multiple variables, but it can be extended like this, and if you're interested
in the specific details, I'll refer you to the textbook
content for the lesson, which again is linked in
the video description. It's actually possible
to multiply together two N-bit integers in a way
that's asymptotically superior to the standard multiplication algorithm. In particular, the Schonhage-Strassen multiplication algorithm
multiplies two N-bit integers with a cost of big-O of
N times the length of N times the length of the length of N. So as N gets large, like
tens of thousands of bits, this is actually a more
efficient way to multiply, but it's complicated and the overhead does make it impractical
for smaller numbers. So let's add multiplication
to our list of examples. And just for simplicity, let's list the cost given by the standard multiplication algorithm, which is big-O of N squared. So once again, we're focusing on the essential nature of this cost, which is that it scales quadratically or sub quadratically if we use asymptotically superior methods. We can also consider the
problem of integer division, which is stated precisely on the screen. Here we're talking about computing a quotient and a remainder, so the answer consists
of two integers, Q and R. Once again, there's a standard
algorithm for division and it also has quadratic
cost and like multiplication, there are in fact asymptotically
superior algorithms. So how do GCD algorithms compare? Well, Euclid's algorithm turns
out to have quadratic cost, and that's what I meant
earlier in the lesson when I said that we can compute GCDs at roughly the same cost as multiplying two numbers together. Once again, there are
asymptotically superior algorithms along similar lines to
addition and multiplication, although the cost is a little
bit higher in this case, as least as far as we know. I'll mention just one more example of a problem to add to this list, and that's modular exponentiation. Here there are three input
numbers, K, M, and N, and the goal is to compute
N to the power K modulo-M, which means the remainder
after we divide by M. It's not at all obvious, but we can compute modular
exponentiation at qubit cost, meaning big-O of N cubed
for N-bit integers. The way to do this one is not to first compute N to the power K and then compute the remainder. Even writing down into the power K could require exponentially many bits, so that approach won't work. Instead, we can use an algorithm
called the power algorithm, which is also called repeated squaring where everything is done modulo-M and we use the bits of
K to iteratively square and multiply in a way
that makes it all work. So this one is more
expensive than the others, but we can still perform
modular exponentiation quite quickly on numbers
with many thousands of bits. So how do the costs of these tasks compare with integer factorization? If we start simple and
think about trial division where we first check to see if two is a factor of the input number and then check three and so on, we'll end up with a pretty
expensive algorithm, big-O of N squared times
two to the power N over two. Each division takes quadratic time and in the worst case, we'll need to search for prime factors up to the square root of the input number, and that's where the N
over two is coming from. If we haven't found a
prime factor at that point, we can just stop because then the input number itself must be prime. This is extremely expensive because N appears as an exponent in this expression, which is to say that this
algorithm has exponential cost. In contrast, all of the examples that we have in this box here have what we call polynomial cost where N gets raised to a power, but that power is always a fixed number that doesn't grow as
the input length grows. In these cases, the powers
are one, two, and three. I'll come back to polynomial versus exponential cost in just a moment, but first, let's finish off
what we know about factoring. Earlier I mentioned
the number field sieve, which is the best
factoring algorithm we have in terms of the way its cost scales. In fact, the cost of this algorithm isn't actually rigorously proven because it's connected with some open questions about number theory, but it is conjectured to have
a cost that looks like this. This time the big-O is in the exponent, but I'm gonna sweep that
detail under the rug. The key takeaway is that it's significantly better
than trial division because although N does
appear in the exponent, it's raised to the power 1/3 and that translates to a much lower cost and that's why this algorithm can handle numbers like RSA250, whereas trial division
most certainly can't. It's still exponential though in the sense that a power of N is appearing in an exponent, and so, as a result, it still can't handle
numbers like RSA1024. Now let me say a little bit more about polynomial cost
versus exponential cost. We say that an algorithm
has polynomial cost if its cost is big-O of N to the power B, where B can be any positive number, but the key is that B has to be fixed, meaning that it doesn't change as the input length end grows. For the examples we saw a moment ago, meaning integer addition, multiplication, division, computing GCDs
and modular exponentiation, we had that B was either
one, two or three. So these are all
polynomial cost algorithms. But when we say polynomial
cost more generally, B could be any positive number. It could even be one million,
but it has to be fixed. As a very rough abstraction, algorithms having polynomial
cost are typically viewed as representing efficient algorithms. It's definitely an oversimplification, but as a rough guideline, you can think about a
polynomial cost algorithm as somehow making use
of a problem's structure to construct a solution as opposed to a brute
force type of approach or an exhaustive search
over a very large space. Of course, an algorithm whose cost scales as N to the one million
on inputs of length N would not reasonably
be viewed as efficient, and we wouldn't expect to be
able to run it on large inputs. But still even an N to
the one million algorithm must be doing something clever that somehow reflects the structure of whatever problem it solves to avoid the exponential scaling in cost that would be associated with an exhaustive search over a large domain. And in practice, the identification of a polynomial cost algorithm is really just a first
step towards efficiency. If a problem is important
and someone discovers the very first polynomial
cost algorithm for it, maybe having a very large exponent, you can be reasonably sure that algorithm designers
will find improvements that cause that exponent to come down. In essence, you can
think about the existence of a polynomial cost
algorithm for a problem as demonstrating the feasibility
of solving that problem on reasonably large inputs, but there still could be a lot of work required to reach true efficiency. In complexity theory, it's typical to say that an algorithm's cost
scales sub-exponentially if the cost is big-O of two to the power N to the power epsilon for every positive real number epsilon, and if it's not sub-exponential, then it's exponential, or it could even be super-exponential like two to the two to the N,
which is doubly exponential, but as long as the output
isn't super-exponentially long, every problem can actually be
computed by a Boolean circuit having exponential cost as it turns out. So the important distinction here is between polynomial
and exponential cost. We don't know of any sub-exponential cost classical algorithms for
integer factorization. For example, the number field sieve's cost is not big-O of two to
the N to the epsilon when epsilon is 1/3 or less, so it doesn't satisfy this definition. It's undoubtedly a very clever algorithm, but it's still a sieve and we're still effectively searching over a very large space to identify factors of a given number. As we'll see in the next lesson, there is in fact a polynomial
cost quantum algorithm for integer factorization. Shor's algorithm has polynomial cost and specifically it has qubit cost, similar to modular exponentiation. Another class of problems that
you might be familiar with is the class of NP-complete problems. I won't go into what NP-complete problems are in this lesson, but you can look them
up if you're interested. It's conjectured that
none of these problems have sub-exponential cost. That's essentially a circuit-based version of what's sometimes called the
exponential time hypothesis, but so far nobody's been able
to figure out how to prove it. In fact, we can't even prove that there aren't linear cost algorithms for NP-complete problems. It's just really tough, at
least as far as we know, to rule out the possibility
of a future discovery of an incredibly creative
algorithm for these problems. Here's a figure that
illustrates the relationship between polynomial and exponential cost. On the X axis, we have the input length to whatever problem we're thinking about, and the Y axis represents the
cost of solving that problem or you can think about
it as the time required to solve the problem if you prefer. An exponential cost algorithm
might scale like this. For short inputs, the cost might not be particularly high, like factoring the
number 12, for instance, but as the input length gets longer, the cost is eventually going to blow up. A polynomial cost algorithm,
on the other hand, might have a scaling that looks like this. The cost will still increase in general as inputs get longer, and it might even cost more than the exponential cost
algorithm for some inputs, but eventually it's gonna
be a lot less expensive than the exponential cost algorithm and it doesn't blow up
in the same extreme way that exponential cost algorithms do. So if we imagine that we
have some maximum cost that we're willing to spend, or a maximum amount of time that we're willing to wait for an answer, if you prefer to think about time, then the exponential cost algorithm will only work for inputs
up to a given length, whereas, in general, the
polynomial cost algorithm will allow us to solve the
problem for larger inputs. And that's why, for instance, we can easily compute GCDs for numbers with many thousands of
bits, but in the worst case, we can't factor numbers of this size. In the last part of the lesson, we'll turn our attention to implementing classical computations
on quantum computers. More specifically, we'll
see that any computation that can be performed by a Boolean circuit can also be performed by a quantum circuit at roughly the same cost. Moreover, this can be
done in a clean manner that's important if we
wanna run these computations as sub routines inside of
larger quantum computations, and I'll explain what I mean by clean when we get further into the discussion. We're going to make use of
Toffoli gates in this process. Recall that Toffoli gates are controlled-controlled-NOT gates, so their action on standard basis states is as you see here on the screen. Notice that we can also
think about Toffoli gates as being like query gates
for the AND function. That's not to say that they are query gates in a literal sense. As I mentioned at the start of the lesson, we're not working within the
query model at this point, but rather, their action follows the same pattern as a query gate. We can think of the binary values A and B as the inputs to the AND function and like a query gate, these values get echoed
as part of the output and the value of the function, meaning the logical AND of A and B gets XORed onto the bottom qubit. Now, we haven't included Toffoli gates in our standard gate set. We could change our mind if we wanted to, but we don't really need to
do that because alternatively, we can implement Toffoli
gates using Hadamard gates, T and T dagger gates
and controlled NOT gates like you see right here. It's not at all obvious
that this particular circuit implements a Toffoli gate, but it does, and you can either take my word for it or multiply everything out for yourself to get some extra practice, or you can just ask Qiskit or Python or any software that multiplies matrices to do the multiplication for you. And you can check out the
textbook content for the lesson to see this done using Qiskit. Now we can move on to simulating Boolean gates with quantum circuits and all we'll need to
do this are NOT gates, controlled NOT gates and Toffoli gates. These operations are all deterministic in addition to being unitary and sometimes we describe
operations like this as being reversible, and in fact, the study of reversible computation predates quantum computing and that's where this entire
procedure actually comes from. The Boolean gates we need to
worry about are NOT gates, Hadamard gates and FANOUT gates, And what we're going to show is that we can implement these gates, or simulate them if you
prefer, using NOT gates, controlled NOT gates and Toffoli gates. To be clear, what we
want are quantum circuits that allow us to compute the values of the Boolean gates that I just mentioned when we give them qubits in
standard basis states as input. For arbitrary quantum state inputs, the action will be determined
by linearity as usual. We don't need to do anything
special with the NOT gates. NOT gates are NOT gates and they can be implemented directly. And OR gates can be simulated
using Toffoli and NOT gates. AND gates are easy, particularly in light of what I just said a few moments ago about Toffoli gates being essentially query gate implementations
of the AND function. To be precise, what we can do is to introduce a new qubit initialized to the zero state and let that qubit be the
target of our Toffoli gate and let the two input
qubits be the controls. Different names are given
to extra qubits like this, which are often introduced
into quantum computations and I'll call them workspace
qubits just to pick a name. Our typical assumption on workspace qubits is that they start out in the zero state and we can often return
them to the zero state when we're done with them, but not always. In the diagram we have a cat zero inside of the dotted rectangle to indicate that the
initialization of this qubit is part of the process, and if you don't have a
fresh qubit like this, you're not gonna be able
to do this simulation. The bottom output qubit
is the one we care about because that's the result
of the AND operation and we're basically
done with the other two so they can stay inside of the box. In fact, you could even imagine that getting rid of these two qubits is part of the job of
simulating this AND gate, kind of like garbage that
needs to be cleaned up. But anyway, that's how we
can simulate an AND gate. Now that we know how to do AND gates, we can do OR gates using
one of the De Morgan laws. If you put NOT gates on the two inputs in the output of an AND
gate, you'll get an OR gate, so it's as simple as that. And the last one is FANOUT, and for this one, all we
need is a controlled NOT gate along with a workspace qubit. Specifically, if we
set the workspace qubit to be the target, we effectively just copy whatever standard basis state comes in, so that acts like a FANOUT gate. So we have all our gates. And now that we have all of our gates, we can move on to circuits. Let's suppose that we have
some Boolean circuit named C, and this can be whatever
Boolean circuit you choose as long as it's composed of
AND or NOT and FANOUT gates. We'll let T be the number of gates in C and we'll give the name F to whatever function it is that C computes. So we can think of F as having the form that you see right here, where N is the number of input bits of C, M is the number of output bits and sigma is the binary alphabet. And what we do is to simply go one by one through the gates of C. We leave the NOT gates alone and we replace the others with the simulations
that I just described. So if C looks like this, then what we get is a circuit
R that looks like this. In this diagram, all
of the workspace qubits we need for the simulations have been pulled out of their boxes and collected on the bottom. So K is the total number of
workspace qubits we need, which is one for each of the gates of C except for the NOT gates. For the output of R, we have the qubits that
correspond to the output of C, and here those qubits
are depicted on the top, although it's not actually going to matter at the end of all of this
whether they're on top or not. To be precise, assuming that we've run R on a standard basis state corresponding to an
arbitrary N-bit string X, these M output qubits will be in the standard basis state
corresponding to F of X. We also have all of the leftover qubits from the AND and the OR gates. We still need to account for them, and if we gather them all together, they'll be in some standard basis state. Whatever the standard basis state is has to be some function of the string X, and we're gonna give
that function the name G. So G is a new function that's
determined by the gates of C that tells us what state
these leftover qubits are in after R is run. G is the next letter after F, so that's one reason to
call this function G, but another reason to call it G is because G is short for garbage. These are all garbage qubits, and in particular, we
have two garbage qubits for each AND gate and for each OR gate. The total number of gates
we need is big-O of T. That's because for each gate of C, we need a bounded number of quantum gates. The OR gate is the most expensive one. For that one, we need three NOT gates along with one Toffoli gate, and a Toffoli gate costs us 15 gates from our standard gate set. So that's at most 18 gates
in R for each gate of C. If you wanted to optimize this, you could, but for the purposes of a
rough asymptotic analysis, we can simply say that
R has big-O of T gates. So we've managed to
compute the same function F that the original Boolean
circuit C computed, and we've done this at a cost that's linear in the size of C. If all we care about is
computing this function F, then we're done. But unfortunately, we do
have these garbage qubits and these garbage qubits
are generally going to ruin the interference patterns that make quantum algorithms work if we try to use this
circuit R as a subroutine inside of a larger quantum algorithm. But we can in fact get
rid of this garbage, and the way that we can do
this is to make use of the fact that the circuit R can be inverted, or in other words, we
can run it in reverse. To illustrate how this works, let's imagine that we first run R and then we run R in reverse, which we can express as R dagger. To be clear about this, what it means is that we apply the gates
of R in the reverse order and we take the inverses
of each of the gates. NOT gates, controlled NOT
gates and Toffoli gates are actually their own inverses, so if we're thinking in
terms of these gates, then R dagger is really just R with the gates supplied
in the reverse order. But in general, we can always run any unitary quantum circuit in reverse in the manner that I just described, meaning that we apply the
gates in the reverse order and we replace each of the
individual gates by its inverse. If we were to do this, we'd find that first running R and then running R in
reverse does nothing at all. So if we started out in a standard basis state like we have here, then we would end in exactly the same standard basis state. Of course, doing just
that isn't very helpful, but what we can do is we can introduce a bunch of extra qubits and specifically, this is M qubits where M is the number of output bits of the circuit C. These additional qubits could
be in any quantum state, but to explain how this works, we're going to assume that they're in a standard basis state corresponding to some string Y. So Y is an M-bit string, and what we can do is to perform a bunch of controlled NOT
gates like you see right here, which effectively XORs the string F of X onto the string Y stored
in these bottom M qubits. And so the output of
the entire computation is as it appears on the screen. X and Y are arbitrary binary strings. X has M-bits and Y has M-bits, and we also need K workspace qubits. When we run R, we compute F of X and that string is stored
in the top M qubits, but we also have a
bunch of garbage qubits, so we XOR the output F of
X onto the bottom M qubits and then run R in reverse
to clean up the garbage. I should point out that it's important that R is deterministic. If these top M qubits could be instead in some superposition of
standard base of stats, this wouldn't work correctly, but this does work when
R is deterministic. So if we compare the quantum
circuit we just built to the original Boolean circuit C, we see that it basically
works in the same way that a query gate works
for an arbitrary function. There is a difference here, which is that we need
these workspace qubits to make it all work, but otherwise we've
effectively built a query gate or a query circuit for whatever function it is that C computes. We also managed, by the way, to return the workspace
qubits to their initial state, so they could in principle be reused as workspace qubits for
subsequent computations. If we want to, we can put a box around the entire quantum circuit and call it Q and the relationship to
C becomes more clear. The circuit Q will be
bigger than C in general. We need a handful of quantum
gates in Q for each gate of C, and we also need the controlled
NOT gates in the middle, but the number is still
linear in the size of C. It should also be noted that this is a completely general procedure which works for any Boolean circuit, and we haven't tried at all to optimize Q. For a specific circuit C, there might very well be
ways to reduce the size of Q, and that includes using methods that reduce the number of
workspace qubits needed in case that's something
that we want to do, and that is the procedure for running classical computations
on a quantum computer. It's pretty simple, but it's an important
thing to be able to do. In particular, now that
we know how to do this, we are free to make use of any classical computations we want inside of quantum computations, provided that we follow this form where the input gets echoed and the output gets XORed
onto a bunch of qubits just like we had for query gates. The cost scales linearly. So for example, we can
create quantum circuits for problems like integer addition, multiplication, division, computing GCDs and modular exponentiation, and this can all be done at the same asymptotic cost
as for Boolean circuits. We do need workspace qubits to do this, but at least they're available to be reused again when
we're done with them. One final remark that's worth mentioning is that this method provides us with a method to implement query gates, which I alluded to in the previous lesson. That is, if we have a query algorithm that takes some function F as input, and we have a way to compute
F with a Boolean circuit, then we can follow this procedure to build a query gate for that function and then run our query algorithm on it. Whether or not that's a useful thing to do depends on the query problem itself and what we're trying to accomplish, but in essence, this
construction provides us with a way of connecting the query model to a more standard model of computation. This is in some sense how Shor's algorithm for integer factorization works. We're going to build
quantum circuits like this out of some of the arithmetic operations we talked about earlier in the lesson. And by doing that, we
give quantum circuits the ability to evaluate those operations on super positions of classical states. And as we will see, that allows us to factor integers efficiently. That's the end of this lesson, which has focused mainly on establishing a foundation
for quantum algorithms in a standard non-black box
computational framework. We talked about computational cost mainly through the lens of problems connected with basic number theory. We discussed polynomial
versus exponential cost for computational tasks, and we saw how classical computations can be implemented by quantum circuits in a clean garbage-free manner that allows them to be used as subroutines inside of larger quantum computations. I hope you'll join me for the next lesson where we'll see how Shor's algorithm puts these ideas to use, allowing us to factorize
integers at polynomial cost. Goodbye until then.

## Quantum Channels ｜ Understanding Quantum Information & Computation ｜ Lesson 10

welcome back to understanding Quantum welcome back to understanding Quantum
information and computation my name is information and computation my name is information and computation my name is
John watus and I'm the technical John watus and I'm the technical John watus and I'm the technical
director for education at IBM director for education at IBM director for education at IBM
Quantum this is the 10th lesson of the Quantum this is the 10th lesson of the Quantum this is the 10th lesson of the
series and it's the second lesson in the series and it's the second lesson in the series and it's the second lesson in the
third unit which is on the general third unit which is on the general third unit which is on the general
formulation of quantum formulation of quantum formulation of quantum
information this lesson is about information this lesson is about information this lesson is about
channels which represent changes in channels which represent changes in channels which represent changes in
systems that store Quantum systems that store Quantum systems that store Quantum
information this includes useful information this includes useful information this includes useful
operations like ones performed by operations like ones performed by operations like ones performed by
unitary Gates and Quantum circuits as unitary Gates and Quantum circuits as unitary Gates and Quantum circuits as
well as changes that we might prefer to well as changes that we might prefer to well as changes that we might prefer to
avoid like avoid like avoid like
noise we can also describe measurements noise we can also describe measurements noise we can also describe measurements
as channels and that's something that as channels and that's something that as channels and that's something that
we'll get into in the next we'll get into in the next we'll get into in the next
lesson the term Channel comes to us from lesson the term Channel comes to us from lesson the term Channel comes to us from
information Theory which among other information Theory which among other information Theory which among other
things studies the information carrying things studies the information carrying things studies the information carrying
capacity of noisy communication capacity of noisy communication capacity of noisy communication
channels so when we think about channels channels so when we think about channels channels so when we think about channels
we're thinking abstractly about we're thinking abstractly about we're thinking abstractly about
communication communication communication
channels it might seem kind of odd to channels it might seem kind of odd to channels it might seem kind of odd to
think about a complicated computation as think about a complicated computation as think about a complicated computation as
a communication Channel but we need a a communication Channel but we need a a communication Channel but we need a
word to refer to the general notion of a word to refer to the general notion of a word to refer to the general notion of a
process that changes information in some process that changes information in some process that changes information in some
way and For Better or Worse the word way and For Better or Worse the word way and For Better or Worse the word
channel has stuck no doubt due in large channel has stuck no doubt due in large channel has stuck no doubt due in large
part to the enormous impact that part to the enormous impact that part to the enormous impact that
information Theory and in particular the information Theory and in particular the information Theory and in particular the
work of Claude Shannon in the 1940s has work of Claude Shannon in the 1940s has work of Claude Shannon in the 1940s has
had on Quantum information and had on Quantum information and had on Quantum information and
computation here is an overview of the computation here is an overview of the computation here is an overview of the
lesson we'll start with the basics lesson we'll start with the basics lesson we'll start with the basics
including the definition of what a including the definition of what a including the definition of what a
channel is as well as how unitary channel is as well as how unitary channel is as well as how unitary
operations in the simplified formulation operations in the simplified formulation operations in the simplified formulation
of quantum information can be described of quantum information can be described of quantum information can be described
as channels we'll also talk about the as channels we'll also talk about the as channels we'll also talk about the
fact that convex combinations of fact that convex combinations of fact that convex combinations of
channels are channels and if you're channels are channels and if you're channels are channels and if you're
wondering why this notion of convexity wondering why this notion of convexity wondering why this notion of convexity
keeps coming up it's because it really keeps coming up it's because it really keeps coming up it's because it really
is a fundamentally important is a fundamentally important is a fundamentally important
mathematical notion in the theory of mathematical notion in the theory of mathematical notion in the theory of
quantum information we'll also see some quantum information we'll also see some quantum information we'll also see some
examples of channels specifically Cubit examples of channels specifically Cubit examples of channels specifically Cubit
channels which will reappear from time channels which will reappear from time channels which will reappear from time
to time throughout the remainder of the to time throughout the remainder of the to time throughout the remainder of the
lesson in the second part of the lesson lesson in the second part of the lesson lesson in the second part of the lesson
we'll talk about three different ways we'll talk about three different ways we'll talk about three different ways
that we can represent channels that we can represent channels that we can represent channels
mathematically and in particular we'll mathematically and in particular we'll mathematically and in particular we'll
talk about Stein spring representations talk about Stein spring representations talk about Stein spring representations
Krauss representations and Choy Krauss representations and Choy Krauss representations and Choy
representations which are named after representations which are named after representations which are named after
three people whose work was important in three people whose work was important in three people whose work was important in
the development of these different the development of these different the development of these different
representations and in the third and representations and in the third and representations and in the third and
final part of the lesson we'll discuss a final part of the lesson we'll discuss a final part of the lesson we'll discuss a
mathematical proof that establishes that mathematical proof that establishes that mathematical proof that establishes that
these three representations are in fact these three representations are in fact these three representations are in fact
equivalent as well as the fact that they equivalent as well as the fact that they equivalent as well as the fact that they
all correctly characterize the all correctly characterize the all correctly characterize the
definition of what a channel is it's a definition of what a channel is it's a definition of what a channel is it's a
really cool mathematical proof in my really cool mathematical proof in my really cool mathematical proof in my
opinion and hopefully it will add some opinion and hopefully it will add some opinion and hopefully it will add some
clarity to these different presentations clarity to these different presentations clarity to these different presentations
how they relate and how they work in how they relate and how they work in how they relate and how they work in
general we'll start with the basics and general we'll start with the basics and general we'll start with the basics and
address the question what are channels address the question what are channels address the question what are channels
as I already suggested at the start of as I already suggested at the start of as I already suggested at the start of
the lesson channels describe changes the lesson channels describe changes the lesson channels describe changes
specifically discreete time changes in specifically discreete time changes in specifically discreete time changes in
systems that store Quantum information systems that store Quantum information systems that store Quantum information
here when we refer to a discrete time here when we refer to a discrete time here when we refer to a discrete time
change we're just thinking about a change we're just thinking about a change we're just thinking about a
change that occurs between two distinct change that occurs between two distinct change that occurs between two distinct
moments in time as opposed to a moments in time as opposed to a moments in time as opposed to a
continuous process for instance that continuous process for instance that continuous process for instance that
makes sense to think about over makes sense to think about over makes sense to think about over
infinitesimally small time time periods infinitesimally small time time periods infinitesimally small time time periods
channels include useful operations like channels include useful operations like channels include useful operations like
unitary operations that are performed by unitary operations that are performed by unitary operations that are performed by
Quantum Gates and circuits as well as Quantum Gates and circuits as well as Quantum Gates and circuits as well as
changes due to noise for instance for changes due to noise for instance for changes due to noise for instance for
the purposes of this lesson though we're the purposes of this lesson though we're the purposes of this lesson though we're
mostly going to be thinking about mostly going to be thinking about mostly going to be thinking about
channels in abstract mathematical terms channels in abstract mathematical terms channels in abstract mathematical terms
and there won't be any particular need and there won't be any particular need and there won't be any particular need
to distinguish between what's useful and to distinguish between what's useful and to distinguish between what's useful and
what's not typical names for channels what's not typical names for channels what's not typical names for channels
include the uppercase Greek letters fi include the uppercase Greek letters fi include the uppercase Greek letters fi
Sai andai and we'll use different Sai andai and we'll use different Sai andai and we'll use different
letters to refer to some specific letters to refer to some specific letters to refer to some specific
channels in a simple situation in which channels in a simple situation in which channels in a simple situation in which
we have some system that's in a state we have some system that's in a state we have some system that's in a state
described by some density Matrix row and described by some density Matrix row and described by some density Matrix row and
we apply a channel f to that system the we apply a channel f to that system the we apply a channel f to that system the
result is a system whose state is result is a system whose state is result is a system whose state is
described by the density Matrix fi of described by the density Matrix fi of described by the density Matrix fi of
row so we're thinking about a given row so we're thinking about a given row so we're thinking about a given
Channel fi as being a function that can Channel fi as being a function that can Channel fi as being a function that can
be applied to density matrices of a be applied to density matrices of a be applied to density matrices of a
particular size or in other words particular size or in other words particular size or in other words
density matrices of a particular system density matrices of a particular system density matrices of a particular system
that we're applying this channel to that we're applying this channel to that we're applying this channel to
there are a couple of requirements that there are a couple of requirements that there are a couple of requirements that
channels must satisfy in order to be channels must satisfy in order to be channels must satisfy in order to be
considered valid channels First channels considered valid channels First channels considered valid channels First channels
are linear mappings this is completely are linear mappings this is completely are linear mappings this is completely
analogous to the situation we have both analogous to the situation we have both analogous to the situation we have both
in the simplified formulation of quantum in the simplified formulation of quantum in the simplified formulation of quantum
information where operations on Quantum information where operations on Quantum information where operations on Quantum
State vectors are necessarily linear as State vectors are necessarily linear as State vectors are necessarily linear as
well as in the standard formulation of well as in the standard formulation of well as in the standard formulation of
classical information where classical information where classical information where
probabilistic operations are linear you probabilistic operations are linear you probabilistic operations are linear you
could choose to question this basic could choose to question this basic could choose to question this basic
requirement of linearity if you really requirement of linearity if you really requirement of linearity if you really
wanted to but if we want a theory that's wanted to but if we want a theory that's wanted to but if we want a theory that's
consistent with both probability Theory consistent with both probability Theory consistent with both probability Theory
and the choice to represent Quantum and the choice to represent Quantum and the choice to represent Quantum
states with density matrices then we states with density matrices then we states with density matrices then we
don't really have any choice and we just don't really have any choice and we just don't really have any choice and we just
have to accept that channels are linear have to accept that channels are linear have to accept that channels are linear
the second requirement is that channels the second requirement is that channels the second requirement is that channels
transform density matrices into density transform density matrices into density transform density matrices into density
matrices that makes perfect sense matrices that makes perfect sense matrices that makes perfect sense
because if we've chosen to represent because if we've chosen to represent because if we've chosen to represent
states of systems with density matrices states of systems with density matrices states of systems with density matrices
and we apply a channel to a system and we apply a channel to a system and we apply a channel to a system
that's in some State then the result has that's in some State then the result has that's in some State then the result has
to be a valid State and therefore to be a valid State and therefore to be a valid State and therefore
represented by a density Matrix there's represented by a density Matrix there's represented by a density Matrix there's
a very important point to add here a very important point to add here a very important point to add here
though and that is that this requirement though and that is that this requirement though and that is that this requirement
has to be true not only if we apply a has to be true not only if we apply a has to be true not only if we apply a
channel to an isolated system whose channel to an isolated system whose channel to an isolated system whose
state is described by a density Matrix state is described by a density Matrix state is described by a density Matrix
but also if we have a compound system but also if we have a compound system but also if we have a compound system
whose state is described by some density whose state is described by some density whose state is described by some density
Matrix and we apply the channel to just Matrix and we apply the channel to just Matrix and we apply the channel to just
part of that system so those are the part of that system so those are the part of that system so those are the
requirements that we place on channels requirements that we place on channels requirements that we place on channels
and we can see that they are very much and we can see that they are very much and we can see that they are very much
analogous to the requirements that we analogous to the requirements that we analogous to the requirements that we
have for operations in both the have for operations in both the have for operations in both the
simplified formulation of quantum simplified formulation of quantum simplified formulation of quantum
information and the standard formulation information and the standard formulation information and the standard formulation
of classical information in the of classical information in the of classical information in the
simplified formulation of quantum simplified formulation of quantum simplified formulation of quantum
information operations are represented information operations are represented information operations are represented
by unitary matrices because they by unitary matrices because they by unitary matrices because they
correspond precisely to the linear Maps correspond precisely to the linear Maps correspond precisely to the linear Maps
that always transform Quantum State that always transform Quantum State that always transform Quantum State
vectors into Quantum State vectors for a vectors into Quantum State vectors for a vectors into Quantum State vectors for a
given system and similarly probabilistic given system and similarly probabilistic given system and similarly probabilistic
operations are represented by stochastic operations are represented by stochastic operations are represented by stochastic
matrices because those are the ones that matrices because those are the ones that matrices because those are the ones that
always transform probability vectors always transform probability vectors always transform probability vectors
into probability vectors here we're into probability vectors here we're into probability vectors here we're
working with density matrices and there working with density matrices and there working with density matrices and there
is this added requirement concerning is this added requirement concerning is this added requirement concerning
compound systems that turns out not to compound systems that turns out not to compound systems that turns out not to
be necessary for Quantum State vectors be necessary for Quantum State vectors be necessary for Quantum State vectors
or probability vectors but otherwise the or probability vectors but otherwise the or probability vectors but otherwise the
requirements are analogous and in some requirements are analogous and in some requirements are analogous and in some
sense our forced Upon Us by the choice sense our forced Upon Us by the choice sense our forced Upon Us by the choice
to represent Quantum states by density to represent Quantum states by density to represent Quantum states by density
matrices here are some further details matrices here are some further details matrices here are some further details
about channels including a convention about channels including a convention about channels including a convention
for how we'll think about the input and for how we'll think about the input and for how we'll think about the input and
output systems of a channel throughout output systems of a channel throughout output systems of a channel throughout
the lesson as well as some more detail the lesson as well as some more detail the lesson as well as some more detail
about the requirement that channels about the requirement that channels about the requirement that channels
always transform density matrices into always transform density matrices into always transform density matrices into
density matrices first every channel is density matrices first every channel is density matrices first every channel is
assumed to have an input system and an assumed to have an input system and an assumed to have an input system and an
output system we can name these systems output system we can name these systems output system we can name these systems
however we choose but throughout this however we choose but throughout this however we choose but throughout this
lesson I'm going to use the name X to lesson I'm going to use the name X to lesson I'm going to use the name X to
refer to the input system and the name y refer to the input system and the name y refer to the input system and the name y
to refer to the output system to refer to the output system to refer to the output system
conceptually speaking the channel conceptually speaking the channel conceptually speaking the channel
transforms the input system into the transforms the input system into the transforms the input system into the
output system so the input and output output system so the input and output output system so the input and output
systems never simultaneously coexist we systems never simultaneously coexist we systems never simultaneously coexist we
start with x the channel is applied or start with x the channel is applied or start with x the channel is applied or
performed and the result is that X has performed and the result is that X has performed and the result is that X has
been transformed into y whose state is been transformed into y whose state is been transformed into y whose state is
determined by the linear mapping determined by the linear mapping determined by the linear mapping
associated with the channel the input associated with the channel the input associated with the channel the input
and output systems for a channel don't and output systems for a channel don't and output systems for a channel don't
need to be the same and in particular need to be the same and in particular need to be the same and in particular
they don't need to have the same number they don't need to have the same number they don't need to have the same number
of classical States however it can be of classical States however it can be of classical States however it can be
that the input and the output systems that the input and the output systems that the input and the output systems
are actually the same system or in other are actually the same system or in other are actually the same system or in other
words x equals y in which case we're words x equals y in which case we're words x equals y in which case we're
simply imagining that fi is changing the simply imagining that fi is changing the simply imagining that fi is changing the
state of the system and indeed this is state of the system and indeed this is state of the system and indeed this is
very often the situation that we're very often the situation that we're very often the situation that we're
interested in but this possibility is interested in but this possibility is interested in but this possibility is
consistent with the first bullet point consistent with the first bullet point consistent with the first bullet point
in the sense that we never have the in the sense that we never have the in the sense that we never have the
input and output systems existing as two input and output systems existing as two input and output systems existing as two
separate systems at the same time and in separate systems at the same time and in separate systems at the same time and in
any case it's going to be convenient for any case it's going to be convenient for any case it's going to be convenient for
the purposes of this lesson to use the purposes of this lesson to use the purposes of this lesson to use
different letters namely X and Y to different letters namely X and Y to different letters namely X and Y to
refer to the input and output systems refer to the input and output systems refer to the input and output systems
just to help us to keep things straight just to help us to keep things straight just to help us to keep things straight
now let's go into a little bit more now let's go into a little bit more now let's go into a little bit more
detail about the requirement that detail about the requirement that detail about the requirement that
channels always transform density channels always transform density channels always transform density
matrices into density matrices into density matrices into density
matrices let's suppose that we introduce matrices let's suppose that we introduce matrices let's suppose that we introduce
a new system Z into the picture let's a new system Z into the picture let's a new system Z into the picture let's
give the name gamma to the classical give the name gamma to the classical give the name gamma to the classical
State set of this new system imagine now State set of this new system imagine now State set of this new system imagine now
that we form a new compound system ZX that we form a new compound system ZX that we form a new compound system ZX
consisting of the new system together consisting of the new system together consisting of the new system together
with the input system X of a channel fi with the input system X of a channel fi with the input system X of a channel fi
and suppose that this compound system is and suppose that this compound system is and suppose that this compound system is
in a state described by a density Matrix in a state described by a density Matrix in a state described by a density Matrix
row this can be an arbitrary Quantum row this can be an arbitrary Quantum row this can be an arbitrary Quantum
state of this pair but whatever state it state of this pair but whatever state it state of this pair but whatever state it
is it's always possible to express the is it's always possible to express the is it's always possible to express the
density Matrix Row in the form that's density Matrix Row in the form that's density Matrix Row in the form that's
shown on the screen where both A and B shown on the screen where both A and B shown on the screen where both A and B
range over the classical State set gamma range over the classical State set gamma range over the classical State set gamma
and each row AB is some Matrix whose and each row AB is some Matrix whose and each row AB is some Matrix whose
rows and columns correspond to whatever rows and columns correspond to whatever rows and columns correspond to whatever
the classical State set of X is and we the classical State set of X is and we the classical State set of X is and we
don't need to give that classical State don't need to give that classical State don't need to give that classical State
set a name at this point just to be set a name at this point just to be set a name at this point just to be
clear each row AB is not necessarily a clear each row AB is not necessarily a clear each row AB is not necessarily a
density Matrix on its own but the entire density Matrix on its own but the entire density Matrix on its own but the entire
Matrix row is necessarily a density Matrix row is necessarily a density Matrix row is necessarily a density
Matrix when we apply the channel 5 to Matrix when we apply the channel 5 to Matrix when we apply the channel 5 to
the system X it gets transformed into Y the system X it gets transformed into Y the system X it gets transformed into Y
and so we're left with the pair and so we're left with the pair and so we're left with the pair
zy and the way that we determine the zy and the way that we determine the zy and the way that we determine the
state of this pair after performing fi state of this pair after performing fi state of this pair after performing fi
on just X is to evaluate the linear on just X is to evaluate the linear on just X is to evaluate the linear
mapping fi on each of the matrices row a mapping fi on each of the matrices row a mapping fi on each of the matrices row a
another way to say this is that we take another way to say this is that we take another way to say this is that we take
the tensor product of the mapping fi the tensor product of the mapping fi the tensor product of the mapping fi
with the identity mapping on Square with the identity mapping on Square with the identity mapping on Square
matrices having rows and columns matrices having rows and columns matrices having rows and columns
corresponding to gamma and that in turn corresponding to gamma and that in turn corresponding to gamma and that in turn
corresponds to doing nothing to this new corresponds to doing nothing to this new corresponds to doing nothing to this new
system Z but this is an equivalent system Z but this is an equivalent system Z but this is an equivalent
description and it doesn't necessitate description and it doesn't necessitate description and it doesn't necessitate
going into any details about tensor going into any details about tensor going into any details about tensor
products of mappings of this form they products of mappings of this form they products of mappings of this form they
do however work in a pretty do however work in a pretty do however work in a pretty
straightforward way that's analogous to straightforward way that's analogous to straightforward way that's analogous to
how they work for vectors and matrices how they work for vectors and matrices how they work for vectors and matrices
one way to think about this visually is one way to think about this visually is one way to think about this visually is
in terms of block matrices and if we in terms of block matrices and if we in terms of block matrices and if we
make the simplifying assumption that the make the simplifying assumption that the make the simplifying assumption that the
classical stat set gamma consists of the classical stat set gamma consists of the classical stat set gamma consists of the
integers between 0 and N minus one for integers between 0 and N minus one for integers between 0 and N minus one for
some positive integer M then it becomes some positive integer M then it becomes some positive integer M then it becomes
easy to write all of this down as is easy to write all of this down as is easy to write all of this down as is
shown right here and now to reiterate shown right here and now to reiterate shown right here and now to reiterate
the requirement that we're placing on the requirement that we're placing on the requirement that we're placing on
five in order for it to be a valid five in order for it to be a valid five in order for it to be a valid
Channel we must always obtain a density Channel we must always obtain a density Channel we must always obtain a density
Matrix from this process and this has to Matrix from this process and this has to Matrix from this process and this has to
be true regardless of what choice we be true regardless of what choice we be true regardless of what choice we
make for the system Z as well as the make for the system Z as well as the make for the system Z as well as the
choice we make for row provided that choice we make for row provided that choice we make for row provided that
it's a density Matrix that might seem it's a density Matrix that might seem it's a density Matrix that might seem
like a pretty strict requirement and it like a pretty strict requirement and it like a pretty strict requirement and it
might also seem like it's very difficult might also seem like it's very difficult might also seem like it's very difficult
to check but as we'll see by the end of to check but as we'll see by the end of to check but as we'll see by the end of
the lesson it's actually not so strict the lesson it's actually not so strict the lesson it's actually not so strict
or difficult to check and in particular or difficult to check and in particular or difficult to check and in particular
we have nice mathematical we have nice mathematical we have nice mathematical
characterizations for this condition characterizations for this condition characterizations for this condition
which are connected with the different which are connected with the different which are connected with the different
representations that we'll talk about a representations that we'll talk about a representations that we'll talk about a
bit later in the lesson now let's take a bit later in the lesson now let's take a bit later in the lesson now let's take a
look at some examples of channels look at some examples of channels look at some examples of channels
starting with an entire category of starting with an entire category of starting with an entire category of
channels called unitary channels in channels called unitary channels in channels called unitary channels in
simple terms these are unitary simple terms these are unitary simple terms these are unitary
operations just like we have in the operations just like we have in the operations just like we have in the
simplified formulation of quantum simplified formulation of quantum simplified formulation of quantum
information viewed as channels to be information viewed as channels to be information viewed as channels to be
precise suppose that U is a unitary precise suppose that U is a unitary precise suppose that U is a unitary
Matrix that represents a unitary Matrix that represents a unitary Matrix that represents a unitary
operation on a system X so in the operation on a system X so in the operation on a system X so in the
simplified formulation of quantum simplified formulation of quantum simplified formulation of quantum
information we would multiply this information we would multiply this information we would multiply this
Matrix to a given Quantum State Vector Matrix to a given Quantum State Vector Matrix to a given Quantum State Vector
to determine the action that that to determine the action that that to determine the action that that
operation has on that Quantum State the operation has on that Quantum State the operation has on that Quantum State the
way that this works for density matrices way that this works for density matrices way that this works for density matrices
is that we multiply the density Matrix is that we multiply the density Matrix is that we multiply the density Matrix
on the left by U and on the right by U on the left by U and on the right by U on the left by U and on the right by U
dagger in mathematical terms we refer to dagger in mathematical terms we refer to dagger in mathematical terms we refer to
this operation as conjugation by The this operation as conjugation by The this operation as conjugation by The
Matrix U this is consistent with KSI Matrix U this is consistent with KSI Matrix U this is consistent with KSI
brasai being the density Matrix that brasai being the density Matrix that brasai being the density Matrix that
describes the state represented by the describes the state represented by the describes the state represented by the
quantum State Vector s and it's pretty quantum State Vector s and it's pretty quantum State Vector s and it's pretty
easy to check that in particular if we easy to check that in particular if we easy to check that in particular if we
were to perform U on the quantum State were to perform U on the quantum State were to perform U on the quantum State
Vector Ki then we'd get U * Ki and its Vector Ki then we'd get U * Ki and its Vector Ki then we'd get U * Ki and its
density Matrix representation is given density Matrix representation is given density Matrix representation is given
by that Vector times its conjugate by that Vector times its conjugate by that Vector times its conjugate
transpose which is U * Ki * brasai * U transpose which is U * Ki * brasai * U transpose which is U * Ki * brasai * U
dagger when we evaluate the conjugate dagger when we evaluate the conjugate dagger when we evaluate the conjugate
transpose of U * Ki so transpose of U * Ki so transpose of U * Ki so
the way that this channel is defined the way that this channel is defined the way that this channel is defined
makes sense for Pure States and when we makes sense for Pure States and when we makes sense for Pure States and when we
combine this with linearity we see that combine this with linearity we see that combine this with linearity we see that
it makes sense for density matrices in it makes sense for density matrices in it makes sense for density matrices in
general owing to the fact that every general owing to the fact that every general owing to the fact that every
density Matrix can be expressed as a density Matrix can be expressed as a density Matrix can be expressed as a
convex combination of pure States this convex combination of pure States this convex combination of pure States this
definition always gives us a valid definition always gives us a valid definition always gives us a valid
channel for any choice of a unitary channel for any choice of a unitary channel for any choice of a unitary
operation u in particular conjugating by operation u in particular conjugating by operation u in particular conjugating by
a unitary Matrix like this doesn't a unitary Matrix like this doesn't a unitary Matrix like this doesn't
change the trace of a matrix and if we change the trace of a matrix and if we change the trace of a matrix and if we
conjugate any positive semi-definite conjugate any positive semi-definite conjugate any positive semi-definite
matrix by any Matrix at all we'll get matrix by any Matrix at all we'll get matrix by any Matrix at all we'll get
another positive semi-definite Matrix another positive semi-definite Matrix another positive semi-definite Matrix
and it's pretty easy to get that and it's pretty easy to get that and it's pretty easy to get that
directly from the definition of positive directly from the definition of positive directly from the definition of positive
semi-definite matrices so density semi-definite matrices so density semi-definite matrices so density
matrices always get mapped to density matrices always get mapped to density matrices always get mapped to density
matrices and all of this remains true matrices and all of this remains true matrices and all of this remains true
even if we tensor you with the identity even if we tensor you with the identity even if we tensor you with the identity
Matrix of any size which is equivalent Matrix of any size which is equivalent Matrix of any size which is equivalent
to applying this channel to just part of to applying this channel to just part of to applying this channel to just part of
a compound system here's a really simple a compound system here's a really simple a compound system here's a really simple
example of a unitary Channel suppose we example of a unitary Channel suppose we example of a unitary Channel suppose we
take U itself to be the identity Matrix take U itself to be the identity Matrix take U itself to be the identity Matrix
this is very obviously a valid Channel this is very obviously a valid Channel this is very obviously a valid Channel
called the identity Channel doing called the identity Channel doing called the identity Channel doing
nothing at all is certainly a linear nothing at all is certainly a linear nothing at all is certainly a linear
operation and if we start with a density operation and if we start with a density operation and if we start with a density
Matrix and we do nothing to it it's Matrix and we do nothing to it it's Matrix and we do nothing to it it's
still going to be a density Matrix so still going to be a density Matrix so still going to be a density Matrix so
this is a valid Channel people use this is a valid Channel people use this is a valid Channel people use
different symbols to denote this channel different symbols to denote this channel different symbols to denote this channel
sometimes it's exactly the same symbol sometimes it's exactly the same symbol sometimes it's exactly the same symbol
we use for the identity Matrix which can we use for the identity Matrix which can we use for the identity Matrix which can
be a little bit confusing but it be a little bit confusing but it be a little bit confusing but it
generally isn't here I'm using the generally isn't here I'm using the generally isn't here I'm using the
letters ID short for identity just to letters ID short for identity just to letters ID short for identity just to
avoid any confusion and this notation is avoid any confusion and this notation is avoid any confusion and this notation is
also pretty common it might seem like a also pretty common it might seem like a also pretty common it might seem like a
very boring Channel but in the context very boring Channel but in the context very boring Channel but in the context
of information transmission it's of information transmission it's of information transmission it's
actually a perfect Channel where the actually a perfect Channel where the actually a perfect Channel where the
receiver gets exactly what the sender receiver gets exactly what the sender receiver gets exactly what the sender
sends which is typically what one wants sends which is typically what one wants sends which is typically what one wants
most in that context another way we can most in that context another way we can most in that context another way we can
obtain lots of examples of channels is obtain lots of examples of channels is obtain lots of examples of channels is
to take convex combinations of channels to take convex combinations of channels to take convex combinations of channels
or in other words to average them or in other words to average them or in other words to average them
together and the fact that we can do together and the fact that we can do together and the fact that we can do
this is an important aspect of channels this is an important aspect of channels this is an important aspect of channels
to explain this in more detail let's to explain this in more detail let's to explain this in more detail let's
start with two channels F0 and 51 let's start with two channels F0 and 51 let's start with two channels F0 and 51 let's
say along with a real number P between 0 say along with a real number P between 0 say along with a real number P between 0
and one and just to be clear we're and one and just to be clear we're and one and just to be clear we're
assuming that these two channels have assuming that these two channels have assuming that these two channels have
the same input system which is X as well the same input system which is X as well the same input system which is X as well
as the same output system which is y and as the same output system which is y and as the same output system which is y and
now consider what happens if we decide now consider what happens if we decide now consider what happens if we decide
to apply the first channel F0 with to apply the first channel F0 with to apply the first channel F0 with
probability p and otherwise with the probability p and otherwise with the probability p and otherwise with the
remaining probability 1 minus P we apply remaining probability 1 minus P we apply remaining probability 1 minus P we apply
the second Channel F1 for instance maybe the second Channel F1 for instance maybe the second Channel F1 for instance maybe
we randomly choose a bit with those we randomly choose a bit with those we randomly choose a bit with those
probabilities and then use this random probabilities and then use this random probabilities and then use this random
bit to determine which channel to apply bit to determine which channel to apply bit to determine which channel to apply
by doing this we're effectively applying by doing this we're effectively applying by doing this we're effectively applying
a new Channel which we might call sigh a new Channel which we might call sigh a new Channel which we might call sigh
that's given by a convex combination of that's given by a convex combination of that's given by a convex combination of
the two channels that we started with the two channels that we started with the two channels that we started with
and which we can express as is written and which we can express as is written and which we can express as is written
on the screen we can always take linear on the screen we can always take linear on the screen we can always take linear
combinations of linear Maps like this by combinations of linear Maps like this by combinations of linear Maps like this by
the way and the way that a linear the way and the way that a linear the way and the way that a linear
combination of linear Maps is defined on combination of linear Maps is defined on combination of linear Maps is defined on
a given input is that we just take the a given input is that we just take the a given input is that we just take the
same linear combination of the outputs same linear combination of the outputs same linear combination of the outputs
that might look sort of trivial but it that might look sort of trivial but it that might look sort of trivial but it
makes sense the output we get from this makes sense the output we get from this makes sense the output we get from this
hypothetical scenario is simply the hypothetical scenario is simply the hypothetical scenario is simply the
weighted average of the outputs we would weighted average of the outputs we would weighted average of the outputs we would
get from the two original channels get from the two original channels get from the two original channels
nevertheless this is a new channel and nevertheless this is a new channel and nevertheless this is a new channel and
if ph0 and 51 were indeed valid channels if ph0 and 51 were indeed valid channels if ph0 and 51 were indeed valid channels
then for sure this new mapping will also then for sure this new mapping will also then for sure this new mapping will also
be a valid Channel another way to say be a valid Channel another way to say be a valid Channel another way to say
that in mathematical terms is that the that in mathematical terms is that the that in mathematical terms is that the
set of all channels from a given input set of all channels from a given input set of all channels from a given input
system to a given output system is system to a given output system is system to a given output system is
convex and that's an important convex and that's an important convex and that's an important
mathematical property of channels we can mathematical property of channels we can mathematical property of channels we can
do this more generally if we have M do this more generally if we have M do this more generally if we have M
channels 5 0 through f m minus one say channels 5 0 through f m minus one say channels 5 0 through f m minus one say
along with the probability vector that along with the probability vector that along with the probability vector that
tells us what the probability is for tells us what the probability is for tells us what the probability is for
each Channel then we get a new channel each Channel then we get a new channel each Channel then we get a new channel
by averaging the channels accordingly so by averaging the channels accordingly so by averaging the channels accordingly so
in some sense this is an endless source in some sense this is an endless source in some sense this is an endless source
of new channels for example the class of of new channels for example the class of of new channels for example the class of
so-called mixed unitary channels is what so-called mixed unitary channels is what so-called mixed unitary channels is what
we get by taking convex combinations of we get by taking convex combinations of we get by taking convex combinations of
unitary channels that is if we have M unitary channels that is if we have M unitary channels that is if we have M
unitary matrices u0 through u m minus unitary matrices u0 through u m minus unitary matrices u0 through u m minus
one all having the same size along with one all having the same size along with one all having the same size along with
the probability Vector then if we take the probability Vector then if we take the probability Vector then if we take
the weighted average of the channels the weighted average of the channels the weighted average of the channels
corresponding to the individual unit corresponding to the individual unit corresponding to the individual unit
matrices as is shown here on the screen matrices as is shown here on the screen matrices as is shown here on the screen
then we'll get a channel and channels then we'll get a channel and channels then we'll get a channel and channels
that can be written like this are called that can be written like this are called that can be written like this are called
mixed unary channels not every channel mixed unary channels not every channel mixed unary channels not every channel
is a mixed unitary Channel we can't get is a mixed unitary Channel we can't get is a mixed unitary Channel we can't get
every channel this way but they do come every channel this way but they do come every channel this way but they do come
up quite a up quite a up quite a
lot next we'll look at a few specific lot next we'll look at a few specific lot next we'll look at a few specific
examples of channels these will all be examples of channels these will all be examples of channels these will all be
examples of channels from one cubit to examples of channels from one cubit to examples of channels from one cubit to
one cubit which are called Cubit one cubit which are called Cubit one cubit which are called Cubit
channels for short the first one is channels for short the first one is channels for short the first one is
called the Cubit reset Channel and the called the Cubit reset Channel and the called the Cubit reset Channel and the
idea is that this channel resets a cubit idea is that this channel resets a cubit idea is that this channel resets a cubit
to the zero State here we're using the to the zero State here we're using the to the zero State here we're using the
capital Greek letter Lambda to denote capital Greek letter Lambda to denote capital Greek letter Lambda to denote
this Channel and the way that you can this Channel and the way that you can this Channel and the way that you can
remember what this channel does is to remember what this channel does is to remember what this channel does is to
think about the block sphere Capital think about the block sphere Capital think about the block sphere Capital
Lambda kind of looks like an arrow Lambda kind of looks like an arrow Lambda kind of looks like an arrow
pointing up and the zero state is at the pointing up and the zero state is at the pointing up and the zero state is at the
top of the block sphere so it's as if top of the block sphere so it's as if top of the block sphere so it's as if
it's pointing to the location of the it's pointing to the location of the it's pointing to the location of the
output the way it's defined in output the way it's defined in output the way it's defined in
mathematical terms is that Lambda of row mathematical terms is that Lambda of row mathematical terms is that Lambda of row
equals the trace of row time c0 bra 0 so equals the trace of row time c0 bra 0 so equals the trace of row time c0 bra 0 so
if we evaluate it on any density me if we evaluate it on any density me if we evaluate it on any density me
Matrix row we'll get the zero State as a Matrix row we'll get the zero State as a Matrix row we'll get the zero State as a
density matrix it's natural to wonder density matrix it's natural to wonder density matrix it's natural to wonder
why we need the trace couldn't we simply why we need the trace couldn't we simply why we need the trace couldn't we simply
Define Lambda of row to be cat zero bra Define Lambda of row to be cat zero bra Define Lambda of row to be cat zero bra
zero without the trace of row in front zero without the trace of row in front zero without the trace of row in front
and the answer to that question is no and the answer to that question is no and the answer to that question is no
channels need to be linear maps and it channels need to be linear maps and it channels need to be linear maps and it
wouldn't be linear if we removed the wouldn't be linear if we removed the wouldn't be linear if we removed the
trace of row also keep in mind that trace of row also keep in mind that trace of row also keep in mind that
sometimes we need to know how a channel sometimes we need to know how a channel sometimes we need to know how a channel
works as a linear mapping on non- works as a linear mapping on non- works as a linear mapping on non-
density Matrix inputs for example as we density Matrix inputs for example as we density Matrix inputs for example as we
saw earlier if we want to know what a saw earlier if we want to know what a saw earlier if we want to know what a
channel does when we apply it to a channel does when we apply it to a channel does when we apply it to a
compound system system we need to compound system system we need to compound system system we need to
evaluate that channel on the individual evaluate that channel on the individual evaluate that channel on the individual
blocks of a larger block Matrix and blocks of a larger block Matrix and blocks of a larger block Matrix and
although the entire block Matrix may be although the entire block Matrix may be although the entire block Matrix may be
a density Matrix the individual blocks a density Matrix the individual blocks a density Matrix the individual blocks
generally won't be so let's see exactly generally won't be so let's see exactly generally won't be so let's see exactly
what this channel does when we apply it what this channel does when we apply it what this channel does when we apply it
to just one part of a compound system to just one part of a compound system to just one part of a compound system
and more specifically what it does when and more specifically what it does when and more specifically what it does when
we apply it to one of two cubits that we apply it to one of two cubits that we apply it to one of two cubits that
form an ebit to be precise suppose A and form an ebit to be precise suppose A and form an ebit to be precise suppose A and
B are cubits that as a pair are in the B are cubits that as a pair are in the B are cubits that as a pair are in the
five plus Bell State and we apply Lambda five plus Bell State and we apply Lambda five plus Bell State and we apply Lambda
to say the first Cubit a to figure out to say the first Cubit a to figure out to say the first Cubit a to figure out
what happens let's first Express the what happens let's first Express the what happens let's first Express the
five plus State as a density Matrix and five plus State as a density Matrix and five plus State as a density Matrix and
for the sake of this example let's use for the sake of this example let's use for the sake of this example let's use
direct notation to do that here's the direct notation to do that here's the direct notation to do that here's the
five plus State as a Quantum State five plus State as a Quantum State five plus State as a Quantum State
vector and here's what we get when we vector and here's what we get when we vector and here's what we get when we
multiply this Vector to its conjugate multiply this Vector to its conjugate multiply this Vector to its conjugate
transpose and expand it all out we can transpose and expand it all out we can transpose and expand it all out we can
then apply the channel to the first then apply the channel to the first then apply the channel to the first
Cubit and evaluate what we get here's Cubit and evaluate what we get here's Cubit and evaluate what we get here's
Lambda applied to the first Cubit and Lambda applied to the first Cubit and Lambda applied to the first Cubit and
here's what we get when we evaluate each here's what we get when we evaluate each here's what we get when we evaluate each
of the terms the trace of k0o br0 is one of the terms the trace of k0o br0 is one of the terms the trace of k0o br0 is one
and so is the trace of cat 1 bra 1 so in and so is the trace of cat 1 bra 1 so in and so is the trace of cat 1 bra 1 so in
those two cases the output of the those two cases the output of the those two cases the output of the
channel is cat 0 bra 0 and that explains channel is cat 0 bra 0 and that explains channel is cat 0 bra 0 and that explains
the two terms in the answer the trace of the two terms in the answer the trace of the two terms in the answer the trace of
Kat zero bra one on the other hand is Kat zero bra one on the other hand is Kat zero bra one on the other hand is
zero as is the trace of Kat 1 bra zero zero as is the trace of Kat 1 bra zero zero as is the trace of Kat 1 bra zero
and so those two terms evaluate to zero and so those two terms evaluate to zero and so those two terms evaluate to zero
and therefore don't contribute anything and therefore don't contribute anything and therefore don't contribute anything
to the answer we can now simplify a to the answer we can now simplify a to the answer we can now simplify a
little bit and what we obtain is the little bit and what we obtain is the little bit and what we obtain is the
zero State for the first Cubit which is zero State for the first Cubit which is zero State for the first Cubit which is
a and the completely mixed state for the a and the completely mixed state for the a and the completely mixed state for the
second Cubit which is B notice in second Cubit which is B notice in second Cubit which is B notice in
particular that we did not get a plus particular that we did not get a plus particular that we did not get a plus
state for B which wouldn't necessarily state for B which wouldn't necessarily state for B which wouldn't necessarily
have been a bad guess if we didn't know have been a bad guess if we didn't know have been a bad guess if we didn't know
anything about density matrices and we anything about density matrices and we anything about density matrices and we
just looked at the quantum State Vector just looked at the quantum State Vector just looked at the quantum State Vector
representation of the state but that's representation of the state but that's representation of the state but that's
not how this channel works and there not how this channel works and there not how this channel works and there
isn't any channel that could possibly isn't any channel that could possibly isn't any channel that could possibly
work that way the channel Lambda is work that way the channel Lambda is work that way the channel Lambda is
called the Cubit reset Channel but the called the Cubit reset Channel but the called the Cubit reset Channel but the
way it works is really more like way it works is really more like way it works is really more like
throwing the input cubit in the trash throwing the input cubit in the trash throwing the input cubit in the trash
and replacing it with a fresh cubit in and replacing it with a fresh cubit in and replacing it with a fresh cubit in
the zero state but this in fact is the the zero state but this in fact is the the zero state but this in fact is the
only Cubit channel that always outputs only Cubit channel that always outputs only Cubit channel that always outputs
the zero State here's another example of the zero State here's another example of the zero State here's another example of
of a cubit Channel known as the of a cubit Channel known as the of a cubit Channel known as the
completely def phasing Channel this one completely def phasing Channel this one completely def phasing Channel this one
is denoted by Delta and the way that it is denoted by Delta and the way that it is denoted by Delta and the way that it
works is that it leaves the diagonal works is that it leaves the diagonal works is that it leaves the diagonal
entries of any 2x2 matrix alone and entries of any 2x2 matrix alone and entries of any 2x2 matrix alone and
zeros out the off diagonal entries Delta zeros out the off diagonal entries Delta zeros out the off diagonal entries Delta
is the Greek letter equivalent of D in is the Greek letter equivalent of D in is the Greek letter equivalent of D in
the Latin alphabet and that's short for the Latin alphabet and that's short for the Latin alphabet and that's short for
diagonal so that's one way to remember diagonal so that's one way to remember diagonal so that's one way to remember
what this channel does we can what this channel does we can what this channel does we can
alternatively Express this channel using alternatively Express this channel using alternatively Express this channel using
direct notation as is shown on the direct notation as is shown on the direct notation as is shown on the
screen and just to be clear the output screen and just to be clear the output screen and just to be clear the output
is always a 2X two Matrix so in is always a 2X two Matrix so in is always a 2X two Matrix so in
particular the two zero outputs refer to particular the two zero outputs refer to particular the two zero outputs refer to
the all zero 2x two Matrix we often do the all zero 2x two Matrix we often do the all zero 2x two Matrix we often do
this and we've seen this before in this this and we've seen this before in this this and we've seen this before in this
series the symbol zero is generally used series the symbol zero is generally used series the symbol zero is generally used
perhaps Ambiguously to mean the zero perhaps Ambiguously to mean the zero perhaps Ambiguously to mean the zero
element of whatever thing we're talking element of whatever thing we're talking element of whatever thing we're talking
about whether it's scalers vectors about whether it's scalers vectors about whether it's scalers vectors
matrices or whatever let's do the same matrices or whatever let's do the same matrices or whatever let's do the same
thing that we did previously for the thing that we did previously for the thing that we did previously for the
Cubit reset Channel and see what happens Cubit reset Channel and see what happens Cubit reset Channel and see what happens
if we apply this channel to half of an if we apply this channel to half of an if we apply this channel to half of an
ebit the setup is the same so I won't go ebit the setup is the same so I won't go ebit the setup is the same so I won't go
through that part and the result is the through that part and the result is the through that part and the result is the
state shown on on the screen which is state shown on on the screen which is state shown on on the screen which is
essentially a shared random bit so what essentially a shared random bit so what essentially a shared random bit so what
this channel did was basically to this channel did was basically to this channel did was basically to
convert an ebit into a perfectly convert an ebit into a perfectly convert an ebit into a perfectly
correlated pair of classical random bits correlated pair of classical random bits correlated pair of classical random bits
and that's consistent with a more and that's consistent with a more and that's consistent with a more
General interpretation of this channel General interpretation of this channel General interpretation of this channel
by zeroing out the off diagonal inges of by zeroing out the off diagonal inges of by zeroing out the off diagonal inges of
a density Matrix we're effectively a density Matrix we're effectively a density Matrix we're effectively
throwing away the quantum stuff in a throwing away the quantum stuff in a throwing away the quantum stuff in a
density Matrix leaving a diagonal density Matrix leaving a diagonal density Matrix leaving a diagonal
density Matrix that represents a density Matrix that represents a density Matrix that represents a
classical probabilistic State another classical probabilistic State another classical probabilistic State another
way to think about this is that this way to think about this is that this way to think about this is that this
channel represents a perfect noiseless channel represents a perfect noiseless channel represents a perfect noiseless
classical channel that transmits a bit classical channel that transmits a bit classical channel that transmits a bit
so if we put a zero state or a one state so if we put a zero state or a one state so if we put a zero state or a one state
or a convex combination of those two or a convex combination of those two or a convex combination of those two
states into the channel then that state states into the channel then that state states into the channel then that state
comes out unchanged but if you try to comes out unchanged but if you try to comes out unchanged but if you try to
put a Quantum State into it like a plus put a Quantum State into it like a plus put a Quantum State into it like a plus
state for instance it's as if the state for instance it's as if the state for instance it's as if the
channel measures that state with a channel measures that state with a channel measures that state with a
standard basis measurement and then standard basis measurement and then standard basis measurement and then
transmits the classical result one final transmits the classical result one final transmits the classical result one final
example of a cubit channel is the example of a cubit channel is the example of a cubit channel is the
completely depolarizing Channel what completely depolarizing Channel what completely depolarizing Channel what
this channel does is to always output this channel does is to always output this channel does is to always output
the completely mixed state so it's the completely mixed state so it's the completely mixed state so it's
similar in some sense to the Cubit reset similar in some sense to the Cubit reset similar in some sense to the Cubit reset
Channel except that we get a rather less Channel except that we get a rather less Channel except that we get a rather less
useful completely mixed state out rather useful completely mixed state out rather useful completely mixed state out rather
than an initialized Cubit a natural way than an initialized Cubit a natural way than an initialized Cubit a natural way
to think about this channel is that it to think about this channel is that it to think about this channel is that it
represents an extreme form of noise and represents an extreme form of noise and represents an extreme form of noise and
as far as Cubit channels are concerned as far as Cubit channels are concerned as far as Cubit channels are concerned
it really doesn't get any noisier than it really doesn't get any noisier than it really doesn't get any noisier than
this I don't have a good pneumonic to this I don't have a good pneumonic to this I don't have a good pneumonic to
share for remembering what this one does share for remembering what this one does share for remembering what this one does
other than to say that Omega is the last other than to say that Omega is the last other than to say that Omega is the last
letter of the Greek alphabet and it's letter of the Greek alphabet and it's letter of the Greek alphabet and it's
sometimes associated with the end of all sometimes associated with the end of all sometimes associated with the end of all
things and perhaps this is what happens things and perhaps this is what happens things and perhaps this is what happens
to all cubits eventually as far as noise to all cubits eventually as far as noise to all cubits eventually as far as noise
goes it's pretty extreme but one thing goes it's pretty extreme but one thing goes it's pretty extreme but one thing
that we can do is to consider a convex that we can do is to consider a convex that we can do is to consider a convex
combination of this Channel and the combination of this Channel and the combination of this Channel and the
identity Channel with perhaps a small identity Channel with perhaps a small identity Channel with perhaps a small
probability Epsilon for complete probability Epsilon for complete probability Epsilon for complete
depolarization this is a less extreme depolarization this is a less extreme depolarization this is a less extreme
form of noise where a given Cubit state form of noise where a given Cubit state form of noise where a given Cubit state
will mostly be left alone but there is will mostly be left alone but there is will mostly be left alone but there is
some small chance that it becomes some small chance that it becomes some small chance that it becomes
completely mixed geometrically we can completely mixed geometrically we can completely mixed geometrically we can
think about this as a slight contraction think about this as a slight contraction think about this as a slight contraction
of the block sphere toward the center of the block sphere toward the center of the block sphere toward the center
and by the way we can do something and by the way we can do something and by the way we can do something
similar with the completely def phasing similar with the completely def phasing similar with the completely def phasing
Channel to model the process of Channel to model the process of Channel to model the process of
decoherence for instance where things decoherence for instance where things decoherence for instance where things
become slightly more classical through become slightly more classical through become slightly more classical through
some process so those are three basic some process so those are three basic some process so those are three basic
examples of Cubit channels and we'll examples of Cubit channels and we'll examples of Cubit channels and we'll
come back to them mainly as a source of come back to them mainly as a source of come back to them mainly as a source of
examples throughout the remainder of the examples throughout the remainder of the examples throughout the remainder of the
lesson next we'll discuss mathematical lesson next we'll discuss mathematical lesson next we'll discuss mathematical
representations of channels starting representations of channels starting representations of channels starting
with a basic question linear mappings with a basic question linear mappings with a basic question linear mappings
from vectors to vectors can always be from vectors to vectors can always be from vectors to vectors can always be
represented by matrices in a familiar represented by matrices in a familiar represented by matrices in a familiar
way which is that the action of the way which is that the action of the way which is that the action of the
linear mapping is described by matrix linear mapping is described by matrix linear mapping is described by matrix
Vector multiplication but channels are Vector multiplication but channels are Vector multiplication but channels are
linear mappings from matrices to linear mappings from matrices to linear mappings from matrices to
matrices not vectors to vectors so in matrices not vectors to vectors so in matrices not vectors to vectors so in
general how can we express channels in general how can we express channels in general how can we express channels in
mathematical terms sometimes for special mathematical terms sometimes for special mathematical terms sometimes for special
channels we'll have a simple formula channels we'll have a simple formula channels we'll have a simple formula
that describes them and that was the that describes them and that was the that describes them and that was the
case in the examples of Cubit channels case in the examples of Cubit channels case in the examples of Cubit channels
that we already discussed but that isn't that we already discussed but that isn't that we already discussed but that isn't
practical in general because there won't practical in general because there won't practical in general because there won't
always be such a nice formula as a point always be such a nice formula as a point always be such a nice formula as a point
of comparison in the simplified of comparison in the simplified of comparison in the simplified
formulation of quantum information we formulation of quantum information we formulation of quantum information we
use unitary matrices to represent use unitary matrices to represent use unitary matrices to represent
operations on Quantum State vectors operations on Quantum State vectors operations on Quantum State vectors
every unitary Matrix represents a valid every unitary Matrix represents a valid every unitary Matrix represents a valid
operation and every valid operation can operation and every valid operation can operation and every valid operation can
be expressed as a unitary Matrix so in be expressed as a unitary Matrix so in be expressed as a unitary Matrix so in
essence what this question is asking is essence what this question is asking is essence what this question is asking is
how do we do this for channels and the how do we do this for channels and the how do we do this for channels and the
answer to this question is that there answer to this question is that there answer to this question is that there
are in fact multiple ways to represent are in fact multiple ways to represent are in fact multiple ways to represent
channels in mathematical terms in channels in mathematical terms in channels in mathematical terms in
particular we'll discuss three different particular we'll discuss three different particular we'll discuss three different
ways of representing channels that are ways of representing channels that are ways of representing channels that are
named after three individuals whose work named after three individuals whose work named after three individuals whose work
played an important part in their played an important part in their played an important part in their
development hin spring Krauss and Choy development hin spring Krauss and Choy development hin spring Krauss and Choy
first we'll see how these different first we'll see how these different first we'll see how these different
representations work and then in the representations work and then in the representations work and then in the
last part of the lesson we'll see how last part of the lesson we'll see how last part of the lesson we'll see how
the three representations relate and in the three representations relate and in the three representations relate and in
particular how we can convert between particular how we can convert between particular how we can convert between
them we'll also see that these them we'll also see that these them we'll also see that these
representations precisely capture the representations precisely capture the representations precisely capture the
requirements that we have on channels requirements that we have on channels requirements that we have on channels
namely that they're linear mappings and namely that they're linear mappings and namely that they're linear mappings and
that they always transform density that they always transform density that they always transform density
matrices into density matrices even when matrices into density matrices even when matrices into density matrices even when
they're applied to just one part of a they're applied to just one part of a they're applied to just one part of a
compound compound compound
system first up are Stein spring system first up are Stein spring system first up are Stein spring
representations Willam Forest Stein representations Willam Forest Stein representations Willam Forest Stein
spring was a mathematician who didn't spring was a mathematician who didn't spring was a mathematician who didn't
work on Quantum information and Stein work on Quantum information and Stein work on Quantum information and Stein
spring representations as I'll describe spring representations as I'll describe spring representations as I'll describe
them are in some sense just a derivative them are in some sense just a derivative them are in some sense just a derivative
of his work on operator theory in the of his work on operator theory in the of his work on operator theory in the
1950s but this work was most certainly 1950s but this work was most certainly 1950s but this work was most certainly
an important part of the development of an important part of the development of an important part of the development of
the mathematics that Quantum channels the mathematics that Quantum channels the mathematics that Quantum channels
are based on the idea behind this way of are based on the idea behind this way of are based on the idea behind this way of
representing channels is that every representing channels is that every representing channels is that every
channel can be implemented in a certain channel can be implemented in a certain channel can be implemented in a certain
way that I'll now describe first we form way that I'll now describe first we form way that I'll now describe first we form
a compound system consisting of the a compound system consisting of the a compound system consisting of the
input to whatever Channel we're input to whatever Channel we're input to whatever Channel we're
implementing together with an implementing together with an implementing together with an
initialized workspace system then a initialized workspace system then a initialized workspace system then a
unitary operation is performed on The unitary operation is performed on The unitary operation is performed on The
Compound system and finally whatever Compound system and finally whatever Compound system and finally whatever
part of the resulting compound system part of the resulting compound system part of the resulting compound system
corresponds to the output of the channel corresponds to the output of the channel corresponds to the output of the channel
is left alone or output and the rest is is left alone or output and the rest is is left alone or output and the rest is
discarded or traced out it's helpful to discarded or traced out it's helpful to discarded or traced out it's helpful to
express this in the form of a diagram express this in the form of a diagram express this in the form of a diagram
and it's simplest to start with the and it's simplest to start with the and it's simplest to start with the
special case in which the input and the special case in which the input and the special case in which the input and the
output systems are the same this diagram output systems are the same this diagram output systems are the same this diagram
by the way is just like a Quantum by the way is just like a Quantum by the way is just like a Quantum
circuit diagram except that the wires circuit diagram except that the wires circuit diagram except that the wires
represent arbitrary systems and not represent arbitrary systems and not represent arbitrary systems and not
necessarily single cubits so we start necessarily single cubits so we start necessarily single cubits so we start
with the input system X being in some with the input system X being in some with the input system X being in some
State row we introduce an initialized State row we introduce an initialized State row we introduce an initialized
workspace system W and in this diagram workspace system W and in this diagram workspace system W and in this diagram
I'm presuming that Zer is a classical I'm presuming that Zer is a classical I'm presuming that Zer is a classical
state of w and we're taking k0 to be the state of w and we're taking k0 to be the state of w and we're taking k0 to be the
initialized state of the system but we initialized state of the system but we initialized state of the system but we
could alternatively fix any pure state could alternatively fix any pure state could alternatively fix any pure state
of w and view that as the initialized of w and view that as the initialized of w and view that as the initialized
state we then perform some unitary state we then perform some unitary state we then perform some unitary
operation on the pair WX X and then operation on the pair WX X and then operation on the pair WX X and then
Trace out the workspace system and Trace out the workspace system and Trace out the workspace system and
output X here in the diagram by the way output X here in the diagram by the way output X here in the diagram by the way
I'm using the ground symbol from I'm using the ground symbol from I'm using the ground symbol from
electrical engineering to indicate that electrical engineering to indicate that electrical engineering to indicate that
W is being discarded that's not W is being discarded that's not W is being discarded that's not
necessarily conventional and you won't necessarily conventional and you won't necessarily conventional and you won't
find that symbol in kcit there you just find that symbol in kcit there you just find that symbol in kcit there you just
ignore the cubits that you're not ignore the cubits that you're not ignore the cubits that you're not
interested in the point is that I just interested in the point is that I just interested in the point is that I just
want to have a symbol that makes it want to have a symbol that makes it want to have a symbol that makes it
clear that we're discarding the system clear that we're discarding the system clear that we're discarding the system
for the purposes of this discussion in for the purposes of this discussion in for the purposes of this discussion in
general we don't necessarily need the general we don't necessarily need the general we don't necessarily need the
input and the output systems to be the input and the output systems to be the input and the output systems to be the
same so we can have for instance an same so we can have for instance an same so we can have for instance an
output system y along with a garbage output system y along with a garbage output system y along with a garbage
system G that gets traced out in order system G that gets traced out in order system G that gets traced out in order
for U to be unitary it has to be a for U to be unitary it has to be a for U to be unitary it has to be a
square Matrix so there's an assumption square Matrix so there's an assumption square Matrix so there's an assumption
here that the pair g y together has the here that the pair g y together has the here that the pair g y together has the
same number of classical States as the same number of classical States as the same number of classical States as the
pair WX for example maybe these systems pair WX for example maybe these systems pair WX for example maybe these systems
consist of cubits and the input system X consist of cubits and the input system X consist of cubits and the input system X
consists of 5 cubits and W consists of consists of 5 cubits and W consists of consists of 5 cubits and W consists of
10 cubits say so U is a unitary 10 cubits say so U is a unitary 10 cubits say so U is a unitary
operation on 15 cubits it could then be operation on 15 cubits it could then be operation on 15 cubits it could then be
that y consists of just four cubits that y consists of just four cubits that y consists of just four cubits
which means that g consists of 11 cubits which means that g consists of 11 cubits which means that g consists of 11 cubits
or maybe these aren't cubits at all all or maybe these aren't cubits at all all or maybe these aren't cubits at all all
that's important is that WX together and that's important is that WX together and that's important is that WX together and
gy together have the same number of gy together have the same number of gy together have the same number of
classical States so that you can be classical States so that you can be classical States so that you can be
unitary a description of a channel like unitary a description of a channel like unitary a description of a channel like
this is called a Stein spring this is called a Stein spring this is called a Stein spring
representation of the channel now it's representation of the channel now it's representation of the channel now it's
pretty straightforward to argue that any pretty straightforward to argue that any pretty straightforward to argue that any
mapping fi that can be described in this mapping fi that can be described in this mapping fi that can be described in this
way is in fact a valid channel in way is in fact a valid channel in way is in fact a valid channel in
particular if we start with the density particular if we start with the density particular if we start with the density
Ma Matrix and then tack on an additional Ma Matrix and then tack on an additional Ma Matrix and then tack on an additional
workspace system then we'll certainly be workspace system then we'll certainly be workspace system then we'll certainly be
left with a density Matrix we already left with a density Matrix we already left with a density Matrix we already
observed that applying a unitary observed that applying a unitary observed that applying a unitary
operation gives us a valid Channel and operation gives us a valid Channel and operation gives us a valid Channel and
if we have a density Matrix of a if we have a density Matrix of a if we have a density Matrix of a
compound system and we Trace out one of compound system and we Trace out one of compound system and we Trace out one of
the systems then again we'll be left the systems then again we'll be left the systems then again we'll be left
with a density Matrix so when we compose with a density Matrix so when we compose with a density Matrix so when we compose
these steps we'll be guaranteed that it these steps we'll be guaranteed that it these steps we'll be guaranteed that it
always transforms density matrices into always transforms density matrices into always transforms density matrices into
density matrices and it's also linear density matrices and it's also linear density matrices and it's also linear
what's not at all clear at this point is what's not at all clear at this point is what's not at all clear at this point is
that every channel can indeed be that every channel can indeed be that every channel can indeed be
implemented in this way for some choice implemented in this way for some choice implemented in this way for some choice
of the workspace system and the garbage of the workspace system and the garbage of the workspace system and the garbage
system as well as the unitary operation system as well as the unitary operation system as well as the unitary operation
U of course but that is true and we'll U of course but that is true and we'll U of course but that is true and we'll
see why by the end of the lesson here's see why by the end of the lesson here's see why by the end of the lesson here's
an example of a Stein spring an example of a Stein spring an example of a Stein spring
representation for the completely def representation for the completely def representation for the completely def
phasing Cubit Channel and to be clear phasing Cubit Channel and to be clear phasing Cubit Channel and to be clear
now the wires are cubits and this is an now the wires are cubits and this is an now the wires are cubits and this is an
ordinary controlled ordinary controlled ordinary controlled
knate it's very simple the input Cubit knate it's very simple the input Cubit knate it's very simple the input Cubit
is on the top and we introduced a is on the top and we introduced a is on the top and we introduced a
workspace Cubit initialized to the zero workspace Cubit initialized to the zero workspace Cubit initialized to the zero
statee we then perform a controlled knot statee we then perform a controlled knot statee we then perform a controlled knot
operation where the input Cubit is the operation where the input Cubit is the operation where the input Cubit is the
control and the workspace Cubit is the control and the workspace Cubit is the control and the workspace Cubit is the
Target and then the workspace Cubit is Target and then the workspace Cubit is Target and then the workspace Cubit is
traced traced traced
out intuitively speaking it makes sense out intuitively speaking it makes sense out intuitively speaking it makes sense
that this should implement the that this should implement the that this should implement the
completely def phasing Channel because completely def phasing Channel because completely def phasing Channel because
the controlled notate kind of acts like the controlled notate kind of acts like the controlled notate kind of acts like
a standard basis measurement on the top a standard basis measurement on the top a standard basis measurement on the top
Cubit by copying its classical State Cubit by copying its classical State Cubit by copying its classical State
onto the bottom Cubit and then the onto the bottom Cubit and then the onto the bottom Cubit and then the
result gets thrown result gets thrown result gets thrown
away but we can also go through a away but we can also go through a away but we can also go through a
calculation and of course it's a good calculation and of course it's a good calculation and of course it's a good
idea to do that because we really idea to do that because we really idea to do that because we really
shouldn't put too much trust in our shouldn't put too much trust in our shouldn't put too much trust in our
intuition we can start by writing down intuition we can start by writing down intuition we can start by writing down
the density Matrix representation of the the density Matrix representation of the the density Matrix representation of the
two cubits after the workspace Cubit has two cubits after the workspace Cubit has two cubits after the workspace Cubit has
been introduced and notice that we're been introduced and notice that we're been introduced and notice that we're
using the kuit ordering Convention as using the kuit ordering Convention as using the kuit ordering Convention as
usual where the bottom Cubit is the left usual where the bottom Cubit is the left usual where the bottom Cubit is the left
one and the top Cubit is on the one and the top Cubit is on the one and the top Cubit is on the
right we can express this density Matrix right we can express this density Matrix right we can express this density Matrix
explicitly as an actual 4x4 Matrix as is explicitly as an actual 4x4 Matrix as is explicitly as an actual 4x4 Matrix as is
shown on the shown on the shown on the
screen and then we compute the effect screen and then we compute the effect screen and then we compute the effect
that the controlled notot gate has on that the controlled notot gate has on that the controlled notot gate has on
this density Matrix so we conjugate by this density Matrix so we conjugate by this density Matrix so we conjugate by
the unitary Matrix associated with this the unitary Matrix associated with this the unitary Matrix associated with this
gate once again keeping in mind the kuit gate once again keeping in mind the kuit gate once again keeping in mind the kuit
ordering convention so this is the ordering convention so this is the ordering convention so this is the
correct unitary correct unitary correct unitary
Matrix and the result is shown Matrix and the result is shown Matrix and the result is shown
here in effect the controlled noock gate here in effect the controlled noock gate here in effect the controlled noock gate
permuted the entries of our original permuted the entries of our original permuted the entries of our original
Matrix around a little bit so now the Matrix around a little bit so now the Matrix around a little bit so now the
possibly non-zero entries are in the possibly non-zero entries are in the possibly non-zero entries are in the
four corners it remains to compute the four corners it remains to compute the four corners it remains to compute the
partial trace and we could do that partial trace and we could do that partial trace and we could do that
directly from the Matrix but an directly from the Matrix but an directly from the Matrix but an
alternative is to convert to the direct alternative is to convert to the direct alternative is to convert to the direct
notation as is shown on the screen and notation as is shown on the screen and notation as is shown on the screen and
then it becomes pretty easy to evaluate then it becomes pretty easy to evaluate then it becomes pretty easy to evaluate
what happens when we Trace out the Cubit what happens when we Trace out the Cubit what happens when we Trace out the Cubit
on the left within each on the left within each on the left within each
term and the result is that just the term and the result is that just the term and the result is that just the
terms corresponding to the diagonal terms corresponding to the diagonal terms corresponding to the diagonal
entries of row survive and so we've def entries of row survive and so we've def entries of row survive and so we've def
phased the Cubit as phased the Cubit as phased the Cubit as
expected as always check this for expected as always check this for expected as always check this for
yourself yourself at your own pace if yourself yourself at your own pace if yourself yourself at your own pace if
you choose to do that it's a good you choose to do that it's a good you choose to do that it's a good
example to get some practice working example to get some practice working example to get some practice working
with density matrices and with partial traces that's not the only way to traces that's not the only way to
implement the completely def phasing implement the completely def phasing implement the completely def phasing
Channel here's an alternative Stein Channel here's an alternative Stein Channel here's an alternative Stein
spring representation for this spring representation for this spring representation for this
channel I won't go through the channel I won't go through the channel I won't go through the
calculation in detail in this video but calculation in detail in this video but calculation in detail in this video but
I will show it briefly in case you'd I will show it briefly in case you'd I will show it briefly in case you'd
like to pause the video and have a like to pause the video and have a like to pause the video and have a
closer closer closer
look the way it works is similar to the look the way it works is similar to the look the way it works is similar to the
first one but the details are first one but the details are first one but the details are
different this time we start with a different this time we start with a different this time we start with a
hadamar gate on the workspace Cubit hadamar gate on the workspace Cubit hadamar gate on the workspace Cubit
which puts it into a plus State and if which puts it into a plus State and if which puts it into a plus State and if
we expand out the tensor product we get we expand out the tensor product we get we expand out the tensor product we get
the Matrix that's shown on the screen to apply the control zgate we screen to apply the control zgate we
conjugate by the associated unitary conjugate by the associated unitary conjugate by the associated unitary
Matrix and the result of doing that is Matrix and the result of doing that is Matrix and the result of doing that is
to inject some minus signs into the to inject some minus signs into the to inject some minus signs into the
Matrix in particular the last row and Matrix in particular the last row and Matrix in particular the last row and
the last column both get multiplied by the last column both get multiplied by the last column both get multiplied by
Nega 1 and this time it would be pretty messy and this time it would be pretty messy
to convert to direct notation so we'll to convert to direct notation so we'll to convert to direct notation so we'll
compute the partial Trace compute the partial Trace compute the partial Trace
directly when we Trace out the first of directly when we Trace out the first of directly when we Trace out the first of
two cubits what we end up getting is the two cubits what we end up getting is the two cubits what we end up getting is the
sum of the two diagonal 2x two blocks sum of the two diagonal 2x two blocks sum of the two diagonal 2x two blocks
and so the final result is again that and so the final result is again that and so the final result is again that
we've defaced the Cubit that was quick and if you're not Cubit that was quick and if you're not
used to these sorts of calculations then used to these sorts of calculations then used to these sorts of calculations then
take your time and do it slowly speed is take your time and do it slowly speed is take your time and do it slowly speed is
not important not important not important
here to finish off this example let me here to finish off this example let me here to finish off this example let me
mention that there is a pretty simple mention that there is a pretty simple mention that there is a pretty simple
idea behind this implementation which is idea behind this implementation which is idea behind this implementation which is
pretty nicely expressed by the last step pretty nicely expressed by the last step pretty nicely expressed by the last step
of the calculation that we just skimmed of the calculation that we just skimmed of the calculation that we just skimmed
through more succinct way of saying this through more succinct way of saying this through more succinct way of saying this
is that if we average a given Cubit is that if we average a given Cubit is that if we average a given Cubit
density Matrix with that density Matrix density Matrix with that density Matrix density Matrix with that density Matrix
conjugated by the poly Z Matrix then conjugated by the poly Z Matrix then conjugated by the poly Z Matrix then
that's the same thing as def that's the same thing as def that's the same thing as def
phasing so the completely def phasing phasing so the completely def phasing phasing so the completely def phasing
Channel happens to be a mixed unitary Channel happens to be a mixed unitary Channel happens to be a mixed unitary
Channel where we either do nothing or in Channel where we either do nothing or in Channel where we either do nothing or in
other words we perform the identity other words we perform the identity other words we perform the identity
operation or we perform a zgate each operation or we perform a zgate each operation or we perform a zgate each
with probability with probability with probability
1/2 good to 1/2 good to 1/2 good to
know so in summary we have two different know so in summary we have two different know so in summary we have two different
Stein spring representations of the Stein spring representations of the Stein spring representations of the
completely def phasing Channel and that completely def phasing Channel and that completely def phasing Channel and that
tells us something important which is tells us something important which is tells us something important which is
that Stein spring representations are that Stein spring representations are that Stein spring representations are
not unique and in fact there are not unique and in fact there are not unique and in fact there are
infinitely many of them for any given infinitely many of them for any given infinitely many of them for any given
Channel Next we discuss cross Channel Next we discuss cross Channel Next we discuss cross
representations of channels which give representations of channels which give representations of channels which give
us a convenient formulaic way of us a convenient formulaic way of us a convenient formulaic way of
expressing channels through matrix expressing channels through matrix expressing channels through matrix
multiplication and addition cross multiplication and addition cross multiplication and addition cross
representations are named after Carl representations are named after Carl representations are named after Carl
Krauss who is a physicist who among Krauss who is a physicist who among Krauss who is a physicist who among
other things worked out a lot of the other things worked out a lot of the other things worked out a lot of the
basic mathematical details of channels basic mathematical details of channels basic mathematical details of channels
and the general formulation of quantum and the general formulation of quantum and the general formulation of quantum
information more broadly in general a information more broadly in general a information more broadly in general a
cross representation is an expression of cross representation is an expression of cross representation is an expression of
a channel taking the form shown here on a channel taking the form shown here on a channel taking the form shown here on
the screen in particular we have a the screen in particular we have a the screen in particular we have a
collection of matrices which here are collection of matrices which here are collection of matrices which here are
named a z through a n minus one for some named a z through a n minus one for some named a z through a n minus one for some
positive integer n and they specify the positive integer n and they specify the positive integer n and they specify the
action of the channel on any given input action of the channel on any given input action of the channel on any given input
row according to the formula in short we row according to the formula in short we row according to the formula in short we
conjugate row by each of these matrices conjugate row by each of these matrices conjugate row by each of these matrices
which are often referred to as cross which are often referred to as cross which are often referred to as cross
matrices in this context and then we sum matrices in this context and then we sum matrices in this context and then we sum
up the result and that gives us the up the result and that gives us the up the result and that gives us the
output all these matrices have to have output all these matrices have to have output all these matrices have to have
the same dimensions and specifically the same dimensions and specifically the same dimensions and specifically
their columns must correspond to the their columns must correspond to the their columns must correspond to the
classical states of the input system and classical states of the input system and classical states of the input system and
the rows must correspond to the the rows must correspond to the the rows must correspond to the
classical states of the output system if classical states of the output system if classical states of the output system if
that's the case then when we take the that's the case then when we take the that's the case then when we take the
product AK * row * AK dagger for any product AK * row * AK dagger for any product AK * row * AK dagger for any
choice of K we get a square Matrix whose choice of K we get a square Matrix whose choice of K we get a square Matrix whose
rows and columns correspond to the rows and columns correspond to the rows and columns correspond to the
classical states of the output system classical states of the output system classical states of the output system
and so it makes sense that the and so it makes sense that the and so it makes sense that the
expression produces a density Matrix of expression produces a density Matrix of expression produces a density Matrix of
the output system now it is not the case the output system now it is not the case the output system now it is not the case
that for an arbitrary choice of these that for an arbitrary choice of these that for an arbitrary choice of these
cross matrices that we get a valid cross matrices that we get a valid cross matrices that we get a valid
Channel there is however a pretty simple Channel there is however a pretty simple Channel there is however a pretty simple
condition on the matrices that condition on the matrices that condition on the matrices that
guarantees that we will get a valid guarantees that we will get a valid guarantees that we will get a valid
Channel and that condition is the Channel and that condition is the Channel and that condition is the
formula displayed right here formula displayed right here formula displayed right here
specifically if we sum up the product of specifically if we sum up the product of specifically if we sum up the product of
AK dagger * AK over all the values of K AK dagger * AK over all the values of K AK dagger * AK over all the values of K
then we must get the identity Matrix and then we must get the identity Matrix and then we must get the identity Matrix and
that will be the identity Matrix whose that will be the identity Matrix whose that will be the identity Matrix whose
rows and columns correspond to the rows and columns correspond to the rows and columns correspond to the
classical states of the input system so classical states of the input system so classical states of the input system so
that condition must be satisfied in that condition must be satisfied in that condition must be satisfied in
order for the mapping defined by the order for the mapping defined by the order for the mapping defined by the
first equation to be a valid Channel and first equation to be a valid Channel and first equation to be a valid Channel and
it turns out that every channel does in it turns out that every channel does in it turns out that every channel does in
fact have a representation that takes fact have a representation that takes fact have a representation that takes
this form as long as we take n to be this form as long as we take n to be this form as long as we take n to be
large enough large enough large enough
here are a few examples of cross here are a few examples of cross here are a few examples of cross
representations starting with the Cubit representations starting with the Cubit representations starting with the Cubit
reset Channel we can obtain a cross reset Channel we can obtain a cross reset Channel we can obtain a cross
representation for this channel by using representation for this channel by using representation for this channel by using
two cross matrices c0 bra 0 and c0 bra two cross matrices c0 bra 0 and c0 bra two cross matrices c0 bra 0 and c0 bra
one so n is equal to two in this example one so n is equal to two in this example one so n is equal to two in this example
we can plug these choices into the we can plug these choices into the we can plug these choices into the
general formula and we have just two general formula and we have just two general formula and we have just two
terms so it's easy to write out the sum terms so it's easy to write out the sum terms so it's easy to write out the sum
and if we simplify and recognize that and if we simplify and recognize that and if we simplify and recognize that
the expression in parenthesis is the the expression in parenthesis is the the expression in parenthesis is the
trace of row then we see that we obtain trace of row then we see that we obtain trace of row then we see that we obtain
the Cubit reset Channel we can also the Cubit reset Channel we can also the Cubit reset Channel we can also
check that the required condition on check that the required condition on check that the required condition on
these matrices is met and that's a these matrices is met and that's a these matrices is met and that's a
simple matter of plugging the CR simple matter of plugging the CR simple matter of plugging the CR
matrices into the equation and matrices into the equation and matrices into the equation and
evaluating and we get the identity evaluating and we get the identity evaluating and we get the identity
Matrix as is required for another Matrix as is required for another Matrix as is required for another
example let's again take a look at the example let's again take a look at the example let's again take a look at the
completely def phasing Channel taking completely def phasing Channel taking completely def phasing Channel taking
the matrices cat 0 br0 and Cat one bra the matrices cat 0 br0 and Cat one bra the matrices cat 0 br0 and Cat one bra
one works and that can be checked by one works and that can be checked by one works and that can be checked by
expanding out the sum and seeing pretty expanding out the sum and seeing pretty expanding out the sum and seeing pretty
directly that we get what we need and directly that we get what we need and directly that we get what we need and
again the required condition is met an again the required condition is met an again the required condition is met an
alternative is to take a z to be the alternative is to take a z to be the alternative is to take a z to be the
identity Matrix ided &lt; tk2 and A1 to be identity Matrix ided &lt; tk2 and A1 to be identity Matrix ided &lt; tk2 and A1 to be
Sigma Z / &lt; tk2 and if we evaluate we Sigma Z / &lt; tk2 and if we evaluate we Sigma Z / &lt; tk2 and if we evaluate we
see that again we get the completely def see that again we get the completely def see that again we get the completely def
phasing Channel based on the observation phasing Channel based on the observation phasing Channel based on the observation
we made about this channel a little bit we made about this channel a little bit we made about this channel a little bit
earlier in the lesson and once again the earlier in the lesson and once again the earlier in the lesson and once again the
required condition is pretty required condition is pretty required condition is pretty
straightforward to straightforward to straightforward to
check as an exercise to work through if check as an exercise to work through if check as an exercise to work through if
you would find that to be helpful you would find that to be helpful you would find that to be helpful
consider the completely depolarizing consider the completely depolarizing consider the completely depolarizing
Channel which outputs the completely Channel which outputs the completely Channel which outputs the completely
mixed state for every density Matrix mixed state for every density Matrix mixed state for every density Matrix
input for this one four cross matrices input for this one four cross matrices input for this one four cross matrices
are required are required are required
it's not possible to come up with a it's not possible to come up with a it's not possible to come up with a
cross representation where n is three or cross representation where n is three or cross representation where n is three or
less here's one choice that works and less here's one choice that works and less here's one choice that works and
here's another so the exercise is to here's another so the exercise is to here's another so the exercise is to
verify that these choices work as well verify that these choices work as well verify that these choices work as well
as to check that the required condition as to check that the required condition as to check that the required condition
is satisfied it's worth pointing out is satisfied it's worth pointing out is satisfied it's worth pointing out
that the second alternative basically that the second alternative basically that the second alternative basically
says that it's possible to completely says that it's possible to completely says that it's possible to completely
depolarize a cubit or in other words to depolarize a cubit or in other words to depolarize a cubit or in other words to
completely randomize it by randomly completely randomize it by randomly completely randomize it by randomly
choosing one of the four poly matrices choosing one of the four poly matrices choosing one of the four poly matrices
including the identity and applying the including the identity and applying the including the identity and applying the
corresponding unitary operation to the corresponding unitary operation to the corresponding unitary operation to the
Cubit and so the completely depolarizing Cubit and so the completely depolarizing Cubit and so the completely depolarizing
channel is another example of a mixed channel is another example of a mixed channel is another example of a mixed
unitary channel the last of the three unitary channel the last of the three unitary channel the last of the three
Channel representations that we'll cover Channel representations that we'll cover Channel representations that we'll cover
is the Choy representation manduin Choy is the Choy representation manduin Choy is the Choy representation manduin Choy
is a mathematician who works on Matrix is a mathematician who works on Matrix is a mathematician who works on Matrix
Theory and other topics and the Choy Theory and other topics and the Choy Theory and other topics and the Choy
representation is named in his honor representation is named in his honor representation is named in his honor
based on a really elegant paper that he based on a really elegant paper that he based on a really elegant paper that he
published in 1975 that in some sense published in 1975 that in some sense published in 1975 that in some sense
ties all the stuff together the choit ties all the stuff together the choit ties all the stuff together the choit
representation of a channel f is a representation of a channel f is a representation of a channel f is a
single Matrix that's conventionally single Matrix that's conventionally single Matrix that's conventionally
denoted jfi and specifically if the denoted jfi and specifically if the denoted jfi and specifically if the
input system has n classical States and input system has n classical States and input system has n classical States and
the output system has M classical States the output system has M classical States the output system has M classical States
then the chy representation of f will be then the chy representation of f will be then the chy representation of f will be
an n * m by n * m Square Matrix before I an n * m by n * m Square Matrix before I an n * m by n * m Square Matrix before I
tell you exactly how the Matrix is tell you exactly how the Matrix is tell you exactly how the Matrix is
defined I'd like to mention a couple of defined I'd like to mention a couple of defined I'd like to mention a couple of
its Key properties which motivate the its Key properties which motivate the its Key properties which motivate the
representation in a way that the actual representation in a way that the actual representation in a way that the actual
definition itself might not at least on definition itself might not at least on definition itself might not at least on
the surface first unlike both Stein the surface first unlike both Stein the surface first unlike both Stein
spring representations and cruss spring representations and cruss spring representations and cruss
representations the chy representation representations the chy representation representations the chy representation
is a so-called faithful representation is a so-called faithful representation is a so-called faithful representation
which means that every channel has just which means that every channel has just which means that every channel has just
one of them and that's pretty important one of them and that's pretty important one of them and that's pretty important
because it says that two channels are because it says that two channels are because it says that two channels are
the same if and only if their Choy the same if and only if their Choy the same if and only if their Choy
representations are the same so if you representations are the same so if you representations are the same so if you
have two different descriptions of have two different descriptions of have two different descriptions of
channels such as two different Stein channels such as two different Stein channels such as two different Stein
spring representations or two different spring representations or two different spring representations or two different
cruss representations or perhaps one of cruss representations or perhaps one of cruss representations or perhaps one of
each and you want to know if they are in each and you want to know if they are in each and you want to know if they are in
fact the same channel you can just fact the same channel you can just fact the same channel you can just
compute the Choy representation and compute the Choy representation and compute the Choy representation and
check whether or not they're equal the check whether or not they're equal the check whether or not they're equal the
second property is that they are simple second property is that they are simple second property is that they are simple
to check conditions on the Choy to check conditions on the Choy to check conditions on the Choy
representation that are true if and only representation that are true if and only representation that are true if and only
if we have a valid channel that property if we have a valid channel that property if we have a valid channel that property
isn't quite so special we can say the isn't quite so special we can say the isn't quite so special we can say the
same thing about the other two same thing about the other two same thing about the other two
representations but it's a good property representations but it's a good property representations but it's a good property
to have nevertheless and we'll discuss to have nevertheless and we'll discuss to have nevertheless and we'll discuss
it further in more detail shortly one it further in more detail shortly one it further in more detail shortly one
more thing to note about the Cho more thing to note about the Cho more thing to note about the Cho
representation is that although we are representation is that although we are representation is that although we are
representing a channel by a matrix this representing a channel by a matrix this representing a channel by a matrix this
Matrix does not directly represent the Matrix does not directly represent the Matrix does not directly represent the
channel as a linear mapping in fact it's channel as a linear mapping in fact it's channel as a linear mapping in fact it's
actually more natural to associate the actually more natural to associate the actually more natural to associate the
Choy representation with the density Choy representation with the density Choy representation with the density
Matrix that represents a certain State Matrix that represents a certain State Matrix that represents a certain State
than it is to think about the Choy than it is to think about the Choy than it is to think about the Choy
representation as describing a linear representation as describing a linear representation as describing a linear
mapping it is however possible to mapping it is however possible to mapping it is however possible to
recover the action of the channel from recover the action of the channel from recover the action of the channel from
its Tri representation using a pretty its Tri representation using a pretty its Tri representation using a pretty
simple formula but it's not so simple as simple formula but it's not so simple as simple formula but it's not so simple as
ordinary matrix ordinary matrix ordinary matrix
multiplication that's enough of a multiplication that's enough of a multiplication that's enough of a
preamble let's get to the actual preamble let's get to the actual preamble let's get to the actual
definition suppose that we have a definition suppose that we have a definition suppose that we have a
channel fi from a system X to a system Y channel fi from a system X to a system Y channel fi from a system X to a system Y
and assume that the classical State set and assume that the classical State set and assume that the classical State set
of the input system X is Sigma we're not of the input system X is Sigma we're not of the input system X is Sigma we're not
going to need to refer to the classical going to need to refer to the classical going to need to refer to the classical
State set of the the output system so State set of the the output system so State set of the the output system so
for the definition there's no need to for the definition there's no need to for the definition there's no need to
name that one the Choy representation of name that one the Choy representation of name that one the Choy representation of
fi is then defined by the expression fi is then defined by the expression fi is then defined by the expression
shown here on the screen we have a sum shown here on the screen we have a sum shown here on the screen we have a sum
that ranges over all classical States A that ranges over all classical States A that ranges over all classical States A
and B for the input system and we're and B for the input system and we're and B for the input system and we're
summing k a bra B tensored with the summing k a bra B tensored with the summing k a bra B tensored with the
channel fi applied to k a bra B so this channel fi applied to k a bra B so this channel fi applied to k a bra B so this
representation is defined by a formula representation is defined by a formula representation is defined by a formula
and there are a couple of different ways and there are a couple of different ways and there are a couple of different ways
that we can think about this formula that we can think about this formula that we can think about this formula
before I discuss them however let me before I discuss them however let me before I discuss them however let me
mention that sometimes the chart mention that sometimes the chart mention that sometimes the chart
representation is defined in a slightly representation is defined in a slightly representation is defined in a slightly
different way where the ordering of the different way where the ordering of the different way where the ordering of the
two tensor factors is reversed meaning two tensor factors is reversed meaning two tensor factors is reversed meaning
that fi of k a bra appears as the first that fi of k a bra appears as the first that fi of k a bra appears as the first
tensor Factor rather than the second but tensor Factor rather than the second but tensor Factor rather than the second but
the two different ways of doing it are the two different ways of doing it are the two different ways of doing it are
equivalent up to this simple symmetry equivalent up to this simple symmetry equivalent up to this simple symmetry
but what that means is that when you're but what that means is that when you're but what that means is that when you're
using this representation it's a good using this representation it's a good using this representation it's a good
idea to clarify which ordering you're idea to clarify which ordering you're idea to clarify which ordering you're
using this ordering is the way it's done using this ordering is the way it's done using this ordering is the way it's done
in kcit and it's also better for in kcit and it's also better for in kcit and it's also better for
describing the representation visually describing the representation visually describing the representation visually
in terms of block matrices so this is in terms of block matrices so this is in terms of block matrices so this is
the one that going to go with to be more the one that going to go with to be more the one that going to go with to be more
precise about the block Matrix precise about the block Matrix precise about the block Matrix
description if we assume that the description if we assume that the description if we assume that the
classical State set Sigma consists of classical State set Sigma consists of classical State set Sigma consists of
the integers from zero up to n minus one the integers from zero up to n minus one the integers from zero up to n minus one
for some positive integer n then we can for some positive integer n then we can for some positive integer n then we can
alternatively write jfi as a block alternatively write jfi as a block alternatively write jfi as a block
Matrix as is shown right here on the Matrix as is shown right here on the Matrix as is shown right here on the
screen in other words the Choy screen in other words the Choy screen in other words the Choy
representation of fi is the Matrix we representation of fi is the Matrix we representation of fi is the Matrix we
get by evaluating fi on each of the get by evaluating fi on each of the get by evaluating fi on each of the
input matrices that have exactly one input matrices that have exactly one input matrices that have exactly one
entry equal to one with all of their entry equal to one with all of their entry equal to one with all of their
entries zero and then arranging them entries zero and then arranging them entries zero and then arranging them
into a Matrix in a pretty natural way it into a Matrix in a pretty natural way it into a Matrix in a pretty natural way it
may not be clear a priori why we would may not be clear a priori why we would may not be clear a priori why we would
want to form a matrix like this but it want to form a matrix like this but it want to form a matrix like this but it
does turn out to be very does turn out to be very does turn out to be very
useful one thing to notice is that the useful one thing to notice is that the useful one thing to notice is that the
set containing the matrices k a bra b as set containing the matrices k a bra b as set containing the matrices k a bra b as
a and b both range over all integers a and b both range over all integers a and b both range over all integers
from 0 to n minus one forms a basis for from 0 to n minus one forms a basis for from 0 to n minus one forms a basis for
the space of all n byn matrices so if the space of all n byn matrices so if the space of all n byn matrices so if
you have this Matrix then you can you have this Matrix then you can you have this Matrix then you can
certainly determine what fi does to an certainly determine what fi does to an certainly determine what fi does to an
arbitrary n byn matrix by linearity arbitrary n byn matrix by linearity arbitrary n byn matrix by linearity
specifically by taking appropriate specifically by taking appropriate specifically by taking appropriate
linear combinations of the blocks so the linear combinations of the blocks so the linear combinations of the blocks so the
Matrix jfi does in fact uniquely Matrix jfi does in fact uniquely Matrix jfi does in fact uniquely
determine fi as a linear determine fi as a linear determine fi as a linear
mapping another way to think about the mapping another way to think about the mapping another way to think about the
Cho representation of a channel is that Cho representation of a channel is that Cho representation of a channel is that
it represents a state as a density it represents a state as a density it represents a state as a density
Matrix when it's normalized specifically Matrix when it's normalized specifically Matrix when it's normalized specifically
by dividing it by the number of by dividing it by the number of by dividing it by the number of
classical states of the input system classical states of the input system classical states of the input system
which I'll continue to denote by which I'll continue to denote by which I'll continue to denote by
lowercase n this will always give us a lowercase n this will always give us a lowercase n this will always give us a
density Matrix which is often referred density Matrix which is often referred density Matrix which is often referred
to as the choice state of the channel to as the choice state of the channel to as the choice state of the channel
here's how this works in more detail here's how this works in more detail here's how this works in more detail
imagine first that we have two identical imagine first that we have two identical imagine first that we have two identical
copies of the input system and we copies of the input system and we copies of the input system and we
consider the quantum State Vector s consider the quantum State Vector s consider the quantum State Vector s
displayed on the screen which represents displayed on the screen which represents displayed on the screen which represents
a so-called maximally entangled state of a so-called maximally entangled state of a so-called maximally entangled state of
this pair of identical systems for this pair of identical systems for this pair of identical systems for
instance if Sigma is the binary alphabet instance if Sigma is the binary alphabet instance if Sigma is the binary alphabet
then this is a f+ state but we can think then this is a f+ state but we can think then this is a f+ state but we can think
about an analogously defined state for about an analogously defined state for about an analogously defined state for
any choice of Sigma and what we get is a any choice of Sigma and what we get is a any choice of Sigma and what we get is a
state that's highly entangled and in state that's highly entangled and in state that's highly entangled and in
fact it's as entangled as you can get fact it's as entangled as you can get fact it's as entangled as you can get
given this choice of systems to obtain given this choice of systems to obtain given this choice of systems to obtain
the density Matrix representation of the the density Matrix representation of the the density Matrix representation of the
state we multiply the state Vector to state we multiply the state Vector to state we multiply the state Vector to
its conjugate transpose as usual and its conjugate transpose as usual and its conjugate transpose as usual and
this is what it looks like and finally this is what it looks like and finally this is what it looks like and finally
if we think about the Choy if we think about the Choy if we think about the Choy
representation of the channel divided by representation of the channel divided by representation of the channel divided by
n then we see that it's the state that n then we see that it's the state that n then we see that it's the state that
we would obtain if we were to apply the we would obtain if we were to apply the we would obtain if we were to apply the
channel fi to the second channel fi to the second channel fi to the second
system in the form of a figure here's system in the form of a figure here's system in the form of a figure here's
what this looks like we have two copies what this looks like we have two copies what this looks like we have two copies
of the system X in this maximally of the system X in this maximally of the system X in this maximally
entangled State s the channel f is entangled State s the channel f is entangled State s the channel f is
applied to the second system which is applied to the second system which is applied to the second system which is
the one on the top in the diagram and the one on the top in the diagram and the one on the top in the diagram and
the resulting state is the choice state the resulting state is the choice state the resulting state is the choice state
of five of course we wouldn't normally of five of course we wouldn't normally of five of course we wouldn't normally
give the same name to two different give the same name to two different give the same name to two different
systems like this it's generally not systems like this it's generally not systems like this it's generally not
helpful to give the same name to two helpful to give the same name to two helpful to give the same name to two
different things but it does make sense different things but it does make sense different things but it does make sense
in this case just to make this in this case just to make this in this case just to make this
connection with Choy representations connection with Choy representations connection with Choy representations
there are a couple of implications that there are a couple of implications that there are a couple of implications that
we can immediately draw from this way of we can immediately draw from this way of we can immediately draw from this way of
looking at Choy representations one is looking at Choy representations one is looking at Choy representations one is
that because the choice state is a that because the choice state is a that because the choice state is a
density Matrix assuming that f is a density Matrix assuming that f is a density Matrix assuming that f is a
valid Channel we conclude that the Choy valid Channel we conclude that the Choy valid Channel we conclude that the Choy
representation of fi must be positive representation of fi must be positive representation of fi must be positive
semi-definite remember that it's a semi-definite remember that it's a semi-definite remember that it's a
requirement that we place on channels requirement that we place on channels requirement that we place on channels
that they always transform density that they always transform density that they always transform density
matrices into density matrices and this matrices into density matrices and this matrices into density matrices and this
is just one special case of this the is just one special case of this the is just one special case of this the
second implication is that if we take second implication is that if we take second implication is that if we take
the choice state for any channel fi and the choice state for any channel fi and the choice state for any channel fi and
then discard the output system y then we then discard the output system y then we then discard the output system y then we
must be left with the completely mixed must be left with the completely mixed must be left with the completely mixed
state on the input system X that's state on the input system X that's state on the input system X that's
because applying a channel to a system because applying a channel to a system because applying a channel to a system
and then throwing the resulting system and then throwing the resulting system and then throwing the resulting system
into the garbage is equivalent to just into the garbage is equivalent to just into the garbage is equivalent to just
throwing the original system into the throwing the original system into the throwing the original system into the
garbage and if we were to do that with garbage and if we were to do that with garbage and if we were to do that with
our maximally entangled State the our maximally entangled State the our maximally entangled State the
density Matrix we'd be left with would density Matrix we'd be left with would density Matrix we'd be left with would
be the identity Matrix divided by n or be the identity Matrix divided by n or be the identity Matrix divided by n or
in other words the completely mixed in other words the completely mixed in other words the completely mixed
state for the system X the idea in more state for the system X the idea in more state for the system X the idea in more
mathematical terms is that composing any mathematical terms is that composing any mathematical terms is that composing any
Channel with the trace on the output Channel with the trace on the output Channel with the trace on the output
system must be equal to the trace on the system must be equal to the trace on the system must be equal to the trace on the
input system and that's because channels input system and that's because channels input system and that's because channels
must preserve Trace by virtue of the must preserve Trace by virtue of the must preserve Trace by virtue of the
fact that they're linear mappings that fact that they're linear mappings that fact that they're linear mappings that
always transform density matrices into always transform density matrices into always transform density matrices into
density matrices once we have this density matrices once we have this density matrices once we have this
equation we can clear the denominator n equation we can clear the denominator n equation we can clear the denominator n
from both sides to get a slightly from both sides to get a slightly from both sides to get a slightly
cleaner condition in words performing cleaner condition in words performing cleaner condition in words performing
the partial trace on the output system the partial trace on the output system the partial trace on the output system
of the Choy representation of a channel of the Choy representation of a channel of the Choy representation of a channel
must give us the identity Matrix on the must give us the identity Matrix on the must give us the identity Matrix on the
input system so in summary Cho input system so in summary Cho input system so in summary Cho
representations of channels are always representations of channels are always representations of channels are always
positive semi-definite and if we Trace positive semi-definite and if we Trace positive semi-definite and if we Trace
out the output system we're necessarily out the output system we're necessarily out the output system we're necessarily
left with the identity Matrix on the left with the identity Matrix on the left with the identity Matrix on the
input system as a turns out these are input system as a turns out these are input system as a turns out these are
not only necessary conditions but not only necessary conditions but not only necessary conditions but
they're also sufficient meaning that if they're also sufficient meaning that if they're also sufficient meaning that if
I is some arbitrary linear mapping and I is some arbitrary linear mapping and I is some arbitrary linear mapping and
these two conditions both happen to be these two conditions both happen to be these two conditions both happen to be
true then it must in fact be that fi is true then it must in fact be that fi is true then it must in fact be that fi is
a valid Channel and we'll see why that a valid Channel and we'll see why that a valid Channel and we'll see why that
is the case by the end of the lesson is the case by the end of the lesson is the case by the end of the lesson
let's take a look at a few examples let's take a look at a few examples let's take a look at a few examples
starting with the completely def phasing starting with the completely def phasing starting with the completely def phasing
channel to compute the Choy channel to compute the Choy channel to compute the Choy
representation it's just a matter of representation it's just a matter of representation it's just a matter of
evaluating the formula the completely evaluating the formula the completely evaluating the formula the completely
defacing Channel leaves k a bra B alone defacing Channel leaves k a bra B alone defacing Channel leaves k a bra B alone
when a equal B and otherwise the output when a equal B and otherwise the output when a equal B and otherwise the output
is zero so the terms in the sum that is zero so the terms in the sum that is zero so the terms in the sum that
survive are the ones where a equals B survive are the ones where a equals B survive are the ones where a equals B
and the result is expressed on the and the result is expressed on the and the result is expressed on the
screen using direct notation we can also screen using direct notation we can also screen using direct notation we can also
view it as a block Matrix Delta leaves view it as a block Matrix Delta leaves view it as a block Matrix Delta leaves
diagonal entries alone and zeros out off diagonal entries alone and zeros out off diagonal entries alone and zeros out off
diagonal entries and so we're left with diagonal entries and so we're left with diagonal entries and so we're left with
a matrix having ones in the upper left a matrix having ones in the upper left a matrix having ones in the upper left
and lower right hand Corners with all of and lower right hand Corners with all of and lower right hand Corners with all of
their entries being zero and that's their entries being zero and that's their entries being zero and that's
consistent with the first expression for consistent with the first expression for consistent with the first expression for
another example let's consider the another example let's consider the another example let's consider the
completely depolarizing Channel again we completely depolarizing Channel again we completely depolarizing Channel again we
can start by writing down the formula can start by writing down the formula can start by writing down the formula
and just like in the previous example we and just like in the previous example we and just like in the previous example we
don't get any contribution to the sum don't get any contribution to the sum don't get any contribution to the sum
when a is not equal to B and this time when a is not equal to B and this time when a is not equal to B and this time
when a equals B we get the completely when a equals B we get the completely when a equals B we get the completely
mixed state and we can simplify that to mixed state and we can simplify that to mixed state and we can simplify that to
obtain 1/2 * the 2x2 identity Matrix obtain 1/2 * the 2x2 identity Matrix obtain 1/2 * the 2x2 identity Matrix
tensor to itself which is 1/2 * the 4x4 tensor to itself which is 1/2 * the 4x4 tensor to itself which is 1/2 * the 4x4
identity Matrix we can also think about identity Matrix we can also think about identity Matrix we can also think about
it in terms of block matrices and it in terms of block matrices and it in terms of block matrices and
naturally we get the same result and as naturally we get the same result and as naturally we get the same result and as
a last example let's consider the a last example let's consider the a last example let's consider the
identity channel for a single Cubit it's identity channel for a single Cubit it's identity channel for a single Cubit it's
an easy one here's the formula and of an easy one here's the formula and of an easy one here's the formula and of
course the identity Channel doesn't do course the identity Channel doesn't do course the identity Channel doesn't do
anything at all and we can think of the anything at all and we can think of the anything at all and we can think of the
result as being twice the density Matrix result as being twice the density Matrix result as being twice the density Matrix
associated with a f+ state and that's associated with a f+ state and that's associated with a f+ state and that's
consistent with what we expect by consistent with what we expect by consistent with what we expect by
thinking about the choice state of this thinking about the choice state of this thinking about the choice state of this
Channel and here it is as a block Matrix Channel and here it is as a block Matrix Channel and here it is as a block Matrix
notice in particular that the Choy notice in particular that the Choy notice in particular that the Choy
representation of the identity channel representation of the identity channel representation of the identity channel
is not the identity matrix it's this is not the identity matrix it's this is not the identity matrix it's this
Matrix we've now seen three different Matrix we've now seen three different Matrix we've now seen three different
ways to represent channels in ways to represent channels in ways to represent channels in
mathematical terms mathematical terms mathematical terms
namely Stein spring representations namely Stein spring representations namely Stein spring representations
cross representations and Choy cross representations and Choy cross representations and Choy
representations and we also have the representations and we also have the representations and we also have the
definition of a channel which is that a definition of a channel which is that a definition of a channel which is that a
channel is a linear mapping that always channel is a linear mapping that always channel is a linear mapping that always
transforms density matrices into density transforms density matrices into density transforms density matrices into density
matrices even when the channel is matrices even when the channel is matrices even when the channel is
applied to just part of a compound applied to just part of a compound applied to just part of a compound
system a very natural and very important system a very natural and very important system a very natural and very important
question to ask at this point is how do question to ask at this point is how do question to ask at this point is how do
we know these things are all equivalent we know these things are all equivalent we know these things are all equivalent
for instance how do we know that every for instance how do we know that every for instance how do we know that every
mapping described by any one of these mapping described by any one of these mapping described by any one of these
representations is in fact a valid representations is in fact a valid representations is in fact a valid
Channel and how do we know that we're Channel and how do we know that we're Channel and how do we know that we're
not leaving any channels out the not leaving any channels out the not leaving any channels out the
remainder of the lesson describes how remainder of the lesson describes how remainder of the lesson describes how
all of this can be reasoned all of this can be reasoned all of this can be reasoned
mathematically if you're willing to mathematically if you're willing to mathematically if you're willing to
accept the equivalence of these things accept the equivalence of these things accept the equivalence of these things
and you're not interested in how it can and you're not interested in how it can and you're not interested in how it can
be reasoned then you should feel free to be reasoned then you should feel free to be reasoned then you should feel free to
skip the rest of the video but I skip the rest of the video but I skip the rest of the video but I
personally think the argument is pretty personally think the argument is pretty personally think the argument is pretty
slick and it's a good example of a slick and it's a good example of a slick and it's a good example of a
particular style of mathematical proof particular style of mathematical proof particular style of mathematical proof
that comes up from time to time in the that comes up from time to time in the that comes up from time to time in the
theory of quantum information I won't go theory of quantum information I won't go theory of quantum information I won't go
through all of the details some of them through all of the details some of them through all of the details some of them
I'll leave for you or you can find them I'll leave for you or you can find them I'll leave for you or you can find them
in the written content for this lesson in the written content for this lesson in the written content for this lesson
where everything is is written out in where everything is is written out in where everything is is written out in
detail but I do hope to present a detail but I do hope to present a detail but I do hope to present a
reasonably clear picture of how it all reasonably clear picture of how it all reasonably clear picture of how it all
works what we want to do is to prove the works what we want to do is to prove the works what we want to do is to prove the
equivalence of a collection of equivalence of a collection of equivalence of a collection of
statements and the way that we can do statements and the way that we can do statements and the way that we can do
this is to set up a cycle of this is to set up a cycle of this is to set up a cycle of
implications in particular we'll start implications in particular we'll start implications in particular we'll start
with the definition of a channel meaning with the definition of a channel meaning with the definition of a channel meaning
a linear mapping that always transforms a linear mapping that always transforms a linear mapping that always transforms
density matrices into density matrices density matrices into density matrices density matrices into density matrices
from this definition we can reason that from this definition we can reason that from this definition we can reason that
the Choy representation of this channel the Choy representation of this channel the Choy representation of this channel
must satisfy the two conditions we've must satisfy the two conditions we've must satisfy the two conditions we've
already discussed and in fact we've already discussed and in fact we've already discussed and in fact we've
already done this this this was already done this this this was already done this this this was
discussed in the context of the choice discussed in the context of the choice discussed in the context of the choice
state of a channel we then forget all state of a channel we then forget all state of a channel we then forget all
about the definition and we work from about the definition and we work from about the definition and we work from
the Choy representation and given our the Choy representation and given our the Choy representation and given our
two conditions on Choy representations two conditions on Choy representations two conditions on Choy representations
we conclude that there must exist a we conclude that there must exist a we conclude that there must exist a
cross representation of the mapping fi cross representation of the mapping fi cross representation of the mapping fi
we then do an analogous thing we forget we then do an analogous thing we forget we then do an analogous thing we forget
about Choy representations and we work about Choy representations and we work about Choy representations and we work
from an arbitrary cross representation from an arbitrary cross representation from an arbitrary cross representation
and from it we can reason that there and from it we can reason that there and from it we can reason that there
must be a Stein spring representation must be a Stein spring representation must be a Stein spring representation
for the mapping fi and by the way these for the mapping fi and by the way these for the mapping fi and by the way these
arguments will be mechanical in some arguments will be mechanical in some arguments will be mechanical in some
sense which allows us to actually sense which allows us to actually sense which allows us to actually
perform the conversions according to the perform the conversions according to the perform the conversions according to the
arrows for specific channels if we want arrows for specific channels if we want arrows for specific channels if we want
to do that the last implication is to to do that the last implication is to to do that the last implication is to
reason that if we have a Stein spring reason that if we have a Stein spring reason that if we have a Stein spring
representation for a mapping fi then representation for a mapping fi then representation for a mapping fi then
that mapping must in fact be a valid that mapping must in fact be a valid that mapping must in fact be a valid
Channel and again we already did that Channel and again we already did that Channel and again we already did that
this time in the context of Stein spring this time in the context of Stein spring this time in the context of Stein spring
representations altogether these four representations altogether these four representations altogether these four
implications reveal that these four implications reveal that these four implications reveal that these four
things are all equivalent because you things are all equivalent because you things are all equivalent because you
can start from any one of them and can start from any one of them and can start from any one of them and
follow the implications transitively to follow the implications transitively to follow the implications transitively to
get to any of the other ones get to any of the other ones get to any of the other ones
we'll start with the first implication we'll start with the first implication we'll start with the first implication
which is that if we have a channel fi which is that if we have a channel fi which is that if we have a channel fi
meaning a linear map that always meaning a linear map that always meaning a linear map that always
transforms density matrices into density transforms density matrices into density transforms density matrices into density
matrices then it has a Cho matrices then it has a Cho matrices then it has a Cho
representation that satisfies the two representation that satisfies the two representation that satisfies the two
conditions that we identified earlier conditions that we identified earlier conditions that we identified earlier
namely that it's positive semi-definite namely that it's positive semi-definite namely that it's positive semi-definite
and if we Trace out the output system we and if we Trace out the output system we and if we Trace out the output system we
get the identity on the input system and get the identity on the input system and get the identity on the input system and
as I mentioned a moment ago we've as I mentioned a moment ago we've as I mentioned a moment ago we've
already covered this implication in the already covered this implication in the already covered this implication in the
context of choice States in particular context of choice States in particular context of choice States in particular
when we divide the Cho represent ation when we divide the Cho represent ation when we divide the Cho represent ation
of a channel by n the number of of a channel by n the number of of a channel by n the number of
classical states of the input system classical states of the input system classical states of the input system
then we get a density Matrix and then we get a density Matrix and then we get a density Matrix and
moreover the reduced state of the input moreover the reduced state of the input moreover the reduced state of the input
system X for that state is the system X for that state is the system X for that state is the
completely mixed state or in other words completely mixed state or in other words completely mixed state or in other words
the identity Matrix divided by n the identity Matrix divided by n the identity Matrix divided by n
clearing the denominator gives us our clearing the denominator gives us our clearing the denominator gives us our
two conditions so this implication is two conditions so this implication is two conditions so this implication is
pretty quick and easy because we already pretty quick and easy because we already pretty quick and easy because we already
covered covered covered
it now let's take a look at the second it now let's take a look at the second it now let's take a look at the second
implication this time we start with the implication this time we start with the implication this time we start with the
Choy representation of a mapping which Choy representation of a mapping which Choy representation of a mapping which
we assume satis the two requirements and we assume satis the two requirements and we assume satis the two requirements and
our goal is to come up with a cross our goal is to come up with a cross our goal is to come up with a cross
representation for this mapping that representation for this mapping that representation for this mapping that
satisfies the requirement that we talked satisfies the requirement that we talked satisfies the requirement that we talked
about earlier this implication is quite about earlier this implication is quite about earlier this implication is quite
mechanical in a sense we'll need a mechanical in a sense we'll need a mechanical in a sense we'll need a
little bit of linear algebra and it'll little bit of linear algebra and it'll little bit of linear algebra and it'll
mainly come down to manipulating mainly come down to manipulating mainly come down to manipulating
matricies and vectors in a pretty clever matricies and vectors in a pretty clever matricies and vectors in a pretty clever
way it starts with the assumption that way it starts with the assumption that way it starts with the assumption that
the Choy representation is positive the Choy representation is positive the Choy representation is positive
semi-definite which is the first of the semi-definite which is the first of the semi-definite which is the first of the
two requirements that we're assuming are two requirements that we're assuming are two requirements that we're assuming are
in place the notion of a positive in place the notion of a positive in place the notion of a positive
semi-definite Matrix is fundamentally semi-definite Matrix is fundamentally semi-definite Matrix is fundamentally
important in Quantum information and not important in Quantum information and not important in Quantum information and not
just in the context of density just in the context of density just in the context of density
matrices it's absolutely critical here matrices it's absolutely critical here matrices it's absolutely critical here
we wouldn't be able to make this work we wouldn't be able to make this work we wouldn't be able to make this work
without it in particular the assumption without it in particular the assumption without it in particular the assumption
that jfi is positive semi-definite that jfi is positive semi-definite that jfi is positive semi-definite
allows us to express it in the form allows us to express it in the form allows us to express it in the form
shown on the screen for some choice of shown on the screen for some choice of shown on the screen for some choice of
vectors s0 through S N minus one and vectors s0 through S N minus one and vectors s0 through S N minus one and
some choice of a positive integer n and some choice of a positive integer n and some choice of a positive integer n and
let me note explicitly that these are let me note explicitly that these are let me note explicitly that these are
not necessarily unit vectors one way not necessarily unit vectors one way not necessarily unit vectors one way
that we can get our hands on an that we can get our hands on an that we can get our hands on an
Expression like this is from the Expression like this is from the Expression like this is from the
spectral the theorem for positive spectral the theorem for positive spectral the theorem for positive
semi-definite matrices which was semi-definite matrices which was semi-definite matrices which was
discussed in the previous lesson here we discussed in the previous lesson here we discussed in the previous lesson here we
don't see any IG values appearing don't see any IG values appearing don't see any IG values appearing
explicitly like we had in a spectral explicitly like we had in a spectral explicitly like we had in a spectral
decomposition but remember that the IG decomposition but remember that the IG decomposition but remember that the IG
values of positive semi- different values of positive semi- different values of positive semi- different
matrices are always non- negative real matrices are always non- negative real matrices are always non- negative real
numbers and also keep in mind that we're numbers and also keep in mind that we're numbers and also keep in mind that we're
not requiring these vectors S 0 through not requiring these vectors S 0 through not requiring these vectors S 0 through
S N minus one to be unit vectors so what S N minus one to be unit vectors so what S N minus one to be unit vectors so what
we can do is to take the square roots of we can do is to take the square roots of we can do is to take the square roots of
the igen values and multiply them into the igen values and multiply them into the igen values and multiply them into
the unit igen vectors we get from a the unit igen vectors we get from a the unit igen vectors we get from a
spectrally comp position to get these spectrally comp position to get these spectrally comp position to get these
vectors s0 through S N minus1 it's vectors s0 through S N minus1 it's vectors s0 through S N minus1 it's
actually not important for this proof actually not important for this proof actually not important for this proof
that these vectors are orthogonal we do that these vectors are orthogonal we do that these vectors are orthogonal we do
get that from a spectral decomposition get that from a spectral decomposition get that from a spectral decomposition
but we're not going to use that here and but we're not going to use that here and but we're not going to use that here and
if we had some different expression of if we had some different expression of if we had some different expression of
jfi like this that didn't come from the jfi like this that didn't come from the jfi like this that didn't come from the
spectral theorem then that would be just spectral theorem then that would be just spectral theorem then that would be just
fine we now decompose each of these fine we now decompose each of these fine we now decompose each of these
vectors as is shown on the screen and vectors as is shown on the screen and vectors as is shown on the screen and
this is just like we would do if these this is just like we would do if these this is just like we would do if these
vectors were Quantum State vectors of a vectors were Quantum State vectors of a vectors were Quantum State vectors of a
pair of systems XY and and we were pair of systems XY and and we were pair of systems XY and and we were
performing a standard basis measurement performing a standard basis measurement performing a standard basis measurement
on the X system to be clear we're not on the X system to be clear we're not on the X system to be clear we're not
measuring anything here we're just measuring anything here we're just measuring anything here we're just
writing these vectors in this form which writing these vectors in this form which writing these vectors in this form which
is always possible and now here comes is always possible and now here comes is always possible and now here comes
the trick we Define cruss matrices a0 the trick we Define cruss matrices a0 the trick we Define cruss matrices a0
through a n minus one by plucking the K through a n minus one by plucking the K through a n minus one by plucking the K
A's off of the front of these vectors s0 A's off of the front of these vectors s0 A's off of the front of these vectors s0
through S N minus one flipping them through S N minus one flipping them through S N minus one flipping them
around so they become bra A's rather around so they become bra A's rather around so they become bra A's rather
than K A's and sticking them out to the than K A's and sticking them out to the than K A's and sticking them out to the
right hand side as is shown in the right hand side as is shown in the right hand side as is shown in the
equation on the screen one way to think equation on the screen one way to think equation on the screen one way to think
about this is that we're basically about this is that we're basically about this is that we're basically
folding up these vectors to form folding up these vectors to form folding up these vectors to form
matrices so that if we stacked The matrices so that if we stacked The matrices so that if we stacked The
Columns of AK on top of one another we'd Columns of AK on top of one another we'd Columns of AK on top of one another we'd
get back s so these matrices have get back s so these matrices have get back s so these matrices have
exactly the same entries as the exactly the same entries as the exactly the same entries as the
corresponding vectors but we've just corresponding vectors but we've just corresponding vectors but we've just
rearranged the entries differently so rearranged the entries differently so rearranged the entries differently so
that we get matrices but we can also that we get matrices but we can also that we get matrices but we can also
think about these metrices purely think about these metrices purely think about these metrices purely
symbolically in terms of direct notation symbolically in terms of direct notation symbolically in terms of direct notation
as is written here and now at this point as is written here and now at this point as is written here and now at this point
there's an obvious question why should there's an obvious question why should there's an obvious question why should
this work well let's not dwell too much this work well let's not dwell too much this work well let's not dwell too much
on why it works let's just verify that on why it works let's just verify that on why it works let's just verify that
it does to do this let's define a new it does to do this let's define a new it does to do this let's define a new
mapping which is the mapping defined by mapping which is the mapping defined by mapping which is the mapping defined by
these matrices where we're treating them these matrices where we're treating them these matrices where we're treating them
as cross matrices what we can now do is as cross matrices what we can now do is as cross matrices what we can now do is
to compute the Choy representation of to compute the Choy representation of to compute the Choy representation of
this new mapping and we'll see that it's this new mapping and we'll see that it's this new mapping and we'll see that it's
equal to the Choy representation of the equal to the Choy representation of the equal to the Choy representation of the
mapping F that we started with and mapping F that we started with and mapping F that we started with and
because Choy representations are because Choy representations are because Choy representations are
faithful this means that the two faithful this means that the two faithful this means that the two
mappings are in fact equal I won't go mappings are in fact equal I won't go mappings are in fact equal I won't go
through the algebra in this video you through the algebra in this video you through the algebra in this video you
can verify that yourself it's quite can verify that yourself it's quite can verify that yourself it's quite
mechanical and it's good practice and mechanical and it's good practice and mechanical and it's good practice and
everything you need for that is right everything you need for that is right everything you need for that is right
here on the screen we haven't said here on the screen we haven't said here on the screen we haven't said
anything yet about the two second anything yet about the two second anything yet about the two second
conditions in the two boxes but they conditions in the two boxes but they conditions in the two boxes but they
turn out to be equivalent and one way to turn out to be equivalent and one way to turn out to be equivalent and one way to
establish that is to verify the equation establish that is to verify the equation establish that is to verify the equation
displayed here on the bottom of the displayed here on the bottom of the displayed here on the bottom of the
screen the letter T here by the way screen the letter T here by the way screen the letter T here by the way
refers to Matrix transposition which you refers to Matrix transposition which you refers to Matrix transposition which you
can alternatively think about as the can alternatively think about as the can alternatively think about as the
linear mapping that turns k a bra B into linear mapping that turns k a bra B into linear mapping that turns k a bra B into
k b bra a and it's just something that's k b bra a and it's just something that's k b bra a and it's just something that's
needed to make the formula work so in needed to make the formula work so in needed to make the formula work so in
summary there's definitely some algebra summary there's definitely some algebra summary there's definitely some algebra
needed for this implication but like I needed for this implication but like I needed for this implication but like I
said it's all pretty mechanical said it's all pretty mechanical said it's all pretty mechanical
basically index gymnastics as some basically index gymnastics as some basically index gymnastics as some
people refer to this sort of thing but people refer to this sort of thing but people refer to this sort of thing but
if you work through it and you give your if you work through it and you give your if you work through it and you give your
brain a chance to digest it you may find brain a chance to digest it you may find brain a chance to digest it you may find
that you come away with a better that you come away with a better that you come away with a better
understanding of how Choy understanding of how Choy understanding of how Choy
representations and cross representations and cross representations and cross
representations representations representations
relate the third implication allows us relate the third implication allows us relate the third implication allows us
to go from a cross representation of a to go from a cross representation of a to go from a cross representation of a
mapping fi where the usual condition is mapping fi where the usual condition is mapping fi where the usual condition is
in place to a Stein spring in place to a Stein spring in place to a Stein spring
representation this one is pretty representation this one is pretty representation this one is pretty
similar in spirit to the previous similar in spirit to the previous similar in spirit to the previous
implication in the sense that it's implication in the sense that it's implication in the sense that it's
mechanical and it can be viewed and mechanical and it can be viewed and mechanical and it can be viewed and
verified in purely algebraic terms the verified in purely algebraic terms the verified in purely algebraic terms the
idea is actually pretty simple what we idea is actually pretty simple what we idea is actually pretty simple what we
can do is to take our cross matrices and can do is to take our cross matrices and can do is to take our cross matrices and
form part of a larger matrix by stacking form part of a larger matrix by stacking form part of a larger matrix by stacking
them on top of one another like is shown them on top of one another like is shown them on top of one another like is shown
on the screen once it's filled out this on the screen once it's filled out this on the screen once it's filled out this
will give us the unitary Matrix U for a will give us the unitary Matrix U for a will give us the unitary Matrix U for a
Stein spring representation unitary Stein spring representation unitary Stein spring representation unitary
matrices have to be square matrices so matrices have to be square matrices so matrices have to be square matrices so
we'll have to fill out as many remaining we'll have to fill out as many remaining we'll have to fill out as many remaining
columns as we need to do that but it columns as we need to do that but it columns as we need to do that but it
won't actually make any difference what won't actually make any difference what won't actually make any difference what
they are as long as we get a unitary they are as long as we get a unitary they are as long as we get a unitary
Matrix if we momentarily put aside the Matrix if we momentarily put aside the Matrix if we momentarily put aside the
concern that U is unitary and just focus concern that U is unitary and just focus concern that U is unitary and just focus
on the linear mapping that's defined by on the linear mapping that's defined by on the linear mapping that's defined by
the Stein spring representation we get the Stein spring representation we get the Stein spring representation we get
from this Matrix U then what we find is from this Matrix U then what we find is from this Matrix U then what we find is
that the mapping is equal to the that the mapping is equal to the that the mapping is equal to the
original mapping fi defined by the cross original mapping fi defined by the cross original mapping fi defined by the cross
representation that we started with representation that we started with representation that we started with
that's another part of the proof that I that's another part of the proof that I that's another part of the proof that I
won't go through in this video you can won't go through in this video you can won't go through in this video you can
find the details in the written content find the details in the written content find the details in the written content
for this lesson but you may for this lesson but you may for this lesson but you may
alternatively wish to take some time and alternatively wish to take some time and alternatively wish to take some time and
work through it for yourself just keep work through it for yourself just keep work through it for yourself just keep
in mind as usual for the series that in mind as usual for the series that in mind as usual for the series that
we're using kets ordering convention to we're using kets ordering convention to we're using kets ordering convention to
interpret the diagram so w and G will interpret the diagram so w and G will interpret the diagram so w and G will
correspond to the tensor factors on the correspond to the tensor factors on the correspond to the tensor factors on the
left hand side in the left hand side in the left hand side in the
equations and by the way if you do go equations and by the way if you do go equations and by the way if you do go
through it you'll find that because we through it you'll find that because we through it you'll find that because we
have k0o bra zero as the tensor Factor have k0o bra zero as the tensor Factor have k0o bra zero as the tensor Factor
on the left hand side in the equation on the left hand side in the equation on the left hand side in the equation
that we need to verify the only portion that we need to verify the only portion that we need to verify the only portion
of U that has any relevance is the part of U that has any relevance is the part of U that has any relevance is the part
given by the CR maty so as long as we given by the CR maty so as long as we given by the CR maty so as long as we
fill out the columns of you in a way fill out the columns of you in a way fill out the columns of you in a way
that makes it unitary then we're good that makes it unitary then we're good that makes it unitary then we're good
but of course this requires that the but of course this requires that the but of course this requires that the
columns formed by a0 through a n minus columns formed by a0 through a n minus columns formed by a0 through a n minus
one when we stack them on top of one one when we stack them on top of one one when we stack them on top of one
another are orthonormal and this is another are orthonormal and this is another are orthonormal and this is
where the condition that we place on where the condition that we place on where the condition that we place on
Cross representations comes into play Cross representations comes into play Cross representations comes into play
and indeed it's precisely equivalent to and indeed it's precisely equivalent to and indeed it's precisely equivalent to
the columns being the columns being the columns being
orthonormal the relevant mathematical orthonormal the relevant mathematical orthonormal the relevant mathematical
relationships needed to draw this relationships needed to draw this relationships needed to draw this
conclusion are shown down here on the conclusion are shown down here on the conclusion are shown down here on the
bottom of the screen so go ahead and bottom of the screen so go ahead and bottom of the screen so go ahead and
pause and verify if you Choose Or again pause and verify if you Choose Or again pause and verify if you Choose Or again
consult the written content for further consult the written content for further consult the written content for further
details in short what's going on here in details in short what's going on here in details in short what's going on here in
both this implication and in the both this implication and in the both this implication and in the
previous one is that we're describing previous one is that we're describing previous one is that we're describing
the same mathematical operations in the same mathematical operations in the same mathematical operations in
different but equivalent ways the different but equivalent ways the different but equivalent ways the
Beating Heart for all of it essentially Beating Heart for all of it essentially Beating Heart for all of it essentially
is matrix is matrix is matrix
multiplication the fourth and final multiplication the fourth and final multiplication the fourth and final
implication establishes that if we have implication establishes that if we have implication establishes that if we have
a Stein spring representation for some a Stein spring representation for some a Stein spring representation for some
mapping then that mapping must in fact mapping then that mapping must in fact mapping then that mapping must in fact
be a valid Channel and as I indicated be a valid Channel and as I indicated be a valid Channel and as I indicated
earlier we we already covered this earlier we we already covered this earlier we we already covered this
implication when we discussed Stein implication when we discussed Stein implication when we discussed Stein
spring representations in summary and in spring representations in summary and in spring representations in summary and in
slightly different terms introducing an slightly different terms introducing an slightly different terms introducing an
initialized workspace system is a valid initialized workspace system is a valid initialized workspace system is a valid
Channel That's because tensoring a fixed Channel That's because tensoring a fixed Channel That's because tensoring a fixed
density Matrix to a given density Matrix density Matrix to a given density Matrix density Matrix to a given density Matrix
always gives us a density Matrix and always gives us a density Matrix and always gives us a density Matrix and
it's a linear mapping by the bilinearity it's a linear mapping by the bilinearity it's a linear mapping by the bilinearity
of tensor products unitary operations of tensor products unitary operations of tensor products unitary operations
are always valid channels as I discussed are always valid channels as I discussed are always valid channels as I discussed
earlier in the lesson tracing out a earlier in the lesson tracing out a earlier in the lesson tracing out a
system is a valid Channel because the system is a valid Channel because the system is a valid Channel because the
partial Trace is a linear mapping that partial Trace is a linear mapping that partial Trace is a linear mapping that
always transforms density matrices into always transforms density matrices into always transforms density matrices into
density density density
matrices and finally compositions of matrices and finally compositions of matrices and finally compositions of
channels are always channels which is channels are always channels which is channels are always channels which is
quite simple but definitely worth quite simple but definitely worth quite simple but definitely worth
observing in its own right and with the observing in its own right and with the observing in its own right and with the
four implications we just covered we've four implications we just covered we've four implications we just covered we've
established the equivalence of our three established the equivalence of our three established the equivalence of our three
Channel representations along with the Channel representations along with the Channel representations along with the
original definition of a channel in original definition of a channel in original definition of a channel in
particular we started with the particular we started with the particular we started with the
definition of a channel we saw how that definition of a channel we saw how that definition of a channel we saw how that
definition implies that its Cho definition implies that its Cho definition implies that its Cho
representation necessarily satisfies two representation necessarily satisfies two representation necessarily satisfies two
basic conditions then we we saw how basic conditions then we we saw how basic conditions then we we saw how
those conditions allow us to find a those conditions allow us to find a those conditions allow us to find a
Krauss representation which in turn Krauss representation which in turn Krauss representation which in turn
allows us to find a Stein spring allows us to find a Stein spring allows us to find a Stein spring
representation and finally we saw that representation and finally we saw that representation and finally we saw that
the existance of a Stein spring the existance of a Stein spring the existance of a Stein spring
representation reveals that we do in representation reveals that we do in representation reveals that we do in
fact have a valid channel of course fact have a valid channel of course fact have a valid channel of course
there are technical details in there and there are technical details in there and there are technical details in there and
it's not necessarily simple especially it's not necessarily simple especially it's not necessarily simple especially
when you're seeing it for the first time when you're seeing it for the first time when you're seeing it for the first time
but what's interesting and noteworthy but what's interesting and noteworthy but what's interesting and noteworthy
here is that these implications are set here is that these implications are set here is that these implications are set
up in a way that does in fact make up in a way that does in fact make up in a way that does in fact make
things as easy as possible and in things as easy as possible and in things as easy as possible and in
mathematics you should never feel feel mathematics you should never feel feel mathematics you should never feel feel
any shame in taking the easy way out for any shame in taking the easy way out for any shame in taking the easy way out for
example it's not at all obvious that example it's not at all obvious that example it's not at all obvious that
every channel has a Stein spring every channel has a Stein spring every channel has a Stein spring
representation or a crow representation representation or a crow representation representation or a crow representation
but it is relatively easy to conclude but it is relatively easy to conclude but it is relatively easy to conclude
that the Choy representation of every that the Choy representation of every that the Choy representation of every
channel satisfies the two conditions channel satisfies the two conditions channel satisfies the two conditions
that we've associated with it and that that we've associated with it and that that we've associated with it and that
allows us to follow a more or less allows us to follow a more or less allows us to follow a more or less
algebraic path to the other two algebraic path to the other two algebraic path to the other two
representations in a similar vein the representations in a similar vein the representations in a similar vein the
last implication from a Stein spring last implication from a Stein spring last implication from a Stein spring
representation back to the definition is representation back to the definition is representation back to the definition is
also relatively straightforward and it also relatively straightforward and it also relatively straightforward and it
allows us to conclude that we only get allows us to conclude that we only get allows us to conclude that we only get
channels and no other mappings from Choy channels and no other mappings from Choy channels and no other mappings from Choy
representations assuming that the representations assuming that the representations assuming that the
required conditions are met and as I required conditions are met and as I required conditions are met and as I
alluded to before the way that we argue alluded to before the way that we argue alluded to before the way that we argue
these implications are sufficiently these implications are sufficiently these implications are sufficiently
mechanical to allow us to easily convert mechanical to allow us to easily convert mechanical to allow us to easily convert
between the representations and if you between the representations and if you between the representations and if you
go back to some of the examples that we go back to some of the examples that we go back to some of the examples that we
covered earlier in the lesson you'll covered earlier in the lesson you'll covered earlier in the lesson you'll
find that it is in fact possible to find that it is in fact possible to find that it is in fact possible to
convert between the representations convert between the representations convert between the representations
sometimes quite easily and sometimes quite easily and sometimes quite easily and
directly and that is the end of the directly and that is the end of the directly and that is the end of the
lesson which has been all about channels lesson which has been all about channels lesson which has been all about channels
we talked about what they are and we we talked about what they are and we we talked about what they are and we
took a look at some basic examples we took a look at some basic examples we took a look at some basic examples we
discussed three ways that channels can discussed three ways that channels can discussed three ways that channels can
be represented and we dove into some be represented and we dove into some be represented and we dove into some
technical details that establish the technical details that establish the technical details that establish the
equivalence of these representations I equivalence of these representations I equivalence of these representations I
hope you'll join me for the next lesson hope you'll join me for the next lesson hope you'll join me for the next lesson
which is all about measurements in the which is all about measurements in the which is all about measurements in the
general formulation of quantum general formulation of quantum general formulation of quantum
information goodbye until then

## Quantum Circuits ｜ Understanding Quantum Information & Computation ｜ Lesson 03

- Welcome back to Understanding
Quantum Information and Computation. My name is John Watrous and I'm the technical director
for IBM Quantum Education. This is the third lesson of the series. In the first two lessons, we talked about the basics
of quantum information. First for single systems in isolation and then for multiple systems. We also discussed standard
basis measurements and unitary operations in both the single and
multiple system settings. In this lesson, we'll introduce
the quantum circuit model, which gives us a standard way to describe quantum computations, and we can also use quantum circuits to describe quantum protocols and other sorts of interactions involving multiple quantum devices. In addition to quantum circuits, we'll also discuss some
important mathematical concepts connected with quantum information, and we'll use them to observe
a few fundamental limitations on quantum information and what we can do with quantum circuits. That'll set us up well for
the lesson following this one where we'll see a few
very important examples and that will conclude the
first unit of the series on the basics of quantum information. Here's an overview of the lesson. We'll start with the quantum circuit model and we'll see how it works. Then we'll talk about the
critically important notion of an inner product, as well as orthonormality and projections, which are connected with inner products. We didn't discuss these things
in the first two lessons, because we really didn't
need to, but now we do. Inner products provide us
with a mathematical tool that we simply could not do without. We just would not get far in understanding quantum information and
computation without inner products. And in the third part of the lesson, we'll take a look at a
few important limitations of quantum information. Ultimately, what we want to do is to explore interesting
things that we can do with quantum information, but in order to do this effectively, we need to be aware of some
things that we can't do. In particular, we'll discuss
the notion of a global phase and why global phases are irrelevant. We'll see a statement of
the no-cloning theorem, which tells us that quantum states can't be copied in general, and we'll see that two quantum states can't be perfectly
discriminated from one another unless they're orthogonal, which is a fundamental
limitation to be aware of. The first topic for the lesson
is the quantum circuit model, which is a model of computation
that provides us with a way to describe quantum computations. The quantum circuit model is
based on a more general notion in computer science of a circuit, and you can define many
different models of computation based on this general notion. The basic idea is that a
circuit is composed of wires which carry information, and gates, which represent operations that are applied to the
information carried by the wires. The term circuit is actually
kind of a strange name to use for this sort of a model
because the word circuit usually refers to a
circular path of some sort. But when we talk about circuits
as models of computation, we're usually talking
about acyclic circuits where circular paths are not allowed. To be more precise, for the
purposes of this series, we're going to demand that
information always flows from left to right in any circuit. So if you prefer, you
can think about circuits as describing sequences of operations where time goes from left to right. Here's an example of a Boolean circuit where the wires carry binary
values, either zero or one, and the gates represent
Boolean logic operations. The wires are represented by lines, and here we have two input
bits labeled X and Y, and if we want to know
what the circuit does on a particular choice of inputs, we propagate the values on
the wires from left to right, depending on the actions
that the gates have on the values that are
carried by the wires. So for instance, we can
take a look at what happens when we start out with the top input wire set to the value one and the bottom input wire
set to the value zero. These two input wires lead
into these two dots right here, which represent fan-out operations, which means that the value is copied so that it can be carried
forward along multiple wires. So if we do this, then these
two wires take the value one, which is the same as this one, and these two wires take the value zero, which is the same as this one. Here we have a NOT gate which
is indicated by the symbol for logical negation, so the
value that comes out is a zero, which is the logical negation of one. And similarly, the value
that comes out of this gate is a one which is the
logical negation of zero. These two gates are AND gates,
this gate is an OR gate, and if we continue on
and propagate the values through the circuit, evaluating
the gates appropriately, then we see that the final value
on this wire here is a one. So the entire circuit describes
a sequence of operations that are composed as the figure suggests, and we end up with a function
from two bits to one bit in this particular case. But in general, there could be any number
of input and output bits. This particular circuit happens to compute the exclusive OR of the two input bits, so the output is a one
if and only if exactly one of the two input bits is set to a one and the other one is set to a zero, but that's not really
important, it's just an example. It's a basic fact about Boolean circuits that any function from any number of bits to any number of bits can be computed by some Boolean circuit, although sometimes you might
need a very large circuit to compute the function
that you're interested in. There are other styles that
are used to describe circuits. For example, this figure
represents exactly the same circuit as we had before, using a style that's perhaps more typical in electrical engineering, where there are conventions for the shapes that represent different gates. We aren't going to use
this particular style, but we will introduce
some symbols of our own for different quantum gates, such as Hadamard gates,
Controlled NOT gates, Toffoli gates, and so on. Just as an aside, here we can see the origin
of the term circuit. In the context of electrical engineering, it's very natural to
think about electricity cycling through a circuit. But in this series we're not
talking about electricity moving through circuits, we're talking about information
moving through circuits. And once again for us, they will always be acyclic, where the information
moves from left to right. Here's a different example of
a different type of circuit called an arithmetic circuit. This time the wires store numbers and the gates represent
arithmetic operations such as addition and multiplication. Here you can see an example
of an arithmetic circuit. It's not important for the sake
of this lesson what it does, it's just an example that's
meant to convey the idea that there are different
notions of circuits. We can also think about
circuits involving randomness, for instance, where the gates represent
probabilistic operations, and if you felt like it, you could probably sit around all day dreaming up new models of computation based on the notion of circuits. In the quantum circuit model,
the wires represent Qubits, and the gates represent both unitary operations and measurements. Here's a very simple example
of a quantum circuit. There's just one Qubit in this circuit, which is represented
by the horizontal line, and the gates, which are
indicated by squares, represent unitary operations that are performed on this Qubit. So there are no measurements
in this particular example. If we want to give the
Qubit a name such as X, we can indicate that in the diagram by putting an X right
here, that's optional. Just like we had for the Boolean circuit from a few moments ago, we can think about this circuit
as representing a sequence of operations where time
goes from left to right. So we start out with X,
we apply the first gate, which is a Hadamard gate
followed by an S gate, then another Hadamard
gate, and finally a T gate. We saw these operations
toward the end of lesson one, and here you can see
their matrix descriptions. So when we perform these
four operations in order, which means first H, then
S, then H again, and then T, the combined operation is given
by this product right here, keeping in mind that the order
essentially gets reversed as we talked about in lesson one. If we perform the matrix multiplication, we get this matrix right here, and so the action of the
circuit on the Qubit X is described by this unitary matrix. Sometimes we'd like to indicate explicitly in a quantum circuit diagram
what input we're feeding in, and we can do that as you see right here. In this case, we're inputting
Ket 0 into the circuit. That's pretty common. We often take Ket 0 to
be the initialized state of a Qubit, but that's not necessary. We could also consider
applying the circuit to a different state. If we choose to, we can also
indicate the output state that we get like this. Here's another example where
this time we have two Qubits named X and Y as you
can see in the diagram. Here we have a Hadamard gate, which is being applied to the Qubit Y, followed by the gate that's indicated by
this symbol right here, which is a Controlled NOT gate. We talked about the
Controlled NOT operation in the previous lesson, and here in this diagram we
can see that the control Qubit is Y, which is indicated by
the small filled in circle, and the target Qubit is X, which is indicated by the large circle with what looks like a plus sign in it. It kind of looks like the
symbol for the exclusive OR so it makes sense. You can imagine that whatever
classical state is stored in Y gets XOR'd onto the Qubit X. And just in the interest of clarity, here's the diagram that
shows the action of this gate on standard basis states, where we assume the A
and B are binary values. For other states, the action is determined
by linearity as always. Now, before we go any further, it will be helpful to
explicitly state a convention that we're going to follow concerning how Qubits are ordered in quantum circuit diagrams like this one. And the convention is that
bottom to top ordering in a quantum circuit diagram is equivalent to left to right ordering as in a tuple or a tensor product. That's a convention we'll
follow in the series and it's also the convention
that's used in Qiskit. So for this particular circuit right here, we're thinking about it as
acting on a pair of Qubits X, Y, X comes first because it's on the bottom, and Y comes last or second in this case, because it's on the top. The way you can think about
this visually, in case it helps, is to imagine rotating
this circuit clockwise so that the inputs are on the top and the outputs are on the bottom, and the ordering of the
Qubits will be correct. Sometimes this ordering trips people up, so just be sure to keep
this convention in mind. So when we think about the action that this circuit describes, what we have first is a
Hadamard gate applied to Y and then a Controlled NOT
gate on the two Qubits with Y being the control
and X being the target. If we wanna write down a unitary matrix that describes the combined
action of these two gates, we can do that. First, we consider the Hadamard gate on Y. We're applying the Hadamard gate to Y and doing nothing to X, which is equivalent to
performing an identity operation on X. And so the action of the Hadamard gate on both Qubits together is
given by the tensor product of the identity matrix and
the matrix representing the Hadamard operation on Y. That looks like this, keeping in mind our
convention that X comes first and Y comes second, so it's
the identity tensored with H. Then we apply the
Controlled NOT operation, which has this matrix
representation, and again, that's consistent with our convention. The second Qubit Y is the control and the first Qubit X is the target. Don't be afraid to pause the video and verify that this makes sense and I'll stop reminding you about that. You can pause these videos, go back and watch parts
as many times as you want, adjust the playback speed if you choose, use them in whatever
way works best for you. Anyway, when we compose the
two operations together, the combined action is described
by this product right here, and these two matrices are
multiplied in this order because the Hadamard is performed first and the Controlled NOT
is performed second, and once again, this is consistent
with what we talked about in lesson one where the ordering
essentially gets reversed. If we perform the matrix multiplication, we get this matrix right here. And we can now read off from this matrix what the circuit does to
standard basis states. If we run the circuit on the input 0, 0, meaning Ket 0 for the first Qubit, Ket 0 for the second Qubit, the result is the Phi+ Bell state, which we can see from the
first column of the matrix. If we feed in 0, 1, we get the Phi- state, one zero gives us the Si+ state, and 1, 1 gives us not Psi-, but -1 X Psi-. The minus sign isn't
really significant here. If you didn't want this to be here, you could perform a swap
operation on the two Qubits to get rid of it and you
wouldn't mess up what happened to the first three standard basis states. But it's really not important
for the sake of this example. So in summary, the circuit has this action right here on standard basis states. When you're thinking about
what a particular circuit does, you don't need to go through
the conversion to matrices and multiply them together, you can also trace through the
action that the circuit has on particular states, and in many cases this will
be the preferred option. In this example, for instance, we can see what the circuit
does to the 0, 0 state by examining the action of the gates. The initial state, which
we've named Pi zero here, just to avoid a name
that would be confusing with the Bell states is 0, 0. We apply the Hadamard gate and the state we get is 0+ which we can expand out like this, and then the Controlled NOT operation takes this state to the Phi+ state, which we can see by thinking
about the action that the Controlled NOT has on
standard basis states, And you can do the same
thing for the other three standard basis states or any
other state for that matter. We can also incorporate classical bits into quantum circuit diagrams. They're indicated by
double lines like you see in this diagram. That is A and B are classical bits and X and Y are Qubits. We have the same two unitary gates as in the previous example, and these symbols right here represent standard basis measurements Specifically after the two
unitary operations are performed, a standard basis measurement
is performed on Y and the result is overwritten
to the classical bit, B, and similarly X is measured and the result is written to the bit A. Here you can see that the Qubits X and Y continue on after they're measured. It's pretty common
though that we don't care all that much about Qubits
after they're measured. Sometimes all we care about
is the measurement result. And in a case like that, we can use the shorthand
style that you see here where measurements have Qubits coming in and classical bits coming out. The understanding here is that
once the Qubits are measured, they're just discarded or ignored and the classical measurement
outcome is carried forward. Now as the series continues, we'll be seeing many
examples of quantum circuits and they'll typically
be much more complicated than the very simple
examples that we just saw. As I mentioned earlier, different symbols are
used for different gates. Single cubic gates are usually
just written as squares with a letter indicating
which operation it is. Sometimes these different
gates are color coded to mean different things, but
we'll ignore that for now. We've already seen the
Controlled NOT gate. Swap gates are denoted
as you see right here, And Toffoli gates and Fredkin gates are denoted as you see right here. Remember that a Toffoli gate is a controlled Controlled NOT gate and a Fredkin gate is
a controlled swap gate. So these symbols are
consistent with the idea that a control Qubit is
indicated by a solid circle with a line connecting it
to the gate it controls. In general, controlled Qubits
don't need to be on the top and these gates don't have
to act on neighboring Qubits, meaning that these gates
can be stretched across other Qubits that they don't act on, and we'll see that
happening in future lessons. One more thing that we often do when using the quantum circuit model is to view arbitrary unitary operations on any number of Qubits
as if they're single gates like you see right here. Just by drawing a rectangle with a label corresponding
to whatever unitary operation it is. In this case it's some
unitary operation U. Of course, we don't know
what U is from this diagram, we have to assume that
U is defined somehow or that we're considering some arbitrary unitary operation U, perhaps. It could be for example that
we just wanna think about U as if it's a subroutine in
the interest of clarity. We also use this notation
that you see right here for a controlled U operation
where the control Qubit is indicated by a solid circle
just like we had before. So that's a brief introduction
to the quantum circuit model. And as I said before, we will see many more
examples of quantum circuits as we continue on in the series. In the next part of the lesson, we're going to take a
look at inner products, orthonormality and projections, which are very important notions in quantum information and computation. We'll start with inner products. Remember that when we
use the direct notation, a Ket is a column vector
and a Bra is a row vector, and specifically the
Bra vector corresponding to a given column vector is the conjugate transpose
of that column vector. So for this vector right here, we obtain the corresponding Bra vector by transposing the vector
as if it's a matrix, which amounts to laying it on its side and then taking the complex conjugate of each of the entries. Now, suppose that we have two Kets with the same number of entries like this. If we form this bracket right here, which simply means that
we take the product of the Bra and the Ket,
as if they're matrices, then we obtain this expression
that you see right here, which is a scaler, or in
other words a complex number. This value is called the inner
product of these two vectors. You can also think about inner products in terms of vectors of the
form that you see right here where the vectors are expressed
as linear combinations of standard basis vectors. The inner product of these vectors is defined in the same way as
before, Bra Psi times Ket Phi which we can express
as a product like this. And if we compute and simplify, we're left with this expression right here for the inner product, which is analogous to what we had before, except this time we're
taking the sum over elements in our classical state set, which is sigma in this particular case. Here's an example that's meant to give you a geometric picture for
how inner products work. Here I've drawn a circle
in a two-dimensional space corresponding to a Qubit, and as long as we restrict our attention to vectors having real number entries, we can draw them as points on this plane. Let's take Phi to be
this vector right here. It's a unit vector, which we can see by
computing the Euclidean norm and that's what this circle represents, it's all of the unit vectors
having real number entries. Let's also say Psi to be
this vector right here, which again is also a unit vector. It's the minus state in fact, but here we're just calling it Psi. If we compute the inner
product of these two vectors, we get one minus the square root of three over two times root two, which happens to be approximately -1/4. The value of this inner product is equal to the cosine of the angle
between the two vectors, which you can see here in the figure. That's something that's
generally true for unit vectors. If we have two unit vectors and they both have real number entries, then the inner product between them is equal to the cosine of
the angle between them. In particular, if the inner
product is equal to zero, like in this example, where we've changed Psi to
be this state right here, then the two vectors must
be at a 90 degree angle to one another. This all generalizes to
vectors in any dimension, by the way, as long as the
entries are real numbers, it's not just for two dimensional vectors. If we have complex number entries, the picture doesn't really
work anymore, but nevertheless, the inner product is every
bit as useful as we will see. The inner product has some
very important properties and we'll take a few moments to observe some of those properties now. First is a connection
between the inner product and the Euclidean norm. If we take the inner product
of any vector with itself, we get the Euclidean norm squared. So we can alternatively say that the Euclidean norm of a vector is equal to the square
root of the inner product of that vector with itself. This isn't a coincidence, this
is a fundamental connection, and here we can see that if we didn't have the complex conjugates in the definition of the inner product, then this formula
wouldn't be true anymore, and we wouldn't have such a useful notion with this fundamental connection
to the Euclidean norm. Next, the inner product
is conjugate symmetric, which means that if we swap
the order of the two arguments, then the value we get for the
inner product gets conjugated. That's expressed by
this equation right here and is quite straightforward to see why this happens for these two expressions for the inner product of the two vectors in the two possible orders. The inner product is linear
in the second argument. Specifically, if we define a vector Phi to be this linear
combination of two vectors, Phi one and Phi two, then the inner product of
any vector Psi with Phi is the same linear combination
of the two inner products, and this follows quite
directly from the fact that matrix multiplication is linear. And we also have that the inner
product is conjugate linear in the first argument, which you can see expressed right here. If Psi is this linear
combination of vectors, then if we take the inner
product of Psi and Phi, then Bra Psi becomes this vector that we see right here where we take the complex
conjugate of beta one and beta two, and so this is the result and that's what's meant
by conjugate linearity. And the last property of the inner product that I wanna mention is
the incredibly useful Cauchy-Schwarz inequality. For any two vectors, Psi and Phi, the absolute value of the inner product can never be larger than the
product of the Euclidean norms of the two vectors. And moreover, the only way that the two
sides of the inequality can be equal is if the two
vectors are linearly dependent, which means that one of them is equal to some scaler times the other. This is in fact one of the
most widely used inequality in all of mathematics and
we use it over and over when we're talking about
quantum information, so it's very good to know about. Next, we'll talk about the
notions of orthogonality and orthonormality which are
connected with inner products and specifically, with
inner products of vectors being equal to zero. To be precise, if we have
two vectors, Psi and Phi, we say that they're orthogonal
if their inner product is equal to zero. And the way that we can think
about orthogonal vectors intuitively is that they're
at right angles to each other, or alternatively, that they're
perpendicular to one another. People sometimes use the term orthogonal to refer to things that have
nothing to do with each other, like saying that one issue
is orthogonal to another, but that's obviously pretty informal and not really meaningful
in a mathematical sense, but the connection is clear if you think about orthogonal
vectors as pointing not in opposite directions, but in completely different directions like north versus west for instance. If we have a set of vectors, we say that the set is an orthogonal set if any two distinct vectors
chosen from that set are necessarily orthogonal to one another, which we can express
as you see right here, An orthonormal set is
simply an orthogonal set of unit vectors. Vectors in orthogonal
sets don't necessarily need to be unit vectors, and in fact you could
even have zero vector being an element of an orthogonal set, but in an orthonormal set, every vector must be a unit vector in addition to the
requirement that the set is an orthogonal set. Given what we know about the relationship between the inner product
and the Euclidean norm, we can express this
condition pretty succinctly as you see right here. And finally, an orthonormal basis is simply an orthonormal
set that forms a basis for whatever vector space
we're talking about. For example, if we take any
classical state set sigma and we think about the set of
all standard basis vectors, we see that this is an orthonormal set and in fact it's an orthonormal basis for the space of all vectors whose entries correspond to
this classical state set sigma. These are all unit vectors and
they're orthogonal in pairs, so it's an orthonormal set. and because it's a basis as well, we have that it's an orthonormal basis. Orthonormal sets are
always linearly independent by the way, so anytime you
have an orthonormal set, it will always be an orthonormal
basis for some subspace, but in this case, we have a basis for the entire space that we're working with. Another example is the set
containing Ket+ and Ket-. These two vectors are orthogonal, their inner product is equal to zero. and they're both unit vectors and they span the
two-dimensional space of vector corresponding to a single Qubit, so they form an orthonormal
basis for that space. Another example is the Bell basis, which is an orthonormal basis for the space of all
four dimensional vectors corresponding to two Qubits. The set containing Ket 0
and Ket+ on the other hand, is not an orthogonal set because the inner product between
these vectors is non-zero. They're both unit vectors and this is a basis for the vector space corresponding to a Qubit, but we don't have orthogonality. Here's a handy fact about orthonormal sets that we'll make use of from time to time. Suppose that we have some
orthonormal set of vectors, specifically, let's suppose
that we have M vectors in this set, Psi one through Psi M, where we're assuming that these vectors are drawn from some N
dimensional vector space, or in other words, each of
the vectors has M entries. As I already mentioned, orthonormal sets are always
linearly independent, so it can't be that M is larger than N, but it could be that M is smaller than N, and that's the case that
we're interested in. And what the fact states is
that if M is smaller than N, then it's always possible
to add additional vectors to the set we started with in order to obtain an
orthonormal basis for our space. To be precise, there must
necessarily exist vectors Psi M+1 up to Psi N so that the entire collection
forms an orthonormal basis. So you can never paint yourself
into a corner, so to speak, by somehow choosing an
orthonormal set of vectors that can't be extended to
form an orthonormal basis. Sometimes you can just look
at the set you start with and figure out how to extend it to form an orthonormal basis, but an alternative that always works is to use a procedure known as the Gram-Schmidt
orthonormalization process to construct these vectors. I won't explain that
procedure in this lesson or in this series because the specifics aren't
really important for us, but of course you can look
it up if you're curious. It's something that can
be performed by a computer in case that's relevant, so
there's no creativity involved, it's just a mechanical procedure. One of the reasons why this
is such a handy fact to have is concerned with a close connection that exists between orthonormal
bases and unitary matrices. In particular, for any square matrix U, the three conditions that you see here are always equivalent, meaning that they're either
all true or they're all false. The first condition says
that U is a unitary matrix, which means that both U dagger times U and U times U dagger are both
equal to the identity matrix. The second condition
says that the rows of U form an orthonormal basis
and the third condition says that the columns of U
form an orthonormal basis. It's actually not too hard
to see why this equivalence is true. Just to keep things simple, let's take a look at a
three by three matrix U as you see right here, where the entries can be any
complex numbers you like. Here on the left hand side I've written U conjugate transposed
just for convenience. If we form three vectors
from the columns of U, which here are named Psi
one, Psi two and Psi three, then if we take the
product of U dagger and U, you can check that each
entry of the product is given by an inner product of vectors as we see right here in this pattern. And now when you think about the condition that U is unitary, which is equivalent to this matrix being equal to the identity matrix, you see that it's equivalent
to the three vectors forming an orthonormal set. The diagonal entries all
have to be equal to one, so these are necessarily unit vectors, and the off diagonal
entries have to be zero, so it's an orthogonal set of vectors. So U being unitary is equivalent to the columns forming an orthonormal set, And that has to be a basis because the number of vectors in the set, which is three in this case, agrees with the dimension of the space that we're working in. You can follow a similar argument for the rows instead of the columns to conclude that U being
unitary is equivalent to the rows farming an orthonormal basis. It doesn't work quite as nicely because we have to make an adjustment to get the complex conjugates
to work in the right way, but the idea is the same. An alternative way to go about it, which is pretty straightforward, is to prove that a matrix is unitary if and only if it's transposed as unitary. And here I do mean to say transpose and not conjugate transpose. It's pretty trivial from the
definition of unitary matrices, that the conjugate transpose
of any unitary matrix is unitary, but it's also the case that the transpose of any unitary matrix is unitary. In any case, we have the
equivalence between these three conditions on any square matrix U, and we can combine this equivalence with the fact about
extending orthonormal sets to form orthonormal bases in order to come up with unitary matrices that do interesting things
or fulfill various needs that we may encounter. Specifically anytime we
have an orthonormal set of M vectors, there will always exist a unitary matrix U whose first M columns are the vectors from that orthonormal set. The remaining columns
can always be filled in so that we obtain a unitary matrix by choosing them so that the columns form an orthonormal basis. This will be a pretty
handy fact for us to have at our disposal. And one more topic for
this section of the lesson is the notion of a projection. A square matrix Pi is
said to be a projection or a projection matrix if the two properties that
are listed here are both true. This is a capital Pi by the way, it's a pretty typical choice for a letter that's used to denote a projection. We often give these typical sorts of names to objects such as matrices just to make them easier to recognize and keep things straight, but you could use whatever name you like. Anyway, the first condition is that Pi is equal to its own conjugate transpose. In general, when we have a matrix that's equal to its own
conjugate transpose, we say that it's a Hermitian matrix. So the first condition can alternatively be stated as Pi is Hermitian. The second condition is
that if we square Pi, meaning that we multiply Pi
to itself, we get Pi again. So squaring Pi doesn't do anything to it. The idea of a projection
intuitively speaking is that it's similar in
spirit to projecting an image onto a screen or dropping
a perpendicular onto a line for instance. When you multiply a
vector by a projection, what you're doing is in some sense leaving a part of it alone
and zeroing out the rest. So if you apply a
projection a second time, it doesn't do anything because
you've already zeroed out the part that the projection zeroes out. An example of a projection
is this matrix right here for any choice of a unit vector Psi. We can check that it satisfies
the required conditions one at a time. For the first one, we compute
the conjugate transpose and this is the conjugate
transpose of a product. And so we can use this formula right here for the conjugate transpose
of a product to help us. What you get is the product
of the conjugate transposes, but the order gets reversed. That happens because of
the transposition part. So if we apply that formula, we obtain this equality right here and we can then simplify to
get back the original matrix. Notice by the way, that we didn't use the fact
that Psi is a unit vector. So in fact, this matrix is
Hermitian for any vector Psi, not just for unit vectors. And for the second one, we square Pi and we obtain
this expression right here and that is equal to the
original matrix this time because Psi is a unit vector, so this inner product
right here is equal to one and that's where the assumption
that Psi is a unit vector comes into play. We can generalize this example and start with an orthonormal set. And if we form this matrix right here, where we sum up the projections corresponding to the individual vectors, then we obtain a projection. Here on the screen, you can see calculations that reveal that and I'll leave it to you to
examine them if you wish. And along similar lines
to the first example, it's the second condition where the orthonormality
of the set gets used. What this projection does if
you multiply it to a vector, is to project that
vector onto the subspace that's spanned by this set of vectors, which in an abstract way is
like dropping a perpendicular to that subspace, and once
you're in that subspace, the projection doesn't do anything. So matrices like this are
examples of projections, and now in fact we've exhausted
all of the possibilities. Every projection matrix takes this form for some choice of an orthonormal set. The zero matrix is a projection, so we do have to make
sure we include that one if we want to get every projection. So just to be clear about this, we are allowing the
possibility that M equals zero and that gives us an empty sum, which we can interpret as the zero matrix. Now that we have the
concept of a projection at our disposal, we can define a more general
class of quantum measurements known as projective measurements. This isn't the most general
notion of a measurement, but it is significantly more general than standard basis measurements and it's going to be very
helpful to incorporate it into our description
of quantum information. A projective measurement is defined by any set of projections
that satisfies the condition that you see right here, which is that the projections
sum to the identity matrix. Here M can be any
positive integer you like so long as these are all projections and this condition is true, the collection defines a
projective measurement. The measurement described
by such a collection works as follows when you
measure any quantum state Psi. First you get an outcome
between one and M, where M is the number of projections. For each possible outcome, K, the probability to get that outcome is the Euclidean norm
squared of the vector you get by multiplying S by Pi
K, the Kth projection. We can also write this
quantity as you see right here. This equality comes from the relationship between the Euclidean
norm and the inner product together with the fact
that Pi K is a projection. From the second form, together with the condition that we have on these projections, we can see right away, by linearity, that our probabilities
will always sum to one, keeping in mind that Psi
has to be a unit vector. And second, assuming that
we get the outcome, K, the state of the system
collapses to the quantum state we get by normalizing whatever
vector we got by projecting. We'll take a look at some
examples in just a moment, but before we do that, let me mention that we don't actually need to number the possible
outcomes from one to M. We can choose whatever names or symbols we want for the outcomes. In particular, if we have a collection of projections that are labeled with symbols
from sum set gamma say, and they satisfy the
same condition as before, which is that they sum
to the identity matrix, then we have a projective measurement that works in exactly
the same way as before. The only only difference here is that we're allowing
the measurement outcomes to be elements of this set gamma as opposed to the integers from one to M. For our first example of
a projective measurement, let's observe that
standard basis measurements are projective measurements. In this case the set of
outcomes of the measurement is the same as the set of classical states of whatever system is being measured. And the set of projections
that describes the measurement is this collection right here, assuming that sigma is
the classical state set of our system. These are all projections
and if we sum them up, we get the identity matrix, so it's a valid projective measurement To see how this works in
a little bit more detail, let's suppose that we
have some arbitrary state that's written in the form
that you see right here. The probability for the
measurement to yield the outcome A, according to the description
from a moment ago, is this value right here, which is equal to the
absolute value squared of the corresponding
entry of the vector Psi, which is what we expect. Conditioned on the outcome being A, the state of the system becomes this, again, according to the
description from a moment ago. If we simplify, we don't
quite get the state we expect, which is Ket A. Instead we get Ket A
multiplied by a scaler, and specifically, this is a scaler having
absolute value equal to one, so it's a complex number
on the unit circle. This is called a global phase and we'll discuss global phases shortly, but for now in short, this
state is equivalent to Ket A. This global phase doesn't
actually have any significance at all and we'll see why
when we get to the discussion of global phases. So (unclear) this issue of global phases, which we'll talk about in just a moment, we see that standard basis measurements are in fact projective measurements. For another example, which is really an extension
of the previous example, if we perform a standard basis
measurement on a system X and we do nothing to a system Y, just like we talked about
in the previous lesson, it's completely equivalent
to measuring the system X, Y. With this projective
measurement right here. We have these projections corresponding to the classical states of X and we're tensoring with the identity on Y which reflects the fact that
we're not doing anything to Y. The description for how
projective measurements work in general tells us
what happens in this example, and I'll leave it to you to
check that this is in fact completely equivalent to what we discussed in the previous lesson. This is all very nice because this projective
measurement description is cleaner in a way and we see that it fits into
this more general picture. Here's one more example of
a projective measurement. This one is a measurement of two Qubits. For this measurement
we have two projections and therefore, two possible
outcomes which are zero and one. The projections are defined
in terms of the Bell states as you see right here. The first projection is
onto the space spanned by the three symmetric Bell states and the second projection is
onto the one dimensional space spanned by the Psi- state, which is the anti-symmetric one, or in other words the
one that gets multiplied by a minus sign if we
perform a swap operation on the two Qubits. We'll see later on in the series why this is an interesting measurement. For now, it's just an example
that connects with something that we've seen before,
which is the Bell basis. One final note on projective measurements is that anytime you have
a projective measurement, you can always implement that measurement using unitary operations and
standard basis measurements. For this particular example of
a measurement, for instance, we can build a quantum circuit that implements the measurement
as you see right here. We imagine that the top
two Qubits are the Qubits that we're measuring with
this projective measurement and what we can do is
introduce a new Qubit that starts off initialized
to the zero state. We perform a Hadamard gate
followed by a Fredkin gate, followed by another Hadamard gate, and if we measure this bottom Qubit using a standard basis measurement, the effect will be exactly the same as if we've measured the top two Qubits with the projective
measurement from this example. We'll get exactly the same probabilities for the two outcomes and the effect that the measurement has on the top two Qubits
will be the same as well. That's not at all obvious. An analysis is required to see that and that analysis isn't shown here. As I've already suggested, we will take a closer look at
this particular measurement in a later lesson and for now
it's just meant as an example. The important point to take away is that something like
this is always possible for any projective measurement, so we don't really gain
any additional capabilities by introducing projective measurements, but it is a very useful concept and we will often speak
of projective measurements as we continue on in the series. The last part of this lesson is on a few important limitations
of quantum information and what we can do with quantum circuits. The first limitation, which maybe isn't so much of a limitation as it is degeneracy, concerns the issue of global phases that I mentioned a short time ago. Here's the definition of what we mean when we refer to a global phase. If we have two quantum
state vectors, Psi and Phi, such that one of them is
equal to the other one multiplied by some complex number alpha, then we say that the states
differ by a global phase or alternatively that they're equivalent up to a global phase. Notice that because quantum state vectors have Euclidean norm equal to one, the only way that this
equation can possibly be true is if the absolute value
of this complex number is equal to one, so alpha
must lie on the unit circle. Let's imagine that we have
two quantum states like this that differ by a global phase and suppose that we measure the state Phi with a standard basis measurement. The probability to get any outcome A is given by this expression right here, and if we substitute
alpha times Psi for Phi, which we can do because we're
assuming that they're equal, we obtain this probability. And if we simplify using the fact that alpha lies in the
complex unit circle, we obtain this probability. Remember that this is the probability that measuring Phi gives the outcome A, but now we see that
this is exactly the same as the probability that measuring
Psi gives the outcome A, so there's absolutely
no difference whatsoever in the probabilities of different outcomes for the two states. A similar argument tells us more generally that for any projective measurement, the probabilities are again identical. So by measuring we can't
tell the difference between two states that
differ by a global phase, even in a statistical sense. Finally, if we apply any unitary
operation to the state Phi, then again by substituting, what we will get is exactly
the same complex number alpha multiplied to whatever we will get by applying the same unitary
operation to the state, Psi. So the two states will still
differ by a global phase. The bottom line is that the two states are completely indistinguishable, so there's really no
difference between them at all in terms of what we could
possibly learn about them from performing unitary
operations and measurements and for this reason, we consider these two
states to be equivalent or if you prefer, that these are just two possibly different vector representations of
the same quantum state. Like I said at the
start of the discussion, it's basically degeneracy
in how quantum states are described by quantum state vectors. In any case, what this means is that we can
simply ignore global phases just like we did effectively when we describe standard
basis measurements as projective measurements. By the way, this degeneracy only happens in the simplified description
of quantum information. When we describe quantum
states using density matrices, as we will do in the
third unit of the series, we will have a unique
matrix representation for every quantum state. The issue of global phases
simply doesn't arise in the general description
of quantum information. It's one more appealing aspect
of the general description of quantum information,
which I keep alluding to, but regardless, global
phases won't bother us because we can just exercise
our freedom to ignore them. Here's a very quick
example just to make sure the notion of a global phase is clear. If we have a minus state, and negative one times a minus state, then these two states
differ by a global phase, and in particular we have
alpha equals negative one. On the other hand, if we have a plus state and a minus state, then we don't have two states
that differ by a global phase. These are orthogonal vectors, so they're linearly independent. To say that two quantum state vectors differ by a global phase
is equivalent to saying that the vectors are linearly dependent, so certainly these vectors
don't differ by a global phase. It's true that the only difference is that this plus sign
turns into this minus sign, but that's what we call a
relative phase difference. That's different from a global phase because we're not
multiplying the entire vector by a complex number. And this is consistent with the fact that we can tell the difference
between these states, in particular, if we
apply a Hadamard operation and then measure, we'll see a zero if the
state was a plus state and a one if the state was a minus state. The next limitation of quantum information that we'll see in this lesson
is the no-cloning theorem. Here's one statement of
the no-cloning theorem and this particular
statement of the theorem is pretty concrete and specific, which will make it easier to talk about. There are other more general ways to formulate the no-cloning theorem, but we'll focus on this one for now. We assume that we have two systems X and Y which have the same classical state set. which consists of the
integer from zero to D-1 where D is at least two. It's not really important what names the classical states have, but by choosing these names it'll help to keep things simple. What's most important
here is that there are at least two different classical
states, so in other words, we're not worrying about the
trivial and uninteresting case where we only have one classical state. What the theorem says is that there's no unitary operation U that we can apply to X, Y so that if we initialize
Y to the zero state, we effectively copy or clone
arbitrary quantum states of X. If we take a look at this
expression right here in isolation, what it says is that for all choices of a quantum state vector Psi of X, applying U to Psi tensored
with the zero state, gives us Psi tensor Psi, or in other words two
independent copies of Psi, so that's what it would mean if we could clone quantum
states and what the theorem says is that this is not possible. You can think about this
in terms of circuits and what the theorem says is
that you'll never be able to come up with a gate or a circuit
that works as you see here. Where one copy of the state Psi comes in along with a collection
of initialized Qubits and two copies of Psi are produced. You can do this for certain
choices of the state Psi, but you can never come up with
a circuit that works this way for every quantum state Psi, and that's what the theorem says. It's quite easy to prove
this theorem, in fact, it essentially boils down to the fact that the transformation we're
looking for isn't linear, but unitary operations are
linear, so it just can't work. Here's an argument in a bit more detail. Suppose that we have a unitary operation U that clones the standard basis
states, Ket 0 and Ket one. We know that the classical
states zero and one must belong to our classical state set because D is at least two. But this means by linearity
that if X is in a plus state, which we can write like this, then the result is this linear combination of these two states right here. However, that's not the right behavior. The requirement on U is that
U cloned is a plus state, which means that we have to
have this equation right here being satisfied. These two states aren't the
same and so it doesn't work and that's all there is to it. Just a few additional remarks
about the no-cloning theorem. First, this statement of the
no-cloning theorem is absolute in some sense. It simply says that a unitary operation U that works exactly like
this doesn't exist. So you might ask whether
or not there exists unitary operation U that
does this approximately with respect to some way of describing an approximation to the statement. There are different
notions of approximation that one might be interested in and we'll see a couple
of them in the third unit of the series, but for these different
notions of approximation, there are quantitative statements
of the no-cloning theorem that put strong limits on how well you can approximately clone. A second remark is that copying or cloning standard basis states most
certainly is possible. For example, a Controlled NOT gate copies the standard basis states of a Qubit. This doesn't contradict
the no-cloning theorem. The point is that cloning in the sense of the no-cloning theorem requires
that this works not only for standard basis states
but for all states. A Controlled NOT gate
certainly doesn't clone a plus state for instance. And finally, it is worth pointing out that there's also a no-cloning theorem for
probabilistic states. It's just not something
that people talk about or care about. If somebody randomly
chooses a bit, for instance, for some arbitrary way of
assigning probabilities to the two values and
then hands you that bit, of course there's no way for you to obtain two independent samples
of that random bit. You just don't know what
the probabilities are. You can copy the bit just like a Controlled NOT
copies standard basis states, but that's not the same thing as cloning the probabilistic state. You just don't have enough
information to do that. Whatever random bit you were given doesn't tell you what
the probabilities were. It's just a bit. The impossibility of cloning
a probabilistic state follows from exactly the same reasoning as in the quantum setting. Cloning is a non-linear process, whereas probabilistic operations, just like unitary operations, are linear. Anyway, it's not surprising
that we can't clone on a probabilistic state and
somehow it's not as interesting as the fact that we can't
clone a quantum state, but the point is simply to observe that this is not a uniquely
quantum phenomenon. The final limitation
of quantum information that we'll discuss in this lesson is that it's not possible
to perfectly discriminate two quantum states that aren't orthogonal. Another way of saying this
is that if two quantum states can be perfectly discriminated, then they have to be orthogonal. What we mean when we refer
to a perfect discrimination of two quantum states, Psi and Phi, is that there's some
way to tell them apart without any chance of error. In terms of the quantum circuit model, we can express this as you
see here on the screen. We have some unitary operation U, which could be implemented
as a quantum circuit or we could just think about U abstractly as a unitary operation
and force input U takes either the state Psi or the state Phi along with a bunch of initialized Qubits, which you can think of as workspace Qubits that somehow assist in this
process of discrimination. After U as applied, the top Qubit is measured with respect to a standard basis measurement and here's the perfect
discrimination part. If the input state was Psi, then the measurement must
give the outcome zero, 100% of the time, and if
the input state was Phi, then the measurement must
give the outcome one, again with 100% chance. It's not important what the
states of these other Qubits are after U was applied. All that matters is this one bit that we get from the measurement because that's the bit that's telling us which of the two states we started with. For some choices of states, Psi and Phi, it will be possible to perform a perfect discrimination like
this and for others it won't. And what we'll observe is that
if it is possible to do this, then Psi and Phi have
to be orthogonal states and that means if Phi and
Psi are not orthogonal, you can't perfectly discriminate them. So let's see why a perfect discrimination implies orthogonality. We'll start by writing down two equations that express what these
two diagrams express. If we think about the quantum state, we get after applying U to a
bunch of initialized Qubits together with the state Psi, we see that it has to
have this form right here, where Pi zero is some quantum state vector representing the state
of all of these Qubits except for the top one. We don't have any information
about this quantum state, Pi zero, it could be anything, but we know that the entire
state has to look like this because that's the only
way that the measurement is guaranteed to give the
outcome zero, 100% of the time. And we have an analogous
equation for the second one. This time the top Qubit
has to be in the one state and similar to Pi zero, the
state Pi one could be anything. And in addition to having
no information about Pi one, we also don't know anything
about the relationship between Pi zero and Pi one. For all we know, these are just two
arbitrary quantum states. What we can do next is
to multiply both sides of these equations on
both sides by U dagger, meaning the conjugate transpose of U. And if we do that, we get the two equations
that you see right here. We're using the fact that U dagger times U is the identity matrix. So that's why we don't see U or U dagger on the left hand side of either equation. And now we can consider the inner product between Psi and Phi. It's the same as the inner
product of Psi and Phi multiplied to the inner
product of the L zero state with itself because the inner product of
any state with itself is one and that includes the L zero state. And now if we look at our two equations, we see that this expression here is equal to the inner product of the vectors represented by the two equations. And in particular it's the inner product between the left hand
sides of these equations. So we can equally well
express this inner product as the inner product of the vectors represented by the right hand
sides of these equations, because the left hand side is
equal to the right hand side for both equations. So we get this expression right here, which is the inner product
of the two right hand sides of the equations or equivalently, it's the conjugate transpose of the first multiplied to the second. U times U dagger is equal
to the identity matrix, because U is unitary and so it goes away. And we can simplify just a little bit to get the product of
these inner products. Now remember that we don't know anything about the relationship
between Pi zero and Pi one, so we don't know what
their inner product is. That's a mystery to us. But whatever it is, we're multiplying it to the inner product of Ket 0 and Ket one, which is zero. So the end result is that
we obtain the value zero, but that's what we wanted to show, the inner product between
Psi and Phi is equal to zero. So the two states are orthogonal. So don't bother trying
to perfectly discriminate two non-orthogonal quantum
states, you just can't do it. There is a converse to the
statement that we just proved, which is that orthogonal quantum states always can be perfectly discriminated, and this is not too hard to
see if we make use of a fact that I mentioned earlier in the lesson. Specifically if Psi and
Phi are orthogonal states, then one thing that we can do is to choose any unitary matrix U whose first two columns
are the two states. We know that it's always
possible to do this and it doesn't matter
for the sake of this task exactly how this unitary matrix is chosen. All that's important is that these are the first two columns. We don't need any additional Qubits. If we simply apply U to
the two possible states, then we'll get these
two possible outcomes. And if we measure the top most Qubit, which remember is the right most Qubit, then we'll get a zero or a one depending on which state we started with. Alternatively, we can consider
a projective measurement with outcomes zero to one
defined as you see right here. in this case, the measurement
actually only depends on Psi. This measurement perfectly
discriminates Psi from any state that's orthogonal to Psi. If we measure Psi with respect to this projective measurement, then we'll get the outcome
zero with probability one as these equations review. And for any state Phi
that's orthogonal to Psi, we'll get the outcome
one with probability one. I won't go through the
details of these calculations, but if you're interested, I encourage you to go through
the details on your own. And that is the end of lesson three. We covered a lot of
material in this lesson, including quantum circuits, inner products and orthonormality, projections and projective measurements, the fact that global phases don't matter, the no-cloning theorem, and the fact that non-orthogonal states can't be perfectly discriminated while orthogonal states can. I hope you'll join me for
the fourth lesson soon, which is the last lesson in
the first unit of the series. In the fourth lesson, we'll talk about three
very important examples, teleportation, superdense coding, and the CHSH inequality or CHSH game as it's sometimes described. Goodbye until then,

## Quantum Error Correction ｜ Understanding Quantum Information & Computation ｜ Lesson 15

welcome back to understanding Quantum welcome back to understanding Quantum
information and computation my name is information and computation my name is information and computation my name is
John watus and I'm the technical John watus and I'm the technical John watus and I'm the technical
director for education at IBM director for education at IBM director for education at IBM
Quantum this is lesson number 15 of the Quantum this is lesson number 15 of the Quantum this is lesson number 15 of the
series and it's the third lesson of the series and it's the third lesson of the series and it's the third lesson of the
fourth unit which is on Quantum error fourth unit which is on Quantum error fourth unit which is on Quantum error
correction thus far we've seen a few correction thus far we've seen a few correction thus far we've seen a few
examples of quantum error correcting examples of quantum error correcting examples of quantum error correcting
codes including the nine Cubit Shore codes including the nine Cubit Shore codes including the nine Cubit Shore
code the 7 Cubit Steen code and the 5 code the 7 Cubit Steen code and the 5 code the 7 Cubit Steen code and the 5
Cubit code and we've also discussed the Cubit code and we've also discussed the Cubit code and we've also discussed the
stabilizer formalism which provid stabilizer formalism which provid stabilizer formalism which provid
provides us with a mathematical provides us with a mathematical provides us with a mathematical
Foundation from which we can describe Foundation from which we can describe Foundation from which we can describe
and analyze these and many other Quantum and analyze these and many other Quantum and analyze these and many other Quantum
error correcting error correcting error correcting
codes the codes that I just mentioned codes the codes that I just mentioned codes the codes that I just mentioned
are undoubtedly interesting and they are undoubtedly interesting and they are undoubtedly interesting and they
represent a natural place to begin an represent a natural place to begin an represent a natural place to begin an
exploration of quantum error exploration of quantum error exploration of quantum error
correction the problem is that they can correction the problem is that they can correction the problem is that they can
only tolerate a very low error rate only tolerate a very low error rate only tolerate a very low error rate
correcting an error on one cubit out of correcting an error on one cubit out of correcting an error on one cubit out of
five seven or nine isn't bad but in all five seven or nine isn't bad but in all five seven or nine isn't bad but in all
likelihood at least as far as we suspect likelihood at least as far as we suspect likelihood at least as far as we suspect
we're going to need to be able to we're going to need to be able to we're going to need to be able to
tolerate a lot more errors than that in tolerate a lot more errors than that in tolerate a lot more errors than that in
order to make large-scale Quantum order to make large-scale Quantum order to make large-scale Quantum
Computing a Computing a Computing a
reality in this lesson we'll take a reality in this lesson we'll take a reality in this lesson we'll take a
first look at some more sophisticated first look at some more sophisticated first look at some more sophisticated
Quantum error correcting code Quantum error correcting code Quantum error correcting code
constructions including codes that can constructions including codes that can constructions including codes that can
tolerate a much higher error rate than tolerate a much higher error rate than tolerate a much higher error rate than
the ones we've seen so far and are the ones we've seen so far and are the ones we've seen so far and are
viewed as promising candidates for viewed as promising candidates for viewed as promising candidates for
practical Quantum error correction in practical Quantum error correction in practical Quantum error correction in
actual physical actual physical actual physical
devices here's a brief overview of the devices here's a brief overview of the devices here's a brief overview of the
lesson we'll begin with a class of lesson we'll begin with a class of lesson we'll begin with a class of
quantum error correcting codes known as quantum error correcting codes known as quantum error correcting codes known as
CSS codes CSS codes CSS codes
named for the researchers that first named for the researchers that first named for the researchers that first
discovered them Robert calderbank Peter discovered them Robert calderbank Peter discovered them Robert calderbank Peter
Shaw and Andrew Shaw and Andrew Shaw and Andrew
Steen the CSS code construction allows Steen the CSS code construction allows Steen the CSS code construction allows
one to take certain pairs of classical one to take certain pairs of classical one to take certain pairs of classical
error correcting codes and combine them error correcting codes and combine them error correcting codes and combine them
into a Quantum error correcting into a Quantum error correcting into a Quantum error correcting
code as a part of that discussion we'll code as a part of that discussion we'll code as a part of that discussion we'll
take a pretty quick look at classical take a pretty quick look at classical take a pretty quick look at classical
linear codes and we'll also talk about linear codes and we'll also talk about linear codes and we'll also talk about
some of the basic properties of CSS some of the basic properties of CSS some of the basic properties of CSS
codes the second part of the lesson is codes the second part of the lesson is codes the second part of the lesson is
on a code known know as the Torah code on a code known know as the Torah code on a code known know as the Torah code
which is a fundamental and really which is a fundamental and really which is a fundamental and really
beautiful example of a Quantum error beautiful example of a Quantum error beautiful example of a Quantum error
correcting code that can tolerate a correcting code that can tolerate a correcting code that can tolerate a
relatively High error rate in fact the relatively High error rate in fact the relatively High error rate in fact the
Torah code isn't a single example of a Torah code isn't a single example of a Torah code isn't a single example of a
Quantum error correcting code but rather Quantum error correcting code but rather Quantum error correcting code but rather
it's an infinite family of codes one for it's an infinite family of codes one for it's an infinite family of codes one for
each positive integer starting from each positive integer starting from each positive integer starting from
Two and in the last part of the lesson Two and in the last part of the lesson Two and in the last part of the lesson
we'll briefly discuss a couple of other we'll briefly discuss a couple of other we'll briefly discuss a couple of other
families of quantum codes including families of quantum codes including families of quantum codes including
surface codes which are closely surface codes which are closely surface codes which are closely
connected to the torque code as well as connected to the torque code as well as connected to the torque code as well as
color codes color codes color codes
people have been studying classical people have been studying classical people have been studying classical
error correcting codes since the 1940s error correcting codes since the 1940s error correcting codes since the 1940s
and it's a fascinating and Rich and it's a fascinating and Rich and it's a fascinating and Rich
subject linear codes are perhaps the subject linear codes are perhaps the subject linear codes are perhaps the
simplest type of classical error simplest type of classical error simplest type of classical error
correcting code and I'll explain exactly correcting code and I'll explain exactly correcting code and I'll explain exactly
what the word linear means in this what the word linear means in this what the word linear means in this
context in just a moment but one simple context in just a moment but one simple context in just a moment but one simple
way to express what they are is that way to express what they are is that way to express what they are is that
they're stabilizer codes that happen to they're stabilizer codes that happen to they're stabilizer codes that happen to
be classical and we'll see shortly that be classical and we'll see shortly that be classical and we'll see shortly that
CSS codes are basically pairs of CSS codes are basically pairs of CSS codes are basically pairs of
classical linear codes that are combined classical linear codes that are combined classical linear codes that are combined
together to create a Quantum error together to create a Quantum error together to create a Quantum error
correcting code so for the sake of that correcting code so for the sake of that correcting code so for the sake of that
discussion we're going to need to discussion we're going to need to discussion we're going to need to
understand a few basic things about understand a few basic things about understand a few basic things about
classical linear classical linear classical linear
codes first we're going to take Sigma to codes first we're going to take Sigma to codes first we're going to take Sigma to
be the binary alphabet for this entire be the binary alphabet for this entire be the binary alphabet for this entire
discussion and now when we refer to a discussion and now when we refer to a discussion and now when we refer to a
classical linear code we mean a set C of classical linear code we mean a set C of classical linear code we mean a set C of
binary strings of length n for some binary strings of length n for some binary strings of length n for some
positive integer n satisfying just one positive integer n satisfying just one positive integer n satisfying just one
basic property which is that if u and v basic property which is that if u and v basic property which is that if u and v
are binary strings in C then the string are binary strings in C then the string are binary strings in C then the string
uxx ORV is also in uxx ORV is also in uxx ORV is also in
C here by the way uxx ORV means the C here by the way uxx ORV means the C here by the way uxx ORV means the
bitwise exclusive or of u and v just bitwise exclusive or of u and v just bitwise exclusive or of u and v just
like we saw multiple times back in like we saw multiple times back in like we saw multiple times back in
lesson five and six in the quantum lesson five and six in the quantum lesson five and six in the quantum
algorithms portion of the algorithms portion of the algorithms portion of the
series and this is what it means for a series and this is what it means for a series and this is what it means for a
classical code to be linear in essence classical code to be linear in essence classical code to be linear in essence
we're thinking about binary strings of we're thinking about binary strings of we're thinking about binary strings of
length n as being n-dimensional vectors length n as being n-dimensional vectors length n as being n-dimensional vectors
where the entries are all either zero or where the entries are all either zero or where the entries are all either zero or
one but instead of ordinary vector one but instead of ordinary vector one but instead of ordinary vector
addition over the real or complex addition over the real or complex addition over the real or complex
numbers we're using addition modulo 2 numbers we're using addition modulo 2 numbers we're using addition modulo 2
which is simply the exor so if we have which is simply the exor so if we have which is simply the exor so if we have
two code words u and v meaning that u two code words u and v meaning that u two code words u and v meaning that u
and v are binary strings in C then U and v are binary strings in C then U and v are binary strings in C then U
plus v modulo 2 or in other words uux or plus v modulo 2 or in other words uux or plus v modulo 2 or in other words uux or
V must also be a code word in C and by V must also be a code word in C and by V must also be a code word in C and by
the way this implication has to be true the way this implication has to be true the way this implication has to be true
even if U is equal to V nothing rules even if U is equal to V nothing rules even if U is equal to V nothing rules
out that possibility and that implies out that possibility and that implies out that possibility and that implies
that every classical linear code must that every classical linear code must that every classical linear code must
contain the all zero string because the contain the all zero string because the contain the all zero string because the
bitwise exor of any string with itself bitwise exor of any string with itself bitwise exor of any string with itself
is the all zero is the all zero is the all zero
string for example the three-bit string for example the three-bit string for example the three-bit
repetition code is a classical linear repetition code is a classical linear repetition code is a classical linear
code in particular there are two code in particular there are two code in particular there are two
possible choices for U and two possible possible choices for U and two possible possible choices for U and two possible
choices for V and it's a pretty trivial choices for V and it's a pretty trivial choices for V and it's a pretty trivial
matter to go through the possible pairs matter to go through the possible pairs matter to go through the possible pairs
to see that we always get a code word to see that we always get a code word to see that we always get a code word
when we take the bitwise when we take the bitwise when we take the bitwise
exor here's another example which is exor here's another example which is exor here's another example which is
called the 743 Hamming code this is one called the 743 Hamming code this is one called the 743 Hamming code this is one
of the very first classical error of the very first classical error of the very first classical error
correcting codes that was ever correcting codes that was ever correcting codes that was ever
discovered and it consists of 16 binary discovered and it consists of 16 binary discovered and it consists of 16 binary
strings of length strings of length strings of length
seven there is a very simple logic seven there is a very simple logic seven there is a very simple logic
behind the selection of these strings behind the selection of these strings behind the selection of these strings
but I won't explain that in this video but I won't explain that in this video but I won't explain that in this video
and I should also mention that you will and I should also mention that you will and I should also mention that you will
sometimes see these strings reversed but sometimes see these strings reversed but sometimes see these strings reversed but
for now the main point is that if we for now the main point is that if we for now the main point is that if we
take any two of these strings and exor take any two of these strings and exor take any two of these strings and exor
them together we'll get another string them together we'll get another string them together we'll get another string
in the in the in the
code so it's a linear code and the code so it's a linear code and the code so it's a linear code and the
notation 743 in single square brackets notation 743 in single square brackets notation 743 in single square brackets
means something analogous to the double means something analogous to the double means something analogous to the double
square bracket not a I mentioned in the square bracket not a I mentioned in the square bracket not a I mentioned in the
previous lesson for stabilizer codes but previous lesson for stabilizer codes but previous lesson for stabilizer codes but
here it's for classical linear here it's for classical linear here it's for classical linear
codes in particular code words have codes in particular code words have codes in particular code words have
seven bits There are 16 code words so we seven bits There are 16 code words so we seven bits There are 16 code words so we
can encode four bits using the code and can encode four bits using the code and can encode four bits using the code and
it happens to be a distance three code it happens to be a distance three code it happens to be a distance three code
meaning that you have to flip at least meaning that you have to flip at least meaning that you have to flip at least
three bits of any one code word to get a three bits of any one code word to get a three bits of any one code word to get a
different code word and that implies different code word and that implies different code word and that implies
that it can correct for up to one bit that it can correct for up to one bit that it can correct for up to one bit
flip error these are simple examples of flip error these are simple examples of flip error these are simple examples of
codes but even the example of the 74 codes but even the example of the 74 codes but even the example of the 74
three Heming code looks kind of three Heming code looks kind of three Heming code looks kind of
mysterious when we simply list all the mysterious when we simply list all the mysterious when we simply list all the
code words so we need better ways of code words so we need better ways of code words so we need better ways of
describing classical linear codes and describing classical linear codes and describing classical linear codes and
there are two natural ways of doing there are two natural ways of doing there are two natural ways of doing
this one way is to describe classical this one way is to describe classical this one way is to describe classical
linear codes by giving a minimal set of linear codes by giving a minimal set of linear codes by giving a minimal set of
code words that generate the code code words that generate the code code words that generate the code
meaning that by taking all the possible meaning that by taking all the possible meaning that by taking all the possible
subsets of these code words and xoring subsets of these code words and xoring subsets of these code words and xoring
them together we get the entire code and them together we get the entire code and them together we get the entire code and
that's expressed in mathematical terms that's expressed in mathematical terms that's expressed in mathematical terms
here on the here on the here on the
screen another way to think about this screen another way to think about this screen another way to think about this
is that we're viewing this code as a is that we're viewing this code as a is that we're viewing this code as a
Subspace of a vector space and this set Subspace of a vector space and this set Subspace of a vector space and this set
of generators is a basis for that of generators is a basis for that of generators is a basis for that
Subspace keeping in mind that this is a Subspace keeping in mind that this is a Subspace keeping in mind that this is a
vector space where arithmetic is done vector space where arithmetic is done vector space where arithmetic is done
modulo 2 if we have M generators we're modulo 2 if we have M generators we're modulo 2 if we have M generators we're
going to end up with a code having two going to end up with a code having two going to end up with a code having two
to the m to the m to the m
elements each generator doubles the elements each generator doubles the elements each generator doubles the
number of elements in the code and number of elements in the code and number of elements in the code and
that's consistent with the fact that that's consistent with the fact that that's consistent with the fact that
classical linear codes over the binary classical linear codes over the binary classical linear codes over the binary
alphabet always include a number of alphabet always include a number of alphabet always include a number of
strings that's a power of strings that's a power of strings that's a power of
two the second natural way to describe a two the second natural way to describe a two the second natural way to describe a
classical linear code is by parity classical linear code is by parity classical linear code is by parity
checks specifically this is a minimal checks specifically this is a minimal checks specifically this is a minimal
list of binary strings having the same list of binary strings having the same list of binary strings having the same
length as the strings in the code such length as the strings in the code such length as the strings in the code such
that the strings in the code are that the strings in the code are that the strings in the code are
precisely the ones whose binary precisely the ones whose binary precisely the ones whose binary
dotproduct with every one of these dotproduct with every one of these dotproduct with every one of these
parity check strings is zero where the parity check strings is zero where the parity check strings is zero where the
binary do product is simply the ordinary binary do product is simply the ordinary binary do product is simply the ordinary
inner product modulo 2 and again that's inner product modulo 2 and again that's inner product modulo 2 and again that's
something that we encountered back in something that we encountered back in something that we encountered back in
lesson five in the context of query lesson five in the context of query lesson five in the context of query
algorithms and the reason they're called algorithms and the reason they're called algorithms and the reason they're called
parity check strings is that a given parity check strings is that a given parity check strings is that a given
string U has binary product equal to string U has binary product equal to string U has binary product equal to
zero with v if and only if the bits of U zero with v if and only if the bits of U zero with v if and only if the bits of U
corresponding to the ones in V have even corresponding to the ones in V have even corresponding to the ones in V have even
parity when we describe a classical parity when we describe a classical parity when we describe a classical
linear code in this way by a minimal linear code in this way by a minimal linear code in this way by a minimal
list of parity check strings each parity list of parity check strings each parity list of parity check strings each parity
check string divides the number of check string divides the number of check string divides the number of
strings in The Code by two so for a strings in The Code by two so for a strings in The Code by two so for a
single code described in these two single code described in these two single code described in these two
different ways we'll always have m + Ral different ways we'll always have m + Ral different ways we'll always have m + Ral
n n n
one important thing to notice here is one important thing to notice here is one important thing to notice here is
that the binary dotproduct is not an that the binary dotproduct is not an that the binary dotproduct is not an
inner product in a formal sense and in inner product in a formal sense and in inner product in a formal sense and in
particular when two strings have binary particular when two strings have binary particular when two strings have binary
dotproduct equal to zero it doesn't mean dotproduct equal to zero it doesn't mean dotproduct equal to zero it doesn't mean
that they're orthogonal in the usual way that they're orthogonal in the usual way that they're orthogonal in the usual way
that we think about that we think about that we think about
orthogonality for example the binary do orthogonality for example the binary do orthogonality for example the binary do
product of the string one one with product of the string one one with product of the string one one with
itself is zero so it is possible that itself is zero so it is possible that itself is zero so it is possible that
one of these parity check strings is one of these parity check strings is one of these parity check strings is
itself in the codee so that's just itself in the codee so that's just itself in the codee so that's just
something to keep in something to keep in something to keep in
mind for example as we already know the mind for example as we already know the mind for example as we already know the
3-bit competition code is a linear code 3-bit competition code is a linear code 3-bit competition code is a linear code
so it can be described in both of these so it can be described in both of these so it can be described in both of these
ways in particular there's only one ways in particular there's only one ways in particular there's only one
choice for a generator that works and choice for a generator that works and choice for a generator that works and
that's that's that's
111 and we can alternatively describe 111 and we can alternatively describe 111 and we can alternatively describe
the code with these two parody check the code with these two parody check the code with these two parody check
strings which should look familiar from strings which should look familiar from strings which should look familiar from
our previous discussions of this our previous discussions of this our previous discussions of this
code and for the 743 hemming code we can code and for the 743 hemming code we can code and for the 743 hemming code we can
use these generators and these parody use these generators and these parody use these generators and these parody
checks and here by the way all of our checks and here by the way all of our checks and here by the way all of our
parody check strings turn out to parody check strings turn out to parody check strings turn out to
themselves be in the code code a final comment about classical code a final comment about classical
linear codes just to connect them to the linear codes just to connect them to the linear codes just to connect them to the
stabilizer formalism these parody check stabilizer formalism these parody check stabilizer formalism these parody check
strings are equivalent to stabilizer strings are equivalent to stabilizer strings are equivalent to stabilizer
generators where we only have Z and generators where we only have Z and generators where we only have Z and
identity polym identity polym identity polym
matrices for example the parity check matrices for example the parity check matrices for example the parity check
strings 11 0 and 011 for the 3-bit strings 11 0 and 011 for the 3-bit strings 11 0 and 011 for the 3-bit
repetition code correspond precisely to repetition code correspond precisely to repetition code correspond precisely to
the stabilizer generators ZZ identity the stabilizer generators ZZ identity the stabilizer generators ZZ identity
and identity ZZ and that's consistent and identity ZZ and that's consistent and identity ZZ and that's consistent
with our discussion of observables of with our discussion of observables of with our discussion of observables of
this sort from the previous this sort from the previous this sort from the previous
lesson next we'll discuss CSS codes lesson next we'll discuss CSS codes lesson next we'll discuss CSS codes
which are stabilizer codes that can be which are stabilizer codes that can be which are stabilizer codes that can be
obtained by combining together certain obtained by combining together certain obtained by combining together certain
pairs of classical linear codes having pairs of classical linear codes having pairs of classical linear codes having
the same number of bits in their the same number of bits in their the same number of bits in their
encodings this doesn't work for any two encodings this doesn't work for any two encodings this doesn't work for any two
classical linear codes the two codes classical linear codes the two codes classical linear codes the two codes
have to have a certain relationship as have to have a certain relationship as have to have a certain relationship as
we'll see shortly but nevertheless this we'll see shortly but nevertheless this we'll see shortly but nevertheless this
construction does open up a lot of construction does open up a lot of construction does open up a lot of
possibilities for Quantum error possibilities for Quantum error possibilities for Quantum error
correcting codes based in part on things correcting codes based in part on things correcting codes based in part on things
that we've learned from over 75 years of that we've learned from over 75 years of that we've learned from over 75 years of
classical coding classical coding classical coding
Theory I've already mentioned that in Theory I've already mentioned that in Theory I've already mentioned that in
the stabilizer formalism stabilizer the stabilizer formalism stabilizer the stabilizer formalism stabilizer
generators containing only Z and generators containing only Z and generators containing only Z and
identity poly matrices are equivalent to identity poly matrices are equivalent to identity poly matrices are equivalent to
parity parity parity
checks for example in the three-bit checks for example in the three-bit checks for example in the three-bit
repetition code we can use the parody repetition code we can use the parody repetition code we can use the parody
check strings 1 1 0 and 011 which check strings 1 1 0 and 011 which check strings 1 1 0 and 011 which
enforce that the leftmost two bits are enforce that the leftmost two bits are enforce that the leftmost two bits are
equal and the rightmost two bits are equal and the rightmost two bits are equal and the rightmost two bits are
equal for every string in the code and equal for every string in the code and equal for every string in the code and
these two parity check strings these two parity check strings these two parity check strings
correspond precisely to the stabilizer correspond precisely to the stabilizer correspond precisely to the stabilizer
generator's ZZ identity and identity generator's ZZ identity and identity generator's ZZ identity and identity
ZZ for the 743 Heming code we can use ZZ for the 743 Heming code we can use ZZ for the 743 Heming code we can use
these parody check strings and they these parody check strings and they these parody check strings and they
correspond to these stabilizer correspond to these stabilizer correspond to these stabilizer
generators where we simply replace each generators where we simply replace each generators where we simply replace each
one by a z and each Zero by the identity one by a z and each Zero by the identity one by a z and each Zero by the identity
you might recognize these stabilizer you might recognize these stabilizer you might recognize these stabilizer
generators by the way because these are generators by the way because these are generators by the way because these are
three of the six stabilizer generators three of the six stabilizer generators three of the six stabilizer generators
for the 7 Cubit steam code that's for the 7 Cubit steam code that's for the 7 Cubit steam code that's
certainly not a coincidence and we'll certainly not a coincidence and we'll certainly not a coincidence and we'll
see exactly what's going on here see exactly what's going on here see exactly what's going on here
shortly we're going to give the name Z shortly we're going to give the name Z shortly we're going to give the name Z
stable ster generators to stabilizer stable ster generators to stabilizer stable ster generators to stabilizer
generators like this meaning that they generators like this meaning that they generators like this meaning that they
only have poly Z and identity tensor only have poly Z and identity tensor only have poly Z and identity tensor
factors so X and Y never occur in a z factors so X and Y never occur in a z factors so X and Y never occur in a z
stabilizer stabilizer stabilizer
generator we can also consider generator we can also consider generator we can also consider
stabilizer generators where only X and stabilizer generators where only X and stabilizer generators where only X and
identity poly matricies appear and we identity poly matricies appear and we identity poly matricies appear and we
can think about stabilizer generators can think about stabilizer generators can think about stabilizer generators
like this as being completely analogous like this as being completely analogous like this as being completely analogous
to Z stabilizer generators except that to Z stabilizer generators except that to Z stabilizer generators except that
they basically describe parity checks in they basically describe parity checks in they basically describe parity checks in
the plus minus basis rather than the the plus minus basis rather than the the plus minus basis rather than the
standard standard standard
bases for example going back to the 743 bases for example going back to the 743 bases for example going back to the 743
Heming code we can think about the Heming code we can think about the Heming code we can think about the
remaining three stabilizer generators remaining three stabilizer generators remaining three stabilizer generators
from the 7 Cubit steam code which happen from the 7 Cubit steam code which happen from the 7 Cubit steam code which happen
to follow exactly the same pattern as to follow exactly the same pattern as to follow exactly the same pattern as
for the Z stabilizer generators for this for the Z stabilizer generators for this for the Z stabilizer generators for this
code but this time we substitute X for code but this time we substitute X for code but this time we substitute X for
one rather than Z and what we get from one rather than Z and what we get from one rather than Z and what we get from
just these three stabilizer generators just these three stabilizer generators just these three stabilizer generators
is a code that includes the 16 states is a code that includes the 16 states is a code that includes the 16 states
that are shown here which we get by that are shown here which we get by that are shown here which we get by
applying hadamar gates to the standard applying hadamar gates to the standard applying hadamar gates to the standard
basis states that correspond to the basis states that correspond to the basis states that correspond to the
strings in the 743 Hamming code and of strings in the 743 Hamming code and of strings in the 743 Hamming code and of
course our code space also includes course our code space also includes course our code space also includes
linear combinations of these states linear combinations of these states linear combinations of these states
naturally stabilizer generators of this naturally stabilizer generators of this naturally stabilizer generators of this
form are called X stabilizer generators form are called X stabilizer generators form are called X stabilizer generators
so no y's or z's are allowed this so no y's or z's are allowed this so no y's or z's are allowed this
time and now we can Define CSS codes in time and now we can Define CSS codes in time and now we can Define CSS codes in
simple simple simple
terms CSS codes are stabilizer codes terms CSS codes are stabilizer codes terms CSS codes are stabilizer codes
that can be expressed using only Z that can be expressed using only Z that can be expressed using only Z
stabilizer generators and X stabilizer stabilizer generators and X stabilizer stabilizer generators and X stabilizer
generators so no y's allowed and no x's generators so no y's allowed and no x's generators so no y's allowed and no x's
and Z's appearing in the same stabilizer and Z's appearing in the same stabilizer and Z's appearing in the same stabilizer
generator and just to be sure that this generator and just to be sure that this generator and just to be sure that this
is clear we should keep in mind that is clear we should keep in mind that is clear we should keep in mind that
there is freedom in how we choose the there is freedom in how we choose the there is freedom in how we choose the
stabilizer generators for stabilizer stabilizer generators for stabilizer stabilizer generators for stabilizer
codes and what this definition is saying codes and what this definition is saying codes and what this definition is saying
is that a CSS code is one for which it is that a CSS code is one for which it is that a CSS code is one for which it
is possible to choose just Z and X is possible to choose just Z and X is possible to choose just Z and X
stabilizer generators but in general you stabilizer generators but in general you stabilizer generators but in general you
could choose different stabilizer could choose different stabilizer could choose different stabilizer
generators that might not be Z or X generators that might not be Z or X generators that might not be Z or X
stabilizer generators if you wanted stabilizer generators if you wanted stabilizer generators if you wanted
to here's just about the simplest to here's just about the simplest to here's just about the simplest
non-trivial example of a CSS code we can non-trivial example of a CSS code we can non-trivial example of a CSS code we can
have where there's both a z stabilizer have where there's both a z stabilizer have where there's both a z stabilizer
generator and an X stabilizer generator generator and an X stabilizer generator generator and an X stabilizer generator
and verifying that requires no more than and verifying that requires no more than and verifying that requires no more than
a glance given that we already know from a glance given that we already know from a glance given that we already know from
the previous lesson that this is a valid the previous lesson that this is a valid the previous lesson that this is a valid
stabilizer code the code space is stabilizer code the code space is stabilizer code the code space is
one-dimensional it's the space span by a one-dimensional it's the space span by a one-dimensional it's the space span by a
five Plus Bel and we can observe that five Plus Bel and we can observe that five Plus Bel and we can observe that
our two stabilizer generators act our two stabilizer generators act our two stabilizer generators act
trivially on this vector by considering trivially on this vector by considering trivially on this vector by considering
the two expressions of an ebit shown on the two expressions of an ebit shown on the two expressions of an ebit shown on
the screen together with the the screen together with the the screen together with the
interpretation of the stabilizer interpretation of the stabilizer interpretation of the stabilizer
generators as parity checks in the 01 generators as parity checks in the 01 generators as parity checks in the 01
and plus minus bases and plus minus bases and plus minus bases
the 7 Cubit steam code is another the 7 Cubit steam code is another the 7 Cubit steam code is another
example of a CSS code because for this example of a CSS code because for this example of a CSS code because for this
choice of generators we only have Z and choice of generators we only have Z and choice of generators we only have Z and
X stabilizer generators and again we X stabilizer generators and again we X stabilizer generators and again we
already know that this is a valid already know that this is a valid already know that this is a valid
stabilizer stabilizer stabilizer
code and the N9 Cubit short code is code and the N9 Cubit short code is code and the N9 Cubit short code is
another example this time we have six Z another example this time we have six Z another example this time we have six Z
stabilizer generators and just two x stabilizer generators and just two x stabilizer generators and just two x
stabilizer generators and that's fine stabilizer generators and that's fine stabilizer generators and that's fine
there doesn't need to be a balance or a there doesn't need to be a balance or a there doesn't need to be a balance or a
symmetry between the two types it is symmetry between the two types it is symmetry between the two types it is
however critical that we actually have a however critical that we actually have a however critical that we actually have a
valid stabilizer code and that means valid stabilizer code and that means valid stabilizer code and that means
that each of the Z stabilizer generators that each of the Z stabilizer generators that each of the Z stabilizer generators
must commute with each of the X must commute with each of the X must commute with each of the X
stabilizer generators so you can't stabilizer generators so you can't stabilizer generators so you can't
simply choose the two types of simply choose the two types of simply choose the two types of
generators independently and I'll say a generators independently and I'll say a generators independently and I'll say a
little bit more about this later in the little bit more about this later in the little bit more about this later in the
lesson next we'll discuss error lesson next we'll discuss error lesson next we'll discuss error
detection and correction for CSS codes detection and correction for CSS codes detection and correction for CSS codes
and what we'll find is that the property and what we'll find is that the property and what we'll find is that the property
that we observed for the 9 Cubit Shore that we observed for the 9 Cubit Shore that we observed for the 9 Cubit Shore
code that we can detect and correct X code that we can detect and correct X code that we can detect and correct X
errors and Z errors independently holds errors and Z errors independently holds errors and Z errors independently holds
in general for CSS codes in general for CSS codes in general for CSS codes
to explain this further let's consider a to explain this further let's consider a to explain this further let's consider a
CSS code such as the 7 Cubit steam code CSS code such as the 7 Cubit steam code CSS code such as the 7 Cubit steam code
but it could be a different CSS code if but it could be a different CSS code if but it could be a different CSS code if
you you you
prefer the basic idea is that the Z prefer the basic idea is that the Z prefer the basic idea is that the Z
stabilizer generators detect X errors stabilizer generators detect X errors stabilizer generators detect X errors
but they're completely oblivious to Z but they're completely oblivious to Z but they're completely oblivious to Z
errors because Z stabilizer generators errors because Z stabilizer generators errors because Z stabilizer generators
always commute with Z always commute with Z always commute with Z
errors in addition the Z stabilizer errors in addition the Z stabilizer errors in addition the Z stabilizer
generators also commute with whatever Z generators also commute with whatever Z generators also commute with whatever Z
Gates we might choose to apply as Gates we might choose to apply as Gates we might choose to apply as
Corrections for Z errors Corrections for Z errors Corrections for Z errors
similarly the X stabilizer generators similarly the X stabilizer generators similarly the X stabilizer generators
detect Z errors but they're completely detect Z errors but they're completely detect Z errors but they're completely
oblivious to X errors and Corrections so oblivious to X errors and Corrections so oblivious to X errors and Corrections so
we can in essence think about the Z we can in essence think about the Z we can in essence think about the Z
stabilizer generators as representing a stabilizer generators as representing a stabilizer generators as representing a
classical linear code that protects classical linear code that protects classical linear code that protects
against bit flips or in other words x against bit flips or in other words x against bit flips or in other words x
errors well the X stabilizer generators errors well the X stabilizer generators errors well the X stabilizer generators
represent a classical linear code in the represent a classical linear code in the represent a classical linear code in the
plus minus basis that completely plus minus basis that completely plus minus basis that completely
independently protects against flips independently protects against flips independently protects against flips
between plus and minus States or in between plus and minus States or in between plus and minus States or in
other words phase flip errors or Z other words phase flip errors or Z other words phase flip errors or Z
errors errors errors
so now it's quite apparent what's going so now it's quite apparent what's going so now it's quite apparent what's going
on with the steam code it's simply a 743 on with the steam code it's simply a 743 on with the steam code it's simply a 743
hemming code for bit flip errors and a hemming code for bit flip errors and a hemming code for bit flip errors and a
743 Hamming code for phas flip 743 Hamming code for phas flip 743 Hamming code for phas flip
errors once again the fact that the Z errors once again the fact that the Z errors once again the fact that the Z
stabilizer generators and X stabilizer stabilizer generators and X stabilizer stabilizer generators and X stabilizer
generators commute is required it generators commute is required it generators commute is required it
wouldn't be a valid stabilizer code if wouldn't be a valid stabilizer code if wouldn't be a valid stabilizer code if
they didn't and you could say that it's they didn't and you could say that it's they didn't and you could say that it's
good fortune that the 743 Heming code good fortune that the 743 Heming code good fortune that the 743 Heming code
works out like this but there are a lot works out like this but there are a lot works out like this but there are a lot
of other choices that can be made for of other choices that can be made for of other choices that can be made for
pairs of classical linear codes that pairs of classical linear codes that pairs of classical linear codes that
will give us a valid stabilizer code will give us a valid stabilizer code will give us a valid stabilizer code
when combined in this when combined in this when combined in this
way now getting back to CSS codes in way now getting back to CSS codes in way now getting back to CSS codes in
general suppose that the Z stabilizer general suppose that the Z stabilizer general suppose that the Z stabilizer
generators allow for the correction of generators allow for the correction of generators allow for the correction of
up to say JX errors and the X stabilizer up to say JX errors and the X stabilizer up to say JX errors and the X stabilizer
generators allow for the correction of generators allow for the correction of generators allow for the correction of
up to k z errors so in the case of the up to k z errors so in the case of the up to k z errors so in the case of the
steing code J and K are both one because steing code J and K are both one because steing code J and K are both one because
the 743 hemming code can correct one bit the 743 hemming code can correct one bit the 743 hemming code can correct one bit
flip but in general these numbers J and flip but in general these numbers J and flip but in general these numbers J and
K could be different K could be different K could be different
it then follows by the discretization of it then follows by the discretization of it then follows by the discretization of
errors that the CSS code can correct for errors that the CSS code can correct for errors that the CSS code can correct for
any error on a number of cubits up to any error on a number of cubits up to any error on a number of cubits up to
the minimum of J and the minimum of J and the minimum of J and
K that's because when we measure the K that's because when we measure the K that's because when we measure the
syndrome an arbitrary error on some syndrome an arbitrary error on some syndrome an arbitrary error on some
number of cubits effectively collapses number of cubits effectively collapses number of cubits effectively collapses
probabilistically into some combination probabilistically into some combination probabilistically into some combination
of X errors Z errors or both and then of X errors Z errors or both and then of X errors Z errors or both and then
the X errors and Z errors can be the X errors and Z errors can be the X errors and Z errors can be
detected and corrected detected and corrected detected and corrected
independently so as long as we have two independently so as long as we have two independently so as long as we have two
classical linear codes that get along in classical linear codes that get along in classical linear codes that get along in
the sense that they give us x and z the sense that they give us x and z the sense that they give us x and z
stabilizer generators that commute the stabilizer generators that commute the stabilizer generators that commute the
CSS code we get by combining them CSS code we get by combining them CSS code we get by combining them
basically inherits the error correction basically inherits the error correction basically inherits the error correction
properties of the two codes in the sense properties of the two codes in the sense properties of the two codes in the sense
that I've just that I've just that I've just
described notice that there is a price described notice that there is a price described notice that there is a price
to be paid though which is that we can't to be paid though which is that we can't to be paid though which is that we can't
encode as many cubits as we could bits encode as many cubits as we could bits encode as many cubits as we could bits
with the two classical codes because the with the two classical codes because the with the two classical codes because the
total number of stabilizer generators total number of stabilizer generators total number of stabilizer generators
for the CSS code is the sum of the for the CSS code is the sum of the for the CSS code is the sum of the
number of parody checks for the two number of parody checks for the two number of parody checks for the two
classical codes and each stabilizer classical codes and each stabilizer classical codes and each stabilizer
generator cuts the dimension of the generator cuts the dimension of the generator cuts the dimension of the
codee space in codee space in codee space in
half for example the 743 Hamming code half for example the 743 Hamming code half for example the 743 Hamming code
allows for the encoding of four allows for the encoding of four allows for the encoding of four
classical bits because we have just classical bits because we have just classical bits because we have just
three parody checks for this code three parody checks for this code three parody checks for this code
whereas the 7 Cubit steam code only whereas the 7 Cubit steam code only whereas the 7 Cubit steam code only
encodes one cubit because it has six encodes one cubit because it has six encodes one cubit because it has six
stabilizer stabilizer stabilizer
generators the last thing I'll explain generators the last thing I'll explain generators the last thing I'll explain
about CSS codes in general in this about CSS codes in general in this about CSS codes in general in this
lesson concerns code spaces of CSS codes lesson concerns code spaces of CSS codes lesson concerns code spaces of CSS codes
and what they look like and this will and what they look like and this will and what they look like and this will
also give us a chance to examine in a also give us a chance to examine in a also give us a chance to examine in a
little bit more detail the relation little bit more detail the relation little bit more detail the relation
relationship that needs to hold between relationship that needs to hold between relationship that needs to hold between
two classical linear codes in order for two classical linear codes in order for two classical linear codes in order for
them to be compatible in that we can them to be compatible in that we can them to be compatible in that we can
combine them together to form a CSS combine them together to form a CSS combine them together to form a CSS
code consider an arbitrary CSS code on N code consider an arbitrary CSS code on N code consider an arbitrary CSS code on N
cubits and let Z1 through Zs be the nbit cubits and let Z1 through Zs be the nbit cubits and let Z1 through Zs be the nbit
parity check strings that correspond to parity check strings that correspond to parity check strings that correspond to
the Z stabilizer the Z stabilizer the Z stabilizer
generators this means that the classical generators this means that the classical generators this means that the classical
linear code described by the Z linear code described by the Z linear code described by the Z
stabilizer generators which we'll call stabilizer generators which we'll call stabilizer generators which we'll call
CZ just to help us to remember that this CZ just to help us to remember that this CZ just to help us to remember that this
is the code corresponding to the Z is the code corresponding to the Z is the code corresponding to the Z
stabilizer generators takes the form stabilizer generators takes the form stabilizer generators takes the form
that's shown here on the that's shown here on the that's shown here on the
screen in words the code contains every screen in words the code contains every screen in words the code contains every
string U whose binary do product with string U whose binary do product with string U whose binary do product with
each of the parody check strings is each of the parody check strings is each of the parody check strings is
zero well if we wanted to we could also zero well if we wanted to we could also zero well if we wanted to we could also
consider a different code derived from consider a different code derived from consider a different code derived from
these same strings which we'll call DZ these same strings which we'll call DZ these same strings which we'll call DZ
and the idea here is that we think about and the idea here is that we think about and the idea here is that we think about
these strings Z1 through Zs not as these strings Z1 through Zs not as these strings Z1 through Zs not as
parity check strings but rather as parity check strings but rather as parity check strings but rather as
generators generators generators
at this point it isn't clear why we at this point it isn't clear why we at this point it isn't clear why we
would want to think about this code but would want to think about this code but would want to think about this code but
we can and it'll turn out to be relevant we can and it'll turn out to be relevant we can and it'll turn out to be relevant
to this to this to this
discussion this code is called the Dual discussion this code is called the Dual discussion this code is called the Dual
code of CZ and it's commonly denoted CZ code of CZ and it's commonly denoted CZ code of CZ and it's commonly denoted CZ
per meaning CZ with the perpendicular per meaning CZ with the perpendicular per meaning CZ with the perpendicular
symbol as a superscript but we're going symbol as a superscript but we're going symbol as a superscript but we're going
to call it DZ and that'll be easy enough to call it DZ and that'll be easy enough to call it DZ and that'll be easy enough
to remember because D is short for to remember because D is short for to remember because D is short for
dual we can also do something similar dual we can also do something similar dual we can also do something similar
with the X stabilizer generators so this with the X stabilizer generators so this with the X stabilizer generators so this
time CX is the classical linear code time CX is the classical linear code time CX is the classical linear code
whose parity check strings correspond to whose parity check strings correspond to whose parity check strings correspond to
the X stabilizer generators and DX is the X stabilizer generators and DX is the X stabilizer generators and DX is
the Dual code of this code as an aside the Dual code of this code as an aside the Dual code of this code as an aside
we can now describe the conditions that we can now describe the conditions that we can now describe the conditions that
must hold in order for CZ and CX to be must hold in order for CZ and CX to be must hold in order for CZ and CX to be
compatible in the sense that we can compatible in the sense that we can compatible in the sense that we can
paare them together as a CSS code in paare them together as a CSS code in paare them together as a CSS code in
terms of these codes terms of these codes terms of these codes
themselves specifically it must be that themselves specifically it must be that themselves specifically it must be that
DZ is contained in CX or equivalently DZ is contained in CX or equivalently DZ is contained in CX or equivalently
that DX is contained in CZ and now it's that DX is contained in CZ and now it's that DX is contained in CZ and now it's
possible to describe the code space of possible to describe the code space of possible to describe the code space of
the CSS code derived from these two the CSS code derived from these two the CSS code derived from these two
codes and one way to describe it is that codes and one way to describe it is that codes and one way to describe it is that
it's the space Spann by all of the it's the space Spann by all of the it's the space Spann by all of the
vectors that take the form that's shown vectors that take the form that's shown vectors that take the form that's shown
on the on the on the
screen in words they look like uniform screen in words they look like uniform screen in words they look like uniform
superpositions over the strings in the superpositions over the strings in the superpositions over the strings in the
Dual code DX of the code corresponding Dual code DX of the code corresponding Dual code DX of the code corresponding
to the X stabilizer generators shifted to the X stabilizer generators shifted to the X stabilizer generators shifted
by or exord with strings in the code CZ by or exord with strings in the code CZ by or exord with strings in the code CZ
corresponding to the Z stabilizer corresponding to the Z stabilizer corresponding to the Z stabilizer
generators and we'll see what this looks generators and we'll see what this looks generators and we'll see what this looks
like in the specific case of the steam like in the specific case of the steam like in the specific case of the steam
code in just a moment this makes sense code in just a moment this makes sense code in just a moment this makes sense
because states in the code space must be because states in the code space must be because states in the code space must be
plus one igen vectors of all the plus one igen vectors of all the plus one igen vectors of all the
stabilizer generators and in particular stabilizer generators and in particular stabilizer generators and in particular
multiplying by any one of the X multiplying by any one of the X multiplying by any one of the X
stabilizer generators is equivalent to stabilizer generators is equivalent to stabilizer generators is equivalent to
xoring by the corresponding X string so xoring by the corresponding X string so xoring by the corresponding X string so
that's why these uniform superpositions that's why these uniform superpositions that's why these uniform superpositions
over DX shifted by different strings over DX shifted by different strings over DX shifted by different strings
appear like appear like appear like
this different choices for the shift this different choices for the shift this different choices for the shift
represented by the string Y in this represented by the string Y in this represented by the string Y in this
expression can give us the same vector expression can give us the same vector expression can give us the same vector
by the way so these states aren't all by the way so these states aren't all by the way so these states aren't all
distinct but collectively they expan the distinct but collectively they expan the distinct but collectively they expan the
entire code entire code entire code
space there is a symmetry here between space there is a symmetry here between space there is a symmetry here between
the x and z stabilizer generators and the x and z stabilizer generators and the x and z stabilizer generators and
the codes that they correspond to and so the codes that they correspond to and so the codes that they correspond to and so
we can use that symmetry to describe we can use that symmetry to describe we can use that symmetry to describe
vectors in the code space in an vectors in the code space in an vectors in the code space in an
analogous way swapping x's and Z's but analogous way swapping x's and Z's but analogous way swapping x's and Z's but
we also need to swap the standard bases we also need to swap the standard bases we also need to swap the standard bases
for the plus minus bases which is why for the plus minus bases which is why for the plus minus bases which is why
the hatam operations appear in this expression now for the example that was expression now for the example that was
promised which is the steam code we end promised which is the steam code we end promised which is the steam code we end
up with these two encoded as possible up with these two encoded as possible up with these two encoded as possible
encodings of the zero and one standard encodings of the zero and one standard encodings of the zero and one standard
basis States in particular we can encode basis States in particular we can encode basis States in particular we can encode
the zero State as a uniform the zero State as a uniform the zero State as a uniform
superposition over the eight strings in superposition over the eight strings in superposition over the eight strings in
the Dual code of the 743 Hamming code the Dual code of the 743 Hamming code the Dual code of the 743 Hamming code
which turns out to be half of the which turns out to be half of the which turns out to be half of the
strings in the 743 Hamming code and we strings in the 743 Hamming code and we strings in the 743 Hamming code and we
can encode the one state in a similar can encode the one state in a similar can encode the one state in a similar
way except that all the strings are exor way except that all the strings are exor way except that all the strings are exor
by the all one string in this case by the all one string in this case by the all one string in this case
because that string happens to be in the because that string happens to be in the because that string happens to be in the
743 hemming code but not in its duol one 743 hemming code but not in its duol one 743 hemming code but not in its duol one
final comment here is that we often final comment here is that we often final comment here is that we often
don't actually care all that much about don't actually care all that much about don't actually care all that much about
what vectors in our code spaces look what vectors in our code spaces look what vectors in our code spaces look
like because we generally don't need to like because we generally don't need to like because we generally don't need to
know this in order to use the know this in order to use the know this in order to use the
codes so if this explanation wasn't codes so if this explanation wasn't codes so if this explanation wasn't
clear it's really not something to worry clear it's really not something to worry clear it's really not something to worry
about too much but it is certainly about too much but it is certainly about too much but it is certainly
natural to ask what vectors in code natural to ask what vectors in code natural to ask what vectors in code
spaces of CSS codes look like and this spaces of CSS codes look like and this spaces of CSS codes look like and this
description provides an answer to that description provides an answer to that description provides an answer to that
question next we'll discuss a specific question next we'll discuss a specific question next we'll discuss a specific
CSS code known as the Torah code which CSS code known as the Torah code which CSS code known as the Torah code which
is an example of something known as a is an example of something known as a is an example of something known as a
topological quantum error correcting topological quantum error correcting topological quantum error correcting
code this code was discovered by Alexa code this code was discovered by Alexa code this code was discovered by Alexa
kayv in kayv in kayv in
1997 in fact the Torah code isn't a 1997 in fact the Torah code isn't a 1997 in fact the Torah code isn't a
single code but rather it's a family of single code but rather it's a family of single code but rather it's a family of
codes one for each positive integer at codes one for each positive integer at codes one for each positive integer at
least least least
two these codes possess some Key two these codes possess some Key two these codes possess some Key
Properties the first key property is Properties the first key property is Properties the first key property is
that all of the stabilizer generators that all of the stabilizer generators that all of the stabilizer generators
have low weight as poly operations in have low weight as poly operations in have low weight as poly operations in
particular they're all going to have particular they're all going to have particular they're all going to have
weight four and this is good because it weight four and this is good because it weight four and this is good because it
means that each of the stabilizer means that each of the stabilizer means that each of the stabilizer
generator measurements doesn't need to generator measurements doesn't need to generator measurements doesn't need to
involve too many involve too many involve too many
cubits sometimes people speak of quantum cubits sometimes people speak of quantum cubits sometimes people speak of quantum
ldpc codes or Quantum low density parity ldpc codes or Quantum low density parity ldpc codes or Quantum low density parity
check codes and this is what is meant by check codes and this is what is meant by check codes and this is what is meant by
this term where in this case low means this term where in this case low means this term where in this case low means
four so the Tor code is an example of a four so the Tor code is an example of a four so the Tor code is an example of a
Quantum ldpc Quantum ldpc Quantum ldpc
Code the Tor code also has what's called Code the Tor code also has what's called Code the Tor code also has what's called
geometric locality which means that not geometric locality which means that not geometric locality which means that not
only do the stabilizer generators have only do the stabilizer generators have only do the stabilizer generators have
low weight but it's also possible to low weight but it's also possible to low weight but it's also possible to
arrange the cubits spatially so that arrange the cubits spatially so that arrange the cubits spatially so that
each of the stabilizer generator each of the stabilizer generator each of the stabilizer generator
measurements only involves cubits that measurements only involves cubits that measurements only involves cubits that
are close together which makes these are close together which makes these are close together which makes these
measurements easier to implement in measurements easier to implement in measurements easier to implement in
principle and finally these codes have principle and finally these codes have principle and finally these codes have
large distance so they can tolerate a large distance so they can tolerate a large distance so they can tolerate a
relatively High rate of relatively High rate of relatively High rate of
Errors here's how these codes Errors here's how these codes Errors here's how these codes
work first let capital L be a positive work first let capital L be a positive work first let capital L be a positive
integer that's at least two and consider integer that's at least two and consider integer that's at least two and consider
an L by L lattice with so-called an L by L lattice with so-called an L by L lattice with so-called
periodic IC periodic IC periodic IC
boundaries for example here's an L by L boundaries for example here's an L by L boundaries for example here's an L by L
lce for Lal 8 and notice that the lines lce for Lal 8 and notice that the lines lce for Lal 8 and notice that the lines
on the right and on the bottom are on the right and on the bottom are on the right and on the bottom are
dotted lines and the idea here is that dotted lines and the idea here is that dotted lines and the idea here is that
we want to think about the dotted line we want to think about the dotted line we want to think about the dotted line
on the right as being the same line as on the right as being the same line as on the right as being the same line as
the line on the left and similarly the the line on the left and similarly the the line on the left and similarly the
dotted line on the bottom is the same dotted line on the bottom is the same dotted line on the bottom is the same
line as the one on line as the one on line as the one on
top so if we actually wanted to realize top so if we actually wanted to realize top so if we actually wanted to realize
this sort of configuration physically this sort of configuration physically this sort of configuration physically
we'd need three we'd need three we'd need three
dimensions in particular we could form a dimensions in particular we could form a dimensions in particular we could form a
cylinder by first matching up the left cylinder by first matching up the left cylinder by first matching up the left
and right sides and then bend the and right sides and then bend the and right sides and then bend the
cylinder around so that the circles at cylinder around so that the circles at cylinder around so that the circles at
the ends which used to be the top and the ends which used to be the top and the ends which used to be the top and
bottom edges of the lattice meet or you bottom edges of the lattice meet or you bottom edges of the lattice meet or you
could match up the top and bottom first could match up the top and bottom first could match up the top and bottom first
and then the sides it works both ways and then the sides it works both ways and then the sides it works both ways
and it doesn't matter which way you do and it doesn't matter which way you do and it doesn't matter which way you do
it for the sake of this it for the sake of this it for the sake of this
discussion what we get is a Taurus or in discussion what we get is a Taurus or in discussion what we get is a Taurus or in
other words a donut although thinking other words a donut although thinking other words a donut although thinking
about it as an inner tube of a tire is about it as an inner tube of a tire is about it as an inner tube of a tire is
perhaps a better image to have in mind perhaps a better image to have in mind perhaps a better image to have in mind
because this isn't aoft because this isn't aoft because this isn't aoft
the lattice has actually become just the the lattice has actually become just the the lattice has actually become just the
surface of a surface of a surface of a
Taurus and this is where the name Torah Taurus and this is where the name Torah Taurus and this is where the name Torah
code comes code comes code comes
from the way that you can move around on from the way that you can move around on from the way that you can move around on
a Taurus like this assuming that you're a Taurus like this assuming that you're a Taurus like this assuming that you're
able to move between adjacent points on able to move between adjacent points on able to move between adjacent points on
the lattice will likely be familiar to the lattice will likely be familiar to the lattice will likely be familiar to
people that have played old school video people that have played old school video people that have played old school video
games where if you move off the top edge games where if you move off the top edge games where if you move off the top edge
of the screen you emerge in the bottom of the screen you emerge in the bottom of the screen you emerge in the bottom
and likewise for the left and right and likewise for the left and right and likewise for the left and right
edges of the edges of the edges of the
screen and for the most part this is the screen and for the most part this is the screen and for the most part this is the
way that I'll describe this lattice as a way that I'll describe this lattice as a way that I'll describe this lattice as a
oos to talking specifically about a oos to talking specifically about a oos to talking specifically about a
Taurus in a three-dimensional space next we imagine that cubits are space next we imagine that cubits are
placed on the edges of the lattice as is placed on the edges of the lattice as is placed on the edges of the lattice as is
Illustrated here on the screen where Illustrated here on the screen where Illustrated here on the screen where
cubits are indicated by solid blue circles notice by the way that the circles notice by the way that the
cubits that are placed on the dotted cubits that are placed on the dotted cubits that are placed on the dotted
lines aren't solid and that's because lines aren't solid and that's because lines aren't solid and that's because
we've already represented those cubits we've already represented those cubits we've already represented those cubits
either by the ones along the top or the either by the ones along the top or the either by the ones along the top or the
left hand side keeping in mind that the left hand side keeping in mind that the left hand side keeping in mind that the
lines on the top and the left are lines on the top and the left are lines on the top and the left are
exactly the same lines as the ones exactly the same lines as the ones exactly the same lines as the ones
indicated by the dotted lines on the indicated by the dotted lines on the indicated by the dotted lines on the
bottom and the bottom and the bottom and the
right and so if we count up all of these right and so if we count up all of these right and so if we count up all of these
cubits we get 2 * l^ 2 cubits and one cubits we get 2 * l^ 2 cubits and one cubits we get 2 * l^ 2 cubits and one
way to see that is to observe that we way to see that is to observe that we way to see that is to observe that we
get l s cubits on horizontal edges and L get l s cubits on horizontal edges and L get l s cubits on horizontal edges and L
squar cubits on vertical squar cubits on vertical squar cubits on vertical
edges so that's the layout of our cubits edges so that's the layout of our cubits edges so that's the layout of our cubits
and to describe the Tor code itself we and to describe the Tor code itself we and to describe the Tor code itself we
just need to describe the stabilizer just need to describe the stabilizer just need to describe the stabilizer
generators and there are two types of generators and there are two types of generators and there are two types of
them them them
first we have Z stabilizer generators first we have Z stabilizer generators first we have Z stabilizer generators
and these correspond to tiles or pletes and these correspond to tiles or pletes and these correspond to tiles or pletes
in the in the in the
figure and second we have X stabilizer figure and second we have X stabilizer figure and second we have X stabilizer
generators and these correspond to generators and these correspond to generators and these correspond to
vertices in the vertices in the vertices in the
figure so for example here's one figure so for example here's one figure so for example here's one
stabilizer generator meaning that we're stabilizer generator meaning that we're stabilizer generator meaning that we're
tensoring together one poly Z operation tensoring together one poly Z operation tensoring together one poly Z operation
for each of the cubits that are touching for each of the cubits that are touching for each of the cubits that are touching
this tile along with identity matrices this tile along with identity matrices this tile along with identity matrices
on all the other cubits and that gives on all the other cubits and that gives on all the other cubits and that gives
us a weight for poly us a weight for poly us a weight for poly
operation and here's an example of an X operation and here's an example of an X operation and here's an example of an X
stabilizer generator where we have poly stabilizer generator where we have poly stabilizer generator where we have poly
X matrices acting on the four cubits X matrices acting on the four cubits X matrices acting on the four cubits
incident to this vertex along with incident to this vertex along with incident to this vertex along with
identity matrices on all of the other identity matrices on all of the other identity matrices on all of the other
cubits so once again it's a weight for cubits so once again it's a weight for cubits so once again it's a weight for
poly poly poly
operation it's important that for both operation it's important that for both operation it's important that for both
of these types of stabilizer generators of these types of stabilizer generators of these types of stabilizer generators
we include the ones that wrap around at we include the ones that wrap around at we include the ones that wrap around at
the periodic boundaries and here's an the periodic boundaries and here's an the periodic boundaries and here's an
example of an x- stabilizer generator example of an x- stabilizer generator example of an x- stabilizer generator
that does that does that does
this now in order to get a valid this now in order to get a valid this now in order to get a valid
stabilizer code we need the stabilizer stabilizer code we need the stabilizer stabilizer code we need the stabilizer
erators to erators to erators to
commute the Z stabilizer generators all commute the Z stabilizer generators all commute the Z stabilizer generators all
commute with one another because Z commute with one another because Z commute with one another because Z
commutes with itself and the identity commutes with itself and the identity commutes with itself and the identity
commutes with everything and likewise commutes with everything and likewise commutes with everything and likewise
for the X stabilizer for the X stabilizer for the X stabilizer
generators it's also clear that a z generators it's also clear that a z generators it's also clear that a z
stabilizer generator and an x- stabilizer generator and an x- stabilizer generator and an x-
stabilizer generator commute when they stabilizer generator commute when they stabilizer generator commute when they
act non-trivially on disjoint sets of act non-trivially on disjoint sets of act non-trivially on disjoint sets of
cubits like for the two generators that cubits like for the two generators that cubits like for the two generators that
are shown are shown are shown
here the remaining possibility is that a here the remaining possibility is that a here the remaining possibility is that a
z stabilizer generator and an X z stabilizer generator and an X z stabilizer generator and an X
stabilizer generator overlap and stabilizer generator overlap and stabilizer generator overlap and
whenever that happens it's always two c whenever that happens it's always two c whenever that happens it's always two c
on which they overlap and so the on which they overlap and so the on which they overlap and so the
stabilizer generators commute because ZZ stabilizer generators commute because ZZ stabilizer generators commute because ZZ
and xx commute now this is not a minimal and xx commute now this is not a minimal and xx commute now this is not a minimal
set of stabilizer set of stabilizer set of stabilizer
generators in particular if we multiply generators in particular if we multiply generators in particular if we multiply
all of the Z stabilizer generators all of the Z stabilizer generators all of the Z stabilizer generators
together we get the identity because together we get the identity because together we get the identity because
every Cubit gets multiplied by z twice every Cubit gets multiplied by z twice every Cubit gets multiplied by z twice
once for each of the tiles that touches once for each of the tiles that touches once for each of the tiles that touches
the the the
Cubit that means that multiplying all of Cubit that means that multiplying all of Cubit that means that multiplying all of
the Z stabilizer generators together the Z stabilizer generators together the Z stabilizer generators together
except for one of them gives us that except for one of them gives us that except for one of them gives us that
last one so they're not independent but last one so they're not independent but last one so they're not independent but
if we remove any one of them we will get if we remove any one of them we will get if we remove any one of them we will get
an independent set and the same is true an independent set and the same is true an independent set and the same is true
for the X stabilizer for the X stabilizer for the X stabilizer
generators so we can remove one of each generators so we can remove one of each generators so we can remove one of each
and it doesn't matter which ones we and it doesn't matter which ones we and it doesn't matter which ones we
remove and that leaves l^2 minus one remove and that leaves l^2 minus one remove and that leaves l^2 minus one
stabilizer generators of each type to be stabilizer generators of each type to be stabilizer generators of each type to be
clear about this and this is something clear about this and this is something clear about this and this is something
that I alluded to in the previous lesson that I alluded to in the previous lesson that I alluded to in the previous lesson
we actually do care about all of these we actually do care about all of these we actually do care about all of these
stabilizer generators and in a strictly stabilizer generators and in a strictly stabilizer generators and in a strictly
operational sense we won't actually pick operational sense we won't actually pick operational sense we won't actually pick
one of each type to ignore but but for one of each type to ignore but but for one of each type to ignore but but for
the sake of analyzing the code we can the sake of analyzing the code we can the sake of analyzing the code we can
imagine that one stabilizer generator of imagine that one stabilizer generator of imagine that one stabilizer generator of
each type has been removed so that we each type has been removed so that we each type has been removed so that we
get a minimal generating set keeping in get a minimal generating set keeping in get a minimal generating set keeping in
mind that we could always infer the mind that we could always infer the mind that we could always infer the
results of these removed generators results of these removed generators results of these removed generators
thinking of them as observables from the thinking of them as observables from the thinking of them as observables from the
results of all of the other stabilizer results of all of the other stabilizer results of all of the other stabilizer
generator observables of the same type generator observables of the same type generator observables of the same type
so the number of cubits that this code so the number of cubits that this code so the number of cubits that this code
encodes is equal to the total number of encodes is equal to the total number of encodes is equal to the total number of
cubits which is 2 * l^2 minus the total cubits which is 2 * l^2 minus the total cubits which is 2 * l^2 minus the total
number of stabilizer generators in a number of stabilizer generators in a number of stabilizer generators in a
minimal gener in set which is 2 * l^2 - minimal gener in set which is 2 * l^2 - minimal gener in set which is 2 * l^2 -
1 and that means we can encode 2 1 and that means we can encode 2 1 and that means we can encode 2
cubits I didn't argue by the way that cubits I didn't argue by the way that cubits I didn't argue by the way that
the code space is non-trivial but indeed the code space is non-trivial but indeed the code space is non-trivial but indeed
it is non-trivial and I'll leave it to it is non-trivial and I'll leave it to it is non-trivial and I'll leave it to
you to convince yourself that there's no you to convince yourself that there's no you to convince yourself that there's no
way to multiply together stabilizer way to multiply together stabilizer way to multiply together stabilizer
generators in a way that gives us generators in a way that gives us generators in a way that gives us
negative -1 times the negative -1 times the negative -1 times the
identity so that is the Torah code identity so that is the Torah code identity so that is the Torah code
itself and at this point it may not be itself and at this point it may not be itself and at this point it may not be
clear at all why this code should be clear at all why this code should be clear at all why this code should be
good for Quantum error good for Quantum error good for Quantum error
correction it remains for us to analyze correction it remains for us to analyze correction it remains for us to analyze
it and what we'll find is that it is in it and what we'll find is that it is in it and what we'll find is that it is in
fact a pretty amazing Quantum error fact a pretty amazing Quantum error fact a pretty amazing Quantum error
correcting correcting correcting
code to understand how the Torah code code to understand how the Torah code code to understand how the Torah code
functions as a Quantum error correcting functions as a Quantum error correcting functions as a Quantum error correcting
code we'll Begin by considering code we'll Begin by considering code we'll Begin by considering
different errors and the syndromes that different errors and the syndromes that different errors and the syndromes that
they they they
generate the first thing to note is that generate the first thing to note is that generate the first thing to note is that
the Tor code is a CSS code because all the Tor code is a CSS code because all the Tor code is a CSS code because all
of our stabilizer generators are either of our stabilizer generators are either of our stabilizer generators are either
Z or X stabilizer Z or X stabilizer Z or X stabilizer
generators and what that means is that X generators and what that means is that X generators and what that means is that X
errors and Z errors can be detected and errors and Z errors can be detected and errors and Z errors can be detected and
possibly corrected possibly corrected possibly corrected
separately and in fact there's a simple separately and in fact there's a simple separately and in fact there's a simple
symmetry between the Z and X stabilizer symmetry between the Z and X stabilizer symmetry between the Z and X stabilizer
generators that allows us to analyze X generators that allows us to analyze X generators that allows us to analyze X
errors and Z errors in essentially the errors and Z errors in essentially the errors and Z errors in essentially the
same way so I'm going to focus on X same way so I'm going to focus on X same way so I'm going to focus on X
errors which are possibly detected by Z errors which are possibly detected by Z errors which are possibly detected by Z
stabilizer generators and you can stabilizer generators and you can stabilizer generators and you can
translate everything to Z errors which translate everything to Z errors which translate everything to Z errors which
are possibly detected by the X are possibly detected by the X are possibly detected by the X
stabilizer generators using exactly the stabilizer generators using exactly the stabilizer generators using exactly the
same same same
ideas here's a figure illustrating our ideas here's a figure illustrating our ideas here's a figure illustrating our
two Square cubits on the edges of an L two Square cubits on the edges of an L two Square cubits on the edges of an L
by lattice with periodic boundaries and by lattice with periodic boundaries and by lattice with periodic boundaries and
we can imagine that this is laid out on we can imagine that this is laid out on we can imagine that this is laid out on
the surface of a the surface of a the surface of a
Taurus at the moment all of the cubits Taurus at the moment all of the cubits Taurus at the moment all of the cubits
are blue indicating that these are are blue indicating that these are are blue indicating that these are
cubits that haven't been affected by cubits that haven't been affected by cubits that haven't been affected by
errors so you should imagine that at errors so you should imagine that at errors so you should imagine that at
this point the state of these cubits is this point the state of these cubits is this point the state of these cubits is
in the code space for this in the code space for this in the code space for this
code as I mentioned our Focus will be on code as I mentioned our Focus will be on code as I mentioned our Focus will be on
X errors and in just a moment we'll use X errors and in just a moment we'll use X errors and in just a moment we'll use
magenta circles to indicate cubits that magenta circles to indicate cubits that magenta circles to indicate cubits that
have been affected by X have been affected by X have been affected by X
errors X errors are detected by Z errors X errors are detected by Z errors X errors are detected by Z
stabilizer generators which correspond stabilizer generators which correspond stabilizer generators which correspond
to tiles in the figure so what we can do to tiles in the figure so what we can do to tiles in the figure so what we can do
is to let White Tiles indicate plus one is to let White Tiles indicate plus one is to let White Tiles indicate plus one
measurement outcomes or in other words measurement outcomes or in other words measurement outcomes or in other words
entries in the syndrome corresponding to entries in the syndrome corresponding to entries in the syndrome corresponding to
that generator while gray tiles will that generator while gray tiles will that generator while gray tiles will
indicate minus one measurement indicate minus one measurement indicate minus one measurement
outcomes you can think of a plus one outcomes you can think of a plus one outcomes you can think of a plus one
outcome or in other words a white tile outcome or in other words a white tile outcome or in other words a white tile
as indicating that the corresponding as indicating that the corresponding as indicating that the corresponding
stabilizer generator is Happy everything stabilizer generator is Happy everything stabilizer generator is Happy everything
seems to be okay for this generator well seems to be okay for this generator well seems to be okay for this generator well
a minus one outcome or a gray tile a minus one outcome or a gray tile a minus one outcome or a gray tile
indicates that the corresponding indicates that the corresponding indicates that the corresponding
stabilizer generator is unhappy because stabilizer generator is unhappy because stabilizer generator is unhappy because
obviously an error must have happened to obviously an error must have happened to obviously an error must have happened to
cause this cause this cause this
outcome so at the moment all of our outcome so at the moment all of our outcome so at the moment all of our
stabilizer generators are happy because stabilizer generators are happy because stabilizer generators are happy because
no errors have occur now if we think no errors have occur now if we think no errors have occur now if we think
about a z stabilizer generator all that about a z stabilizer generator all that about a z stabilizer generator all that
it's doing is measuring the parity of it's doing is measuring the parity of it's doing is measuring the parity of
the 4 cubits that touch the tile with the 4 cubits that touch the tile with the 4 cubits that touch the tile with
respect to the standard basis so a plus respect to the standard basis so a plus respect to the standard basis so a plus
one outcome doesn't indicate that no X one outcome doesn't indicate that no X one outcome doesn't indicate that no X
errors have occurred on these four errors have occurred on these four errors have occurred on these four
cubits but rather it indicates that an cubits but rather it indicates that an cubits but rather it indicates that an
even number of X errors have occurred even number of X errors have occurred even number of X errors have occurred
whereas a minus one outcome indicates whereas a minus one outcome indicates whereas a minus one outcome indicates
that an odd number of X errors have that an odd number of X errors have that an odd number of X errors have
occurred so now let's introduce an X occurred so now let's introduce an X occurred so now let's introduce an X
error and see what error and see what error and see what
happens the two Z stabilizer generators happens the two Z stabilizer generators happens the two Z stabilizer generators
corresponding to the tiles that touch corresponding to the tiles that touch corresponding to the tiles that touch
this Cubit are now unhappy because this this Cubit are now unhappy because this this Cubit are now unhappy because this
x error anti-c commutes with both of x error anti-c commutes with both of x error anti-c commutes with both of
these generators or equivalently in these generators or equivalently in these generators or equivalently in
terms of the standard basis we're now in terms of the standard basis we're now in terms of the standard basis we're now in
a state where both of these sets of four a state where both of these sets of four a state where both of these sets of four
cub bits corresponding to the two tiles cub bits corresponding to the two tiles cub bits corresponding to the two tiles
have odd have odd have odd
parity so that's the effect of a single parity so that's the effect of a single parity so that's the effect of a single
X error so if we were to measure this X error so if we were to measure this X error so if we were to measure this
syndrome and we knew or we assumed that syndrome and we knew or we assumed that syndrome and we knew or we assumed that
just One X error had occurred we'd be just One X error had occurred we'd be just One X error had occurred we'd be
able to easily infer which Cubit had able to easily infer which Cubit had able to easily infer which Cubit had
flipped and we could simply flip it back flipped and we could simply flip it back flipped and we could simply flip it back
with an xgate now let's see what happens with an xgate now let's see what happens with an xgate now let's see what happens
when we introduce additional X errors when we introduce additional X errors when we introduce additional X errors
and in particular let's see what happens and in particular let's see what happens and in particular let's see what happens
when a second X error occurs on another when a second X error occurs on another when a second X error occurs on another
Cubit touching one of the gray tiles Cubit touching one of the gray tiles Cubit touching one of the gray tiles
let's say the gray tile on the top like let's say the gray tile on the top like let's say the gray tile on the top like
this that tile is now happy because two this that tile is now happy because two this that tile is now happy because two
x errors have occurred on the cubits x errors have occurred on the cubits x errors have occurred on the cubits
touching this tile and two is even so touching this tile and two is even so touching this tile and two is even so
these two x errors together commute with these two x errors together commute with these two x errors together commute with
the stabilizer generator for this the stabilizer generator for this the stabilizer generator for this
tile another way to say this is that tile another way to say this is that tile another way to say this is that
this second X error has shifted all the this second X error has shifted all the this second X error has shifted all the
standard Bas of states for these four standard Bas of states for these four standard Bas of states for these four
cubits from odd parity back to even cubits from odd parity back to even cubits from odd parity back to even
parity the tile to the left of this new parity the tile to the left of this new parity the tile to the left of this new
X error on the other hand is now unhappy we can continue this process of adding we can continue this process of adding
new X errors and as long as we form a new X errors and as long as we form a new X errors and as long as we form a
chain of adjacent X errors where chain of adjacent X errors where chain of adjacent X errors where
adjacent means that the cubits touch the adjacent means that the cubits touch the adjacent means that the cubits touch the
same tile we'll effectively push this same tile we'll effectively push this same tile we'll effectively push this
minus1 measurement outcome around and minus1 measurement outcome around and minus1 measurement outcome around and
what we find is that we always have what we find is that we always have what we find is that we always have
minus one syndrome measurement outcomes minus one syndrome measurement outcomes minus one syndrome measurement outcomes
at the end points of chains of adjacent at the end points of chains of adjacent at the end points of chains of adjacent
errors so we can always detect chains of errors so we can always detect chains of errors so we can always detect chains of
adjacent X errors as long as they have adjacent X errors as long as they have adjacent X errors as long as they have
end end end
points there could by the way be points there could by the way be points there could by the way be
multiple chains but but we'll come back multiple chains but but we'll come back multiple chains but but we'll come back
to this possibility to this possibility to this possibility
later it is possible however that we can later it is possible however that we can later it is possible however that we can
have a chain of adjacent X errors that have a chain of adjacent X errors that have a chain of adjacent X errors that
doesn't have n points which is to say doesn't have n points which is to say doesn't have n points which is to say
that our chain of Errors could form a that our chain of Errors could form a that our chain of Errors could form a
closed closed closed
loop so let's add some more errors so loop so let's add some more errors so loop so let's add some more errors so
that they form a closed loop like this that they form a closed loop like this that they form a closed loop like this
for for for
instance we now have an even number of X instance we now have an even number of X instance we now have an even number of X
errors on every tile so every stabilizer errors on every tile so every stabilizer errors on every tile so every stabilizer
generator is now generator is now generator is now
happy and what we found is that Clos happy and what we found is that Clos happy and what we found is that Clos
Loops of adjacent X errors are not Loops of adjacent X errors are not Loops of adjacent X errors are not
detected by this code because we obtain detected by this code because we obtain detected by this code because we obtain
the all plus1 syndrome in such a the all plus1 syndrome in such a the all plus1 syndrome in such a
case so that might seem kind of case so that might seem kind of case so that might seem kind of
disappointing in fact we only need four disappointing in fact we only need four disappointing in fact we only need four
adjacent X errors to form a closed loop adjacent X errors to form a closed loop adjacent X errors to form a closed loop
and we're hoping for something better and we're hoping for something better and we're hoping for something better
than a distance 4 than a distance 4 than a distance 4
code however some closed Loops of code however some closed Loops of code however some closed Loops of
adjacent X errors are themselves in the adjacent X errors are themselves in the adjacent X errors are themselves in the
stabilizer remember that we have an X stabilizer remember that we have an X stabilizer remember that we have an X
stabilizer generator for each vertex in stabilizer generator for each vertex in stabilizer generator for each vertex in
the lettuce and if we mult multiply the lettuce and if we mult multiply the lettuce and if we mult multiply
together adjacent X stabilizer together adjacent X stabilizer together adjacent X stabilizer
generators we can form closed Loops of X generators we can form closed Loops of X generators we can form closed Loops of X
operations such as we have right here operations such as we have right here operations such as we have right here
for the closed loop that we constructed for the closed loop that we constructed for the closed loop that we constructed
just a few moments just a few moments just a few moments
ago so some closed Loops of X errors ago so some closed Loops of X errors ago so some closed Loops of X errors
like the one that's shown here aren't like the one that's shown here aren't like the one that's shown here aren't
actually errors at all because they are actually errors at all because they are actually errors at all because they are
in the stabilizer so they do nothing at in the stabilizer so they do nothing at in the stabilizer so they do nothing at
all to the code space there are however all to the code space there are however all to the code space there are however
some closed Loops of X errors that are some closed Loops of X errors that are some closed Loops of X errors that are
not in the stabilizer and therefore they not in the stabilizer and therefore they not in the stabilizer and therefore they
represent non-trivial errors that go represent non-trivial errors that go represent non-trivial errors that go
undetected by the code and there's a undetected by the code and there's a undetected by the code and there's a
pretty simple way to characterize the pretty simple way to characterize the pretty simple way to characterize the
different types of different types of different types of
loops loops that cross every line in the loops loops that cross every line in the loops loops that cross every line in the
lattice meaning every vertical line from lattice meaning every vertical line from lattice meaning every vertical line from
top to bottom and every horizontal line top to bottom and every horizontal line top to bottom and every horizontal line
from left to right an even number of from left to right an even number of from left to right an even number of
times are always products of X times are always products of X times are always products of X
stabilizer stabilizer stabilizer
generators that includes the example generators that includes the example generators that includes the example
shown on the screen as well as Loops shown on the screen as well as Loops shown on the screen as well as Loops
that may be much more complicated that may be much more complicated that may be much more complicated
including the possibility that they including the possibility that they including the possibility that they
cross the periodic boundaries but cross the periodic boundaries but cross the periodic boundaries but
nevertheless by multiplying these closed nevertheless by multiplying these closed nevertheless by multiplying these closed
Loops of x Errors By X stabilizer Loops of x Errors By X stabilizer Loops of x Errors By X stabilizer
generators they can effectively be generators they can effectively be generators they can effectively be
shrunk down to shrunk down to shrunk down to
nothing and it's certainly True by the nothing and it's certainly True by the nothing and it's certainly True by the
way that we can only get loops like this way that we can only get loops like this way that we can only get loops like this
by multiplying together X stabilizer by multiplying together X stabilizer by multiplying together X stabilizer
generators because every X stabilizer generators because every X stabilizer generators because every X stabilizer
generator puts an even number of X generator puts an even number of X generator puts an even number of X
operations on every horizontal line and operations on every horizontal line and operations on every horizontal line and
on every vertical on every vertical on every vertical
line the other type of closed loop we line the other type of closed loop we line the other type of closed loop we
can have is one that crosses at least can have is one that crosses at least can have is one that crosses at least
one line an odd number of times such as one line an odd number of times such as one line an odd number of times such as
the one that's shown here which crosses the one that's shown here which crosses the one that's shown here which crosses
every horizontal line just once every horizontal line just once every horizontal line just once
and here's another example that crosses and here's another example that crosses and here's another example that crosses
each vertical line just once so these each vertical line just once so these each vertical line just once so these
are examples of non-trivial errors that are examples of non-trivial errors that are examples of non-trivial errors that
go undetected by the code but the key go undetected by the code but the key go undetected by the code but the key
here is that the only way to form a here is that the only way to form a here is that the only way to form a
closed loop like this is to go around closed loop like this is to go around closed loop like this is to go around
the Taurus either around the hole in the the Taurus either around the hole in the the Taurus either around the hole in the
middle of the Taurus or through the hole middle of the Taurus or through the hole middle of the Taurus or through the hole
or it could be or it could be or it could be
both this is a topological property both this is a topological property both this is a topological property
concerning aurus it's all about the concerning aurus it's all about the concerning aurus it's all about the
holes basically and that's why this is holes basically and that's why this is holes basically and that's why this is
called a topological Quantum error Cor called a topological Quantum error Cor called a topological Quantum error Cor
code in terms of this visualization of a code in terms of this visualization of a code in terms of this visualization of a
two-dimensional lattice with periodic two-dimensional lattice with periodic two-dimensional lattice with periodic
boundaries a chain in this category must boundaries a chain in this category must boundaries a chain in this category must
loop around one or both of the periodic loop around one or both of the periodic loop around one or both of the periodic
boundaries in a way that makes it boundaries in a way that makes it boundaries in a way that makes it
impossible to contract that Loop to a impossible to contract that Loop to a impossible to contract that Loop to a
point by multiplying it by X stabilizer point by multiplying it by X stabilizer point by multiplying it by X stabilizer
generators such a chain might be more generators such a chain might be more generators such a chain might be more
complicated than the one that's complicated than the one that's complicated than the one that's
Illustrated here but this is the Illustrated here but this is the Illustrated here but this is the
shortest that such a loop can possibly shortest that such a loop can possibly shortest that such a loop can possibly
be because it has to somehow loop around be because it has to somehow loop around be because it has to somehow loop around
one or both of the periodic bound one or both of the periodic bound one or both of the periodic bound
boundaries and what this implies is that boundaries and what this implies is that boundaries and what this implies is that
the minimum weight of a non-trivial the minimum weight of a non-trivial the minimum weight of a non-trivial
undetectable error is L any closed loop undetectable error is L any closed loop undetectable error is L any closed loop
with fewer errors than that must fall with fewer errors than that must fall with fewer errors than that must fall
into the first category and is therefore into the first category and is therefore into the first category and is therefore
in the stabilizer and chains of errors in the stabilizer and chains of errors in the stabilizer and chains of errors
that don't form closed Loops are that don't form closed Loops are that don't form closed Loops are
detected by the detected by the detected by the
code and that's it that's why this code code and that's it that's why this code code and that's it that's why this code
works and what we end up with is a 2 l^2 works and what we end up with is a 2 l^2 works and what we end up with is a 2 l^2
2 L stabilizer code we're using 2 L S 2 L stabilizer code we're using 2 L S 2 L stabilizer code we're using 2 L S
cubits to code 2 cubits and the code has cubits to code 2 cubits and the code has cubits to code 2 cubits and the code has
distance distance distance
L next I'll say just a little bit about L next I'll say just a little bit about L next I'll say just a little bit about
correcting errors with the Torah code correcting errors with the Torah code correcting errors with the Torah code
though I won't go too far into the though I won't go too far into the though I won't go too far into the
details if we measure the syndrome and details if we measure the syndrome and details if we measure the syndrome and
we see something other than the all we see something other than the all we see something other than the all
plus1 syndrome where I'm speaking here plus1 syndrome where I'm speaking here plus1 syndrome where I'm speaking here
about just the Z stabilizer generators about just the Z stabilizer generators about just the Z stabilizer generators
then the minus one outcomes indicate to then the minus one outcomes indicate to then the minus one outcomes indicate to
us the end points of one or more chains us the end points of one or more chains us the end points of one or more chains
of adjacent X of adjacent X of adjacent X
errors and we can attempt to correct errors and we can attempt to correct errors and we can attempt to correct
these Errors By pairing together the these Errors By pairing together the these Errors By pairing together the
minus one outcomes and forming a chain minus one outcomes and forming a chain minus one outcomes and forming a chain
of X Corrections between them and as of X Corrections between them and as of X Corrections between them and as
we're doing this it makes sense to we're doing this it makes sense to we're doing this it makes sense to
choose shortest paths for our choose shortest paths for our choose shortest paths for our
Corrections for example if we measure a Corrections for example if we measure a Corrections for example if we measure a
syndrome such as the one that's shown syndrome such as the one that's shown syndrome such as the one that's shown
here then we could choose this path here then we could choose this path here then we could choose this path
between the unhappy tiles and apply X between the unhappy tiles and apply X between the unhappy tiles and apply X
Gates as Corrections along this Gates as Corrections along this Gates as Corrections along this
path of course the actual chain of path of course the actual chain of path of course the actual chain of
Errors causing the syndrome could have Errors causing the syndrome could have Errors causing the syndrome could have
been different such as the one that's been different such as the one that's been different such as the one that's
shown here but that's okay as as long as shown here but that's okay as as long as shown here but that's okay as as long as
the errors and the corrections together the errors and the corrections together the errors and the corrections together
fall into the fall into the fall into the
stabilizer it could also be that our stabilizer it could also be that our stabilizer it could also be that our
Corrections complete a loop around the Corrections complete a loop around the Corrections complete a loop around the
Taurus which is bad because in doing Taurus which is bad because in doing Taurus which is bad because in doing
this we've caused a logical this we've caused a logical this we've caused a logical
error so this strategy of correcting error so this strategy of correcting error so this strategy of correcting
along shortest paths corrects low weight along shortest paths corrects low weight along shortest paths corrects low weight
errors but it might not work for higher errors but it might not work for higher errors but it might not work for higher
weight errors but of course we can't weight errors but of course we can't weight errors but of course we can't
expect anything more from any Quantum expect anything more from any Quantum expect anything more from any Quantum
error cting code more likely perhaps error cting code more likely perhaps error cting code more likely perhaps
depending upon the noise model is that depending upon the noise model is that depending upon the noise model is that
we observe a bunch of minus ones in the we observe a bunch of minus ones in the we observe a bunch of minus ones in the
syndrome and in this case we can use syndrome and in this case we can use syndrome and in this case we can use
efficient classical algorithms to help efficient classical algorithms to help efficient classical algorithms to help
us to pair them up and find a lowest us to pair them up and find a lowest us to pair them up and find a lowest
weight error representing the weight error representing the weight error representing the
syndrome as I mentioned in the previous syndrome as I mentioned in the previous syndrome as I mentioned in the previous
lesson this may not always correct the lesson this may not always correct the lesson this may not always correct the
most likely explanation for the syndrome most likely explanation for the syndrome most likely explanation for the syndrome
but it does work pretty well in general but it does work pretty well in general but it does work pretty well in general
and it always succeeds in correcting and it always succeeds in correcting and it always succeeds in correcting
errors having weight strictly less than errors having weight strictly less than errors having weight strictly less than
half the distance L and I should also half the distance L and I should also half the distance L and I should also
mention that there are other strategies mention that there are other strategies mention that there are other strategies
that are better Suited Ed to different that are better Suited Ed to different that are better Suited Ed to different
error error error
models it's been over 25 years since the models it's been over 25 years since the models it's been over 25 years since the
Tor code was discovered and in that time Tor code was discovered and in that time Tor code was discovered and in that time
there's been quite a lot of work on there's been quite a lot of work on there's been quite a lot of work on
Quantum error correcting codes including Quantum error correcting codes including Quantum error correcting codes including
other topological Quantum error other topological Quantum error other topological Quantum error
correcting codes inspired by the Torah correcting codes inspired by the Torah correcting codes inspired by the Torah
code as well as codes based on different code as well as codes based on different code as well as codes based on different
ideas I certainly can't give you a ideas I certainly can't give you a ideas I certainly can't give you a
comprehensive list of all of the comprehensive list of all of the comprehensive list of all of the
different code constructions that are different code constructions that are different code constructions that are
known but I will scratch the surface known but I will scratch the surface known but I will scratch the surface
just a little bit and briefly mention a just a little bit and briefly mention a just a little bit and briefly mention a
couple of well-known couple of well-known couple of well-known
examples first as a turns out it isn't examples first as a turns out it isn't examples first as a turns out it isn't
really essential that the Tor code has really essential that the Tor code has really essential that the Tor code has
periodic periodic periodic
boundaries that is it's possible to cut boundaries that is it's possible to cut boundaries that is it's possible to cut
out just a portion of the Tor code and out just a portion of the Tor code and out just a portion of the Tor code and
lay it flat on a two-dimensional surface lay it flat on a two-dimensional surface lay it flat on a two-dimensional surface
rather than on a Taurus and what we get rather than on a Taurus and what we get rather than on a Taurus and what we get
by doing that is called a surface by doing that is called a surface by doing that is called a surface
code for instance if we cut the edges of code for instance if we cut the edges of code for instance if we cut the edges of
a lattice in the pattern that's shown a lattice in the pattern that's shown a lattice in the pattern that's shown
here with so-called rough edges at the here with so-called rough edges at the here with so-called rough edges at the
top and bottom and smooth edges at the top and bottom and smooth edges at the top and bottom and smooth edges at the
sides and correspondingly we include sides and correspondingly we include sides and correspondingly we include
edge cases for the stabilizer generators edge cases for the stabilizer generators edge cases for the stabilizer generators
then we end up with a code that can then we end up with a code that can then we end up with a code that can
encode just one Cub at this time but the encode just one Cub at this time but the encode just one Cub at this time but the
code nevertheless inherits the most code nevertheless inherits the most code nevertheless inherits the most
important characteristics of the Tor important characteristics of the Tor important characteristics of the Tor
code in particular non-trivial code in particular non-trivial code in particular non-trivial
undetectable errors for this code must undetectable errors for this code must undetectable errors for this code must
correspond to chains of errors that correspond to chains of errors that correspond to chains of errors that
either stretch from the left Edge to the either stretch from the left Edge to the either stretch from the left Edge to the
right Edge or from the top to the bottom right Edge or from the top to the bottom right Edge or from the top to the bottom
depending on whether they're chains of X depending on whether they're chains of X depending on whether they're chains of X
errors or Z errors errors or Z errors errors or Z errors
respectively it's also possible to cut respectively it's also possible to cut respectively it's also possible to cut
the edges diagonally to obtain what are the edges diagonally to obtain what are the edges diagonally to obtain what are
sometimes called rotated service codes sometimes called rotated service codes sometimes called rotated service codes
and doing that can be helpful for making and doing that can be helpful for making and doing that can be helpful for making
the codes more economical in terms of the codes more economical in terms of the codes more economical in terms of
how many cubits are how many cubits are how many cubits are
used for example if we take this surface used for example if we take this surface used for example if we take this surface
code and we throw away some of the code and we throw away some of the code and we throw away some of the
cubits leaving a square oriented cubits leaving a square oriented cubits leaving a square oriented
diagonally we obtain another service diagonally we obtain another service diagonally we obtain another service
code so long as we specify the code so long as we specify the code so long as we specify the
stabilizer generators on the boundary in stabilizer generators on the boundary in stabilizer generators on the boundary in
the right way diagrams like this are the right way diagrams like this are the right way diagrams like this are
helpful for making clear how the helpful for making clear how the helpful for making clear how the
stabilizer generators are defined where stabilizer generators are defined where stabilizer generators are defined where
black tiles including the rounded ones black tiles including the rounded ones black tiles including the rounded ones
at the edges indicate X stabilizer at the edges indicate X stabilizer at the edges indicate X stabilizer
generators and the white ones indicate Z generators and the white ones indicate Z generators and the white ones indicate Z
stabilizer generators the ones on the stabilizer generators the ones on the stabilizer generators the ones on the
boundary by the way are just 2 Cubit XX boundary by the way are just 2 Cubit XX boundary by the way are just 2 Cubit XX
or ZZ stabilizer generators this or ZZ stabilizer generators this or ZZ stabilizer generators this
time and you'll commonly see these time and you'll commonly see these time and you'll commonly see these
diagrams rotated by 45° another interesting class of codes 45° another interesting class of codes
which also fall into the general which also fall into the general which also fall into the general
category of topological codes are color category of topological codes are color category of topological codes are color
codes I won't explain exactly how they codes I won't explain exactly how they codes I won't explain exactly how they
work but one way to think about them is work but one way to think about them is work but one way to think about them is
that they're geometric generalizations that they're geometric generalizations that they're geometric generalizations
of the 7 Cubit steam code of the 7 Cubit steam code of the 7 Cubit steam code
so let's consider the seven cubits team so let's consider the seven cubits team so let's consider the seven cubits team
code and suppose that we number our code and suppose that we number our code and suppose that we number our
cubits using the KCET ordering cubits using the KCET ordering cubits using the KCET ordering
Convention as is shown here on the Convention as is shown here on the Convention as is shown here on the
screen we can then associate these seven screen we can then associate these seven screen we can then associate these seven
cubits with the vertices of the graph cubits with the vertices of the graph cubits with the vertices of the graph
that's shown here and what we find is that's shown here and what we find is that's shown here and what we find is
that the stabilizer generators match up that the stabilizer generators match up that the stabilizer generators match up
with the tiles or the faces that are with the tiles or the faces that are with the tiles or the faces that are
formed by the edges of the graph and for formed by the edges of the graph and for formed by the edges of the graph and for
each tile we always have a z stabilizer each tile we always have a z stabilizer each tile we always have a z stabilizer
generator and an X stabilizer generator generator and an X stabilizer generator generator and an X stabilizer generator
that act non-trivially on precisely that act non-trivially on precisely that act non-trivially on precisely
those cubits that are found at the those cubits that are found at the those cubits that are found at the
vertices of that vertices of that vertices of that
tile this is nice because just like for tile this is nice because just like for tile this is nice because just like for
the Torah code we have geometric the Torah code we have geometric the Torah code we have geometric
locality for this code so we don't need locality for this code so we don't need locality for this code so we don't need
to move cubits over long distances to to move cubits over long distances to to move cubits over long distances to
measure stabilizer generators and the measure stabilizer generators and the measure stabilizer generators and the
fact that the Z and X stabilizer fact that the Z and X stabilizer fact that the Z and X stabilizer
generators always act non-trivially on generators always act non-trivially on generators always act non-trivially on
exactly the same sets of cubits is also exactly the same sets of cubits is also exactly the same sets of cubits is also
nice for reasons connected with fault nice for reasons connected with fault nice for reasons connected with fault
tolerant Quantum computation which I'll tolerant Quantum computation which I'll tolerant Quantum computation which I'll
say more about in the next say more about in the next say more about in the next
lesson color code are quantum error lesson color code are quantum error lesson color code are quantum error
correcting codes that generalize this correcting codes that generalize this correcting codes that generalize this
basic pattern where we do essentially basic pattern where we do essentially basic pattern where we do essentially
the same thing except with different the same thing except with different the same thing except with different
graphs for example here's a graph with graphs for example here's a graph with graphs for example here's a graph with
19 vertices that 19 vertices that 19 vertices that
works this gives us a code that encodes works this gives us a code that encodes works this gives us a code that encodes
1 Cubit into 19 and it happens to have 1 Cubit into 19 and it happens to have 1 Cubit into 19 and it happens to have
distance five these are called color distance five these are called color distance five these are called color
codes by the way because one of the codes by the way because one of the codes by the way because one of the
required conditions on the graph is that required conditions on the graph is that required conditions on the graph is that
the faces can be three-colored meaning the faces can be three-colored meaning the faces can be three-colored meaning
that we can color the faces with three that we can color the faces with three that we can color the faces with three
colors and we never have two faces of colors and we never have two faces of colors and we never have two faces of
the same color sharing an edge the the same color sharing an edge the the same color sharing an edge the
colors don't actually matter for the colors don't actually matter for the colors don't actually matter for the
code itself we simply have x and z code itself we simply have x and z code itself we simply have x and z
stabilizer generators for each face stabilizer generators for each face stabilizer generators for each face
regardless of its color but the colors regardless of its color but the colors regardless of its color but the colors
do turn out to be important for do turn out to be important for do turn out to be important for
analyzing how it analyzing how it analyzing how it
works so those are two very well-known works so those are two very well-known works so those are two very well-known
families of codes but there are many families of codes but there are many families of codes but there are many
other constructions that are known other constructions that are known other constructions that are known
including ones that are based on ideas including ones that are based on ideas including ones that are based on ideas
similar to these code families as well similar to these code families as well similar to these code families as well
as completely different as completely different as completely different
ones a specific example of a Quantum ones a specific example of a Quantum ones a specific example of a Quantum
error correcting code that was recently error correcting code that was recently error correcting code that was recently
discovered by some of my colleagues at discovered by some of my colleagues at discovered by some of my colleagues at
IBM is known as the gross code which is IBM is known as the gross code which is IBM is known as the gross code which is
so named because a gross is a dozen so named because a gross is a dozen so named because a gross is a dozen
dozen or dozen or dozen or
144 in particular this code encodes 12 144 in particular this code encodes 12 144 in particular this code encodes 12
cubits into 144 cubits and it has cubits into 144 cubits and it has cubits into 144 cubits and it has
distance 12 I won't explain how this distance 12 I won't explain how this distance 12 I won't explain how this
code Works in detail it's similar to the code Works in detail it's similar to the code Works in detail it's similar to the
Tor code except that each of the Tor code except that each of the Tor code except that each of the
stabilizer generators now involves two stabilizer generators now involves two stabilizer generators now involves two
additional cubits that are just a little additional cubits that are just a little additional cubits that are just a little
bit further bit further bit further
away it doesn't have a planer embedding away it doesn't have a planer embedding away it doesn't have a planer embedding
like a surface code but it does have a like a surface code but it does have a like a surface code but it does have a
so-called Baner impeding meaning that so-called Baner impeding meaning that so-called Baner impeding meaning that
two layers of cubits are needed rather two layers of cubits are needed rather two layers of cubits are needed rather
than one it falls into a family of codes than one it falls into a family of codes than one it falls into a family of codes
called byari bicycle codes and what's called byari bicycle codes and what's called byari bicycle codes and what's
particularly remarkable about them is particularly remarkable about them is particularly remarkable about them is
that they're much more efficient than that they're much more efficient than that they're much more efficient than
the Torah code or a surface code in the the Torah code or a surface code in the the Torah code or a surface code in the
sense that a lot more cubits can be sense that a lot more cubits can be sense that a lot more cubits can be
encoded while simultaneously they encoded while simultaneously they encoded while simultaneously they
tolerate roughly the same rate of tolerate roughly the same rate of tolerate roughly the same rate of
Errors for the gross code an additional Errors for the gross code an additional Errors for the gross code an additional
144 cubits are needed for the syndrome 144 cubits are needed for the syndrome 144 cubits are needed for the syndrome
measure measure measure
so the total number of cubits required so the total number of cubits required so the total number of cubits required
is 288 which makes it a very promising is 288 which makes it a very promising is 288 which makes it a very promising
candidate for today's candidate for today's candidate for today's
technology and that concludes this technology and that concludes this technology and that concludes this
lesson which has been on constructions lesson which has been on constructions lesson which has been on constructions
of quantum error creating codes of quantum error creating codes of quantum error creating codes
including CSS codes the Torah code and a including CSS codes the Torah code and a including CSS codes the Torah code and a
glimpse at other Quantum Code glimpse at other Quantum Code glimpse at other Quantum Code
constructions including surface codes constructions including surface codes constructions including surface codes
and color and color and color
codes I hope you will join me for the codes I hope you will join me for the codes I hope you will join me for the
next and final lesson of the series on next and final lesson of the series on next and final lesson of the series on
fault tolerant Quantum computation which fault tolerant Quantum computation which fault tolerant Quantum computation which
addresses the challenge of Performing addresses the challenge of Performing addresses the challenge of Performing
comp computations un encoded Quantum comp computations un encoded Quantum comp computations un encoded Quantum
States using operations and Computing States using operations and Computing States using operations and Computing
components that are themselves prone to components that are themselves prone to components that are themselves prone to
errors goodbye until then

## Quantum Query Algorithms ｜ Understanding Quantum Information & Computation ｜ Lesson 05

- Welcome back to "Understanding Quantum
Information and Computation." My name is John Watrous, and I'm the Technical Director
for IBM Quantum Education. This is the fifth lesson of the series, and it's the first lesson in
the second unit of the series, which is on Quantum Algorithms. In this unit, we'll investigate
computational advantages of quantum information, that is, we'll take a
look at what we can do with quantum computers and the advantages that quantum computers might potentially have
over classical computers. We're gonna be focusing on what we can do with a single quantum computer as opposed to a distributed
setting, for instance, where multiple quantum computers interact over a network of some sort. There are, in fact, quantum advantages to be found in distributed settings, where communication and
cryptography come into play, but we're not going to be
talking about those things in this unit. This unit is about quantum algorithms running on single quantum computers. Let's start with a natural question. What advantages might a quantum
computer potentially offer? The main one is that quantum computers might provide faster solutions to some computational problems. Time is precious, and although the computers we
use every day are pretty fast, they're still way too slow to solve certain computational problems just because of the inherent
computational difficulty of those problems. This potential that quantum computers might allow us to solve problems that classical computers
are too slow to solve is really what's driven
quantum algorithms research for the past few decades. There are other computational
resources besides time that can be considered, like the amount of computer memory required for computations, but quantum computers can't
offer too much savings in terms of memory, as it turns out, and classical computer
memory is pretty inexpensive particularly compared with quantum memory. So there just isn't
very much promise here. We could also think about other resources like the amount of energy
needed for computation, but you don't actually
need much energy at all to compute classically, particularly if you're
willing to slow things down. So it really is the time
required for computations, that's our main focus. Here's an overview for the lesson, which is about a specific
model of computation known as the query model. You can think of this model as being kind of like a Petri dish for developing algorithmic ideas, both quantum and classical. It's not meant to be a practical model that describes the sorts
of computational problems we encounter in real life. We'll move away from the query model in subsequent lessons of this unit and focus instead on models motivated by more practical
notions of computation, but the query model is a
very good place to start. It's a simple model that allows us to explore the
potential of quantum computers without a lot of technical
details getting in the way, and it also allows us to isolate some of the
really important ideas behind quantum computing and how it works. Quantum computing did, in fact, first develop within this model, and the ideas that were
developed within it very directly inspired
important quantum algorithms such as Shor's algorithm for factoring. In the first section of the lesson, we'll take a look at specifically what the query model of computation is, and in the second, third,
and fourth sections, we'll discuss three
quantum query algorithms, namely Deutsch's algorithm, the Deutsch-Jozsa algorithm,
and Simon's algorithm, which reveal increasingly
impressive advantages of quantum computers
over classical computers within the query model of computation. To explain the query model of computation, let's first begin with a more standard
abstraction of computation. Here's a picture that
represents a computation in very simple and basic terms. We have an input, which usually takes the
form of a binary string, although we could use other
symbols if we wanted to. The input is provided to the computation, which is represented by a blue
rectangle in this picture, and the result of the computation is that some output is produced, and again, that's most
typically a binary string. Of course, this picture
is not at all specific about how the computation itself works. We can formulate different
specific computational models to do that, such as Turing machines
and Boolean circuit, or quantum circuits if we're thinking about
quantum computation. It's true that this picture
doesn't represent the fact that many of the
computers we use every day are continuously receiving
input and producing output, essentially interacting with us and with other computers through
the internet, for instance. But we're not trying to represent
those interactions here. This is about modeling
isolated computational tasks. For example, the input might be the
binary encoding of a number or a representation of a matrix or a description of a molecule or really anything else we have in mind, and there's some computational task being performed on that input alone. The key point here is
that the entire input is provided to the computation, and nothing about the
input is hidden from it. The query model of
computation works differently. In the query model, the input is made available
in the form of a function which the computation can
access by making queries, or in other words, by evaluating the function
on different inputs. Here you can see that represented
in the form of a picture. Similar to what we had before, the computation is represented
by a blue rectangle, but instead of receiving an input directly in the form of a string
of bits, for instance, it accesses the input by effectively asking questions
and receiving answers. We sometimes use the
terms oracle and black box to refer to the mechanism
that provides the input. There is a distinction between
an oracle and a black box in complexity theory, but that won't be relevant at all for the purposes of this discussion. Anyway, these terms are meant
to convey the basic idea. In ancient Greece, you
could go to an oracle, like the Oracle at Delphi,
and ask an important question, and the oracle would answer
that specific question, but she certainly wouldn't tell you everything that she knew. Along similar lines, when
we think about a black box, we have in mind some
sort of process or system where we can choose what goes in and we can see what comes out, but we don't have any understanding
of its inner workings. And that's the case here. The input to the problem
takes the form of a function, and all that we can do is
to evaluate that function, but otherwise, we don't have
any knowledge or understanding of how it works. Throughout the lesson,
we'll reserve the letter f for the function that
represents the input, and we'll make the assumption that it always takes the
form that you see right here. Sigma denotes the binary alphabet, so f is a function from strings
of bits to strings of bits, and specifically, we'll
assume that the input to f is a string of bits having length n, and the output is a string
of bits having length m, where n and m are positive integers. For different problems, we'll
have restrictions on n and m, but in all cases, the input function will
take a form like this. When we say that a
computation makes a query, what we mean is that the
function f is evaluated once, that is, some string x of length N is selected by the computation, and the string f of x is
then made available to it by the oracle or the black box. And finally, the very simple way that we'll measure the computational cost of a query algorithm is to count how many queries it requires. So, although it is true that
we are really most interested in how long computations take, when we study the query model, we generally keep things
simple and just count queries, and that's what we will do in this lesson. Now, let's take a look at a
few examples of query problems, meaning computational
problems in the query model, and here, by the way, I'm just gonna describe
the problems themselves, and at this point, I won't say anything
about how to solve them. The first problem is the OR problem. In this case, the input is a function
from n bits to just 1 bit, so m is equal to 1 for this problem, and the required output for
the problem is either 1 or 0. If there is some n bits string x for which f of x is equal to 1, then the correct answer is 1, and otherwise, if f of x is
always 0 for every string x, then the correct answer is 0. So if we think about the function f as providing us with random
access to two to the n bits, one for each n bit string x, then what the it's asking
for is the OR of those bits, and that's why it's called the OR problem. Another example is the parity problem, which is similar to the OR problem except that asks for the parity, or in other words, the eXclusive OR of all of the single-bit
output values of the function. Another way to say that is
that the correct answer is 0 if the number of n bits strings x for which f of x is equal to 1 is even, and the correct answer is
1 if that number is odd. An example of a problem where m is not necessarily equal to 1 is the minimum problem. Here, the function f takes the usual form, and there are no restrictions on n and m, and the correct answer is whatever output value
of the function comes first in the lexicographic
or dictionary ordering of binary strings. Alternatively, if you wanna
think about strings of length m as representing non-negative
integers in binary notation, then the correct answer
is the smallest possible or minimum value that
comes out of this function. Sometimes we also consider query problems where we have a promise
on the input function, or in other words, some
sort of guarantee on it, and we don't worry about functions that don't meet that promise. Functions that don't satisfy the promise are considered as "don't care" inputs. And we're just not responsible for what happens on those inputs. Here's an example of a query problem with a promise called unique search. It's related to the OR
problem, but it's different. The input function has the same
form as for the OR problem, which is that it takes
n bits strings to bits. This time, we're promised that
there's exactly one string Z that causes f to evaluate to 1, with f of x being equal to 0
for every string x besides z, and the correct output
is this unique string z. Of course, there are
choices for the function f that don't satisfy this condition, but we don't place any requirements at all on algorithms for this problem when they're given such
functions as input. We only care about the functions that do satisfy the promise. Now, all four of the
problems I just mentioned are pretty natural problems, meaning that it's easy to explain them, and it's easy to imagine situations in which a need to solve them might arise, but some query problems
aren't like this at all. In fact, sometimes, we consider highly
contrived query problems, where it's difficult to imagine that anyone would ever
actually wanna solve them. Simon's problem, which we'll
see later in the lesson, kind of falls into this category, but there are some other ones that are much worse in some sense, but they, nevertheless, actually turn out to be pretty important to the study of the query model. We don't study these problems because we necessarily want
to solve them in practice. The way you can think about it is that we are looking for
extremes of various sorts, in particular, when we're working within
this Petri dish of a model, a natural way to explore
quantum algorithmic ideas is to formulate highly-contrived problems, where we can best exploit the
power of quantum computers. And this can actually
shed a great deal of light on how quantum computers
can potentially be useful in more practical settings. We can formalize the notion
of a query in different ways depending on what computational
model we're working with. For example, in the Turing
machine model of computation, which we haven't talked about and we won't get into in this series, there's a certain way of
formalizing the notion of a query that makes sense for that model. For circuit models, we define special types of
gates called query gates, and that's how queries
are formally represented. For Boolean circuits, there's a very simple way of doing this, which is simply to imagine
that we have query gates that evaluate the input function directly, like you can see here on the screen. The idea is that we can
use these query gates to build circuits that solve whatever
problem we're looking at, and in order to work correctly, the circuits have to give the right answer for whatever input function f is given, assuming that it meets the promise in the case of a promise problem. Here's a simple example
of a Boolean circuit that solves the parity
problem when n is equal to 1, so the input function f is from
1 bit to 1 bit in this case. What this circuit does is to evaluate the function
f on the inputs 0 and 1 using these two query gates. It then plugs the two output values it
gets from the query gates into the Boolean circuit from lesson three that computes the XOR, which is the same thing as the parity. To be clear, these two
input values right here are not inputs into the problem,
meaning the parity problem, when we're working with the query model, the input to the problem
is the function f, and you should think about these
two inputs into the circuit as being hard-coded
values that never change, they're essentially part of the algorithm. We want to compute the XOR of f 0 and f 1, so we initialized these
two wires in this way to make it work. We've used two query gates
here, this one and this one, and so the algorithm represented
by this Boolean circuit makes two queries, and if we think about the
parity problem for a moment for functions from one bit to one bit, we see that it's absolutely necessary for any Boolean circuit
that solves this problem to make at least two queries. If you only make one query so that you only know one
output value of the function f and not the other, you don't actually have
any information at all about the parity, because the other output value
could equally well be 0 or 1, leading to either of the
two possible solutions to the problem. So in terms of the number
of queries that it makes, this Boolean circuit is as good as it gets for the parity problem
when n is equal to 1. There's one other point about query gates that's worth mentioning before we move on, and that is that when we're designing and analyzing query algorithms, we don't worry at all about how
query gates are implemented. It's assumed that these
query gates are given to us, and the difficulty of building
them isn't our concern. You can think of the
query gates themselves as being the input if you like, and it's not our job to create the input, we're just trying to
solve a particular problem given the input. Of course, when f is a function
from one bit to one bit, there are only four possible choices, and it would be easy enough
to implement any one of them, but in general, for functions
with many input bits, it could be extremely difficult to implement query gates
for those functions. To be clear, this isn't
to say that the issue of how query gates are
implemented isn't important, but rather that it isn't considered to be part of the cost
of a query algorithm. This is consistent with the idea that this isn't really
about practical computing or actually building circuits per se, it's about working within
a theoretical framework that tells us interesting
things about computation. We will have more to say about this when we make the jump from this model to a more standard model of computation, and that's mainly what the next lesson, Lesson 6, will be about. Now let's turn our attention
to quantum circuits. For quantum circuits, we're going to change the
definition of query gates to make them unitary. We want to be able to make
queries within quantum circuits and for them to operate
linearly on quantum states so that we can effectively
make queries in super position. It might seem like we're moving
the goalposts, so to speak, by changing what we mean by a query in comparison to what we just
saw for Boolean circuits, but there is justification for it that we'll discuss in more
detail in the next lesson. For now, it's just a definition. Specifically, the
definition is as follows, given any function f of
the same form as before, meaning that it maps n bits
strings to m bits strings, the query gate Uf acts on n + m qubits and is defined by the
action described here on standard basis states. We'll see a figure
representing this definition in just a moment, but first, let me make clear the notation
that's being used here, and that is whenever we
use the symbol for the XOR as an operation between two
strings of the same length, we're referring to the
bitwise eXclusive OR, and here you can see an example. In the definition, we have that both y and f of
x are strings of length m, so this string right here
represents the string we get by XORing those two strings
together, one bit at a time, and here's the diagram
explaining how these gates work. The top n qubits correspond to
the argument of the function. And when they start out
in a standard basis state, cat x in this diagram, that standard basis state
goes through unchanged and gets echoed as part of the output. The bottom m qubits correspond to the output
of the function f, and when they start out in
any standard basis state, cat y in this diagram, what happens is that the
output of the function f of x gets XORed onto the string y. This is what happens for
standard basis states, and if we wanna know
what happens in general for other quantum states, we
use linearity, as we always do. A reason to define query gates like this where we echo the input and we use the bitwise XOR for the output is that these gates are always unitary for any choice of the function f. This is, in fact, a
deterministic operation. And if we were to express it as a matrix for any choice of a function F, we'd get a permutation matrix, meaning one with a single one in each row and in each column, and all other entries equal to 0. And permutation matrices
are always unitary. In fact, for this operation, we'll get that the operation
is its own inverse, so Uf dager equals Uf. If all we wanna do with these
gates is to evaluate f of x for some string x, that's easy, we can just set y to
be the all-zero string. So when we take the bit y's eXclusive OR, we end up with just f of x. Here, by the way, you can see the notation that's used to denote the all-zero string. It looks like 0 to the power M, but that's not what it means. It means the string of zeros
having length equal to M. When we're talking about
strings as opposed to numbers, exponents typically mean that we repeat that string
some number of times, so this is 0 repeated m times. It's just a convenient and precise way of describing this string. Once again, when we're
working with the query model, we aren't worrying about how
to implement these query gates. That's the same issue
we had with query gates for Boolean circuit. It's not our job to prepare the input, we're given the input in the
form of these query gates. I will point out though, that if we know how to
implement the function f using a Boolean circuit, then it is possible to
translate that implementation pretty directly to a quantum
circuit implementation of Uf, and we'll see exactly how
that works in the next lesson. As an aside, it is quite straightforward to create a Boolean circuit
that operates like this, using a single classical
query gate for the function f, but that is quite different from having a quantum
circuit implementation of this unitary gate. The point, however, is that
for classical query algorithms, having a query gate that works like this is, in fact, equivalent to having an ordinary
classical query gate for f, in the sense that either one can be implemented using the other. In this section of the lesson, we'll discuss Deutsch's algorithm, which is a very simple
quantum query algorithm, but it really is a gem. This algorithm comes from a 1985 paper written by David Deutsch, was one of the very first people to work on the theoretical
foundations of quantum computing. The algorithm itself represents a very small part
of Deutsch's 1985 paper, and in fact, the algorithm we now
call Deutsch's algorithm is actually an improved version of the algorithm that Deutsch described. It provides a very modest example of how quantum computers can
offer computational advantages, but you have to start somewhere, and it really was an important spark that got things started. Deutsch's algorithm solves a query problem that's often called Deutsch's problem in the context of quantum computing, but in fact, it's none other
than the parity problem for functions from one bit to one bit, which we've already talked about. As we've already seen, there are four functions
from one bit to one bit, and they're described here on the screen by their tables of values. The first and last of these
functions are constant, while the middle two are balanced in the sense that the two
possible output values appear the same number of times. For the two constant functions,
we see that the parity, or in other words, the eXclusive OR of the two outputs is 0, and for the two balance functions, the parity of the outputs is 1. So an equivalent way to
describe the parity problem in this particular case, where the function f takes
a single bit as input, or in other words, Deutsch's problem, is like you see here. The input is a function f
from one bit to one bit, and the output is 0 if f is constant and 1 if f is balanced. As I've already suggested, every classical query algorithm
that solves this problem must make at least two queries to f. If you learn just one of
the two output values of f, you still have no information at all about the parity of the two outputs. And this, by the way, is true
even for classical algorithms that make use of randomness. Randomness can sometimes be very useful for solving query problems, and we'll see an example
a bit later in the lesson, but randomness doesn't help here. So our algorithm from before is optimal among classical query algorithms
for Deutsch's problem. Now we'll take a look
at Deutsche's algorithm, which is a quantum query algorithm that solves Deutsch's
problem using a single query. And here is the algorithm
described as a quantum circuit. We have two qubits
initialized like you see here, with the top one initialized to cat 0 and the bottom one initialized to cat 1, and we'll see why we initialize these qubits like this shortly. We perform Hadamard
gates on the two qubits, feed them into the query gate, perform another Hadamard
gate on the top qubit, and then measure it. And the claim is that
the measurement outcome gives us, with certainty, the
correct answer to the problem. So let's analyze the
circuit one step at a time to see exactly how it works. The state of the two qubits after the first layer of Hadamard gates is the tensor product of
the minus and plus states. The Hadamard on top maps
the 0 state to a plus state, and the Hadamard on the bottom maps the 1 state to a minus state. The top qubit is on the right, and the bottom qubit is on the left, so this is the state that we get. It's very important that the bottom qubit is in a minus state, by the way, this wouldn't work if we
initialize the bottom qubit to a 0 state instead of a 1 state. We're going to expand this state
halfway, like you see here. The reason for doing that
isn't clear at this point, but it will be convenient
in just a moment. Next, let's think about
the state of the two qubits after the query gate is performed. The way the query gate
works is that it computes F on the standard basis
state of the top qubit and XOR is the value
onto the bottom qubit, and here we can see the
result of doing that. For the first term, the standard basis state
of the top qubit is 0, so f of zero gets XORed
onto the bottom qubit, and that works by linearity. And it's similar for the second term, except that the standard basis state for the top qubit is 1, so we XOR f of 1 onto the bottom qubit. And at this point, we can
simplify this expression by observing the formula
that you can see down here, which works when a is either 0 or 1. And we can simply check this for those two values separately. If a is equal to 0, then
XORing with a does nothing, and negative 1 to the power 0 is 1, so the two sides are equal. And if a is equal to 1, we get cat 1 minus cat
0 when we take the XOR, which is negative 1
times cat 0 minus cat 1. And if we use that formula
to simplify our state, where we take a is equal
to f of 0 on the left and a is equal to f of 1 on the right, we get the expression
that you see right here. So when we compare the two
quantum state vectors π1 and π2, we see that what the Uf gate does is effectively to put
one of the two values, f of 0 or f of 1, into the
exponent of negative 1, or in other words, into
the phase of each term. This is the reason for
initializing the bottom qubit to 1 and performing a Hadamard gate on it to create the minus state,
which allows this to happen. It's called the phase kickback phenomenon, and we'll come back to it in a few moments to have another look at it. It's pretty important, and we'll see it happening a
few more times in the lesson, as well as in later lessons. We can now simplify by pulling
the minus state back outside to get the expression
that you see right here. As an aside, it's kind
of interesting to notice that there isn't actually
any entanglement here. Both of these states are product states. So, well, entanglement
is extremely interesting, doesn't happen to be
playing any role at all in this particular algorithm. Let's clean this up a bit so that we have a little bit more room. What we can now do is to
pull a factor of negative 1 raised to the power f of zero outside to get the expression of the
state that you see right here. You'll notice that right here, we have eXclusive OR of f 0 and f 1 in the exponent of negative 1. We might expect that to
be f of 1 minus f of 0 because we pull the factor of negative 1 to the power f of 0 out, but we get the same
value if we take the XOR because these are bits, and the only thing that matters when we have an integer in
the exponent of negative 1 is whether that integer is even or odd. Another way to express this
state is like we have here, where we're just considering
the two possible cases for the XOR. and that's a convenient
way to express the state as we consider the action
of the final Hadamard gate together with the measurement. When we perform the last
Hadamard gate on the top qubit, the plus state is
transformed into a 0 state and the minus state is
transformed into a 1 state, which we can write more
succinctly like this. And finally, when the
measurement is performed, the result we obtain is
therefore the XOR of f 0 and f 1, which is the value we're looking for, and that's how Deutsch's algorithm works. It's pretty simple, and if we ask ourselves what
it is that makes it work, allowing us to compute
the parity of two bits with a single query rather than two, a reasonable answer is
that it's interference. We're effectively
computing f of 0 and f of 1 at the same time, because we first performed
the Hadamard gate on the top qubit to put the input into f into
a super position of 0 and 1. And by means of the phase kickback together with the final Hadamard gate, we're essentially creating
constructive interference for the correct answer and destructive interference
for the wrong answer. So we don't see the wrong answer, we only see the correct answer. Let me now say a bit more about the phase kickback phenomenon just to see it from a
slightly different angle. First, let's observe
this very simple formula where b and c are arbitrary binary values. In short, XORing a bit b by the value 1 is the same thing as performing a NOT gate or an x operation on b, and of course, exploring
a bit b by the value 0 doesn't do anything to it, which is the same thing as performing the
identity operation on it. And we can express the identity operation as x to the power 0. So if we perform a Uf gate on two qubits in the standard basis
state cat b tensor cat a, we obtain this outcome, just by the definition of the Uf gate. and by using the formula, we can alternatively write that state like you see right here. This formula works for all choices of b, so by linearity, it works
for an arbitrary state SI in place of cat b. And in particular, if we want to, we can choose SI to be the minus state. And now here's the key to why the phase x kickback
phenomenon happens. It's because the x operation
applied to the minus state gives us negative 1 times the minus state. In mathematical terms, we say that the minus
state is an eigenvector of the x operation, with eigen value equal to negative 1. Eigenvectors and eigen values
are critically important in linear algebra as well
as in quantum information, and we'll have a lot
more to say about them as the series continues. But for now, all we need
is this simple formula. And using it, we can
simplify our state vector. And that right there is essentially the phase
kickback phenomenon written as a formula. It works because the minus
state is an eigenvector of the NOT operation having
eigen value negative one. We will generalize this in a
pretty major way in Lesson 7, when we turn to the phase
estimation procedure, which is what drives
Shor's factoring algorithm. We can see this formula in action in the analysis of Deutsch's
algorithm as follows. Prior to the query gate, our state is the tensor product of the minus and plus states. We then apply the query gate, and expanding out the plus
state gives us this state. We now apply the phase kickback formula to get this expression, and then the analysis
carries on as before. It's the same as before, but here we're just placing
a focus on the phase kickback so that we can recognize it and we can also be ready for
it when it comes up again. Next we'll turn to the
Deutsch-Jozsa algorithm, which is an extension
of Deutsch's algorithm that actually solves a couple
of different query problems. The advantage that
Deutsch's algorithm provides over classical algorithms
is pretty modest, one query versus two. The Deutsch-Jozsa algorithm increases that advantage somewhat, and I'll explain what I mean
by that when we get there. The Deutsch-Jozsa algorithm can be viewed as a natural and direct way of extending Deutsch's algorithm from functions from one bit to one bit to functions from n bits to one bit, or another words, functions of this form. As a quantum circuit, the Deutsch-Jozsa
algorithm looks like this, and I'm going to refer to this
as the Deutsch-Jozsa circuit just to draw a minor
distinction between this circuit and the Deutsch-Jozsa algorithm itself because there's also a
classical post-processing step. The idea is that if we run
this circuit and then measure, we'll obtain some n bits string y that tells us something
about the function f. At this point, by the way, I haven't actually said
what the query problem is that we might hope to
solve with this circuit, that's coming up very soon. But the point is that
by running this circuit, we'll gain some information about f that can potentially help
us to solve query problems where f is the input. And like I said a moment ago, we can actually use this circuit to solve a couple of
different query problems. First, let's take a look
at the query problem known as the Deutsch-Jozsa problem, which is the problem that
the Deutsch-Jozsa circuit was originally intended to solve. The Deutsch-Jozsa problem is a generalization of Deutsch's problem, where the goal is to determine whether a given function
is constant or balanced. Notice that this isn't the natural way to generalize the parity problem
to functions of this form. Here we're not computing the parity, the task is to output 0 if
the function f is constant and 1 if the function f is balanced. The first thing to notice about
this problem is that when n, the number of input bits to
the function f, is at least 2, there are choices for the function that are neither constant nor balanced. For example, here's a
function from 2 bits to 1 bit that's neither constant nor balanced. It's not constant because both binary values
appear among the outputs, and it's not balanced
because the two output values appear with different frequencies. 0 appears three times
and 1 appears just once. In order to be balanced,
the outputs 0 and 1 have to occur exactly
the same number of times. The fact that there are functions that are neither constant nor balanced when n is at least 2 is okay though, we're just gonna consider
functions like that as "don't care" inputs. So in other words, this
is a promise problem. And here's a precise
statement of the problem. We're promised that the input function f is either constant or balanced, and the goal is to figure
out which one it is. And we're not responsible for what happens if the input
doesn't meet the promise. The Deutsch-Jozsa algorithm,
which solves this problem, is first to run the Deutsch-Jozsa circuit and then output 0 if the string that we
obtain from the measurements is the all-zero string
and output 1 otherwise. Another way to say that is that we output the OR of
the bits that we measure. To analyze the Deutsch-Jozsa algorithm, it'll be helpful to
first take a few moments to think about Hadamard gates. We can, of course, describe the Hadamard
operation as a matrix, but we can also describe
it in terms of its action on standard basis states,
as you see right here. These two equations can be
combined into a single equation, which describes how a
Hadamard operation works for a being either of
the two binary values, and we can go a little bit further and express what we
obtain as a sum like this. Using that formula, we can come up with a formula that describes how a layer
of Hadamard gates works, where we apply one Hadamard
gate to each of n qubits. Here we're describing this
action on standard basis states. And to be clear about the notation, when we write h tensor n like this, where the tensor n part is a superscript, what we mean is the n fold
tensor product of H with itself, or in other words, n copies
of h all tensor together. We're applying this operation
to a standard basis state, and here I'm following the
convention that Qiskit uses to index binary strings, just because it's a good time
to introduce that convention. So x0 is a bit, x1 is a bit,
and so on with n bits in total, and the bits are ordered as
you see here on the screen. You can think about this
way of indexing bits as corresponding to their significance if we view strings as
representing integers in binary notation, for instance. If we apply a Hadamard gate
to each of these qubits, we can alternatively express the result as the n fold tensor product of a Hadamar gate applied to each qubit, and then we can apply our formula to each one of the tensor factors. And here we're using the
names y and minus 1 down to y1 in place of b in the formula, so these are binary values. And we're making sure to use a new name each time we use the formula. If we then expand everything out using the multilinearity
of tensor products, we get the expression
that you see right here. And to be clear, the n sums are being
combined into a single sum, but we're still summing
over all the possible values for the individual binary values. Now let's clean things up so that we can go a little bit further. And the next step is to
introduce a new operation on strings called the binary dot product. This is an operation on binary strings having the same length. And what the binary dot product of two strings x and y
of the same length is, is the eXclusive OR or the parity of the products of the individual bits. You could also say that it's the XOR of the logical and of the individual bits, because the and is the
same thing as the product for binary values. Yet another way to describe
it is that it's 1 or 0 depending on whether
the sum of the products of the individual bits is odd or even. It's very much like an inner product if we think about strings
as vectors of binary values, but where we compute Modulo-2, or in other words, we take the remainder after dividing by 2. It's not really an inner product though, so it deserves a different name. In the case at hand, we can use the binary dot
product to make our formula for the action of a layer of Hadamard gate on standard basis states more succinct, and here it is right here. We're using the fact that
because this sum right here is appearing in the
exponent of negative 1, all that matters is whether
or not it's even or odd, but otherwise, it's basically just a condensed
version of our formula. By the way, the binary
dot product is symmetric in the sense that x dot
y and y dot x are equal, so we are free to swap the
ordering whenever that's useful. And now that we have that formula, it'll be pretty easy to analyze
the Deutsch-Jozsa circuit. We start out with n qubits in the 0 state and the bottom qubit in the 1 state, and we perform Hadamard
gates on all of them. So this is the state of the
n + 1 qubits that we get. We are treating the
bottom qubit separately because that's gonna be convenient. And again, we're gonna see the phase
kickback phenomenon happening. We can get this expression
by using our formula, where we substitute the
all-zero string for x and substitute x for y, noting that the binary dot
product of the all-zero string with any string is 0. But we can also just reason this directly. We're taking the tensor
product of n plus states, and this is what we get, tensor to the minus state. Then the UF gate is applied, and here's where the phase
kickback phenomenon happens. The value f of x is getting
XORed onto the minus state for each x in the sum. So f of x gets kicked into the phase. We now apply another
layer of Hadamard gates just to the top n qubits, and this time, we really
do need the formula. And when we apply it and do just a little
bit of simplification, we get this state right here. So that's our final state just
prior to the measurements. The state looks a little bit complicated, and it's difficult to simplify it because we don't really know
anything about the function f. But remember, that all
we really need to know is whether or not the measurements
all yield the outcome 0, because that's what determines what our answer is to the problem. If we see that every
measurement outcome is 0, we conclude that the function is constant. And if any one of the
measurement outcomes is 1, we conclude that the function is balanced. So let's calculate the probability to get the all-zero string
from the measurements. And that turns out to be the one result that's actually easy to calculate, because the binary dot
product of the all-zero string with any string is zero. If we examine our vector and
we think about what happens when y is the all-zero string, we get this expression right
here for the probability to get the all-zero string
from the measurements. And considering the two possible cases where f is constant and f is balanced, we see that the probability to
get the all-zero string is 1 in the case that f is constant, and zero when f is balanced. When f is constant, the sum is either 2 to the
n or negative 2 to the n, and dividing by two to the n and taking the absolute
value squared yields 1. And when f is balanced, the sum equals 0. That's because we'll get
positive 1 and negative 1 an equal number of times,
so they all cancel out. So that's it. The Deutsch-Jozsa algorithm
solves the Deutsch-Jozsa problem using a single query, and there's no error. Assuming the operations
are all done perfectly, it's correct every single time. So how does that compare with
what we can do classically? Well, if we insist on a
deterministic algorithm that gets the answer
correct every single time, then in the worst case, we need 2 to the n minus 1 plus 1 queries, and that's because we
have to query the function on more than half of its inputs to be sure about the solution. Even if we query the function on 2 to the n minus 1
different input strings, it's possible that we'll
see the same output value every single time, and that could be because
the function is constant, or it could be that it's actually balanced and we've just been incredibly unlucky having seen the same output
value every single time. So that's a huge advantage
of quantum over classical, a single query versus
exponentially many queries. However, if we use a
probabilistic query algorithm and we accept a small probability that it answers incorrectly, then we actually only need a few queries. In particular, if we
randomly choose k strings and evaluate the function
on those random strings, then we are pretty likely to be able to figure out the right answer. Specifically, we answer 0 or constant when we get the same function
value every single time and 1 or balanced whenever we see two
different function values. If we do that, we'll always be correct when
the function is constant because, of course, we'll always see the same
output value in this case. And if f is balanced, we're very likely to see
two different output values. The chance that we don't see
both output values in this case is the same as the chance that we have of flipping a coin k times and getting the same
result every single time. So for example, if we
set k to be equal to 11, the probability of getting a wrong answer is less than a 10th of a percent. So especially when we consider the fact that quantum computers are never going to be perfect in reality, we don't actually get that
big of an advantage here, but we're making progress, and we do have a quantifiable advantage of quantum over classical that goes beyond Deutsch's algorithm. We can also use the Deutsch-Jozsa circuit to solve a different query problem known as the Bernstein-Vazirani problem. Here's a statement of that problem. The input is a function
from n bits to 1 bit, just like the Deutsch-Jozsa problem, and again, it's a promise problem. This time, the promise is that there exists some n bits string s for which the function f
works as you see right here, where the output is given
by the binary dot product of the input with this string s, and the goal is simply
to find the string s. So we can imagine that the
string s is hidden in some sense, but we can learn about it
by making queries to f, and our goal is to find
this hidden string. To be clear, most functions
won't satisfy this promise, but as usual, we only care about the
input functions that do. As it turns out, the Deutsch-Jozsa circuit actually solves this problem
right out of the box. If we run it for a
function like we have here that satisfies the
promise for some string s, the measurements will reveal the string s with no post-processing required. So let's see how this works. We've already analyzed
the Deutsch-Jozsa circuit, so we can actually just
skip right to the end. Here's the expression that we
had from before for the state just prior to the measurements, and that expression was
valid for any function f. And now, if we substitute
in what we're promised about the function f and simplify, we get the minus state
for the bottom qubit tensored with cat s for the top n qubits. And so we get s when we measure. There is some work to do to perform the simplifications
that I've shown here, and I won't go through that in this video, but there is more detail about it in the textbook content for this series, which you can find by following the link in the description of the video, or just try it for
yourself for some practice. So what we found is that
the Deutsch-Jozsa circuit solves the Bernstein-Vazirani
problem with a single query. It's not too hard to show
that any classical algorithm, whether it's deterministic
or probabilistic, needs at least n queries to solve the Bernstein-Vazirani problem for the simple reason that we need n bits of
information to recover s, and each query gives us at
most 1 bit of information. So we get another advantage of quantum over classical
query algorithms, one query for quantum algorithms, n queries for classical algorithms. It's worth taking just a moment to speak about the
nomenclature that's used in the context of this problem and a little bit about the history. People sometimes refer to the algorithm that I just described as being the Bernstein-Vazirani algorithm, but it's really just the
Deutsch-Jozsa circuit, and that's something that was,
in fact, very clearly stated in Bernstein and Vazirani's work. So it is a little bit strange that it's called the
Bernstein-Vazirani algorithm because it's pretty much the same thing as the Deutsch-Jozsa algorithm minus the post-processing. Bernstein and Vazirani did,
however, go a step further. They defined a recursive
version of this problem, where you basically
have to solve instances of the Bernstein-Vazirani problem to unlock new instances of the problem that are arranged in a
complicated tree-like pattern. It's a highly contrived problem, but the point is that by
defining a problem in this way, they could effectively stretch out the one versus n query separation between quantum and classical. And what they obtained is
the very first known example of a so-called super polynomial advantage of quantum over probabilistic algorithms. This recursive version of the
Bernstein-Vazirani problem also turned out much later
to be very interesting in computational complexity
theory for different reasons. Anyway, the point is that
Bernstein and Vazirani certainly deserve recognition for their important contributions, but it really doesn't make
sense to refer to this as the Bernstein-Vazirani algorithm, it's basically just the
Deutsch-Jozsa algorithm. In the last part of the lesson, we'll take a look at one
more quantum query algorithm known as Simon's algorithm. You'll see a clear similarity
between this algorithm and the Deutsch-Jozsa algorithm, but it is different, and more importantly, it reveals a much stronger advantage of quantum over classical
query algorithms. Specifically, it gives us a
provable exponential advantage of quantum over classical algorithms, even probabilistic ones. The problem it solves is a bit artificial, and it's hard to imagine a situation in which solving this
problem would be useful, but it did directly inspire
Peter Shor's discovery of a quantum algorithm
for integer factorization, and that very strongly supports
this idea of the query model as a Petri dish in which quantum algorithmic
ideas can be developed. Let's start with the query problem that Simon's algorithm solves, which, as you might have guessed,
is called Simon's problem. Here's a statement of the problem, and we're going to need to unpack it to understand what it's all about. The first thing to
notice is that this time, the input function f maps n
bits strings to m bits strings where m is some positive integer that isn't equal to one in general. It's a promise problem. And like the Bernstein-Vazirani problem, the promise involves some string s that we can view as being a hidden string, and also like the
Bernstein-Vazirani problem, the goal is to output
this hidden string s. The promise is more
complicated this time though, so let's take it apart and
understand what it's saying. To begin, let's just read it. It says that there exists a string s having length equal to n such that a particular "if"
and "only if" condition is true for all choices of n bits strings x and y. Specifically, it must
be the case that f of x is equal to f of y if and only if one of
two possibilities holds. The first possibility is that x equals y, and the second possibility is that by taking the
bitwise eXclusive OR of x in the hidden string s, we get Y. This turns out to be a
pretty strict promise in the sense that most
functions won't satisfy it. Only very special functions
satisfy this condition. Now, to get a better grasp on this promise and therefore on the problem itself, we'll consider two main cases. The first case is that s
is the all-zero string. In this case, we can simplify the "if"
and "only if" statement in the promise because the bitwise
eXclusive OR of any string x with the all-zero string
is simply the string x. So the condition simplifies
to f of x is equal to f of y if and only if x equals y. And that's just another way of saying that f is a one to one function. In other words, the string f of x must be unique
for each possible string x. So if f is a one to one function, then the promise is indeed satisfied, specifically for s being
the all-zero string, and in this case, that's
the correct output. The second case is that s
is not the all-zero string, and what this implies is that the function f must be two to one. So for every input string x, there's exactly one other string that f maps to the same
string that x maps to, and moreover, the other string
must be the string we get by taking the bitwise
eXclusive OR of x with s. Another way of saying this
is that every n bit string x has a partner, and that partner is the string that we get by taking the bitwise eXclusive OR with s. Any two partners must give
us the same output string if we plug them into f, and no other strings are allowed to give us
that same output string. So these are necessarily distinct strings that partners produce. Here's an example of a function like this described by its table of values. So this is a function
from 3 bits to 5 bits. If we stare at this table
of values for a while, we find that the promise is
satisfied for the string 011. 000 and 011 are partners. XORing either one of them
by s gives us the other, and they take the same output value, which is 10011 in this particular case, and no other strings produce
that same output string. 001 and 010 are partners because XORing either
one by s gives the other, and they produce the same output value, which again, isn't produced
by any other string. And the situation is
similar for 100 and 111 as well as 101 and 110. So if the input to the
problem is this function, the correct answer is the string 011. Like I said before, most functions don't satisfy this promise, it's a very specific condition. It should also be noted that
if the promise is satisfied, it can only be satisfied for one string s. So there's always only one
correct answer to this problem. So that is Simon's problem. Like I said, it's hard to imagine a practical situation in which you'd wanna solve this problem, but that isn't the point. The point is to try to figure out what quantum computers are good at, and it turns out that they're far superior to classical algorithms for
solving this particular problem. Now let's take a look
at Simon's algorithm, which solves Simon's problem. The algorithm consists of
running the quantum circuit that you see here several times, followed by a classical
post-processing step, and we'll be more specific about what several
times means a bit later. The circuit looks very similar
to the Deutsch-Jozsa circuit, this time because the function
f maps n bits to m bits, the query gate is different. Rather than just a single
qubit on the bottom, we now have m qubits on the bottom and n on the top like before. A key difference is that we're
not setting up a minus state or multiple minus states on the bottom. These bottom qubits are all
initialized to the 0 state, and those states go straight
into the query gate. So there won't be any phase kickback involved in this algorithm, it's just not how this one works. Other than the differences
I just mentioned though, it looks very similar to
the Deutsch-Jozsa circuit. We apply a layer of Hadamard gates, then we apply a query gate one time, and then we apply another
layer of Hadamard gates followed by standard basis measurements of the top n qubits. The idea is that each time we
run this circuit and measure, we'll get some string y that tells us something
about the hidden string s in Simon's problem. And by running the circuit
several times independently, we will have gathered enough
statistical evidence about s to be able to figure out what
it is with high confidence. Now let's analyze the quantum
circuit for Simon's algorithm to see what it does. The state after the first
layer of Hadamard gates looks like this, similar to what we had for
the Deutsch-Jozsa algorithm. Except this time, the bottom m qubits are all in the 0 state
rather than in minus states. After the query gate is performed, we see that the value f of x has been written onto the
bottom or leftmost m qubits, and that's because f of x has been XORed onto the all-zero string. So again, there's no phase kickback happening in this algorithm. We then perform the second
layer of Hadamard gates. And using the same formula as before for the action of a
layer of Hadamard gates on a standard basis
state, cat x in this case, as it was in the original formula, we obtain this state right here, where the action of the Hadamard gates has been made more apparent. But if we want to, we can also write the state
in a more compact form like we have right here. At this point, we measure the top n qubits to obtain some end bit string y, and we just need to figure
out what the probabilities are for the different strings. And that'll help us to understand
what information we get from this string y about the string s that's hidden inside of the function f. We're going to need
some more room for this, and we really don't need
the circuit diagram anymore, so let's get rid of it and give ourselves some
more space for the analysis. Here's the state just
prior to the measurements, which are standard basis measurements of the top or rightmost n qubits. So the measurement
corresponds to the second cat. Let's denote by p of y, the probability that the measurements give us the string y for each possible y. And if we follow the rules from Lesson 2, we get that each of these probabilities is the Euclidean norm squared of whatever vector is tensored
to the standard basis state, corresponding to why in
our quantum state vector. That gives us this expression right here. Now, to get a better handle
on these probabilities, we're gonna need to make
use of some notation, and you might already be
familiar with this notation because it's pretty common in mathematics. First, when we refer to the
range of the function f, we just mean the set of all the strings that can come out of this function. So it's the set of all f of x ranging over all the possible
choices of an input string x. Second, we're gonna use this notation that you see right here to mean the set of all the input strings x that f maps to a given string z, for whatever choice of
z we wanna consider. It kind of looks like the
inverse of the function f, but it's not really the inverse because f is not necessarily invertible. We also see that we have
a set as the argument on the left hand side, and in this case, it's the set containing
just a single string z, and that's the clue we need to recognize that we're not talking about
the inverse of the function f, this is called the pre-image. The pre-image can be
defined more generally for any set given as an argument, not just the set containing
a single element, but this is all that we're gonna need. It's not the greatest
notation in the world because it's easy to
confuse it with the inverse, but it's clear enough
and it's widely used, so we're gonna go with it. And now if we use this notation, we can rewrite the vector that we're taking the
Euclidean norm squared of to get the probability for
measuring each string y. We're doing this because we need to see
what the coefficients are for each individual standard basis state, and that's not clear from
this expression right here, because, in general, we may
have multiple choices of x giving us the same string f of x. In essence, all we're doing
is splitting up the sum over x by first summing overall
z in the range of f, and then inside of that sum, summing over all x in the pre-image of z, or to be more accurate, the pre-image of the set
containing the string z. That lets us replace f of x with z inside of the cat right here. And the sum overall, the different strings
x that f maps to each z are collected into the
coefficient for each cat. We're basically putting strings into bins, summing over the bins, and then summing over
the strings in each bin, but that's equivalent to just
summing over all the strings. So we do obtain an expression that's equivalent to the
one that we started with, and now we can compute the
Euclidean norm squared. It's just the sum of the squares of the absolute values
of the coefficients, and so, we get this expression right here for our probabilities. Of course, give yourself
time to verify this, if you choose to do that, it takes time to get used to all this and to be able to perform
these calculations, and the more practice you give yourself, the easier it'll become. So far, we haven't actually
made use of the promise in Simon's problem at all. Our analysis of these probabilities actually works for any function f, but now it's time to think about Simon's
problems specifically. We're going to break things down into the same two cases
that we had before. The first case is that s
is the all-zero string, and the second case is that
s is not the all-zero string. In the first case, that
s is the all-zero string, things are pretty simple. In this case, as we saw before, we have that f is a one to one function, so there's a single string x that gets mapped to each of the
strings z in the range of f. That makes the absolute value squared of the sum over those strings equal to 1 because there's just a
single term in the sum, it's either plus 1 or minus 1, but it doesn't matter which because we take the
absolute value squared, and so we get one. There are 2 to the n
elements in the range of f, and we know that because f is one to one, so there's one string in the
range for each input string. And if we evaluate the entire expression, we find that each string y appears with probability
one over two to the n. So in other words, the
measurements give us a string y chosen uniformly at random. That might not seem all that useful, but we will, in fact, be able
to distinguish this behavior from the behavior we get in the case that s is not the all-zero string, and we'll see how that goes shortly. For now though, we simply observe that if
f satisfies the promise in Simon's problem in the case that s is the all-zero string, then the result of the measurements is a uniform random string. Now let's move on to the second case, which is that the promise
is satisfied for a string s that's not the all zero-string. In this case, as we discussed before, each string z in the range of f has exactly two strings that map to it, because every string has a partner, and each pair of partners
maps to some unique string, and specifically, each string's partner is the string we get by XORing with s. So if we pick any string
z in the range of f, and we give the name w to either one of the
strings that f maps to z, then the XOR of w with s is the
other string that maps to z. Looking at just the one term in our sum that corresponds to whatever string z in the range that we have in mind, we see that we can
write it more explicitly as we have right here. We have two strings in the pre-image of z, w and w XORed with s. So we can write the sum
as you see on the screen, and now what we can do is we can pull out a factor of
negative 1 to the power w.y. That factor is going to be
either plus 1 or minus 1, so it goes away when we
take the absolute value, and what we're left with is
this expression right here. The details aren't all shown here, and it's not necessarily
obvious that it works this way, but you can check that these
two expressions are equal by going back to the definition
of the binary dot product and thinking about what happens when these things are in
the exponent of negative 1, not unlike what we had for the
Bernstein-Vazirani problem. Finally, we can break this
expression up into two cases. It's 4 when the binary dot
product of s and y is 0, and it's 0 when the binary
dot product of s and y is 1. Notice in particular that
there's no dependence at all here on w or z, it only depends on s and y. The reason this happens is
because of the very specific and special nature of the promise. In essence, this is why the promise was formulated in this way, so it would all work out like this. Again, every string has a partner, which we get by XORing with
this one hidden string s, and that's what makes this all happen. We can now go back to the
expression for the probabilities and plug things in. When s is not the all-zero string, there are 2 to the n minus 1
elements in the range of f, which is half of the
number of input strings, and plugging in the
values we just calculated gives this formula right here. So this time, y is not a
completely random string, but rather y is uniform over exactly half of the n bit strings, and specifically, it's
uniform over those strings that have binary dot
product equal to 0 with s. Now that we know how the circuit
for Simon's algorithm works and specifically what
the probabilities are for the different measurement outcomes, it remains to explain how this helps us to solve Simon's problem. Summarizing what we just learned, if it's the case that the function f satisfies the promise in Simon's problem, when the hidden string s
is the all-zero string, we obtain a completely random
string from the measurements, where each end bit string
y is equally likely. If on the other hand, the function f satisfies the promise for a different string s, one that's not equal
to the all-zero string, then we get a random string y having binary dot product
equal to 0 with s, and among all of those strings,
each y is equally likely. But we'll never get a string y having binary dot product
equal to 1 with s. The question becomes, "Is this enough information
to determine s?" And the answer, as I've
already suggested, is yes, provided that we run the
circuit enough times. Specifically, let's suppose
that we run the circuit k times, where K is equal to n plus r, where r is some number that's going to control the
likelihood that we recover s. Just as an example, if
we choose r to be 10, we'll be successful with
probability greater than 99.9%, so we don't need r to be all that large. In general, the probability of failure is less than the probability
of flipping a fair coin r times and getting heads every single time. So if 99.9% isn't good
enough, then take r to be 20, and the chance of failure is
less than one in a million, or choose whatever r
makes you comfortable. You can't guarantee a correct solution, but you can make the
probability of error negligible for all intents and purposes. Here we are naming these
strings y1 through yk, where we're using superscripts to help us to name these strings, just so that we can reserve subscripts to refer to the individual
bits of those strings. So these aren't exponents
or anything like that, these are just the names that we're giving to these k strings. Each one has length n because we got them
from running the circuit for Simon's algorithm. So we can write down the
individual bits of these k strings like you see right here. What we do now is to create a matrix m, where each entry of this matrix is a bit, and we get these bits from the strings that we just measured, just like as shown right here. So this matrix has k rows and n columns, and its entries are all binary values. Now, we don't know what s is. That's what we're trying to find, but just imagine for a
moment that we did know s, and we formed a column
vector from the bits of s, just like is shown right here. If we were to multiply that vector by m and then take the result modulo-2, or another way of saying that is that we replace addition
with the eXclusive OR, then we will necessarily
get the all-zero vector. That's because each of the
entries in this resulting vector is none other than the
binary dot product of s with one of the rows of m. If S happens to be the all-zero string, we'll always get the zero vector from the binary dot product. And if s is not the all-zero string, we'll get this from what we
know about the probabilities for the strings y1 through yk to appear. In particular, the
binary dot product with s is always zero for all of these strings. A simple way of saying that is that this vector
formed from the bits of s is always in the null
space of the matrix m, assuming that we're doing all
of our arithmetic modulo-2, but of course, we don't know what s is, we're trying to find s. But what we can do is that we can compute the
null space of this matrix m, once again, modulo-2. Specifically, we can do that using the technique of
Gaussian elimination, which is the procedure that's often taught in introductory linear algebra courses. It works perfectly well when we do all of our arithmetic modulo-2, and it can be done quite
efficiently with a computer. And if we compute the null space of m, given the specific way that these k strings have
been randomly chosen, we will be able to recover s with probability greater than
1 minus 2 to the negative r. To be specific, what we will see with probability greater than
1 minus 2 to the negative r is that the null space
includes the all-zero vector, as it always does for every matrix, as well as the vector corresponding to s, whether it's the all-zero vector or not. And most importantly, no other vectors will
appear in the null space. It's actually not too hard to prove this, although I won't go through it in detail. The idea is that if we
have some other string that isn't the all-zero
vector and it isn't s, then each one of the random
strings, y1 through yk, is basically going to knock
it out of the null space with probability a half. So after k shots, it's not
at all likely to survive, and in fact, none of the
strings that don't belong are likely to survive. And a very simple fact
from probability theory, known as the union bound, can
be used to establish that. I haven't explained all
of the details here, but hopefully, you get the idea that by computing the null
space modulo-2 of this matrix m, we are very likely to recover s, and that's how Simon's algorithm
solves Simon's problem. There's just one thing left
to say about Simon's problem, and it concerns the classical
difficulty of this problem, that is, how many queries does a classical query algorithm need to solve Simon's problem? The answer is, a lot, in general. There are different precise
statements that can be made about the classical
difficulty of this problem, and here's just one of them. If we have any probabilistic
query algorithm and that algorithm makes fewer than 2 to the power n over 2
minus 1, minus 1 queries, so that's a number of queries
that's exponential in n, then that algorithm will fail
to solve Simon's problem, with probability at least a half. Sometimes, proving
impossibility results like this can be very challenging, but this one isn't too difficult to prove through an elementary
probabilistic analysis. I won't do that in this video lesson, and I'll just instead focus
on the intuition behind it. We're trying to find the hidden string s, but so long as we don't query the function on two strings having
the same output value, we'll get very limited
information about s. Basically, all we learn is
that the hidden string s is not the eXclusive OR of any two distinct
strings that we've queried, And if we query the function at fewer than this number of queries, there will still be a lot of choices of s that haven't been ruled out. This isn't a formal proof,
but that is the basic idea. So in summary, Simon's algorithm provides us with a striking advantage of quantum over classical
algorithms within the query model. In particular, Simon's
algorithm solves Simon's problem with a number of queries that's linear in the number of input
bits n to our function, whereas any classical algorithm,
even if it's probabilistic, needs to make a number of
queries that's exponential in n in order to solve Simon's problem with a reasonable probability of success. It's true that Simon's problem is a somewhat artificial problem that presumably nobody
really wants to solve, but that's secondary. Sometimes ideas need to be developed before they're put into practice. And indeed, Peter Shor has clearly stated that it was Simon's algorithm that inspired his now
famous quantum algorithm for integer factorization, which we'll see in a couple of lessons. And that is the end of Lesson 5. In this lesson, we've taken a look at the
query model of computation, and we've seen a progression of examples of quantum algorithms
that exhibit advantages over classical algorithms
within this model. There is, in fact, a lot more
known about the query model and quantum algorithms for other problems that
I've not discussed. So if you find this
model to be interesting, there's quite a lot
more to learn about it. Our next step will be to move
away from the query model and toward a model of computation
that's more representative of the sorts of problems that we might hope to solve in practice. I hope you'll join me for Lesson 6, where will focus mainly on
laying down a foundation for studying quantum algorithms. And from there, we'll move on to some
specific quantum algorithms, including Shor's algorithm
for integer factorization in subsequent lessons. Goodbye until then!

## Single Systems ｜ Understanding Quantum Information & Computation ｜ Lesson 01

- Welcome to Understanding
Quantum Information and Computation. My name is John Watrous, and I'm the technical director
of IBM Quantum Education. The purpose of this series is to provide you with
a solid understanding of quantum information and computation, and that means getting
into the technical details of quantum information and seeing how quantum
computing really works, what it can do and what it can't do. If you haven't already
watched the overview video for this series, I encourage you to check that out. In that video, you'll find information about what sort of background
preparation is recommended for this series, as well as an outline of
the topics to be covered. Also, be sure to check
out the Qiskit textbook, which covers the same
material as this video series in a written form, and it also includes
interactive components and interfaces to hardware and software designed to help you to better understand quantum information and computation. You'll find links to the overview video and the Qiskit textbook
in the description. This is lesson one of
unit one of the series. Unit one covers the basics
of quantum information, and in the first lesson of this unit, we'll focus on how
quantum information works for single systems, meaning that we have
just one physical system or a device of some sort that
stores quantum information and we're considering how that
one system or device works in isolation. In the lesson following this one, we'll discuss how
quantum information works when we have multiple systems and that will allow us to describe interesting
and important concepts such as quantum algorithms that make use of multiple qubits and quantum protocols
involving multiple individuals that process quantum information and transmit it back and
forth to one another. But before getting into those discussions, it makes sense to start with the
comparatively simple setting of a single system in isolation, partly because it's good to start simple, but also because as we will see, the description of how
quantum information works for single systems in isolation leads very naturally and logically to the description of how
quantum information works for multiple systems. Here's a brief overview of the lesson. We're gonna start by discussing
classical information. Now, the purpose of this lesson is not to explain how
classical information works, it's to explain how
quantum information works, but to explain how
quantum information works, it's very helpful to begin
with classical information and to use it as a
familiar point of reference when we discuss quantum information. At a mathematical level, quantum and classical information actually have pretty similar descriptions with some key differences, of course. In fact, quantum information
really isn't separate from classical information at all. It's an extension of
classical information. The fact of the matter
is, at least in my view, that you really can't
understand quantum information if you don't understand
classical information, so starting with classical
information makes sense. I do expect that this
content will be familiar to some viewers, but even if you are familiar with it, I don't recommend that you skip it. In addition to highlighting the aspects of classical information that are most relevant to
understanding quantum information, we're gonna introduce the
Dirac notation in this part, which is a standard way
that we're gonna use to describe vectors and
matrices throughout the series. As it turns out, the Dirac notation isn't
specific to quantum information. It can equally well be used in the context of classical information as well as many other settings where vectors and matrices arise. We'll then move on to quantum information, and specifically, we'll
discuss the representation of quantum states as vectors, a simple type of measurement called a standard basis measurement, and unitary operations, which describe discreet time
changes in quantum systems. Again, all in the setting
of single systems. Before we get started with lesson itself, it will be helpful to clarify that there are actually two descriptions of quantum information, the simplified description, which is what we'll be
focusing on in this lesson and throughout this first unit and the general description, which is covered in a later unit, specifically in the
third unit of the series. The simplified description,
true to its name, is simpler and it's
typically learned first. In the simplified description, as I've already suggested, quantum states are represented by vectors and operations are represented
by unitary matrices. This description of quantum
information is sufficient for an understanding of
most quantum algorithms, for instance, so it isn't oversimplified, it's the real thing. However, as we will
discover as we go along, it does have some limitations. For instance, if we wanna
model the effects of noise on quantum systems, this description turns out to be lacking. The general description, on
the other hand, is more general and ultimately, it's more
powerful in a mathematical sense. In this description, quantum states are represented by a special class of matrices
called density matrices, and more general class of
measurements and operations can be described including
noise, for instance. It's entirely consistent with
the simplified description and in fact, you can essentially view
the simplified description as a special case of
the general description, and it also includes classical information as a special case. It's also really quite
beautiful in how it all works, so I strongly encourage you to learn about this general description when the time is right, but the simplified description is the more natural place to start, and so that is what we will do. We're going to begin by talking about classical information with an eye on the aspects
of classical information that are most relevant
to quantum information and the way that it's
described mathematically. Imagine that we have a
physical system or a device of some sort that stores information. We're gonna give this system the name X, but there's nothing
important about this name. We could use any other
name if we preferred, but just to pick a name,
we're gonna call the system X. We're gonna make an assumption about X, which is that there is a
finite set of classical states that X can possibly be
in at any given moment. Here, when I refer to a classical state, I mean a configuration of the system that can be recognized and
described unambiguously without any uncertainty or error. Really, this is something that you can think about intuitively. At a mathematical level, we simply define the
set of classical states, we choose it however we want to or in whatever way best
describes the device that we're working with. Let us give the name sigma to
this set of classical states, and again, let me emphasize
that we're assuming that sigma is a finite set. The set of classical
states of this system X is not infinitely large like
the set of all natural numbers. It's finite, and that's just an
assumption that we're making. For example, it could be that X is a bit, in which case, the classical state set is the set containing two
elements: zero and one. Sometimes we refer to this
set as the binary alphabet. Another example is that X is
an ordinary six-sided die, in which case, the classical states are the numbers one through six. Of course, we're talking
about an abstraction here. An actual six-sided die may
have many different attributes such as its precise
position, its orientation, its temperature, and so on. But with respect to the information that's most typically relevant when we're talking about a six-sided die, the classical states are
the numbers one through six indicated by the number of dots on whichever side of the die is facing up. If X is a switch on a
standard sort of electric fan that you might have in your home, then perhaps the classical states are the settings high,
medium, low and off. These are just examples. We could imagine systems having different classical
state sets than these, but hopefully, they convey the idea that this notion of a classical state is something that should be
familiar and can be understood in simple and intuitive terms. Mathematically speaking, the set of classical states
is just a finite set, and of course, this set must
also be non empty as well. There has to be at least
one classical state that the system can be in and there has to be at
least two classical states if the system is going to be useful at all for storing information. Now, in some situations, we might know that X is
in some particular state, but in many situations, that arise in the setting
of information processing, there may be uncertainty about the classical state of a
system at a particular moment so that each possible classical state has some probability associated with it. For example, if X is a bit, then perhaps it's the case that X is in the classical state zero with probability three quarters and it's in the state one
with probability one quarter. We're going to refer to this
as a probabilistic state, and we could express
this probabilistic state, as you see right here, simply by indicating what
the probabilities are for the different classical states. A more succinct way to express
this probabilistic state is by a vector, specifically the representation
that you see right here is a column vector, and this vector indicates
what the probabilities are for the two possible classical states. To be more specific, the first entry indicates
what the probability is for X to be in the state zero, and the second entry tells
us what the probability is for the system to be in the state one. This is simply the natural way to order the binary alphabet, or at least, it's the way that
we are trained to do this. With zero first and one second. Going forward, we're gonna use
the term probability vector to indicate a vector like this. Specifically, it's a vector whose entries are all
non-negative real numbers and where the sum of the
entries is equal to one, so it makes sense to
think about these entries as being probabilities. Here in this example, there are just two entries because there's just two
classical states of the system we're talking about, but in general, there could
be any number of entries with the understanding
being that there's one entry for each classical state of whatever system we're talking about. We're going to continue
discussing classical information, probability vectors and
so on in just a moment, but first, it's gonna be really helpful to introduce some notation
for describing vectors known as the Dirac notation. In fact, this is just the first
part of the Dirac notation. There are a couple of other
aspects of the Dirac notation that we'll see a little
bit later in the lesson. This notation is gonna be
quite central to the way that we describe quantum information, so it's essential to
understand how it works, but fortunately, it's very simple and it really doesn't have
anything specifically to do with quantum information, so we can introduce it now
in the familiar context of classical information. As before, we assume that we have a set sigma of
classical states of some system, and let's assume that we have an ordering
of these classical states, just like how we refer to an
ordering of the binary alphabet in the previous example. Another way of saying this
is that the elements of sigma are placed in correspondence
with the integer from one to however many
elements are contained in sigma, which is what this notation right here with two bars around sigma means. That's the number of
elements in the set sigma. So there's a first state,
a second state, and so on. The specific ordering or
correspondence that we choose isn't actually gonna matter all that much. What's really important is
that we have an ordering and we stick to it. For many classical state
sets that we'll encounter, there will be a standard ordering just like we have with
the binary alphabet, and if there isn't a standard ordering, we can just choose one. But whatever it is, the understanding is that we stick to it. Now, here's the notation
that's being introduced. For any choice of a classical
state a from our set sigma, we write this right here, which is read as ket a
to mean the column vector that has a one in entry corresponding to a and a zero for all other entries. For example, if we return
to the binary alphabet, which, as you might have guessed, is a very important example that we'll return to again and again. Then ket zero and ket one are the vectors that are defined right here. Ket zero is the vector
that has a one in the entry corresponding to the classical state zero, which is the first entry, and a zero for all other entries, and in this case, there's
only one other entry. Similarly, ket one is
this vector right here where we have a one in the
entry corresponding to one, which is the second entry, and a zero for all other entries, which, again, is just one entry. Here's another example. In this example, we have a
classical state set sigma corresponding to the four suits in a standard deck of playing cards: clubs, diamonds, hearts, and spades. In this case, there really isn't a
universally agreed upon ordering of these suits. For many card games, there is no ordering at all. They're just four different suits and that's all that matters, and there are different card
games that do order these suits in different ways, but it doesn't really matter. We can just choose an
ordering like the one that you see right here. This happens to be alphabetical ordering according to the names of these four suits written in English, Assuming that we've picked
this particular ordering, then we define these four vectors: ket clubs, ket diamonds,
ket hearts, and ket spades as you see right here, and these vectors correspond
to the general description that you see up here. Given that we've chosen
this particular ordering for these four suits. We can do the same thing for any classical state
set that we choose. Keeping in mind that classical
state set always means finite and non empty set. Vectors of this form are
called standard basis vectors, and it's a very basic fact about vectors that every vector can be
expressed in a unique way as a linear combination
of standard basis vectors. For instance, going back to the example from just a moment ago, the probability vector
we saw can be expressed as you see right here. It's basically just an alternative way of expressing vectors that's gonna be very handy for several reasons going forward. And with that notation now in our hands, let's return to the discussion
of classical information and probabilistic states, in particular. And let's consider what happens
when we measure a system while it's in some probabilistic state. We might not ordinarily
refer to a measurement or the act of measuring
something in a classical setting, but the term makes sense. In essence, when we talk
about measuring the system X in this context, we just mean that we look at it and we recognize whatever state it's in. Of course, when we look at the system, we don't see a probabilistic state, we see a classical state, and the particular
classical state that we see is one that we can
imagine is chosen randomly according to the probabilities. Now, you might object on
philosophical grounds to the idea that the classical state we see is somehow chosen at random at the moment that we measure or we look at the system, the system simply was in some particular
classical state all along and we just happen to have
learned what the state was, but for the sake of thinking about the mathematical
framework we're working with, it's okay to think about it this way and doing so will allow us to draw a parallel with
quantum information a little bit later. Let's suppose that the
classical state that we see is the classical state a, which could be any state in our set sigma. This changes the probabilistic
state of X in general, at least from our point of view. The probabilities no longer
have any significance because we now know what
the classical state of X is. It's a, and there's no longer
any uncertainty about this. So we would now say this
that the probability that the system is in the state a is one. We know this because we just looked at it and we saw that the classical state is a, so the new probabilistic state of X is one in which the probability of
being in the classical state a is equal to one. And that probabilistic state is represented by the
probability vector ket a. We have a one in the entry corresponding to the classical state a and a zero in all other entries. For example, if we again
consider the situation where X is a bit and we have the same
probabilistic state as before, then if we measure X, we can imagine that a
transition takes place. With probability three quarters, we see the classical state zero, and by measuring, we transition to ket zero
being our probabilistic state, and with probability one quarter, we transition to ket one. You can think about this as a transition of knowledge if you prefer as opposed to an actual
physical transition, but what's important for
the sake of this lesson is to recognize how the mathematics works. It's kind of trivial actually, but by recognizing this triviality, the analogous description
for the quantum setting might seem a little bit less mysterious. One final note is that to another person who didn't happen to see what
the classical state of X was when we measured it, the probabilistic state
naturally wouldn't change as a result of us having measured it. That's okay. Different individuals can
have different knowledge of a particular system and correspondingly choose
different probabilistic states to represent their
knowledge of that system. In the last part of this discussion of classical information, we'll talk about classical operations that can be performed on a system, starting with deterministic operations. When we perform an operation on a system, its state generally changes as a result, and in this context, the word deterministic means that the result of performing
the operation depends entirely on whatever classical
state the system was in prior to the operation being performed. In other words, there's
no element of chance when a deterministic
operation is performed, so there's no randomness
or uncertainty involved. In mathematical terms, deterministic operations
are described by functions. Every function f of the form
that you see right here, which means that both the input and the output of the function correspond to elements of
our classical state set sigma describes a deterministic operation. The classical state a is
transformed into the state f of a for each choice of a state a. For every function f of this form, there will always be a unique matrix M satisfying this condition
that you see right here, and that is that by multiplying
M to the vector ket a gives us the vector ket f of a, and that's true for every
choice of a classical state a. It's not too hard to write down
an explicit description of M given whatever function f
of this form we've chosen. It will always have exactly
one, one in each column with every other entry equal to zero. And here's the formula. The b, a entry of M is equal to one if b is equal to f of a and otherwise, it's equal to zero, and by this, we mean
this is the entry of M whose row corresponds to b and whose column corresponds to a. We'll see a few examples in just a moment, and we'll see why this formula works. Now, it might not be clear why we would want to take a function and express it as a matrix like this, but it does make sense to do this. One of the reasons, and perhaps, it's the main reason, is that if we have a system
in a probabilistic state and we perform a deterministic
operation on that system, then the resulting probabilistic
state can be obtained by matrix-vector multiplication, i.e., if our system is
in a probabilistic state that's represented by
some probability vector v and we perform the deterministic
operation described by M on that system, then the resulting probabilistic state will be given by M times v. Here, by the way, this arrow with a little
bar on it like this is just a common notation in mathematics for indicating how one
thing gets mapped to or transformed into another thing. You could just use an ordinary arrow, but this is conventional and it's recognizable and maybe it's a little bit more specific than just an arrow, which could mean many different things. Now let's take a look at an example or a collection of examples, really. Let's consider what happens when we take our classical state set sigma to be once again the binary alphabet. There aren't too many different functions of the form we've been discussing when this is our classical state set and to be more precise, there are just four
functions of this form. Here, those four functions are described by their so-called tables of values. The first table describes
the first function, which we've named f1. For each possible input on the left, we list the corresponding
output on the right, so f1 of zero is equal to zero, and f1 of one is, again, equal to zero, and that is to say that f1 is
the constant zero function. It always takes the value zero. The second function f2
is the identity function. The output is always equal to the input as you can see from the second table. The third function f3 is
described by the third table. f3 of zero is equal to one, and f3 of one is equal to zero. This function is better
known as the not function or logical negation. We sometimes also call it a bit flip because it flips zero
to one and one to zero. The last function f4 is,
again, a constant function just like the first one, but this one is the constant one function because it always takes the value one. And those are the only four functions from the binary alphabet to itself. Here are the four matrices that correspond to these four functions that are described by
the formula from before, which you can see right here, and if you're so inclined, you can pause the video and check this. Just remember that with matrices, the order of the indices is always rows first, column second. So the b, a entry corresponds
to row b and column a or the row that corresponds
to the classical state b and the column that corresponds
to the classical state a. So for example, if we look
at the first function, we have the f1 of zero is equal to zero and correspondingly, the
zero, zero entry of M1 is equal to one. Well, the one, zero
entry which is right here is equal to zero, and you can do the same thing for the case where the input is equal to one and similarly, for the
other three functions. You can also check that this
formula from before is true and you can just go one by one through all the different
possibilities to verify this. And in fact, if you think about the way that
matrix multiplication works, really you can just kind of eyeball this. When you multiply a matrix to a standard basis factor like this, you can imagine that
you're taking the input and dropping it into the
top of one of the matrices in whichever column corresponds
to the classical state inside of the ket, and the output that you
get is simply that column as a probability vector. So for example, M3 multiplied to ket
zero gives you zero, one as a probability vector, which is ket one. M3 multiplied to ket
one gives you one, zero, which is ket zero and so on. We'll discuss operations on
probabilistic states more in just a moment, but first, let's introduce the second part of the Dirac notation. This is gonna be helpful
for talking about operations and thinking about them as major Cs, and it's also gonna be a standard tool that we'll continue to use going forward. The setup is just like it was before. We have whatever classical state set sigma that we're working with and we assume that we have some way of
ordering those classical states. We then write this thing
that you see right here where this time the
angled bar is on the left rather than the right to mean
the row vector having a one in the entry that
corresponds to a and a zero for all of the other entries corresponding to all the
other classical states. So the difference between
this thing and ket a is that this is a row vector and
ket a is a column vector, but otherwise, the vectors
are defined in a similar way. We read this as bra a, I'm not sure if that's
supposed to be funny, but the idea is that the names bra and ket are two halves of the word bracket, and we'll have more to say
about that in just a moment. But first, let's take a look
at a very quick example. For the binary alphabet, we have that bra zero is
this row vector right here analogous to ket a, except it's a row vector
instead of a column vector and bra one is this vector right here. Now in general, if we multiply a row
vector to a column vector, just thinking about these as matrices that happen to have just
one row or just one column, then we get a one by one matrix, which we can think
about as being a scaler. Here, by the way, you should
think about these stars as representing arbitrary numbers. In particular, if we
multiply the row vector bra a to the column vector ket b, then there are two things that can happen. One is that a and b are equal, so they're the same classical state, in which case, the ones will line up and the product will be equal to one or it could be the case
the a and b are different, and so the ones don't line up and the product is equal to zero. A simple way of expressing
these observations is like this right here. And just to keep things tidy, we write this product like this with just one bar in the middle and it means exactly the same
thing as this right here. And now you can see how the
bra and the ket go together to form a bracket, which is
also called an inner product, and that's something
that's quite important that we'll come back to
in lesson number three. Multiplying a column
vector to a row vector, on the other hand, gives us a matrix as you can see right here. So for example, going back yet
again to the binary alphabet, if we multiply ket zero to bra zero, what we get is this matrix right here, and we can go through
the other possibilities to see what happens. Ket zero times bra one gives
us this matrix right here where the one has now moved
over to this position. Ket one, bra zero looks like this and here's ket one, bra one. What we see happening is that there is just a single one in a matrix that's otherwise all zeros, and the position of that one is determined by which
classical states we choose in the bra and the ket. More precisely, and
this is true in general, for any choice of a classical state set, the matrix that we get by
multiplying ket a to bra b is the matrix with a one in the a, b entry and a zero for all other entries. And now that we have both the
first and the second parts of the Dirac notation, we have a very handy tool for connecting operations with matrices. So let's go back briefly
to deterministic operations and recall that a bit
earlier in the lesson, it was stated that for every function f of
this form right here, we have a deterministic operation that transforms a into f of a for each possible classical state a. And we said that for
any function like this, there's always a unique
matrix M that acts like this on standard basis states. Using the Dirac notation, we can now very easily express this matrix like you see right here, and we can see how sometimes
it's possible to work with matrices entirely
within the framework of the Dirac notation without ever explicitly converting
to vectors and matrices, meaning rectangles filled with numbers. In this case, we can check that the
required formula is true by multiplying M to ket a. We substitute our
expression for M like this and we can remove the parenthesis and snap together the bra
b with a ket a like this. And that's because matrix
multiplication is linear, in this case, in the first argument and it's associated so we can remove the parentheses. And now, recalling that we have a fixed a that we're talking about as we sum over all of the
possible choices of b, this bracket right here is
always gonna be equal to zero when b is not equal to a, so there won't be any
contribution to the sum, and when b is equal to a, the bracket will be equal to one and so we'll be left with
this one term right here. That's the formula we wanted and so we have a very nice and simple way of expressing a function as a matrix. In addition to deterministic operations, we also have operations that
themselves introduce randomness or uncertainty about the
classical state of a system. We use the term probabilistic operation to mean operations like this, and to be precise, when we say probabilistic operation, we mean operations that
might introduce randomness, so deterministic operations are actually a special case
of probabilistic operations that just don't happen to
introduce any randomness. Here's a simple example where
some randomness is introduced. This is an operation on a single bit. The way that it works is that if the classical state
of the bit is a zero, then nothing happens. The bit stays as a zero, but if the classical
state of the bit is a one, then the operation flips the bit to zero with probability equal to 1/2. There's no special name
for this operation, at least not that I'm aware of. It's just an example, but you could imagine performing
this operation on a bit. Just like deterministic operations, probabilistic operations can
be represented by matrices and their actions on probabilistic states are given by matrix-vector multiplication just like before. This time, it's not necessarily the case that the matrices have exactly one, one in each column and zeros otherwise. Probabilistic operations are
more generally represented by stochastic matrices. These are matrices whose entries are all
non-negative real numbers and where the entries in
each column sum to one. That's equivalent to saying
that stochastic matrices are matrices where every column
forms a probability vector. That makes sense when you think about
the action of a matrix on a standard basis vector. You consider whatever
classical state you wish and you imagine dropping that state into the top of the matrix in whatever column corresponds
to that classical state, and that column tells you what
the probabilistic state is that comes out as a probability vector. The probabilistic operation
in our example, for instance, is described by this matrix right here. If we perform this operation
on the classical state zero, then the probabilistic state we get is given by the first column, which is, again, the classical state zero or equivalently, the probabilistic
state where zero occurs with probability one. If instead, we perform this operation on the classical state one, then we effectively just randomize the bit and correspondingly, we get
this probability vector here where the probability for
each of the possible states is equal to 1/2. In general, if we perform this operation on any probabilistic state, then the effect is essentially
just a weighted average between the two columns because that's how a matrix-vector
multiplication works. It's linear, so we just average the two
possible outcomes accordingly. When we think about probabilistic
operations in this way, it's natural to think about
them as potentially introducing or injecting randomness. We can just read the columns one by one and we can see the randomness
that they introduce for each possible classical state. You can also think about
probabilistic operations in a slightly different way, which is that they're random choices of deterministic operations. For instance, in our example, we can think about this operation as being equivalent to
flipping a fair coin and either performing the
constant zero function or the identity function
each with probability 1/2, and you can see that reflected
right here by this equation. That's always possible for
any probabilistic operation, and it's pretty intuitive. If you made random choices
as part of some process, you could always imagine making those random choices in advance and then hard coding those
choices into some collection of deterministic operations. It's sometimes quite helpful to think about probabilistic
operations in this way, and we'll see an example
of this down the road in a couple of lessons. The last thing to say
about classical information for this lesson concerns composing
probabilistic operations, which just means performing
one after the other. Let's suppose that we
have some system named X and M1 through Mn are stochastic matrices representing probabilistic
operations on X. Imagine first that we have
a probability vector v, which represents a probabilistic
state of X at some moment, and then we first apply the first probabilistic operation to X, which is represented by the matrix M1, and then we apply the second
probabilistic operation, which is represented by M2. If we do that, then the
probability vector we get after applying just the first
operation is M1 times v, and then when we apply
the second operation, the resulting probability vector is M2 multiplied to whatever vector we obtained after applying M1. So it looks like this. Now, matrix multiplication
is an associative operation, which means that it doesn't matter where we put these parentheses. So we could just as easily
put the parentheses like this. That's very simple, but it's also interesting because it reveals that we
can think about the operation where we first apply M1 and then we apply M2
as a single operation. In other words, the
probabilistic operation we get by composing the first and
second probabilistic operation is represented by the
matrix product, M2 times M1, and that's the same operation regardless of what probabilistic
state we started with. We don't need to know anything about the probability vector v to think about or to describe
this composed operation. Notice that the order
here is M2 on the left and M1 on the right, which is because the
matrix on the right, M1, is the one that gets
multiplied to the vector v, whereas M2 gets multiplied
to whatever vector we get after multiplying by M1. So we always have to keep in mind that the order in some
sense gets reversed. The operation performed
first is the one on the right and the operation that gets performed last or in this case, second,
is the one on the left. If we don't stop with the second operation and we apply all of them in
order starting with the first and ending with the last one, which is Mn, then the composition of all
of these operations together is given by this product
that you see right here, and you see that the ordering
once again is reversed for exactly the same reason as before. So in short, compositions
of probabilistic operations are represented by products
of stochastic matrices that represent them. Keeping in mind that this is
the ordering of the matrices in the product. You will always get a stochastic matrix by taking a product of
stochastic matrices like this, and sometimes we express this fact by saying that the stochastic
matrices are closed under multiplication, and the way that you
can think about this is that you can't get out of the
set of stochastic matrices by multiplying them because the set is closed. Getting back to the
ordering of the matrices in this product, it's important to note that the
ordering really does matter. Matrix multiplication is not commutative. If you change the ordering of
the matrices in the product, you might change the result, and we don't need to look any further than the deterministic
operations to see this. Here's a simple example of
two stochastic matrices, both of which represent
deterministic operations where the ordering matters. M1 represents the constant zero function. Assuming, we're imagining that these two operations are on bits, just like in the example from earlier. M2 represents the not
operation or a bit flip. If you perform M1 first
and then you perform M2, the result is this matrix right here, which represents the
constant one function. If you set a bit to zero
and then you flip it, it's the same thing as just
setting that bit to one. On the other hand, if you perform M2 first
and then you perform M1, this is the result right here, and this makes sense because if you flip a bit
and then you set it to zero, it's the same thing as
just setting it to zero. It didn't matter that you flipped it. Obviously, these two
matrices are not the same, and so we have a very
simple example revealing that the order does matter. Of course, this is not surprising at all. We all know that the order in which
operations are performed matters or at least, it can matter. For example, if you light a
match and then you blow on it, the result is very different
from blowing on the match first and then lighting it. At this point, we've talked quite a lot
about classical information and now it's finally time to
discuss quantum information. And what we'll find is that
mathematically speaking, quantum information works
in a very similar way to classical information
with some key differences. The first key difference
is right at the start, how we define a state, in this case, a quantum state of a system, and there is a sense in which this one choice how
we define a quantum state largely determines how
quantum information works. We'll also define how
measurements and operations work, and in the next lesson, we'll discuss how
quantum information works not just for single systems
but for multiple systems, but these things actually
follow pretty naturally once the notion of a quantum state of a single system in
isolation has been established. Here's the definition. A quantum state of a system is represented by a column vector whose indices are placed in correspondence with the
classical states of that system. So we're assuming here that
the system we're talking about has some set of classical states, just like when we talked
about classical information and probabilistic states. The entries in a quantum state
vector are complex numbers as opposed to the probabilistic case where the entries are
non-negative real numbers. And this time, the sum of the absolute
values squared of the entries must equal one as opposed to the sum being equal to one as we have in the probabilistic case. The complex numbers that appear
in a quantum state vector are sometimes called amplitudes and they play a role that's
similar to probabilities, but they aren't probabilities and they don't have the
same interpretation. Now, I can't give you a good reason for why quantum states
should be defined like this other than to say that
physicists have discovered that this definition
is physically relevant and it allows us to describe and model
quantum mechanical systems. In other words, we choose this definition
because it works, not because there are
any particular reasons why quantum states should
be defined like this. With respect to the
mathematics of this notion of a quantum state, the first step towards understanding it and working with it is
to recall this definition for the Euclidean norm of a vector, which you can think of geometrically as the length of a vector, just like you would think about the length of a two-dimensional real vector if you drew it on a piece
of paper as an arrow starting from the origin in the usual way. Specifically, if we have a
column vector v like this, which we assume has
complex number entries, alpha one through alpha n, then the Euclidean norm of v is defined like you see right here. This is the notation we
use for the Euclidean norm putting these double bars around whatever vector we're talking about. In this case, it's v, and the way the Euclidean norm is defined is that it's the square root of the sum of the absolute values squared
of the entries of the vector. So another way to define what
a quantum state vector is is that a quantum state vector is a column vector having
complex number entries whose Euclidean norm is equal to one, which is to say that it's a unit vector with respect to the Euclidean norm. That's because the sum of
the absolute value squared is equal to one if and only if the Euclidean
norm is equal to one. The square root doesn't change anything when the sum of the absolute
values squared is equal to one. So that's what a quantum state vector is. It's a column vector with indices that correspond to the
classical states of a system having complex number entries and Euclidean norm equal to one. What that means or what it implies or how we should interpret this
notion is a different matter and in some sense, the study of quantum
information and computation is an exploration of what
this definition implies, at least, in terms of
information processing. This is simply the starting point. As an aside, let me mention that there
are other so-called norms besides the Euclidean norm and this notation right
here with the double bars around a vector doesn't always
mean the Euclidean norm. In different contexts, this notation could refer
to a different norm. For example, the sum
of the absolute values with no squares or square
root is a different norm, often called the one norm. There are quite a lot of
different norms, in fact. Sometimes you'll see
this notation being used with a subscript after
the second set of bars that provides more information about what norm we're talking about. And when we do that, it's pretty typical that the number two as a subscript
means the Euclidean norm. But in this series, whenever we have a
column vector like this, this double bar notation all by itself means the Euclidean norm. Here are a few examples
of quantum state vectors and these particular quantum state vectors are qubit state vectors, which means that the
classical states of our system are zero and one. The word qubit is short for quantum bit, which is really just a bit
that can be in a quantum state. The first two examples are really simple. The standard basis vectors
ket zero and ket one are quantum state vectors. They're both column vectors, and the indices correspond
to the classical states is zero and one, and the entries of these
vectors are complex numbers. For these particular vectors, the entries are zero and one, which are real numbers, and in fact, they're integers, but they're also complex numbers that happen to have
imaginary part equal to zero. Each vector has one entry equal to one and one entry equal to zero. So when we sum the absolute
values of the entries, we get one in both cases. So the conditions
required for these vectors to be quantum state vectors are satisfied. The next two examples are very commonly encountered qubit states called the plus state and the minus state. They're denoted as you
see here on the screen, either with a ket plus
or a ket minus like this, and they're defined as
you can see right here. Again, the entries of these
vectors are complex numbers. This time, they're either positive one over square root of two or negative one over square root of two, and when we sum the
absolute values squared, we get 1/2 plus 1/2, which is one. By the way, putting a
plus sign or a minus sign inside of a ket like this
might seem a little strange because these aren't classical
states of our system, but this notation is used nevertheless, and we'll come back to this momentarily. But for now, in short, we can use the Dirac notation to put whatever name we want for a vector inside of a ket. The vector doesn't have to
be a standard basis vector when we use the Dirac notation. Here's a final qubit state
example for the moment, anyway. It's a quantum state vector of a qubit that doesn't have a special name, and it's not particularly significant. The entries of this vector are one plus two times i
over three and negative 2/3. These are complex numbers, and if you compute the
absolute values squared of these two entries, you'll get 5/9 for the first one and 4/9 for the second one. So the sum of the absolute
value squared is equal to one. And here's one last example. This time for a system
whose classical states are the four card suits. I don't know why we would have a system whose classical states
are the four card suits being in a quantum state, but it is possible in principle and it's just meant as an example. The entries corresponding
to the different suits are 1/2 for clubs, negative
i over two for diamonds, zero for hearts because it doesn't appear in the sum and one over square
root of two for spades. And here's what it looks
like as a column vector, assuming that we've ordered
the classical states as we did before, which is the order that
you see right here. Taking the absolute value
squared of these entries gives one quarter for the first one, also one quarter for the next one. The absolute value squared
of zero is equal to zero, so that entry doesn't
contribute to the sum. And finally, for the last one, we get 1/2. So the sum is equal to one as is required for this vector
to be a quantum state vector. Now just a moment ago, we used the notation
ket plus and ket minus to referred to two qubit state vectors that aren't standard basis vectors. And I'd like to return
to this point briefly and explain how the Dirac notation is used for arbitrary vectors and not just for standard basis vectors. As I've already mentioned, we can use whatever name we
want inside of a bra or ket to refer to a vector. Kets are column vectors
and bras are row vectors just like before. As an example, the Greek letter
psi is very commonly used inside of a ket to refer to some arbitrary vector, and here, we're using it to
give a name to the state vector from before that doesn't
have a special name. When we do this, the let our psi doesn't
necessarily have any meaning all by itself. It's just the name of a vector, and it's inside of a ket to help us to clarify
that it's a column vector. This can be a little
bit confusing sometimes, and there is a potential for ambiguity if the letter psi could be associated with a classical state of
some system for instance, but it generally doesn't
cause any problems, and you don't have to use
the Dirac notation like this if you don't want to. Using a lower case letter such
as u, v, or w without a ket is also a fine way to
denote a column vector or more specifically,
a quantum state vector. It's your choice what notation
you use to express yourself. You just need to make
sure that number one, the meaning of what you
are trying to express is clear to others. And number two, that you understand what
others are trying to express even when their preferences for notation aren't the same as yours. There is one very important rule for using the Dirac notation
for arbitrary vectors and that is that if you
have a column vector that you've decided to write
as ket psi, for instance, or it could be a different
name inside of a ket, then the row vector bra psi is understood to be
the conjugate transpose of the vector ket psi. And here, this is written as an equation where this dagger right here refers to the conjugate transpose. What that is specifically is
the row vector you get by, number one, transposing the vector, meaning that you flip
the vector on its side and you change it from a column into a row without actually changing the entries. And number two, taking the complex conjugate
of each of the entries. You can actually perform
those two operations, the transpose and the
entry-wise complex conjugate in either order or those
two operations commute. So for the vector in this example, given that we've decided to
call this vector ket psi, it's understood that bra psi
means this vector right here where we've transposed the vector by turning ket zero and ket
one into bra zero and bra one, and we've taken the complex
conjugate of each entry, which is why one plus
two times i over three turns into one minus two
times i over three right here. Here's what these vectors look like when they're written
explicitly as a column vector and a row vector. Notice, by the way, that
this rule is consistent with the notation we use when we have classical states
inside of the bra and the ket. In that case, the entries are
all zero except for one, one, and so taking the complex
conjugate doesn't do anything in that case. We've now seen how quantum states are represented as vectors, but it's not clear at this
point what the meaning or the implications are in terms of what you
can do with the system in a quantum state or how you can interact with it. The first thing we need to do to bring these issues into focus
is to discuss measurements. Intuitively speaking,
measurements provide a mechanism for extracting classical
information from quantum systems. When we look at a system
when it's in a quantum state, we don't see a quantum state just like we don't see
probabilistic states, we see classical states, and this notion of a measurement is what provides the interface. If we, as humans, want to know something about a quantum system, we have to measure it, and in doing so, we'll extract classical information about whatever quantum
state that system was in. There are different
notions of measurements. The difference is one
of generality really, and we're going to start with the simplest and most basic one, which is typically called a
standard basis measurement. We will generalize this
in subsequent lessons. We'll talk about so-called
projective measurements in lesson three, and then later on in the
third unit of the series, we'll talk about general measurements, which are also called positive
operator valued measures or POVMs for short. But for now, we'll restrict our attention to standard basis measurements. When a measurement is
performed on a system, there will be some set
of possible outcomes of that measurement, which are classical outcomes, and in the case of a
standard basis measurement, those possible outcomes are
precisely the classical states of whatever system is being measured. Each of those classical states will be the outcome of the measurement with some probability, and that probability is
the absolute value squared of the entry corresponding
to that classical state of whatever quantum state
vector the system was in immediately prior to being measured. We know that the absolute value squared of any complex number is a non-negative real number, and we know from the definition
of quantum state vectors that the sum of the absolute
value squared of the entries of a quantum state vector is equal to one. So this makes sense. Each classical state will appear as the outcome of the
measurement with some probability and the probabilities sum to one. Here are a few examples. First, suppose we have a
qubit in the plus state. If we measure this qubit with respect to a standard
basis measurement, we get either a zero or
a one as the outcome. The probability of getting a zero is the absolute value
squared of one over root two, which is 1/2, and that's the same
probability of getting a one. So if you measure a plus state, you'll get a uniform random bit. If the system is in a minus
state rather than a plus state, then measuring works in
exactly the same way. The probabilities are, again, 1/2 for each possible outcome. The only difference here is that the plus sign turned into a minus sign, but when we take the absolute
value, nothing changes. If we measure this qubit state, the probability to get the outcome zero is the absolute value squared of one plus two times i
over three, which is 5/9, and the probability to get the outcome one is the absolute value squared
of negative 2/3, which is 4/9. And finally, measuring
the standard basis state, ket zero gives the outcome
zero with certainty, and likewise, measuring the state ket one yields the outcome one with certainty. That's because the absolute
value squared of one is one. Similar to what we had in
the probabilistic setting. We can associate these quantum
states, ket zero in ket one, with the system being in
those classical states, and this example is consistent
with that interpretation. Now, just like we had in
the probabilistic setting, if we measure a system when
it's in a quantum state, then the state will change, in general, as a result of having
performed that measurement. In particular, and again, this is exactly like we had
in the probabilistic setting. If we measure a system and the outcome of the measurement
is the classical state a, then the new quantum state
of the system becomes ket a. So for this state right
here, for instance, we see the same kind
of behavior that we had in the probabilistic setting. If we measure then with probability 5/9, we obtain the outcome zero, in which case, the state
transitions to ket zero, and with probability
4/9, the outcome is one, and the state transitions to ket one. Sometimes this phenomenon is referred to as a collapse
of the quantum state, and it's the kind of
thing that keeps people that study the foundations
of quantum mechanics awake at night. But from a purely
mathematical perspective, it's really quite simple
and very much analogous to what we have in the probabilistic case. The source of tension here, in some sense, concerns the fundamental
nature of quantum states and what they represent
and how they differ from probabilistic states. But at a mathematical level, this is perhaps what you might expect. Notice, by the way, that if
you measured a second time, then you would get exactly
the same result as you did for the first measurement. So there's a limit on how
much classical information can be extracted from a quantum state, and we'll come back to this point soon in another lesson. We've talked about quantum states as well as measurements, specifically standard basis measurements, which provide a way to
extract classical information from quantum states. The remaining topic for
this lesson is operations, specifically unitary operations, which describe how
quantum states of systems can be changed. Naturally, given that quantum
state vectors are different from probability vectors, you would expect that set
of allowable operations on quantum states is different than what we have for
classical information, and indeed, that's the case. Whereas operations on probabilistic states are represented by stochastic matrices, operations on quantum state vectors are represented by unitary matrices. Here's the definition of unitary matrices. A square matrix U having complex
number entries is unitary if it satisfies the equalities
that you see right here. The dagger represents
the conjugate transpose, which we already saw for vectors when we talked about the Dirac
notation a few moments ago. Here we're performing the
conjugate transpose on a matrix rather than a vector, but it's essentially the same thing. We transpose the matrix, which just means that we
swap rows and columns, so the JK entry becomes the KJ entry, and also we take the complex
conjugate of each entry. Also, this notation right here
means the identity matrix. These two equalities right
here are actually equivalent. If you have one, then you
automatically have the other. They're both equivalent to
saying that you is invertible, and the inverse is equal
to the conjugate transpose. That's only true for square
matrices, by the way. If you have a matrix that isn't square, then one of those two
equalities might be true, but then the other one won't be true. But here, we're talking
about square matrices. So this is the standard
definition for unitary matrices, and it's nice because it's easy to check
if a matrix is unitary. You just multiply the matrix to its conjugate transpose on either side, and you see if you get
the identity matrix. There's an equivalent way to characterize unitarian matrices, and that is that a
square matrix is unitary if and only if it never
changes the Euclidean norm of any vector when you multiply. An N by N matrix U is unitary if and only if the
Euclidean norm of U times v is equal to the Euclidean norm v for every n-dimensional column vector v with complex number entries. And therefore, if v is
a quantum state vector, then U times v is also
a quantum state vector because quantum state vectors are simply the vectors having
Euclidean norm equal to one. In fact, we can say a little bit more, which is that the unitary matrices are precisely the matrices that always transform
quantum state vectors into quantum state vectors. That's a similar situation to what we have for stochastic matrices
and probability vectors. The stochastic matrices
are precisely the matrices that always transform probability vectors into probability vectors. So once we've decided that operations should be represented by matrices, which is the same as saying that transformations act linearly on vectors representing states, these choices for the sets
of allowable operations follow naturally, at least,
in a mathematical sense. Now let's take a look at some examples of qubit unitary operations. We'll see many more examples
of unitary operations in subsequent lessons, including unitary operations and systems having more than two classical states. But for now, we'll focus just
on qubit unitary operations. We're going to be seeing these
particular examples arising again and again, so it's definitely worth getting
to know these operations. The first collection of examples are the so-called Pauli operations. These are the operations that correspond to the Pauli matrices, which are shown here. They're all unitary, and you can check that for each one according to the definition from before. These particular matrices
happen to be equal to their own conjugate transposes, which is to say that they're
all Hermitian matrices. So checking that they're
all unitary, in this case, is a matter of squaring each one and seeing that the result
is the identity matrix. The names you see for these
matrices are pretty standard, and you'll also see sigma
x, sigma y, and sigma z called simply X, Y, and Z. Do be aware, though, that the
capital letters X, Y, and Z are also commonly used for other purposes, even in the context of
quantum information, but they are very common
names for these matrices. The sigma x or X operation
is also called a bit flip or a not operation, which we've already seen
in the classical context. And the sigma z or Z operation
is also called a phase flip. And here, you can see the
action of these operations on the standard basis factors, which kind of explains
where these names come from. Certainly, it makes sense to
refer to sigma x as a bit flip based on this action right here. It simply flips the bit from
zero to one or one to zero. It also makes sense to
call sigma z a phase flip based on this action right here. But if that's not crystal clear
at the moment, that's okay. We'll encounter this
operation over and over and the significance
of putting a minus sign in front of the ket one
basis vector and not ket zero and why that's called a phase flip will be more clear later on. The next example is
the Hadamard operation, which is represented by
this matrix right here, which is pretty much always named H. Checking that H is unitary is once again a
straightforward calculation, which if I step out of the way, you can see right here. Similar to the Pauli matrices, H is its own conjugate transpose. So we have this equality right here, and when we perform the multiplication, we get the identity matrix as is required. The third example is
really an entire class of unitary operations
known as phase operations. These are operations
represented by any matrix that takes this form
that you see right here where theta is any real number. So this entry right here will
always be some complex number on the unit circle. Matrices like this are always unitary, which I will leave to you to verify. This time, the conjugate transpose won't be equal to the original matrix unless this number here happens
to be a one or a minus one, in which case, we end up with
either the identity matrix or sigma z, which we've
already encountered. Specifically, transposition
won't do anything to this matrix, but taking the complex
conjugate of each entry changes this entry right here
to e to the minus i theta. These two particular operations right here are particularly important
examples of phase operations. For the first one, we take
theta to be pi over two, and we get this matrix right here. This is commonly called an
S operation or an S gate when we're talking about circuits, which we won't discuss in this lesson, but that's an important
topic that's coming up soon. The second one is this operation. It's a T operation or a T gate. And for this one, theta
is equal to pi over four. And so when we compute the exponential, we get this value right here, which is one plus i over
the square root of two. Here are just a few quick examples to see a couple of these
operations in action. Here's the action of
the Hadamard operation on ket zero and ket one. If we go through the
matrix-vector multiplication, we see that what we obtain are the plus state and the
minus state respectively. If we perform the Hadamard operation on the plus state and the
minus state, on the other hand, we get back to ket zero and ket one. So if we clean things up a little bit, we can see the action more clearly. The Hadamard operation
transforms back and forth between the zero state and the plus state and between the one state
and the minus state. Concerning these two equations right here, notice that this gives us a simple way to detect the difference between a plus state and a minus state. As we saw earlier in the lesson, if we measure a plus
state and a minus state with respect to the
standard basis measurement, we get a uniform random bit in both cases, which doesn't help us to
detect the difference. But if we perform a Hadamard
operation and then we measure, we'll see a zero if the
original state was a plus state and a one if the original
state was a minus state. So these two states are indeed different, and they can be discriminated
perfectly by the method that I just described, which is to first perform
a Hadamard operation and then to measure. Going back to the example of
a qubit state we saw before that isn't particularly special, we can see what the Hadamard
operation does to the state by converting to the forms
that you see right here and performing the multiplication. And here is the result
which we can convert back to the Dirac notation if we wish, and that's one way to compute
these sorts of actions. Here's another example that illustrates that you don't always
have to convert explicitly to a matrix vector form. You can also perform these
computations directly using the Dirac notation. This example concerns the T operation, and here it is right
here just for reference. This operation has this action right here on standard basis states. And if we want to compute
the action of the T operation on a plus state, for instance, we can start by expanding the
definition of the plus state and then use linearity
to express the result as you see right here. We can now just plug in
these expressions up here, do just a little bit of
simplification for the second term, and we obtain the result. If for whatever reason we'd like to apply a Hadamard
operation to the result, we can do a similar thing. First, we substitute the expression for T acting on the plus state and we use linearity like we did before. And now, if we substitute the
expressions we already have for the Hadamard operation
acting on ket zero and ket one, we obtain this. And now, we can expand the
plus state and the minus state to get this expression right here, which we can simplify just by adding, and here is the final result. There's nothing particularly
special about this example. It's just meant to
illustrate this alternative but equivalent way of
calculating the actions of these operations. You can use whichever
way is more convenient in the situation at hand. One final point about
unitary operations is that compositions of unitary
operations are represented by matrix multiplication just like in the probabilistic setting, i.e., you can compute the action of a sequence of unitary operations by simply multiplying
the matrices together. And what you'll get is
a single unitary matrix representing the combined action of the sequence of operations. Just like we had for stochastic matrices, the unitary matrices are
closed under multiplication, so you'll always get a unitary matrix when you compose unitary operations. You have to make sure
you multiply the matrices in the correct order. But the way it works is just like it was for
stochastic operations where the matrix for the
first operation you perform will always appear on the right-hand side
of the matrix product, and the last one will appear
on the left-hand side. So just like before, the order is reversed in the sense that you compose from right to left rather than left to right. Here's an interesting example that gives rise to an operation known as the square root of not operation. Suppose that we first
apply a Hadamard operation followed by an S operation, followed by another Hadamard operation. The combined action of
these three operations is described by this product. Here's the first Hadamard operation, here's the S operation, and here's the second Hadamard operation. If we perform the matrix multiplication, which isn't shown here explicitly, but you can do this for
yourself if you wish, the result is this matrix right here. It's a unitary matrix, and we can either check that directly or we can trust in the fact that the unitary matrices are
closed under multiplication. It has to be unitary because these three matrices are unitary as we've already seen. We don't have any particular
quantum state vector in mind here. We can imagine performing
this sequence of operations on any quantum state vector, and this matrix describes the action. The reason why this combined operation, meaning the one described
by this matrix right here is called a spirit of not operation, is that by performing it
twice, we get a not operation, or in other words, a
sigma x or an X operation. Performing the operation
twice is represented by squaring the matrix or
multiplying it to itself. And if you do this, you'll see that the result
is the Pauli x matrix. And that's kind of peculiar, and it gives you just a hint that you can do some interesting things with quantum information that you can't do with
classical information. There's no classical operation, meaning one represented
by a stochastic matrix that gives us a not operation
if we perform it twice. So it's a nice example that reveals that quantum operations
behave very differently from classical operations. And that's it for lesson one. In this lesson, we discussed how quantum information works for single systems in isolation, and in the next lesson, we'll talk about how
quantum information works for multiple systems. Thanks for watching. If you have any questions, feel free to leave them in
the comments section below.

## The Stabilizer Formalism ｜ Understanding Quantum Information & Computation ｜ Lesson 14

welcome back to understanding Quantum welcome back to understanding Quantum
information and computation my name is information and computation my name is information and computation my name is
John watus and I'm the technical John watus and I'm the technical John watus and I'm the technical
director for education at IBM director for education at IBM director for education at IBM
Quantum this is lesson number 14 of the Quantum this is lesson number 14 of the Quantum this is lesson number 14 of the
series and it's the second lesson of the series and it's the second lesson of the series and it's the second lesson of the
fourth unit which is on Quantum error fourth unit which is on Quantum error fourth unit which is on Quantum error
correction in the previous lesson we correction in the previous lesson we correction in the previous lesson we
took a first look at Quantum error took a first look at Quantum error took a first look at Quantum error
correction including the nine Cubit correction including the nine Cubit correction including the nine Cubit
Shore code which establishes that Shore code which establishes that Shore code which establishes that
Quantum error correction is possible in Quantum error correction is possible in Quantum error correction is possible in
theory we also discussed the theory we also discussed the theory we also discussed the
discretization of Errors which allows us discretization of Errors which allows us discretization of Errors which allows us
to correct for arbitrary Quantum errors to correct for arbitrary Quantum errors to correct for arbitrary Quantum errors
on cubits by effectively projecting them on cubits by effectively projecting them on cubits by effectively projecting them
onto bit flips and phase onto bit flips and phase onto bit flips and phase
flips in this lesson we'll discuss the flips in this lesson we'll discuss the flips in this lesson we'll discuss the
stabilizer formalism which is a stabilizer formalism which is a stabilizer formalism which is a
mathematical tool through which we can mathematical tool through which we can mathematical tool through which we can
specify and analyze a broad class of specify and analyze a broad class of specify and analyze a broad class of
quantum error correcting codes known as quantum error correcting codes known as quantum error correcting codes known as
stabilizer stabilizer stabilizer
codes this includes the 9 Cubit Shore codes this includes the 9 Cubit Shore codes this includes the 9 Cubit Shore
code along with many other examples code along with many other examples code along with many other examples
including codes that are likely to be including codes that are likely to be including codes that are likely to be
much better suited to real world Quantum much better suited to real world Quantum much better suited to real world Quantum
Computing devices than the shore Cod Computing devices than the shore Cod Computing devices than the shore Cod
code not every Quantum error correcting code not every Quantum error correcting code not every Quantum error correcting
code is a stabilizer code but many are code is a stabilizer code but many are code is a stabilizer code but many are
including every example that we'll see including every example that we'll see including every example that we'll see
in this in this in this
series there are certainly interesting series there are certainly interesting series there are certainly interesting
and potentially useful Quantum error and potentially useful Quantum error and potentially useful Quantum error
correcting codes that aren't stabilizer correcting codes that aren't stabilizer correcting codes that aren't stabilizer
codes but they tend to be rather more codes but they tend to be rather more codes but they tend to be rather more
complicated and in any case there's no complicated and in any case there's no complicated and in any case there's no
question that the stabilizer formalism question that the stabilizer formalism question that the stabilizer formalism
is a Cornerstone in the foundation of is a Cornerstone in the foundation of is a Cornerstone in the foundation of
quantum error correction and we'll be quantum error correction and we'll be quantum error correction and we'll be
relying heavily upon it for the relying heavily upon it for the relying heavily upon it for the
remainder of the unit here's a brief remainder of the unit here's a brief remainder of the unit here's a brief
overview of the lesson overview of the lesson overview of the lesson
we'll start out with a short discussion we'll start out with a short discussion we'll start out with a short discussion
of poly matrices and tensor products of of poly matrices and tensor products of of poly matrices and tensor products of
poly matrices which can represent both poly matrices which can represent both poly matrices which can represent both
operations on cubits as well as operations on cubits as well as operations on cubits as well as
measurements of cubits as I'll explain measurements of cubits as I'll explain measurements of cubits as I'll explain
and in this context we often refer to and in this context we often refer to and in this context we often refer to
them as them as them as
observables we'll then go back and have observables we'll then go back and have observables we'll then go back and have
a second look at the repetition code and a second look at the repetition code and a second look at the repetition code and
we'll see how we can describe it through we'll see how we can describe it through we'll see how we can describe it through
poly operations and poly operations and poly operations and
observables and that will both inform observables and that will both inform observables and that will both inform
and lead into a general discussion of and lead into a general discussion of and lead into a general discussion of
stabilizer codes including several stabilizer codes including several stabilizer codes including several
examples basic properties of stabilizer examples basic properties of stabilizer examples basic properties of stabilizer
codes and how we can go about performing codes and how we can go about performing codes and how we can go about performing
the fundamental tasks of encoding the fundamental tasks of encoding the fundamental tasks of encoding
detecting errors and correcting those detecting errors and correcting those detecting errors and correcting those
errors we'll begin the lesson with a errors we'll begin the lesson with a errors we'll begin the lesson with a
brief discussion of poly operations brief discussion of poly operations brief discussion of poly operations
mainly to set the stage for the mainly to set the stage for the mainly to set the stage for the
stabilizer formalism but also to observe stabilizer formalism but also to observe stabilizer formalism but also to observe
a few key facts and establish some basic a few key facts and establish some basic a few key facts and establish some basic
terminology first here are the poly terminology first here are the poly terminology first here are the poly
matrices including the 2x2 identity matrices including the 2x2 identity matrices including the 2x2 identity
Matrix as well as the three non-identity Matrix as well as the three non-identity Matrix as well as the three non-identity
poly matrices which will denote by poly matrices which will denote by poly matrices which will denote by
capital X X Y and Z throughout the capital X X Y and Z throughout the capital X X Y and Z throughout the
remainder of the remainder of the remainder of the
series all four of these matrices are series all four of these matrices are series all four of these matrices are
both unitary and herian and we can both unitary and herian and we can both unitary and herian and we can
identify them with operations on a cubit identify them with operations on a cubit identify them with operations on a cubit
including operations we might choose to including operations we might choose to including operations we might choose to
perform as well as errors as we perform as well as errors as we perform as well as errors as we
discussed in the previous discussed in the previous discussed in the previous
lesson the non-identity poly matrices X lesson the non-identity poly matrices X lesson the non-identity poly matrices X
Y and Z anticommute with one another Y and Z anticommute with one another Y and Z anticommute with one another
meaning that XY is equal to -1 * YX XZ meaning that XY is equal to -1 * YX XZ meaning that XY is equal to -1 * YX XZ
is equal to negative ZX and YZ is equal is equal to negative ZX and YZ is equal is equal to negative ZX and YZ is equal
to negative z y and you can easily check to negative z y and you can easily check to negative z y and you can easily check
these relations by doing the these relations by doing the these relations by doing the
multiplications these anti-commutation multiplications these anti-commutation multiplications these anti-commutation
relations are very simple but they're relations are very simple but they're relations are very simple but they're
critically important in the stabilizer critically important in the stabilizer critically important in the stabilizer
formalism and elsewhere and specifically formalism and elsewhere and specifically formalism and elsewhere and specifically
as we'll see these minus signs that pop as we'll see these minus signs that pop as we'll see these minus signs that pop
out when we exchange The Ordering of two out when we exchange The Ordering of two out when we exchange The Ordering of two
different non-identity poly matrices in different non-identity poly matrices in different non-identity poly matrices in
a product correspond precisely to the a product correspond precisely to the a product correspond precisely to the
detection of errors in the stabilizer detection of errors in the stabilizer detection of errors in the stabilizer
formalism we also have the formalism we also have the formalism we also have the
multiplication rules listed here each multiplication rules listed here each multiplication rules listed here each
polymatrix is its own inverse which is polymatrix is its own inverse which is polymatrix is its own inverse which is
consistent with the fact that they're consistent with the fact that they're consistent with the fact that they're
both unitary and herian and if you both unitary and herian and if you both unitary and herian and if you
multiply any two different non-identity multiply any two different non-identity multiply any two different non-identity
poly matrices together you'll always get poly matrices together you'll always get poly matrices together you'll always get
plus or minus I times the remaining plus or minus I times the remaining plus or minus I times the remaining
non-identity non-identity non-identity
polymatrix in particular up to a global polymatrix in particular up to a global polymatrix in particular up to a global
phase Y is equivalent to X time Z which phase Y is equivalent to X time Z which phase Y is equivalent to X time Z which
explains our focus on x and z errors in explains our focus on x and z errors in explains our focus on x and z errors in
Quantum error correction X represents a Quantum error correction X represents a Quantum error correction X represents a
bit flip Z represents a phase flip and bit flip Z represents a phase flip and bit flip Z represents a phase flip and
so up to a global phase y represents so up to a global phase y represents so up to a global phase y represents
both of these errors occurring on the both of these errors occurring on the both of these errors occurring on the
same Cubit these four matrices all represent Cubit these four matrices all represent
operations or errors or the absence of operations or errors or the absence of operations or errors or the absence of
an error on a single Cubit but it's an error on a single Cubit but it's an error on a single Cubit but it's
often convenient to tensor them together often convenient to tensor them together often convenient to tensor them together
to perform operations or errors on to perform operations or errors on to perform operations or errors on
multiple multiple multiple
cubits as a point of terminology cubits as a point of terminology cubits as a point of terminology
whenever we refer to an N Cubit poly whenever we refer to an N Cubit poly whenever we refer to an N Cubit poly
operation we mean a tensor product of operation we mean a tensor product of operation we mean a tensor product of
any n poly matrices such as the examples any n poly matrices such as the examples any n poly matrices such as the examples
shown shown shown
here it's also common to use the term here it's also common to use the term here it's also common to use the term
poly operation when there's a global poly operation when there's a global poly operation when there's a global
phase phase phase
but to keep things as simple as possible but to keep things as simple as possible but to keep things as simple as possible
I'm going to use the term poly operation I'm going to use the term poly operation I'm going to use the term poly operation
to mean simply a tensor product of poly to mean simply a tensor product of poly to mean simply a tensor product of poly
matrices the weight of an incubate poly matrices the weight of an incubate poly matrices the weight of an incubate poly
operation is simply the number of operation is simply the number of operation is simply the number of
non-identity poly matrices in the tensor non-identity poly matrices in the tensor non-identity poly matrices in the tensor
product for example the n-fold tensor product for example the n-fold tensor product for example the n-fold tensor
product of the 2 x two identity Matrix product of the 2 x two identity Matrix product of the 2 x two identity Matrix
with itself is a weight zero poly with itself is a weight zero poly with itself is a weight zero poly
operation a tensor product of two polyx operation a tensor product of two polyx operation a tensor product of two polyx
matrices and a whole bunch of identity matrices and a whole bunch of identity matrices and a whole bunch of identity
matrices has weight two and so on matrices has weight two and so on matrices has weight two and so on
intuitively speaking the weight of an N intuitively speaking the weight of an N intuitively speaking the weight of an N
Cubit poly operation is the number of Cubit poly operation is the number of Cubit poly operation is the number of
cubits on which it acts cubits on which it acts cubits on which it acts
non-trivially and it's typical that non-trivially and it's typical that non-trivially and it's typical that
Quantum error correcting codes are Quantum error correcting codes are Quantum error correcting codes are
designed so that they can detect and designed so that they can detect and designed so that they can detect and
correct errors represented by poly correct errors represented by poly correct errors represented by poly
operations up to a certain weight we can operations up to a certain weight we can operations up to a certain weight we can
think about poly operations in various think about poly operations in various think about poly operations in various
ways including as operations performed ways including as operations performed ways including as operations performed
intentionally by Quantum Gates as well intentionally by Quantum Gates as well intentionally by Quantum Gates as well
as errors and we'll also see shortly as errors and we'll also see shortly as errors and we'll also see shortly
that poly operations can describe that poly operations can describe that poly operations can describe
measurements sometimes it's also useful measurements sometimes it's also useful measurements sometimes it's also useful
to think about poly operations as to think about poly operations as to think about poly operations as
generators of certain sets in an generators of certain sets in an generators of certain sets in an
algebraic sense that you may recognize algebraic sense that you may recognize algebraic sense that you may recognize
if you're familiar with group theory if if you're familiar with group theory if if you're familiar with group theory if
you're not familiar with group Theory you're not familiar with group Theory you're not familiar with group Theory
that's okay it's really not essential that's okay it's really not essential that's okay it's really not essential
for this lesson but if you're interested for this lesson but if you're interested for this lesson but if you're interested
in studying Quantum error correction in in studying Quantum error correction in in studying Quantum error correction in
Greater depth you may find that some Greater depth you may find that some Greater depth you may find that some
basic group theory is very helpful for basic group theory is very helpful for basic group theory is very helpful for
doing that anyway let's suppose that P1 doing that anyway let's suppose that P1 doing that anyway let's suppose that P1
through PR are n Cubit poly operations through PR are n Cubit poly operations through PR are n Cubit poly operations
so P1 is an ncit poly operation P2 is an so P1 is an ncit poly operation P2 is an so P1 is an ncit poly operation P2 is an
ncit poly operation and so ncit poly operation and so ncit poly operation and so
on when we refer to the set generated by on when we refer to the set generated by on when we refer to the set generated by
P1 through p r what we mean is the set P1 through p r what we mean is the set P1 through p r what we mean is the set
of all matrices that we can obtain by of all matrices that we can obtain by of all matrices that we can obtain by
multiplying these matrices together in multiplying these matrices together in multiplying these matrices together in
any combination and in any order we any combination and in any order we any combination and in any order we
choose taking each one as many times as choose taking each one as many times as choose taking each one as many times as
we like we'll see a few examples in just we like we'll see a few examples in just we like we'll see a few examples in just
a moment but first let me mention the a moment but first let me mention the a moment but first let me mention the
notation that we use to denote this set notation that we use to denote this set notation that we use to denote this set
the set generated by P1 through PR and the set generated by P1 through PR and the set generated by P1 through PR and
it's P1 through PR separated by commas it's P1 through PR separated by commas it's P1 through PR separated by commas
inside of angled brackets it does kind inside of angled brackets it does kind inside of angled brackets it does kind
of look like we could easily confuse of look like we could easily confuse of look like we could easily confuse
this notation with direct notation but this notation with direct notation but this notation with direct notation but
that doesn't generally happen and this that doesn't generally happen and this that doesn't generally happen and this
is the traditional notation in group is the traditional notation in group is the traditional notation in group
Theory here's an example of the set Theory here's an example of the set Theory here's an example of the set
generated by X Y and Z so n is equal to generated by X Y and Z so n is equal to generated by X Y and Z so n is equal to
one in this example because there are no one in this example because there are no one in this example because there are no
tensor products and we're taking all tensor products and we're taking all tensor products and we're taking all
three non-identity poly matrices as three non-identity poly matrices as three non-identity poly matrices as
generators it turns out that there are generators it turns out that there are generators it turns out that there are
16 matrices that can be generated by 16 matrices that can be generated by 16 matrices that can be generated by
forming products that include these forming products that include these forming products that include these
three matrices and you can reason this three matrices and you can reason this three matrices and you can reason this
through the multiplication rules that we through the multiplication rules that we through the multiplication rules that we
saw saw saw
earlier specifically we can of course earlier specifically we can of course earlier specifically we can of course
get X Y and Z as well as the identity get X Y and Z as well as the identity get X Y and Z as well as the identity
and in fact we can also get any of these and in fact we can also get any of these and in fact we can also get any of these
four matrices time I 1 or negative I and four matrices time I 1 or negative I and four matrices time I 1 or negative I and
that's that's that's
it this set is commonly known as the it this set is commonly known as the it this set is commonly known as the
poly poly poly
group we could also consider the set group we could also consider the set group we could also consider the set
generated by just x and z and we end up generated by just x and z and we end up generated by just x and z and we end up
getting half of the poly group in this getting half of the poly group in this getting half of the poly group in this
case case case
and one final example at least for now and one final example at least for now and one final example at least for now
we can consider the set generated by X we can consider the set generated by X we can consider the set generated by X
tensor x and z tensor Z so n is equal to tensor x and z tensor Z so n is equal to tensor x and z tensor Z so n is equal to
two in this case and we end up getting two in this case and we end up getting two in this case and we end up getting
just four elements this time owing to just four elements this time owing to just four elements this time owing to
the fact that X tensor x and z tensor Z the fact that X tensor x and z tensor Z the fact that X tensor x and z tensor Z
commute with one commute with one commute with one
another poly matrices and N Cubit poly another poly matrices and N Cubit poly another poly matrices and N Cubit poly
operations more generally describe operations more generally describe operations more generally describe
unitary operations whether they're unitary operations whether they're unitary operations whether they're
errors or they're intentional but they errors or they're intentional but they errors or they're intentional but they
also describe also describe also describe
measurements to be more precise starting measurements to be more precise starting measurements to be more precise starting
with just the three non-identity poly with just the three non-identity poly with just the three non-identity poly
matrices we can associate each of these matrices we can associate each of these matrices we can associate each of these
matrices with a projective measurement matrices with a projective measurement matrices with a projective measurement
defined in terms of its igen values and defined in terms of its igen values and defined in terms of its igen values and
igen vectors and here on the screen we igen vectors and here on the screen we igen vectors and here on the screen we
have spectral decompositions of these have spectral decompositions of these have spectral decompositions of these
matrices that will help to clarify matrices that will help to clarify matrices that will help to clarify
this in essence when we refer to an X this in essence when we refer to an X this in essence when we refer to an X
measurement a y measurement or a z measurement a y measurement or a z measurement a y measurement or a z
measurement we mean the projective measurement we mean the projective measurement we mean the projective
measurement defined by the projections measurement defined by the projections measurement defined by the projections
onto the igen spaces of these matrices onto the igen spaces of these matrices onto the igen spaces of these matrices
and commonly we think about the igen and commonly we think about the igen and commonly we think about the igen
value vales as being the corresponding value vales as being the corresponding value vales as being the corresponding
measurement measurement measurement
outcomes for example an X measurement is outcomes for example an X measurement is outcomes for example an X measurement is
a measurement with respect to the plus a measurement with respect to the plus a measurement with respect to the plus
minus basis of a cubit or alternatively minus basis of a cubit or alternatively minus basis of a cubit or alternatively
we can describe it by the set containing we can describe it by the set containing we can describe it by the set containing
the projections Kat plus BR plus and K the projections Kat plus BR plus and K the projections Kat plus BR plus and K
minus bra minus bra minus bra
minus a z measurement on the other hand minus a z measurement on the other hand minus a z measurement on the other hand
is nothing more than a standard basis is nothing more than a standard basis is nothing more than a standard basis
measurement although like I said we measurement although like I said we measurement although like I said we
sometimes identify the igen values with sometimes identify the igen values with sometimes identify the igen values with
the measurement outcomes and if we do the measurement outcomes and if we do the measurement outcomes and if we do
that then the outcomes are plus and that then the outcomes are plus and that then the outcomes are plus and
minus one as opposed to 0 and one minus one as opposed to 0 and one minus one as opposed to 0 and one
respectively we can in fact do something respectively we can in fact do something respectively we can in fact do something
along these lines for arbitrary hermi along these lines for arbitrary hermi along these lines for arbitrary hermi
matrices and the term observable is matrices and the term observable is matrices and the term observable is
commonly used in this situation for poly commonly used in this situation for poly commonly used in this situation for poly
matrices and encubate poly operations matrices and encubate poly operations matrices and encubate poly operations
more generally we can perform such more generally we can perform such more generally we can perform such
measurements non-destructively using measurements non-destructively using measurements non-destructively using
phase phase phase
estimation and just to be sure that this estimation and just to be sure that this estimation and just to be sure that this
diagram is clear here we're performing diagram is clear here we're performing diagram is clear here we're performing
the measurements non-destructively on the measurements non-destructively on the measurements non-destructively on
the top Cubit whereas the bottom cubit the top Cubit whereas the bottom cubit the top Cubit whereas the bottom cubit
is always initialized to a plus state or is always initialized to a plus state or is always initialized to a plus state or
in other words a hadamard applied to a in other words a hadamard applied to a in other words a hadamard applied to a
zero State because that's how phase zero State because that's how phase zero State because that's how phase
estimation estimation estimation
Works notice that because the I values Works notice that because the I values Works notice that because the I values
are just plus one and minus one in this are just plus one and minus one in this are just plus one and minus one in this
case we only need one control Cubit case we only need one control Cubit case we only need one control Cubit
which is the bottom cubid in this which is the bottom cubid in this which is the bottom cubid in this
diagram any more than that would just be diagram any more than that would just be diagram any more than that would just be
a waste because there is only one bit of a waste because there is only one bit of a waste because there is only one bit of
precision to worry about the ion value precision to worry about the ion value precision to worry about the ion value
is either plus one or minus one and the is either plus one or minus one and the is either plus one or minus one and the
measurement is already perfect just like measurement is already perfect just like measurement is already perfect just like
this the outcome of this measurement by this the outcome of this measurement by this the outcome of this measurement by
the way meaning the standard basis the way meaning the standard basis the way meaning the standard basis
measurement in the circuit will be zero measurement in the circuit will be zero measurement in the circuit will be zero
for the plus one igen value outcome for the plus one igen value outcome for the plus one igen value outcome
which corresponds to the plus state in which corresponds to the plus state in which corresponds to the plus state in
the case of an X observable and the the case of an X observable and the the case of an X observable and the
outcome is one for the minus one ion outcome is one for the minus one ion outcome is one for the minus one ion
value or the minus state in this value or the minus state in this value or the minus state in this
case everything I just said extends case everything I just said extends case everything I just said extends
naturally to ncit poly operations naturally to ncit poly operations naturally to ncit poly operations
provided that we keep in mind that when provided that we keep in mind that when provided that we keep in mind that when
we think about an observable we're we think about an observable we're we think about an observable we're
talking about measuring the igen values talking about measuring the igen values talking about measuring the igen values
not measuring with respect to a basis of not measuring with respect to a basis of not measuring with respect to a basis of
igen igen igen
vectors for example consider the two vectors for example consider the two vectors for example consider the two
Cubit poly operation Z tensor Cubit poly operation Z tensor Cubit poly operation Z tensor
Z once again there are two igen values Z once again there are two igen values Z once again there are two igen values
plus one and minus one so the plus one and minus one so the plus one and minus one so the
measurement is described by two measurement is described by two measurement is described by two
projections notice in particular that projections notice in particular that projections notice in particular that
the measurement that we associate with the measurement that we associate with the measurement that we associate with
this observable is not the same as this observable is not the same as this observable is not the same as
measuring both cubits with a z measuring both cubits with a z measuring both cubits with a z
measurement or in other words a standard measurement or in other words a standard measurement or in other words a standard
basis measurement but rather it's a two basis measurement but rather it's a two basis measurement but rather it's a two
outcome projective measurement where the outcome projective measurement where the outcome projective measurement where the
projections are onto the spaces spend by projections are onto the spaces spend by projections are onto the spaces spend by
all of the IG vectors corresponding to all of the IG vectors corresponding to all of the IG vectors corresponding to
each of the two igen values plus one and each of the two igen values plus one and each of the two igen values plus one and
minus minus minus
one another way of thinking about this one another way of thinking about this one another way of thinking about this
is that the igen values of a tensor is that the igen values of a tensor is that the igen values of a tensor
product are products of the IG values of product are products of the IG values of product are products of the IG values of
the individual matricies that we're the individual matricies that we're the individual matricies that we're
taking the tensor products of and taking the tensor products of and taking the tensor products of and
because negative 1 * negative 1 is the because negative 1 * negative 1 is the because negative 1 * negative 1 is the
same as positive 1 * positive 1 for same as positive 1 * positive 1 for same as positive 1 * positive 1 for
instance this measurement doesn't instance this measurement doesn't instance this measurement doesn't
distinguish between 0 0 and 1 distinguish between 0 0 and 1 distinguish between 0 0 and 1
one so if we were to measure say a five one so if we were to measure say a five one so if we were to measure say a five
plus Bell state with the measurement plus Bell state with the measurement plus Bell state with the measurement
described by this observable we would be described by this observable we would be described by this observable we would be
certain to obtain the outcome certain to obtain the outcome certain to obtain the outcome
corresponding to the I Value Plus One corresponding to the I Value Plus One corresponding to the I Value Plus One
and the state would be unchanged as and the state would be unchanged as and the state would be unchanged as
opposed to collapsing to 0 0 or 1 one opposed to collapsing to 0 0 or 1 one opposed to collapsing to 0 0 or 1 one
which is what would happen if we which is what would happen if we which is what would happen if we
performed the standard basis performed the standard basis performed the standard basis
measurements on the two measurements on the two measurements on the two
cubits so a zensor z measurement is not cubits so a zensor z measurement is not cubits so a zensor z measurement is not
the same thing as performing to Z the same thing as performing to Z the same thing as performing to Z
measurements measurements measurements
independently but we can in fact perform independently but we can in fact perform independently but we can in fact perform
measurements like this using phase measurements like this using phase measurements like this using phase
estimat in precisely the same way as estimat in precisely the same way as estimat in precisely the same way as
before using a single control Cubit as before using a single control Cubit as before using a single control Cubit as
is shown in the circuit diagram which is shown in the circuit diagram which is shown in the circuit diagram which
generalizes to any n Cubit poly generalizes to any n Cubit poly generalizes to any n Cubit poly
operation in place of Z tensor operation in place of Z tensor operation in place of Z tensor
Z when we have just Z poly matrices by Z when we have just Z poly matrices by Z when we have just Z poly matrices by
the way we can simplify the circuit like the way we can simplify the circuit like the way we can simplify the circuit like
this and what this reveals which is also this and what this reveals which is also this and what this reveals which is also
evident from the projections that Define evident from the projections that Define evident from the projections that Define
the measurement is that performing a z the measurement is that performing a z the measurement is that performing a z
tensor Z measurement on two cubits is tensor Z measurement on two cubits is tensor Z measurement on two cubits is
equivalent to measuring their parity equivalent to measuring their parity equivalent to measuring their parity
thinking in terms of the standard basis thinking in terms of the standard basis thinking in terms of the standard basis
and this generalizes to the tensor and this generalizes to the tensor and this generalizes to the tensor
product of any number of poly Z product of any number of poly Z product of any number of poly Z
matrices next we'll take a second look matrices next we'll take a second look matrices next we'll take a second look
at the three-bit repetition code this at the three-bit repetition code this at the three-bit repetition code this
time phrasing it in terms of poly time phrasing it in terms of poly time phrasing it in terms of poly
operations this will be our first rather operations this will be our first rather operations this will be our first rather
simple example of a stabilizer code and simple example of a stabilizer code and simple example of a stabilizer code and
we'll discuss how to generalize this we'll discuss how to generalize this we'll discuss how to generalize this
notion shortly but it is helpful to see notion shortly but it is helpful to see notion shortly but it is helpful to see
how it works in this simple case how it works in this simple case how it works in this simple case
first recall that when we apply the first recall that when we apply the first recall that when we apply the
3-bit repetition code to cubits we 3-bit repetition code to cubits we 3-bit repetition code to cubits we
encode a given Cubit State Alpha 0 plus encode a given Cubit State Alpha 0 plus encode a given Cubit State Alpha 0 plus
beta 1 as Alpha 0000 0 plus beta 11 1 1 beta 1 as Alpha 0000 0 plus beta 11 1 1 beta 1 as Alpha 0000 0 plus beta 11 1 1
and we'll give this three Cubit encoding and we'll give this three Cubit encoding and we'll give this three Cubit encoding
the name Sai for convenience so by the name Sai for convenience so by the name Sai for convenience so by
definition s is a valid three Cubit definition s is a valid three Cubit definition s is a valid three Cubit
encoding of a cubit state but if we want encoding of a cubit state but if we want encoding of a cubit state but if we want
to check this it suffices to check that to check this it suffices to check that to check this it suffices to check that
the following two equations are true the the following two equations are true the the following two equations are true the
first equation states that applying Z first equation states that applying Z first equation states that applying Z
operations to the leftmost two cubits of operations to the leftmost two cubits of operations to the leftmost two cubits of
s does nothing to S which is to say that s does nothing to S which is to say that s does nothing to S which is to say that
SII is an igen vector of Z tensor Z SII is an igen vector of Z tensor Z SII is an igen vector of Z tensor Z
tensor Identity or just ZZ identity for tensor Identity or just ZZ identity for tensor Identity or just ZZ identity for
short with ion value one and the second short with ion value one and the second short with ion value one and the second
equation is similar except that Z equation is similar except that Z equation is similar except that Z
operations are applied to the rightmost operations are applied to the rightmost operations are applied to the rightmost
two two two
cubits the idea is if we think about S cubits the idea is if we think about S cubits the idea is if we think about S
as a linear combination of standard as a linear combination of standard as a linear combination of standard
basis States then the first equation basis States then the first equation basis States then the first equation
implies that we can only have non-zero implies that we can only have non-zero implies that we can only have non-zero
coefficients for standard basis states coefficients for standard basis states coefficients for standard basis states
where the leftmost two bits have even where the leftmost two bits have even where the leftmost two bits have even
parity or equivalently are equal and the parity or equivalently are equal and the parity or equivalently are equal and the
second equation implies that we can only second equation implies that we can only second equation implies that we can only
have non-zero coefficients for standard have non-zero coefficients for standard have non-zero coefficients for standard
basis States for which the rightmost two basis States for which the rightmost two basis States for which the rightmost two
bits have even bits have even bits have even
parity another way to think about this parity another way to think about this parity another way to think about this
is to view these two poly operations as is to view these two poly operations as is to view these two poly operations as
observables and if we were to measure observables and if we were to measure observables and if we were to measure
these two observables using the these two observables using the these two observables using the
simplified circuit I mentioned just a simplified circuit I mentioned just a simplified circuit I mentioned just a
moment ago then we would be certain to moment ago then we would be certain to moment ago then we would be certain to
obtain a measurement outcome obtain a measurement outcome obtain a measurement outcome
corresponding to plus one igen values corresponding to plus one igen values corresponding to plus one igen values
because s is an igen Vector of both because s is an igen Vector of both because s is an igen Vector of both
observables with igen value one but this observables with igen value one but this observables with igen value one but this
simplified version of the combined simplified version of the combined simplified version of the combined
circuit for independent ly measuring the circuit for independent ly measuring the circuit for independent ly measuring the
observables is none other than our observables is none other than our observables is none other than our
parity check circuit for the 3-bit parity check circuit for the 3-bit parity check circuit for the 3-bit
repetition code and so we see that our repetition code and so we see that our repetition code and so we see that our
two equations together are equivalent to two equations together are equivalent to two equations together are equivalent to
the parity check circuit outputting 0 0 the parity check circuit outputting 0 0 the parity check circuit outputting 0 0
which is the syndrome that indicates which is the syndrome that indicates which is the syndrome that indicates
that no errors have been that no errors have been that no errors have been
detected the three Cubit poly operations detected the three Cubit poly operations detected the three Cubit poly operations
ZZ identity and identity ZZ are called ZZ identity and identity ZZ are called ZZ identity and identity ZZ are called
stabilizer generators for this code and stabilizer generators for this code and stabilizer generators for this code and
the stabilizer of the code is the set the stabilizer of the code is the set the stabilizer of the code is the set
generated by the stabilizer generators generated by the stabilizer generators generated by the stabilizer generators
this set the stabilizer is a this set the stabilizer is a this set the stabilizer is a
fundamentally important mathematical fundamentally important mathematical fundamentally important mathematical
object associated with this code and object associated with this code and object associated with this code and
we'll discuss further the role that it we'll discuss further the role that it we'll discuss further the role that it
plays as the lesson continues for now plays as the lesson continues for now plays as the lesson continues for now
I'll just point out that we could have I'll just point out that we could have I'll just point out that we could have
made a different choice for the made a different choice for the made a different choice for the
generators for instance we could have generators for instance we could have generators for instance we could have
selected Z identity Z in place of either selected Z identity Z in place of either selected Z identity Z in place of either
of the generators we did select and the of the generators we did select and the of the generators we did select and the
code and the stabilizer would be exactly code and the stabilizer would be exactly code and the stabilizer would be exactly
the the the
same next we'll consider bit flip same next we'll consider bit flip same next we'll consider bit flip
detection for the three-bit repetition detection for the three-bit repetition detection for the three-bit repetition
code with a focus on the interactions code with a focus on the interactions code with a focus on the interactions
and relationships among the poly and relationships among the poly and relationships among the poly
operations that are involved namely the operations that are involved namely the operations that are involved namely the
stabilizer generators and the errors stabilizer generators and the errors stabilizer generators and the errors
themselves suppose that we've encoded a themselves suppose that we've encoded a themselves suppose that we've encoded a
cubit using the three-bit repetition cubit using the three-bit repetition cubit using the three-bit repetition
code and a bit flip error occurs on the code and a bit flip error occurs on the code and a bit flip error occurs on the
leftmost leftmost leftmost
Cubit this causes our state side to be Cubit this causes our state side to be Cubit this causes our state side to be
transformed according to the action of transformed according to the action of transformed according to the action of
an X operation or X error on the an X operation or X error on the an X operation or X error on the
leftmost leftmost leftmost
Cubit as we already know we can detect Cubit as we already know we can detect Cubit as we already know we can detect
this error by performing the parody this error by performing the parody this error by performing the parody
checks for the three-bit repetition checks for the three-bit repetition checks for the three-bit repetition
code or equivalently we can measure the code or equivalently we can measure the code or equivalently we can measure the
encoding non-destructively treating the encoding non-destructively treating the encoding non-destructively treating the
stabilizer generators as stabilizer generators as stabilizer generators as
observables let's start with the first observables let's start with the first observables let's start with the first
stabilizer generator and see what stabilizer generator and see what stabilizer generator and see what
happens our state has been affected by happens our state has been affected by happens our state has been affected by
an X error and we'd like to understand an X error and we'd like to understand an X error and we'd like to understand
how the measurement of the stabilizer how the measurement of the stabilizer how the measurement of the stabilizer
generator as an observable is influenced generator as an observable is influenced generator as an observable is influenced
by this by this by this
error x and z anticommute whereas error x and z anticommute whereas error x and z anticommute whereas
everything commutes with the identity everything commutes with the identity everything commutes with the identity
Matrix and so it follows that ZZ Matrix and so it follows that ZZ Matrix and so it follows that ZZ
identity anti-c commutes with X identity identity anti-c commutes with X identity identity anti-c commutes with X identity
identity so we get a minus sign when the identity so we get a minus sign when the identity so we get a minus sign when the
order is order is order is
changed and we also know that ZZ changed and we also know that ZZ changed and we also know that ZZ
identity doesn't do anything to S identity doesn't do anything to S identity doesn't do anything to S
because s is a valid encoding of a cubit because s is a valid encoding of a cubit because s is a valid encoding of a cubit
and valid encodings are always plus one and valid encodings are always plus one and valid encodings are always plus one
igen vectors of both stabilizer igen vectors of both stabilizer igen vectors of both stabilizer
generators that leaves us with negative generators that leaves us with negative generators that leaves us with negative
X identity identity applied to S and X identity identity applied to S and X identity identity applied to S and
therefore X identity identity applied to therefore X identity identity applied to therefore X identity identity applied to
S is an igen Vector of ZZ identity S is an igen Vector of ZZ identity S is an igen Vector of ZZ identity
with igen value negative with igen value negative with igen value negative
one we can also do something similar one we can also do something similar one we can also do something similar
with the second stabilizer generator but with the second stabilizer generator but with the second stabilizer generator but
this time the error commutes with the this time the error commutes with the this time the error commutes with the
stabilizer generator and so our stabilizer generator and so our stabilizer generator and so our
Corrupted State is again an igon Vector Corrupted State is again an igon Vector Corrupted State is again an igon Vector
of the stabilizer generator but this of the stabilizer generator but this of the stabilizer generator but this
time the igen value is positive time the igen value is positive time the igen value is positive
one so our Corrupted State is an IG one so our Corrupted State is an IG one so our Corrupted State is an IG
Vector of both stabilizer generators and Vector of both stabilizer generators and Vector of both stabilizer generators and
whether the igen value is plus one or whether the igen value is plus one or whether the igen value is plus one or
minus one is determined by whether the minus one is determined by whether the minus one is determined by whether the
error commutes or anti-c commutes with error commutes or anti-c commutes with error commutes or anti-c commutes with
each stabilizer generator for the first each stabilizer generator for the first each stabilizer generator for the first
one the error and the stabilizer one the error and the stabilizer one the error and the stabilizer
generator anticommute and for the second generator anticommute and for the second generator anticommute and for the second
one they commute and it's always going one they commute and it's always going one they commute and it's always going
to be one or the other because any two to be one or the other because any two to be one or the other because any two
poly operations either commute or poly operations either commute or poly operations either commute or
anticommute meanwhile the actual State anticommute meanwhile the actual State anticommute meanwhile the actual State
side doesn't really play an important side doesn't really play an important side doesn't really play an important
role here except for the fact that the role here except for the fact that the role here except for the fact that the
stabilizer generators act trivially on stabilizer generators act trivially on stabilizer generators act trivially on
this this this
state so when we're thinking about the state so when we're thinking about the state so when we're thinking about the
detection of Errors we really don't need detection of Errors we really don't need detection of Errors we really don't need
to concern ourselves with the specific to concern ourselves with the specific to concern ourselves with the specific
encoded State all that matters is encoded State all that matters is encoded State all that matters is
whether the error commutes or anti-c whether the error commutes or anti-c whether the error commutes or anti-c
commutes with each stabilizer commutes with each stabilizer commutes with each stabilizer
generator so let's make a table that has generator so let's make a table that has generator so let's make a table that has
one row for each stabilizer generator one row for each stabilizer generator one row for each stabilizer generator
and one column for each and one column for each and one column for each
error here I've only included the errors error here I've only included the errors error here I've only included the errors
corresponding to a single bit flip as corresponding to a single bit flip as corresponding to a single bit flip as
well as no error at all which is well as no error at all which is well as no error at all which is
indicated by the identity tensed with indicated by the identity tensed with indicated by the identity tensed with
itself three times we could add more itself three times we could add more itself three times we could add more
columns for other errors if we wanted columns for other errors if we wanted columns for other errors if we wanted
but for now we'll focus on just these but for now we'll focus on just these but for now we'll focus on just these
errors the entry in the table is either errors the entry in the table is either errors the entry in the table is either
plus one or minus one depending on plus one or minus one depending on plus one or minus one depending on
whether the error and the stabilizer whether the error and the stabilizer whether the error and the stabilizer
generator commute or anticommute so for generator commute or anticommute so for generator commute or anticommute so for
each error in the table we can look at each error in the table we can look at each error in the table we can look at
the corresponding column to see how that the corresponding column to see how that the corresponding column to see how that
error will transform any given encoding error will transform any given encoding error will transform any given encoding
into a plus one or a minus one igen into a plus one or a minus one igen into a plus one or a minus one igen
vector of each stabilizer generator vector of each stabilizer generator vector of each stabilizer generator
these columns correspond precisely to these columns correspond precisely to these columns correspond precisely to
the syndrome we would obtain from the the syndrome we would obtain from the the syndrome we would obtain from the
parity checks which are equivalent to parity checks which are equivalent to parity checks which are equivalent to
the non-destructive measurements of the the non-destructive measurements of the the non-destructive measurements of the
stabilizer generators as stabilizer generators as stabilizer generators as
observables of course the entes in this observables of course the entes in this observables of course the entes in this
table are plus ones and minus ones table are plus ones and minus ones table are plus ones and minus ones
rather than zeros and ones respectively rather than zeros and ones respectively rather than zeros and ones respectively
and if you prefer you can think about and if you prefer you can think about and if you prefer you can think about
the syndromes as being the corresponding the syndromes as being the corresponding the syndromes as being the corresponding
binary strings rather than Columns of binary strings rather than Columns of binary strings rather than Columns of
plus ones and minus ones but there's a plus ones and minus ones but there's a plus ones and minus ones but there's a
direct correspondence and just to make direct correspondence and just to make direct correspondence and just to make
the connection between the syndromes and the connection between the syndromes and the connection between the syndromes and
the igen values more clear and direct the igen values more clear and direct the igen values more clear and direct
I'm going to refer to these vectors of I'm going to refer to these vectors of I'm going to refer to these vectors of
plus ones and minus ones as plus ones and minus ones as plus ones and minus ones as
syndromes as before the syndromes tell syndromes as before the syndromes tell syndromes as before the syndromes tell
us something about whatever error took us something about whatever error took us something about whatever error took
place and in particular if we know that place and in particular if we know that place and in particular if we know that
one of these four possible errors one of these four possible errors one of these four possible errors
occurred the syndrome indicates to us occurred the syndrome indicates to us occurred the syndrome indicates to us
which one it was and we can correct it which one it was and we can correct it which one it was and we can correct it
by applying the error again if we by applying the error again if we by applying the error again if we
choose next I'll say just a little bit choose next I'll say just a little bit choose next I'll say just a little bit
more about the syndromes and in more about the syndromes and in more about the syndromes and in
particular we'll take a closer look at particular we'll take a closer look at particular we'll take a closer look at
their algebraic their algebraic their algebraic
aspects our encodings are three Cubit aspects our encodings are three Cubit aspects our encodings are three Cubit
States so they're unit vectors in an States so they're unit vectors in an States so they're unit vectors in an
eight dimensional complex Vector space eight dimensional complex Vector space eight dimensional complex Vector space
and what the four possible syndromes do and what the four possible syndromes do and what the four possible syndromes do
is to slice this eight dimensional space is to slice this eight dimensional space is to slice this eight dimensional space
up into four four two-dimensional up into four four two-dimensional up into four four two-dimensional
subspaces meaning that the vectors in subspaces meaning that the vectors in subspaces meaning that the vectors in
any one of these subspaces will always any one of these subspaces will always any one of these subspaces will always
give us the same give us the same give us the same
syndrome in more detail this diagram syndrome in more detail this diagram syndrome in more detail this diagram
illustrates how the eight-dimensional illustrates how the eight-dimensional illustrates how the eight-dimensional
space is divided space is divided space is divided
up each stabilizer generator splits the up each stabilizer generator splits the up each stabilizer generator splits the
space into two subspaces of equal space into two subspaces of equal space into two subspaces of equal
Dimension namely the space of plus one Dimension namely the space of plus one Dimension namely the space of plus one
IG vectors and the space of minus one I IG vectors and the space of minus one I IG vectors and the space of minus one I
vectors for example the plus one vectors vectors for example the plus one vectors vectors for example the plus one vectors
of ZZ identity are linear combinations of ZZ identity are linear combinations of ZZ identity are linear combinations
of standard basis States for which the of standard basis States for which the of standard basis States for which the
leftmost two bits have even parity and leftmost two bits have even parity and leftmost two bits have even parity and
the minus one igen vectors are linear the minus one igen vectors are linear the minus one igen vectors are linear
combinations of standard basis States combinations of standard basis States combinations of standard basis States
for which the leftmost two bits have odd for which the leftmost two bits have odd for which the leftmost two bits have odd
parity and the situation is similar for parity and the situation is similar for parity and the situation is similar for
the other stabilizer generator except the other stabilizer generator except the other stabilizer generator except
that it's the rightmost two bits rather that it's the rightmost two bits rather that it's the rightmost two bits rather
than the leftmost two than the leftmost two than the leftmost two
bits so these four subspaces are easy to bits so these four subspaces are easy to bits so these four subspaces are easy to
describe in this case owing to the fact describe in this case owing to the fact describe in this case owing to the fact
that this is a very simple that this is a very simple that this is a very simple
code in particular the Subspace code in particular the Subspace code in particular the Subspace
corresponding to the syndrome plus one corresponding to the syndrome plus one corresponding to the syndrome plus one
+1 is is the space spend by C 000000 and +1 is is the space spend by C 000000 and +1 is is the space spend by C 000000 and
K 111 which is the space of valid K 111 which is the space of valid K 111 which is the space of valid
encodings and that's called the code encodings and that's called the code encodings and that's called the code
space and in general the spaces are space and in general the spaces are space and in general the spaces are
spanned by the standard Bas of states spanned by the standard Bas of states spanned by the standard Bas of states
that are shown in the corresponding that are shown in the corresponding that are shown in the corresponding
squares although it's not necessarily as squares although it's not necessarily as squares although it's not necessarily as
clear or as easy to see the syndromes clear or as easy to see the syndromes clear or as easy to see the syndromes
also partition all of the three Cubit also partition all of the three Cubit also partition all of the three Cubit
poly operations which represent the poly operations which represent the poly operations which represent the
possible poly errors that could take possible poly errors that could take possible poly errors that could take
place into four equal siiz collections place into four equal siiz collections place into four equal siiz collections
depending upon which syndrome that error depending upon which syndrome that error depending upon which syndrome that error
would would would
cause for example any poly operation cause for example any poly operation cause for example any poly operation
that commutes with both stabilizer that commutes with both stabilizer that commutes with both stabilizer
generators results in the syndrome plus1 generators results in the syndrome plus1 generators results in the syndrome plus1
plus1 and among the 64 possible three plus1 and among the 64 possible three plus1 and among the 64 possible three
Cubit poly operations there are exactly Cubit poly operations there are exactly Cubit poly operations there are exactly
16 of them in this category including 16 of them in this category including 16 of them in this category including
the three that are mentioned on the the three that are mentioned on the the three that are mentioned on the
screen and likewise for the other three screen and likewise for the other three screen and likewise for the other three
syndromes that's not obvious but it is syndromes that's not obvious but it is syndromes that's not obvious but it is
true and this basic property that the true and this basic property that the true and this basic property that the
syndromes partition both the state space syndromes partition both the state space syndromes partition both the state space
in which the encodings live and all of in which the encodings live and all of in which the encodings live and all of
the possible polyer in the space into the possible polyer in the space into the possible polyer in the space into
equal siiz pieces is something that's equal siiz pieces is something that's equal siiz pieces is something that's
true in general for stabilizer codes true in general for stabilizer codes true in general for stabilizer codes
which will soon Define precisely which will soon Define precisely which will soon Define precisely
although it's mostly just an aside at although it's mostly just an aside at although it's mostly just an aside at
this point it's worth mentioning that this point it's worth mentioning that this point it's worth mentioning that
poly operations that commute with both poly operations that commute with both poly operations that commute with both
stabilizer generators or equivalently stabilizer generators or equivalently stabilizer generators or equivalently
poly operations that result in the poly operations that result in the poly operations that result in the
syndrome plus1 +1 but are not themselves syndrome plus1 +1 but are not themselves syndrome plus1 +1 but are not themselves
proportional to elements of the proportional to elements of the proportional to elements of the
stabilizer turn out to behave just like stabilizer turn out to behave just like stabilizer turn out to behave just like
single Cubit poly operations on The single Cubit poly operations on The single Cubit poly operations on The
Logical Cubit that we Logical Cubit that we Logical Cubit that we
encoded for example xxx commutes with encoded for example xxx commutes with encoded for example xxx commutes with
both stabilizer generators but it's not both stabilizer generators but it's not both stabilizer generators but it's not
proportional to any element of the proportional to any element of the proportional to any element of the
stabilizer and indeed it's clear if you stabilizer and indeed it's clear if you stabilizer and indeed it's clear if you
think about this code for a moment that think about this code for a moment that think about this code for a moment that
the effect that this operation has on an the effect that this operation has on an the effect that this operation has on an
encoding is equivalent to an X gate on encoding is equivalent to an X gate on encoding is equivalent to an X gate on
the logical Cubit that was encoded and the logical Cubit that was encoded and the logical Cubit that was encoded and
again this is something that generalizes again this is something that generalizes again this is something that generalizes
to all stabilizer to all stabilizer to all stabilizer
codes now it's time to Define stabilizer codes now it's time to Define stabilizer codes now it's time to Define stabilizer
codes in general and the way that it codes in general and the way that it codes in general and the way that it
works is that we do something completely works is that we do something completely works is that we do something completely
analogous to what we just discussed for analogous to what we just discussed for analogous to what we just discussed for
the three-bit repetition code the three-bit repetition code the three-bit repetition code
except that we pick different stabilizer except that we pick different stabilizer except that we pick different stabilizer
generators which all refer to as P1 generators which all refer to as P1 generators which all refer to as P1
through PR for the sake of this through PR for the sake of this through PR for the sake of this
explanation these stabilizer generators explanation these stabilizer generators explanation these stabilizer generators
must all be n Cubit poly operations must all be n Cubit poly operations must all be n Cubit poly operations
where n is the number of cubits used for where n is the number of cubits used for where n is the number of cubits used for
the encoding and we can't just pick any the encoding and we can't just pick any the encoding and we can't just pick any
old set of stabilizer generators they old set of stabilizer generators they old set of stabilizer generators they
have to satisfy a few basic have to satisfy a few basic have to satisfy a few basic
conditions first the stabilizer conditions first the stabilizer conditions first the stabilizer
generators Must ALL commute with one generators Must ALL commute with one generators Must ALL commute with one
another in the case of the three-bit another in the case of the three-bit another in the case of the three-bit
repetition code we didn't bother to take repetition code we didn't bother to take repetition code we didn't bother to take
note of this but it is straightforward note of this but it is straightforward note of this but it is straightforward
to observe that ZZ identity and identity to observe that ZZ identity and identity to observe that ZZ identity and identity
ZZ do in fact commute because Z commutes ZZ do in fact commute because Z commutes ZZ do in fact commute because Z commutes
with itself and everything commutes with with itself and everything commutes with with itself and everything commutes with
the the the
identity for stabilizer codes with more identity for stabilizer codes with more identity for stabilizer codes with more
complicated stabilizer generators on the complicated stabilizer generators on the complicated stabilizer generators on the
other hand including codes where we have other hand including codes where we have other hand including codes where we have
both x's and Z's appearing in the both x's and Z's appearing in the both x's and Z's appearing in the
stabilizer generators this is an stabilizer generators this is an stabilizer generators this is an
important condition that needs to be important condition that needs to be important condition that needs to be
checked one way to think about the need checked one way to think about the need checked one way to think about the need
for this condition is that we're going for this condition is that we're going for this condition is that we're going
to be treating the stabilizer generators to be treating the stabilizer generators to be treating the stabilizer generators
as obs observables and by requiring that as obs observables and by requiring that as obs observables and by requiring that
they all commute we're ensuring that the they all commute we're ensuring that the they all commute we're ensuring that the
order in which we perform these order in which we perform these order in which we perform these
measurements doesn't measurements doesn't measurements doesn't
matter second the stabilizer generators matter second the stabilizer generators matter second the stabilizer generators
must form a so-called minimal generating must form a so-called minimal generating must form a so-called minimal generating
set which means that if we removed any set which means that if we removed any set which means that if we removed any
one of them we'd get a smaller one of them we'd get a smaller one of them we'd get a smaller
stabilizer and one way to express this stabilizer and one way to express this stabilizer and one way to express this
mathematically is shown here on the mathematically is shown here on the mathematically is shown here on the
screen strictly speaking this condition screen strictly speaking this condition screen strictly speaking this condition
isn't really essential to the way that a isn't really essential to the way that a isn't really essential to the way that a
stabilizer code Works in an operational stabilizer code Works in an operational stabilizer code Works in an operational
sense and sometimes it does make sense sense and sometimes it does make sense sense and sometimes it does make sense
to think about non-minimal generating to think about non-minimal generating to think about non-minimal generating
sets as we'll see in the next lesson but sets as we'll see in the next lesson but sets as we'll see in the next lesson but
for the sake of analyzing stabilizer for the sake of analyzing stabilizer for the sake of analyzing stabilizer
codes and explaining their properties codes and explaining their properties codes and explaining their properties
we're going to assume that this we're going to assume that this we're going to assume that this
condition is in condition is in condition is in
place basically this condition place basically this condition place basically this condition
guarantees that each observable that we guarantees that each observable that we guarantees that each observable that we
measure to obtain the syndrome is adding measure to obtain the syndrome is adding measure to obtain the syndrome is adding
information about possible errors as information about possible errors as information about possible errors as
opposed to being redundant and giving opposed to being redundant and giving opposed to being redundant and giving
results that could easily be inferred results that could easily be inferred results that could easily be inferred
from the other stabilizer generators as from the other stabilizer generators as from the other stabilizer generators as
observables observables observables
and finally it must be the case that at and finally it must be the case that at and finally it must be the case that at
least one nonzero Vector is fixed by all least one nonzero Vector is fixed by all least one nonzero Vector is fixed by all
of the stabilizer generators and that of the stabilizer generators and that of the stabilizer generators and that
turns out to be equivalent to demanding turns out to be equivalent to demanding turns out to be equivalent to demanding
that -1 time the identity on N cubits is that -1 time the identity on N cubits is that -1 time the identity on N cubits is
not contained in the stabilizer that's not contained in the stabilizer that's not contained in the stabilizer that's
not obvious but the point is that it is not obvious but the point is that it is not obvious but the point is that it is
possible to choose a minimal generating possible to choose a minimal generating possible to choose a minimal generating
set of n Cubit poly operations that all set of n Cubit poly operations that all set of n Cubit poly operations that all
commute with one another and yet no commute with one another and yet no commute with one another and yet no
Vector is simultaneously a plus one igen Vector is simultaneously a plus one igen Vector is simultaneously a plus one igen
vector of all of them and we don't want vector of all of them and we don't want vector of all of them and we don't want
to concern ourselves with this to concern ourselves with this to concern ourselves with this
possibility because codes where there possibility because codes where there possibility because codes where there
aren't any valid encodings aren't aren't any valid encodings aren't aren't any valid encodings aren't
interesting so those are the interesting so those are the interesting so those are the
requirements on the stabilizer requirements on the stabilizer requirements on the stabilizer
generators for a stabilizer code and if generators for a stabilizer code and if generators for a stabilizer code and if
we do have a set of generators that we do have a set of generators that we do have a set of generators that
satisfy these conditions then we have a satisfy these conditions then we have a satisfy these conditions then we have a
valid stabilizer code and we can valid stabilizer code and we can valid stabilizer code and we can
consider its properties there are many consider its properties there are many consider its properties there are many
different stabilizer codes with various different stabilizer codes with various different stabilizer codes with various
properties that make them interesting or properties that make them interesting or properties that make them interesting or
not but they all satisfy certain basic not but they all satisfy certain basic not but they all satisfy certain basic
properties that are reminiscent of the properties that are reminiscent of the properties that are reminiscent of the
ones that we've already observed for the ones that we've already observed for the ones that we've already observed for the
three-bit repetition code three-bit repetition code three-bit repetition code
we'll take a look at some examples we'll take a look at some examples we'll take a look at some examples
shortly but before that let me mention shortly but before that let me mention shortly but before that let me mention
that when we refer to the code space of that when we refer to the code space of that when we refer to the code space of
a stabilizer code we mean the Subspace a stabilizer code we mean the Subspace a stabilizer code we mean the Subspace
of the N Cubit State space that includes of the N Cubit State space that includes of the N Cubit State space that includes
every Vector that's simultaneously a every Vector that's simultaneously a every Vector that's simultaneously a
plus one igen vector of every one of the plus one igen vector of every one of the plus one igen vector of every one of the
stabilizer stabilizer stabilizer
generators this is the Subspace of the generators this is the Subspace of the generators this is the Subspace of the
en Cubit State space in which valid en Cubit State space in which valid en Cubit State space in which valid
encodings live and we'll have more to encodings live and we'll have more to encodings live and we'll have more to
say about this Subspace as the lesson say about this Subspace as the lesson say about this Subspace as the lesson
continues now let's take a look at a few continues now let's take a look at a few continues now let's take a look at a few
examples of stabiliz examples of stabiliz examples of stabiliz
codes here's the example we've already codes here's the example we've already codes here's the example we've already
discussed the ordinary 3-bit repetition discussed the ordinary 3-bit repetition discussed the ordinary 3-bit repetition
code and we can also Express the code and we can also Express the code and we can also Express the
modified version of that code from the modified version of that code from the modified version of that code from the
previous lesson which can detect a phase previous lesson which can detect a phase previous lesson which can detect a phase
flip instead of a bit flip as a flip instead of a bit flip as a flip instead of a bit flip as a
stabilizer code AS is shown stabilizer code AS is shown stabilizer code AS is shown
here this time the stabilizer generators here this time the stabilizer generators here this time the stabilizer generators
measure XX observables rather than ZZ measure XX observables rather than ZZ measure XX observables rather than ZZ
observables so these are basically observables so these are basically observables so these are basically
parity checks in the plus minus basis parity checks in the plus minus basis parity checks in the plus minus basis
rather than the standard rather than the standard rather than the standard
basis in both cases we can can basis in both cases we can can basis in both cases we can can
immediately see that we have a valid immediately see that we have a valid immediately see that we have a valid
stabilizer code because the stabilizer stabilizer code because the stabilizer stabilizer code because the stabilizer
generators commute these are minimal generators commute these are minimal generators commute these are minimal
generating sets and the code space is generating sets and the code space is generating sets and the code space is
non-trivial as we already know but we non-trivial as we already know but we non-trivial as we already know but we
can also see pretty clearly that we can also see pretty clearly that we can also see pretty clearly that we
can't generate negative -1 time the can't generate negative -1 time the can't generate negative -1 time the
identity and here's the 9 Cubit Shore identity and here's the 9 Cubit Shore identity and here's the 9 Cubit Shore
code which is also a stabilizer code code which is also a stabilizer code code which is also a stabilizer code
expressed by stabilizer expressed by stabilizer expressed by stabilizer
generators in this case we basically generators in this case we basically generators in this case we basically
have three copies of the three- bit have three copies of the three- bit have three copies of the three- bit
repetition code one for each of the repetition code one for each of the repetition code one for each of the
three blocks of three cubits as well as three blocks of three cubits as well as three blocks of three cubits as well as
the last two stabilizer generators which the last two stabilizer generators which the last two stabilizer generators which
take a form that's reminiscent of the take a form that's reminiscent of the take a form that's reminiscent of the
circuit that we saw in the previous circuit that we saw in the previous circuit that we saw in the previous
lesson for detecting phase flips for lesson for detecting phase flips for lesson for detecting phase flips for
this this this
code another way of thinking about these code another way of thinking about these code another way of thinking about these
last two stabilizer generators is that last two stabilizer generators is that last two stabilizer generators is that
they take exactly the same form as we they take exactly the same form as we they take exactly the same form as we
have for the three-bit repetition code have for the three-bit repetition code have for the three-bit repetition code
for phase flips except that we for phase flips except that we for phase flips except that we
substitute XXX for x and the reason for substitute XXX for x and the reason for substitute XXX for x and the reason for
that is that XXX corresponds to an X that is that XXX corresponds to an X that is that XXX corresponds to an X
operation on The Logical Cubit encoded operation on The Logical Cubit encoded operation on The Logical Cubit encoded
using the 3-bit repetition code but using the 3-bit repetition code but using the 3-bit repetition code but
that's an aside and it's not something that's an aside and it's not something that's an aside and it's not something
to worry about right now in case it to worry about right now in case it to worry about right now in case it
isn't isn't isn't
clear next we'll take a look at a few clear next we'll take a look at a few clear next we'll take a look at a few
more examples but before that let me more examples but before that let me more examples but before that let me
mention that people often omit the mention that people often omit the mention that people often omit the
tensor product symbols when describing tensor product symbols when describing tensor product symbols when describing
stabilizer codes because it tends to stabilizer codes because it tends to stabilizer codes because it tends to
make them easier to make them easier to make them easier to
read so here's what these codes look read so here's what these codes look read so here's what these codes look
like without the tensor product symbols like without the tensor product symbols like without the tensor product symbols
but of course these are tensor products but of course these are tensor products but of course these are tensor products
and not ordinary Matrix products for and not ordinary Matrix products for and not ordinary Matrix products for
instance we're just leading out the instance we're just leading out the instance we're just leading out the
tensor product symbols so that it's less tensor product symbols so that it's less tensor product symbols so that it's less
cluttered and the patterns are more clear here's another example it's called clear here's another example it's called
the 7 Cubit steam code and it has some the 7 Cubit steam code and it has some the 7 Cubit steam code and it has some
pretty remarkable pretty remarkable pretty remarkable
features we'll come back to this one features we'll come back to this one features we'll come back to this one
from time to time throughout the from time to time throughout the from time to time throughout the
remaining lessons of the unit but for remaining lessons of the unit but for remaining lessons of the unit but for
now we can simply verify that it is a now we can simply verify that it is a now we can simply verify that it is a
valid stabilizer valid stabilizer valid stabilizer
code in particular the first three code in particular the first three code in particular the first three
stabilizer generators clearly commute stabilizer generators clearly commute stabilizer generators clearly commute
with one another because Z commutes with with one another because Z commutes with with one another because Z commutes with
itself and the identity commutes with itself and the identity commutes with itself and the identity commutes with
everything and the situation is similar everything and the situation is similar everything and the situation is similar
for the last three so it remains to for the last three so it remains to for the last three so it remains to
check that if we take one of the Z check that if we take one of the Z check that if we take one of the Z
stabilizer generators and one of the X stabilizer generators and one of the X stabilizer generators and one of the X
stabilizer generators then they commute stabilizer generators then they commute stabilizer generators then they commute
and it's easy enough to go through the and it's easy enough to go through the and it's easy enough to go through the
nine possible pairings to check nine possible pairings to check nine possible pairings to check
that in all of these cases we'll always that in all of these cases we'll always that in all of these cases we'll always
have an X and A Z matching up in the have an X and A Z matching up in the have an X and A Z matching up in the
same position an even number of times so same position an even number of times so same position an even number of times so
the two generators will commute just the two generators will commute just the two generators will commute just
like xx and ZZ like xx and ZZ like xx and ZZ
commute it's a minimal generating set commute it's a minimal generating set commute it's a minimal generating set
and it has a non-trivial code space but and it has a non-trivial code space but and it has a non-trivial code space but
I won't try to argue that in this video I won't try to argue that in this video I won't try to argue that in this video
and instead I'll leave that for you to and instead I'll leave that for you to and instead I'll leave that for you to
think about if you think about if you think about if you
wish the 7 Cubit steam code is similar wish the 7 Cubit steam code is similar wish the 7 Cubit steam code is similar
to the 9 Cubit Shore code in the sense to the 9 Cubit Shore code in the sense to the 9 Cubit Shore code in the sense
that it encodes a single Cubit and that it encodes a single Cubit and that it encodes a single Cubit and
allows for the correction of an allows for the correction of an allows for the correction of an
arbitrary error on one cubit but it only arbitrary error on one cubit but it only arbitrary error on one cubit but it only
requires seven cubits rather than nine requires seven cubits rather than nine requires seven cubits rather than nine
that's not the fewest number of cubits that's not the fewest number of cubits that's not the fewest number of cubits
required for an error correcting code to required for an error correcting code to required for an error correcting code to
encode a cubit and correct for an encode a cubit and correct for an encode a cubit and correct for an
arbitrary error on on a single arbitrary error on on a single arbitrary error on on a single
Cubit here's a stabilizer code that's Cubit here's a stabilizer code that's Cubit here's a stabilizer code that's
usually just called the five Cubit code usually just called the five Cubit code usually just called the five Cubit code
that also encodes a single Cubit and can that also encodes a single Cubit and can that also encodes a single Cubit and can
correct an arbitrary single Cubit error correct an arbitrary single Cubit error correct an arbitrary single Cubit error
and five is the smallest number of and five is the smallest number of and five is the smallest number of
cubits for which this is cubits for which this is cubits for which this is
possible here's another example of a possible here's another example of a possible here's another example of a
stabilizer code although it isn't really stabilizer code although it isn't really stabilizer code although it isn't really
a code so to speak in the sense that the a code so to speak in the sense that the a code so to speak in the sense that the
code space is code space is code space is
one-dimensional in particular the code one-dimensional in particular the code one-dimensional in particular the code
space for this code is the space spend space for this code is the space spend space for this code is the space spend
by an ebit or in other words a five plus by an ebit or in other words a five plus by an ebit or in other words a five plus
spell spell spell
state so you can't actually encode any state so you can't actually encode any state so you can't actually encode any
cubits using this code but it's still a cubits using this code but it's still a cubits using this code but it's still a
valid stabilizer code where the code valid stabilizer code where the code valid stabilizer code where the code
space happens to be space happens to be space happens to be
one-dimensional and it's possible to do one-dimensional and it's possible to do one-dimensional and it's possible to do
something similar for a GHz something similar for a GHz something similar for a GHz
state so those are some examples of state so those are some examples of state so those are some examples of
stabilizer codes and we'll see stabilizer codes and we'll see stabilizer codes and we'll see
infinitely many more examples in the infinitely many more examples in the infinitely many more examples in the
next next next
lesson suppose that we're given a lesson suppose that we're given a lesson suppose that we're given a
stabilizer stabilizer stabilizer
code a natural question to ask about it code a natural question to ask about it code a natural question to ask about it
perhaps the very first question that perhaps the very first question that perhaps the very first question that
comes to mind comes to mind comes to mind
is how many cubits does it is how many cubits does it is how many cubits does it
encode that question has a very simple encode that question has a very simple encode that question has a very simple
answer and to explain it let's be answer and to explain it let's be answer and to explain it let's be
precise about the stabilizer code being precise about the stabilizer code being precise about the stabilizer code being
considered we have R and Cubit considered we have R and Cubit considered we have R and Cubit
stabilizer generators P1 through p r stabilizer generators P1 through p r stabilizer generators P1 through p r
that satisfy the three requirements that that satisfy the three requirements that that satisfy the three requirements that
I mentioned earlier namely that the I mentioned earlier namely that the I mentioned earlier namely that the
stabilizer generators all commute with stabilizer generators all commute with stabilizer generators all commute with
one another this is a minimal generating one another this is a minimal generating one another this is a minimal generating
set so we wouldn't get the same set so we wouldn't get the same set so we wouldn't get the same
stabilizer if we removed one of the stabilizer if we removed one of the stabilizer if we removed one of the
generators and there's at at least one generators and there's at at least one generators and there's at at least one
nonzero Vector in the code space as the theorem shown on the screen space as the theorem shown on the screen
indicates it must then be that the code indicates it must then be that the code indicates it must then be that the code
space of the stabilizer code has space of the stabilizer code has space of the stabilizer code has
Dimension 2 ^ nus r or another way of Dimension 2 ^ nus r or another way of Dimension 2 ^ nus r or another way of
saying that is that we can encode n saying that is that we can encode n saying that is that we can encode n
minus r cubits using this minus r cubits using this minus r cubits using this
code So intuitively speaking we have n code So intuitively speaking we have n code So intuitively speaking we have n
cubits in our encodings and each cubits in our encodings and each cubits in our encodings and each
stabilizer generator effectively takes a stabilizer generator effectively takes a stabilizer generator effectively takes a
cubit away in terms of how many cubits cubit away in terms of how many cubits cubit away in terms of how many cubits
we can encode we can encode we can encode
this theorem isn't too hard to prove and this theorem isn't too hard to prove and this theorem isn't too hard to prove and
I'll show you a proof in just a moment I'll show you a proof in just a moment I'll show you a proof in just a moment
but first we'll take a look at just a but first we'll take a look at just a but first we'll take a look at just a
few examples starting with the ordinary few examples starting with the ordinary few examples starting with the ordinary
3-bit repetition 3-bit repetition 3-bit repetition
code we have three cubits in our code we have three cubits in our code we have three cubits in our
encodings so n is equal to three and encodings so n is equal to three and encodings so n is equal to three and
there are R equals 2 stabilizer there are R equals 2 stabilizer there are R equals 2 stabilizer
generators and so we can encode one generators and so we can encode one generators and so we can encode one
cubit using this cubit using this cubit using this
code notice that this is not about which code notice that this is not about which code notice that this is not about which
or how many errors can be detected or or how many errors can be detected or or how many errors can be detected or
corrected that's a different matter this corrected that's a different matter this corrected that's a different matter this
is just a statement about the dimension is just a statement about the dimension is just a statement about the dimension
of the codee of the codee of the codee
space for another example consider the 5 space for another example consider the 5 space for another example consider the 5
Cubit Cubit Cubit
code here we have five cubits and four code here we have five cubits and four code here we have five cubits and four
stabilizer generators so once again the stabilizer generators so once again the stabilizer generators so once again the
code space has Dimension two which means code space has Dimension two which means code space has Dimension two which means
that we can encode one cubit using this that we can encode one cubit using this that we can encode one cubit using this
code and one more example for now for code and one more example for now for code and one more example for now for
the ebit stabilizer code we have two the ebit stabilizer code we have two the ebit stabilizer code we have two
cubits and two stabilizer generators so cubits and two stabilizer generators so cubits and two stabilizer generators so
the code space is one-dimensional and as the code space is one-dimensional and as the code space is one-dimensional and as
I mentioned mentioned earlier it's the I mentioned mentioned earlier it's the I mentioned mentioned earlier it's the
space span by the five plus Bell State space span by the five plus Bell State space span by the five plus Bell State
we'll see more examples as we continue we'll see more examples as we continue we'll see more examples as we continue
on including codes where we can encode on including codes where we can encode on including codes where we can encode
more than one cubit but for now let's more than one cubit but for now let's more than one cubit but for now let's
see how we can prove the see how we can prove the see how we can prove the
theorem the first thing that we need to theorem the first thing that we need to theorem the first thing that we need to
observe is that because the stabilizer observe is that because the stabilizer observe is that because the stabilizer
generators commute and form a minimal generators commute and form a minimal generators commute and form a minimal
generating set and because every poly generating set and because every poly generating set and because every poly
operation is its own inverse every operation is its own inverse every operation is its own inverse every
element in the stabilizer can be element in the stabilizer can be element in the stabilizer can be
expressed in a unique way as a product expressed in a unique way as a product expressed in a unique way as a product
where we raise each stabilizer generator where we raise each stabilizer generator where we raise each stabilizer generator
PK to the power AK where each AK is PK to the power AK where each AK is PK to the power AK where each AK is
either Z or one and then we multiply the either Z or one and then we multiply the either Z or one and then we multiply the
results results results
together another way to say this is that together another way to say this is that together another way to say this is that
we get every element of the stabilizer we get every element of the stabilizer we get every element of the stabilizer
by multiplying together some subset of by multiplying together some subset of by multiplying together some subset of
the stabilizer generators and we never the stabilizer generators and we never the stabilizer generators and we never
get the same stabilizer element from get the same stabilizer element from get the same stabilizer element from
different subsets and one thing that different subsets and one thing that different subsets and one thing that
this implies is that the only way to this implies is that the only way to this implies is that the only way to
express the N Cubit identity operation express the N Cubit identity operation express the N Cubit identity operation
in this way is to take all of the powers in this way is to take all of the powers in this way is to take all of the powers
to be to be to be
zero next let's Define pi K to be the zero next let's Define pi K to be the zero next let's Define pi K to be the
projection onto the plus one igen space projection onto the plus one igen space projection onto the plus one igen space
of PK for each K from 1 to R and there's of PK for each K from 1 to R and there's of PK for each K from 1 to R and there's
a simple formula for this projection we a simple formula for this projection we a simple formula for this projection we
can simply average PK with the identity can simply average PK with the identity can simply average PK with the identity
on N cubits which effectively leaves the on N cubits which effectively leaves the on N cubits which effectively leaves the
plus one IG values alone and shifts all plus one IG values alone and shifts all plus one IG values alone and shifts all
the minus one IG values to zero I the minus one IG values to zero I the minus one IG values to zero I
values now the code space is defined as values now the code space is defined as values now the code space is defined as
the set of all vectors that are the set of all vectors that are the set of all vectors that are
simultaneously plus one I vectors of all simultaneously plus one I vectors of all simultaneously plus one I vectors of all
of the stabilizer generators given that of the stabilizer generators given that of the stabilizer generators given that
the stabilizer generators commute the the stabilizer generators commute the the stabilizer generators commute the
projections must also commute and that projections must also commute and that projections must also commute and that
implies that the product of these implies that the product of these implies that the product of these
projections is also a projection and in projections is also a projection and in projections is also a projection and in
particular it's the projection onto the particular it's the projection onto the particular it's the projection onto the
intersection of the subspaces that the intersection of the subspaces that the intersection of the subspaces that the
individual projections project onto or individual projections project onto or individual projections project onto or
in other words it's the projection onto in other words it's the projection onto in other words it's the projection onto
the code the code the code
space we can expand out this product and space we can expand out this product and space we can expand out this product and
what we get is the average over every what we get is the average over every what we get is the average over every
element of the element of the element of the
stabilizer and finally because the stabilizer and finally because the stabilizer and finally because the
dimension of any Subspace is equal to dimension of any Subspace is equal to dimension of any Subspace is equal to
the trace of the projection onto that the trace of the projection onto that the trace of the projection onto that
Subspace we find that the dimension of Subspace we find that the dimension of Subspace we find that the dimension of
the code space is the average of the the code space is the average of the the code space is the average of the
traces over the elements of the traces over the elements of the traces over the elements of the
stabilizer but with the exception of the stabilizer but with the exception of the stabilizer but with the exception of the
ubit identity operation all of the ubit identity operation all of the ubit identity operation all of the
elements of the stabilizer must have elements of the stabilizer must have elements of the stabilizer must have
Trace equal to zero because the three Trace equal to zero because the three Trace equal to zero because the three
non-identity polym matrices all have non-identity polym matrices all have non-identity polym matrices all have
Trace equal to Trace equal to Trace equal to
zero the trace of the N Cubit identity zero the trace of the N Cubit identity zero the trace of the N Cubit identity
operation is 2 to the n and so we find operation is 2 to the n and so we find operation is 2 to the n and so we find
that the dimension of the code space is that the dimension of the code space is that the dimension of the code space is
2 the nus r which proves the 2 the nus r which proves the 2 the nus r which proves the
theorem next I'll say just a little bit theorem next I'll say just a little bit theorem next I'll say just a little bit
about how cubits can be encoded using about how cubits can be encoded using about how cubits can be encoded using
stabilizer codes but to do that we first stabilizer codes but to do that we first stabilizer codes but to do that we first
need to introduce Clifford need to introduce Clifford need to introduce Clifford
operations Clifford operations are operations Clifford operations are operations Clifford operations are
unitary operations on any number of unitary operations on any number of unitary operations on any number of
cubits that can be implemented by cubits that can be implemented by cubits that can be implemented by
Quantum circuits with a restricted set Quantum circuits with a restricted set Quantum circuits with a restricted set
of gates namely hadamard Gates S Gates of gates namely hadamard Gates S Gates of gates namely hadamard Gates S Gates
and controlled knot and controlled knot and controlled knot
Gates notice that t gates are not Gates notice that t gates are not Gates notice that t gates are not
included nor are tofly Gates or fredin included nor are tofly Gates or fredin included nor are tofly Gates or fredin
Gates and not only are those Gates not Gates and not only are those Gates not Gates and not only are those Gates not
included in this list but in fact it's included in this list but in fact it's included in this list but in fact it's
not possible to implement those Gates not possible to implement those Gates not possible to implement those Gates
using the ones listed here so they're using the ones listed here so they're using the ones listed here so they're
not Clifford not Clifford not Clifford
operations poly operations on the other operations poly operations on the other operations poly operations on the other
hand are Clifford operations and hand are Clifford operations and hand are Clifford operations and
specifically they can be implemented specifically they can be implemented specifically they can be implemented
using sequences of hadam Art and Es using sequences of hadam Art and Es using sequences of hadam Art and Es
Gates so that's a simple way to define Gates so that's a simple way to define Gates so that's a simple way to define
Clifford operations but it doesn't Clifford operations but it doesn't Clifford operations but it doesn't
really explain why they're defined in really explain why they're defined in really explain why they're defined in
this way or what's special about this this way or what's special about this this way or what's special about this
particular collection of particular collection of particular collection of
gates here's the real reason that gates here's the real reason that gates here's the real reason that
Clifford operations are defined like Clifford operations are defined like Clifford operations are defined like
this this this
up to a global phase the ncub Clifford up to a global phase the ncub Clifford up to a global phase the ncub Clifford
operations are precisely the unitary operations are precisely the unitary operations are precisely the unitary
operations that always transform poly operations that always transform poly operations that always transform poly
operations into poly operations by operations into poly operations by operations into poly operations by
conjugation to be more precise an nbit conjugation to be more precise an nbit conjugation to be more precise an nbit
unitary operation U is a Clifford unitary operation U is a Clifford unitary operation U is a Clifford
operation up to a global phase Factor if operation up to a global phase Factor if operation up to a global phase Factor if
and only if for every ncit poly and only if for every ncit poly and only if for every ncit poly
operation conjugating that poly operation conjugating that poly operation conjugating that poly
operation by U gives us plus or minus operation by U gives us plus or minus operation by U gives us plus or minus
one times some possibly different n one times some possibly different n one times some possibly different n
Cubit poly operation Cubit poly operation Cubit poly operation
ation you can easily check that this ation you can easily check that this ation you can easily check that this
property is true for hadamar Gates S property is true for hadamar Gates S property is true for hadamar Gates S
Gates and c not Gates so it necessarily Gates and c not Gates so it necessarily Gates and c not Gates so it necessarily
holds for circuits built from these holds for circuits built from these holds for circuits built from these
Gates what's harder to prove is that it Gates what's harder to prove is that it Gates what's harder to prove is that it
goes in the other direction which is goes in the other direction which is goes in the other direction which is
that if we're given a particular nbit that if we're given a particular nbit that if we're given a particular nbit
unitary operation U that satisfies this unitary operation U that satisfies this unitary operation U that satisfies this
property then it must be possible to property then it must be possible to property then it must be possible to
implement it up to a global phase using implement it up to a global phase using implement it up to a global phase using
just hadamard s and cot just hadamard s and cot just hadamard s and cot
Gates I won't go through the proof of Gates I won't go through the proof of Gates I won't go through the proof of
that implication but it is true and you that implication but it is true and you that implication but it is true and you
can look it up if you're can look it up if you're can look it up if you're
interested Clifford operations are not interested Clifford operations are not interested Clifford operations are not
Universal for Quantum computation Universal for Quantum computation Universal for Quantum computation
meaning that we can't run arbitrary meaning that we can't run arbitrary meaning that we can't run arbitrary
Quantum computations using just hadamard Quantum computations using just hadamard Quantum computations using just hadamard
s and cot gates in fact for a given s and cot gates in fact for a given s and cot gates in fact for a given
value of n there are only finitely many value of n there are only finitely many value of n there are only finitely many
n Cubit Clifford operations so it's a n Cubit Clifford operations so it's a n Cubit Clifford operations so it's a
discrete set of operations so unlike a discrete set of operations so unlike a discrete set of operations so unlike a
universal set of quantum Gates we can't universal set of quantum Gates we can't universal set of quantum Gates we can't
approximate arbitrary unitary operations approximate arbitrary unitary operations approximate arbitrary unitary operations
to any desired level of accuracy and in to any desired level of accuracy and in to any desired level of accuracy and in
fact performing Clifford operations on fact performing Clifford operations on fact performing Clifford operations on
standard basis States followed by standard basis States followed by standard basis States followed by
standard basis measurements can't allow standard basis measurements can't allow standard basis measurements can't allow
us to perform computations that are us to perform computations that are us to perform computations that are
outside of the reach of classical outside of the reach of classical outside of the reach of classical
algorithms because we can efficiently algorithms because we can efficiently algorithms because we can efficiently
simulate computations of this form simulate computations of this form simulate computations of this form
classically this fact is known as the classically this fact is known as the classically this fact is known as the
goisman Kil theorem and the standard goisman Kil theorem and the standard goisman Kil theorem and the standard
proof of that theorem is in some sense proof of that theorem is in some sense proof of that theorem is in some sense
based on the stabilizer formalism in based on the stabilizer formalism in based on the stabilizer formalism in
particular the way that it works is that particular the way that it works is that particular the way that it works is that
we can keep track of the way that we can keep track of the way that we can keep track of the way that
Clifford operation transform poly Clifford operation transform poly Clifford operation transform poly
operations by conjugation which can be operations by conjugation which can be operations by conjugation which can be
done efficiently and then infer from done efficiently and then infer from done efficiently and then infer from
that what the computations do to that what the computations do to that what the computations do to
cubits so what does this have to do with cubits so what does this have to do with cubits so what does this have to do with
stabilizer stabilizer stabilizer
codes well a stabilizer code provides us codes well a stabilizer code provides us codes well a stabilizer code provides us
with a code space of a certain Dimension with a code space of a certain Dimension with a code space of a certain Dimension
and we have the freedom to use that code and we have the freedom to use that code and we have the freedom to use that code
space however we space however we space however we
choose so nothing forces us to encode choose so nothing forces us to encode choose so nothing forces us to encode
cubits into that space in a specific way cubits into that space in a specific way cubits into that space in a specific way
but it will always be possible ref form but it will always be possible ref form but it will always be possible ref form
an encoding using a Clifford operation an encoding using a Clifford operation an encoding using a Clifford operation
as the as the as the
encoder that's nice because Clifford encoder that's nice because Clifford encoder that's nice because Clifford
operations are relatively simple operations are relatively simple operations are relatively simple
compared with arbitrary unitary compared with arbitrary unitary compared with arbitrary unitary
operations there are ways to optimize operations there are ways to optimize operations there are ways to optimize
their implementation which works along their implementation which works along their implementation which works along
similar lines to the proof of the gsman similar lines to the proof of the gsman similar lines to the proof of the gsman
canil theorem and as a result circuits canil theorem and as a result circuits canil theorem and as a result circuits
for implementing them never need to be for implementing them never need to be for implementing them never need to be
all that all that all that
large in particular we can always large in particular we can always large in particular we can always
perform an encoding using a Clifford perform an encoding using a Clifford perform an encoding using a Clifford
operation that requires a little bit operation that requires a little bit operation that requires a little bit
less than a quadratic number of gates less than a quadratic number of gates less than a quadratic number of gates
because that's actually true of every because that's actually true of every because that's actually true of every
Clifford Clifford Clifford
operation so the upshot is that for a operation so the upshot is that for a operation so the upshot is that for a
given stabilizer code it's always given stabilizer code it's always given stabilizer code it's always
possible to encode cubits efficiently possible to encode cubits efficiently possible to encode cubits efficiently
using that code and the mathematical using that code and the mathematical using that code and the mathematical
tools that are useful for understanding tools that are useful for understanding tools that are useful for understanding
Clifford operations can help with this Clifford operations can help with this Clifford operations can help with this
process for example here's an encoder process for example here's an encoder process for example here's an encoder
for the 7 Cubit steam code it's a for the 7 Cubit steam code it's a for the 7 Cubit steam code it's a
Clifford operation and as it turns out Clifford operation and as it turns out Clifford operation and as it turns out
this one doesn't even need S this one doesn't even need S this one doesn't even need S
Gates for General stabilizer codes eror Gates for General stabilizer codes eror Gates for General stabilizer codes eror
detection works in the following detection works in the following detection works in the following
way as before we'll assume that we have way as before we'll assume that we have way as before we'll assume that we have
n Cubit stabilizer generators P1 through n Cubit stabilizer generators P1 through n Cubit stabilizer generators P1 through
p r and we'll let e be an N Cubit poly p r and we'll let e be an N Cubit poly p r and we'll let e be an N Cubit poly
operation representing a hypothetical operation representing a hypothetical operation representing a hypothetical
error on an error on an error on an
encoding we're only considering poly encoding we're only considering poly encoding we're only considering poly
operations as Errors By the way because operations as Errors By the way because operations as Errors By the way because
the discretization of Errors works in the discretization of Errors works in the discretization of Errors works in
the same way for arbitrary stabilizer the same way for arbitrary stabilizer the same way for arbitrary stabilizer
codes as it does for the 9 Cubit Shore codes as it does for the 9 Cubit Shore codes as it does for the 9 Cubit Shore
code the way that we check for errors is code the way that we check for errors is code the way that we check for errors is
to measure each of the stabilizer to measure each of the stabilizer to measure each of the stabilizer
generators as generators as generators as
observables there are R stabilizer observables there are R stabilizer observables there are R stabilizer
generators so this means our outcomes generators so this means our outcomes generators so this means our outcomes
where each one is a plus one or a minus where each one is a plus one or a minus where each one is a plus one or a minus
one or a binary value if you prefer to one or a binary value if you prefer to one or a binary value if you prefer to
associate zero with plus one and one associate zero with plus one and one associate zero with plus one and one
with minus one and these R outcomes with minus one and these R outcomes with minus one and these R outcomes
together form the syndrome just like we together form the syndrome just like we together form the syndrome just like we
had for the three-bit repetition code had for the three-bit repetition code had for the three-bit repetition code
when R was equal to when R was equal to when R was equal to
two in general there are three possible two in general there are three possible two in general there are three possible
cases for the error e in terms of its cases for the error e in terms of its cases for the error e in terms of its
detection and they are as follows first detection and they are as follows first detection and they are as follows first
it could be that e is in the stabilizer it could be that e is in the stabilizer it could be that e is in the stabilizer
meaning that is possible to generate E meaning that is possible to generate E meaning that is possible to generate E
from the stabilizer generators or from the stabilizer generators or from the stabilizer generators or
slightly more generally it could be that slightly more generally it could be that slightly more generally it could be that
e is equivalent to a stabilizer element e is equivalent to a stabilizer element e is equivalent to a stabilizer element
up to a global up to a global up to a global
phase in this case e isn't really an phase in this case e isn't really an phase in this case e isn't really an
error at all because it has no effect in error at all because it has no effect in error at all because it has no effect in
the code space other than to possibly the code space other than to possibly the code space other than to possibly
inject a global phase which doesn't have inject a global phase which doesn't have inject a global phase which doesn't have
any effect so we don't need to worry at any effect so we don't need to worry at any effect so we don't need to worry at
all about errors like this whatever all about errors like this whatever all about errors like this whatever
non-trivial action it might have happens non-trivial action it might have happens non-trivial action it might have happens
outside of the code space so it doesn't outside of the code space so it doesn't outside of the code space so it doesn't
concern concern concern
us much more of a concern is the us much more of a concern is the us much more of a concern is the
possibility that e is not in the possibility that e is not in the possibility that e is not in the
stabilizer even up to a global phase but stabilizer even up to a global phase but stabilizer even up to a global phase but
nevertheless it does commute with every nevertheless it does commute with every nevertheless it does commute with every
single one of our stabilizer single one of our stabilizer single one of our stabilizer
generators remember that it's the generators remember that it's the generators remember that it's the
anti-commutation of the error with a anti-commutation of the error with a anti-commutation of the error with a
stabilizer generator that causes a minus stabilizer generator that causes a minus stabilizer generator that causes a minus
one to appear somewhere in the syndrome one to appear somewhere in the syndrome one to appear somewhere in the syndrome
but that doesn't happen in this case so but that doesn't happen in this case so but that doesn't happen in this case so
this is an error that actually does this is an error that actually does this is an error that actually does
change vectors in the code space in some change vectors in the code space in some change vectors in the code space in some
non-al way and we know this because non-al way and we know this because non-al way and we know this because
otherwise it would be proportional to a otherwise it would be proportional to a otherwise it would be proportional to a
stabilizer element but it goes stabilizer element but it goes stabilizer element but it goes
undetected by the undetected by the undetected by the
code so this is the bad case we have an code so this is the bad case we have an code so this is the bad case we have an
error that may potentially be changing error that may potentially be changing error that may potentially be changing
whatever Quantum information we've whatever Quantum information we've whatever Quantum information we've
encoded but the code is oblivious to encoded but the code is oblivious to encoded but the code is oblivious to
this fact for example for the three-bit this fact for example for the three-bit this fact for example for the three-bit
repetition code XXX falls into this repetition code XXX falls into this repetition code XXX falls into this
category category category
and the last possibility is that the a e and the last possibility is that the a e and the last possibility is that the a e
anti-c commutes with at least one of the anti-c commutes with at least one of the anti-c commutes with at least one of the
stabilizer generators and this means stabilizer generators and this means stabilizer generators and this means
that the code detects this error because that the code detects this error because that the code detects this error because
we'll get a syndrome with at least one we'll get a syndrome with at least one we'll get a syndrome with at least one
minus one somewhere in it which minus one somewhere in it which minus one somewhere in it which
indicates that something is wrong and indicates that something is wrong and indicates that something is wrong and
we'll need to try to correct this error we'll need to try to correct this error we'll need to try to correct this error
somehow so those are the somehow so those are the somehow so those are the
possibilities as a point of terminology possibilities as a point of terminology possibilities as a point of terminology
when we refer to the distance of a when we refer to the distance of a when we refer to the distance of a
stabilizer code what we mean is the stabilizer code what we mean is the stabilizer code what we mean is the
minimum weight of a poly operation falls minimum weight of a poly operation falls minimum weight of a poly operation falls
into the second category meaning that it into the second category meaning that it into the second category meaning that it
changes the code space in some changes the code space in some changes the code space in some
non-trivial way but the code doesn't non-trivial way but the code doesn't non-trivial way but the code doesn't
detect detect detect
this and when we say that a stabilizer this and when we say that a stabilizer this and when we say that a stabilizer
code is an nmd stabilizer code using code is an nmd stabilizer code using code is an nmd stabilizer code using
double square brackets as is shown on double square brackets as is shown on double square brackets as is shown on
the screen what that means is that the screen what that means is that the screen what that means is that
encodings are n cubits in length the encodings are n cubits in length the encodings are n cubits in length the
code is capable of encoding M cubits so code is capable of encoding M cubits so code is capable of encoding M cubits so
the code space has Dimension 2 the m and the code space has Dimension 2 the m and the code space has Dimension 2 the m and
its distance is D so an error has to its distance is D so an error has to its distance is D so an error has to
have weight weit D or more to change the have weight weit D or more to change the have weight weit D or more to change the
logical state of whatever Cubit we've logical state of whatever Cubit we've logical state of whatever Cubit we've
been coded without being detected as an been coded without being detected as an been coded without being detected as an
error we'll now take a look at the 7 error we'll now take a look at the 7 error we'll now take a look at the 7
Cubit steam code as an example and we'll Cubit steam code as an example and we'll Cubit steam code as an example and we'll
determine what its distance determine what its distance determine what its distance
is here are the stabilizer generators is here are the stabilizer generators is here are the stabilizer generators
for this code and recall that the for this code and recall that the for this code and recall that the
distance is the minimum weight of an distance is the minimum weight of an distance is the minimum weight of an
encubate poly operation that commutes encubate poly operation that commutes encubate poly operation that commutes
with all of the stabilizer generators with all of the stabilizer generators with all of the stabilizer generators
but is itself not in the stabilizer nor but is itself not in the stabilizer nor but is itself not in the stabilizer nor
is it proportional to an element of the is it proportional to an element of the is it proportional to an element of the
stabilizer stabilizer stabilizer
what we'll determine is that this code what we'll determine is that this code what we'll determine is that this code
has distance has distance has distance
three the way that we'll determine this three the way that we'll determine this three the way that we'll determine this
is to First consider any poly operation is to First consider any poly operation is to First consider any poly operation
having weight at most two and we'll having weight at most two and we'll having weight at most two and we'll
conclude that the only way that it can conclude that the only way that it can conclude that the only way that it can
compute with all six stabilizer compute with all six stabilizer compute with all six stabilizer
generators is for to be the identity generators is for to be the identity generators is for to be the identity
which is always an element of the which is always an element of the which is always an element of the
stabilizer so let's suppose that the stabilizer so let's suppose that the stabilizer so let's suppose that the
possibly non-identity polym matricies possibly non-identity polym matricies possibly non-identity polym matricies
that form this operation which we'll that form this operation which we'll that form this operation which we'll
call P and Q appear in the leftmost two call P and Q appear in the leftmost two call P and Q appear in the leftmost two
positions positions positions
this is just one case and we do have to this is just one case and we do have to this is just one case and we do have to
consider all of the other possible consider all of the other possible consider all of the other possible
locations for these possibly locations for these possibly locations for these possibly
non-identity poly matrices but the non-identity poly matrices but the non-identity poly matrices but the
argument is basically the same for all argument is basically the same for all argument is basically the same for all
of the possible of the possible of the possible
locations well by assumption this locations well by assumption this locations well by assumption this
operation commutes with all sticks operation commutes with all sticks operation commutes with all sticks
stabilizer generators so in particular stabilizer generators so in particular stabilizer generators so in particular
it commutes with this one and it also it commutes with this one and it also it commutes with this one and it also
commutes with this commutes with this commutes with this
one Q lines up with the identity Matrix one Q lines up with the identity Matrix one Q lines up with the identity Matrix
in both of these generators which is why in both of these generators which is why in both of these generators which is why
we chose these two and we have identity we chose these two and we have identity we chose these two and we have identity
Matrix IES in the rightmost five Matrix IES in the rightmost five Matrix IES in the rightmost five
positions of this error and so we positions of this error and so we positions of this error and so we
conclude that P must commute with both x conclude that P must commute with both x conclude that P must commute with both x
and z because otherwise this error would and z because otherwise this error would and z because otherwise this error would
anticommute with one of these two anticommute with one of these two anticommute with one of these two
generators but the only polymatrix that generators but the only polymatrix that generators but the only polymatrix that
commutes with both x and z is the commutes with both x and z is the commutes with both x and z is the
identity so P must be the identity so P must be the identity so P must be the
identity and now that we know that we identity and now that we know that we identity and now that we know that we
can choose two more stabilizer can choose two more stabilizer can choose two more stabilizer
generators that have an X and a z in the generators that have an X and a z in the generators that have an X and a z in the
second position from left and we can second position from left and we can second position from left and we can
draw a similar conclusion about Q draw a similar conclusion about Q draw a similar conclusion about Q
so there's no way for a weight atmost 2 so there's no way for a weight atmost 2 so there's no way for a weight atmost 2
error to go undetected by this code error to go undetected by this code error to go undetected by this code
unless it's just the identity operation unless it's just the identity operation unless it's just the identity operation
which is in the stabilizer so this isn't which is in the stabilizer so this isn't which is in the stabilizer so this isn't
actually an actually an actually an
error on the other hand there are weight error on the other hand there are weight error on the other hand there are weight
three poly operations that commute with three poly operations that commute with three poly operations that commute with
all six of these stabilizer generators all six of these stabilizer generators all six of these stabilizer generators
but aren't proportional to stabilizer but aren't proportional to stabilizer but aren't proportional to stabilizer
elements and here are two elements and here are two elements and here are two
examples so we've concluded that this examples so we've concluded that this examples so we've concluded that this
code has distance code has distance code has distance
three the last thing that we'll discuss three the last thing that we'll discuss three the last thing that we'll discuss
in this lesson is correcting errors for in this lesson is correcting errors for in this lesson is correcting errors for
stabilizer stabilizer stabilizer
codes as usual we'll assume our code is codes as usual we'll assume our code is codes as usual we'll assume our code is
specified by nbit stabilizer generators specified by nbit stabilizer generators specified by nbit stabilizer generators
P1 through p and let's start by thinking P1 through p and let's start by thinking P1 through p and let's start by thinking
a little bit about all of the possible a little bit about all of the possible a little bit about all of the possible
errors that could take errors that could take errors that could take
place there are two to the r possible place there are two to the r possible place there are two to the r possible
syndromes and as I mentioned earlier in syndromes and as I mentioned earlier in syndromes and as I mentioned earlier in
the lesson these syndromes partition all the lesson these syndromes partition all the lesson these syndromes partition all
of the possible en cupit poly operations of the possible en cupit poly operations of the possible en cupit poly operations
into equal siiz collections so that into equal siiz collections so that into equal siiz collections so that
means 4 the N / 2 the r poly operations means 4 the N / 2 the r poly operations means 4 the N / 2 the r poly operations
within each set so when we see a within each set so when we see a within each set so when we see a
particular syndrome it could be that any particular syndrome it could be that any particular syndrome it could be that any
one of these four to the N divided by 2 one of these four to the N divided by 2 one of these four to the N divided by 2
to the r errors is to the r errors is to the r errors is
responsible we are overcounting a little responsible we are overcounting a little responsible we are overcounting a little
bit here though because some of these bit here though because some of these bit here though because some of these
errors have exactly the same effect on errors have exactly the same effect on errors have exactly the same effect on
the code space in particular if we have the code space in particular if we have the code space in particular if we have
an error E and we multiply e By Any an error E and we multiply e By Any an error E and we multiply e By Any
stabilizer element s then we're going to stabilizer element s then we're going to stabilizer element s then we're going to
get another error that has exactly the get another error that has exactly the get another error that has exactly the
same effect in the code space as e so same effect in the code space as e so same effect in the code space as e so
these are equivalent errors these are equivalent errors these are equivalent errors
the stabilizer has two to the r elements the stabilizer has two to the r elements the stabilizer has two to the r elements
and if we account for this then we find and if we account for this then we find and if we account for this then we find
that for each syndrome there are four to that for each syndrome there are four to that for each syndrome there are four to
the N minus r INE equivalent classes of the N minus r INE equivalent classes of the N minus r INE equivalent classes of
errors that could have caused that one errors that could have caused that one errors that could have caused that one
syndrome and what that means is that syndrome and what that means is that syndrome and what that means is that
unless R is equal to n in which case we unless R is equal to n in which case we unless R is equal to n in which case we
have a one-dimensional code space and have a one-dimensional code space and have a one-dimensional code space and
therefore can't encode any cubits at all therefore can't encode any cubits at all therefore can't encode any cubits at all
it's not going to be possible to correct it's not going to be possible to correct it's not going to be possible to correct
every error that we can detect instead every error that we can detect instead every error that we can detect instead
what we have to do is choose just one what we have to do is choose just one what we have to do is choose just one
correction operation for each syndrome correction operation for each syndrome correction operation for each syndrome
in the hopes of correcting just one in the hopes of correcting just one in the hopes of correcting just one
class of equivalent class of equivalent class of equivalent
errors so how should we choose what errors so how should we choose what errors so how should we choose what
correction to perform for each correction to perform for each correction to perform for each
syndrome a natural strategy is to choose syndrome a natural strategy is to choose syndrome a natural strategy is to choose
the lowest weight poly operation that the lowest weight poly operation that the lowest weight poly operation that
causes whatever syndrome we measure with causes whatever syndrome we measure with causes whatever syndrome we measure with
the idea being that lower weight poly the idea being that lower weight poly the idea being that lower weight poly
operations represent more likely operations represent more likely operations represent more likely
explanations for that explanations for that explanations for that
syndrome this might not actually be the syndrome this might not actually be the syndrome this might not actually be the
case for some noise models and another case for some noise models and another case for some noise models and another
natural strategy is to somehow compute natural strategy is to somehow compute natural strategy is to somehow compute
the most likely error that causes the the most likely error that causes the the most likely error that causes the
given syndrome but for now we'll just given syndrome but for now we'll just given syndrome but for now we'll just
consider the strategy where we attempt consider the strategy where we attempt consider the strategy where we attempt
to correct errors by selecting the to correct errors by selecting the to correct errors by selecting the
lowest weight poly operation causing the lowest weight poly operation causing the lowest weight poly operation causing the
syndrome for a distance D stabilizer syndrome for a distance D stabilizer syndrome for a distance D stabilizer
code this strategy will always allow us code this strategy will always allow us code this strategy will always allow us
to correct errors having weight strictly to correct errors having weight strictly to correct errors having weight strictly
less than half of D or in other words less than half of D or in other words less than half of D or in other words
weight at most D minus1 over2 and I'll weight at most D minus1 over2 and I'll weight at most D minus1 over2 and I'll
explain briefly why this explain briefly why this explain briefly why this
is this will show for instance that the is this will show for instance that the is this will show for instance that the
7 Cubit steam code can correct for any 7 Cubit steam code can correct for any 7 Cubit steam code can correct for any
weight one poly error and by the weight one poly error and by the weight one poly error and by the
discretization of errors this means that discretization of errors this means that discretization of errors this means that
just like the 9 Cubit Shore code the just like the 9 Cubit Shore code the just like the 9 Cubit Shore code the
steam code can correct for an arbitrary steam code can correct for an arbitrary steam code can correct for an arbitrary
error on one cubit so here's how this error on one cubit so here's how this error on one cubit so here's how this
works consider that this circle works consider that this circle works consider that this circle
represents all of the possible poly represents all of the possible poly represents all of the possible poly
operations that result in the all plus1 operations that result in the all plus1 operations that result in the all plus1
syndrome which is the syndrome that syndrome which is the syndrome that syndrome which is the syndrome that
suggests that no errors have occurred suggests that no errors have occurred suggests that no errors have occurred
and nothing is and nothing is and nothing is
wrong among these operations we have wrong among these operations we have wrong among these operations we have
elements of the stabilizer or really elements of the stabilizer or really elements of the stabilizer or really
operations that are proportional to operations that are proportional to operations that are proportional to
elements of the stabilizer and we also elements of the stabilizer and we also elements of the stabilizer and we also
have non-trivial errors that change the have non-trivial errors that change the have non-trivial errors that change the
code space somehow but they aren't code space somehow but they aren't code space somehow but they aren't
detected by the detected by the detected by the
code and We Know by the definition of code and We Know by the definition of code and We Know by the definition of
distance that every poly operation in distance that every poly operation in distance that every poly operation in
this category must have weight at least this category must have weight at least this category must have weight at least
D because D is defined as the minimum D because D is defined as the minimum D because D is defined as the minimum
weight of these weight of these weight of these
operations we also have poly operations operations we also have poly operations operations we also have poly operations
that cause other syndromes to appear and that cause other syndromes to appear and that cause other syndromes to appear and
we could consider each syndrome in turn we could consider each syndrome in turn we could consider each syndrome in turn
if we wish but here we're just imagining if we wish but here we're just imagining if we wish but here we're just imagining
that one such syndrome s has appeared now imagine that e is an eror in this now imagine that e is an eror in this
category that has weight strictly less category that has weight strictly less category that has weight strictly less
than half of the distance than half of the distance than half of the distance
D well we don't know that this is D well we don't know that this is D well we don't know that this is
necessarily the minimum weight poly necessarily the minimum weight poly necessarily the minimum weight poly
operation in this circle so let's take C operation in this circle so let's take C operation in this circle so let's take C
to be the minimum weight operation in to be the minimum weight operation in to be the minimum weight operation in
this set or one of them in case there's this set or one of them in case there's this set or one of them in case there's
a a a
tie so it could be that c is equal to e tie so it could be that c is equal to e tie so it could be that c is equal to e
or possibly not but because C has or possibly not but because C has or possibly not but because C has
minimum weight we know that it can't minimum weight we know that it can't minimum weight we know that it can't
possibly have weight larger than the possibly have weight larger than the possibly have weight larger than the
weight of E because C is also in the set weight of E because C is also in the set weight of E because C is also in the set
and it has minimal weight so the and it has minimal weight so the and it has minimal weight so the
correction operation C must have weight correction operation C must have weight correction operation C must have weight
strictly less than half of strictly less than half of strictly less than half of
D so what happens if we apply this D so what happens if we apply this D so what happens if we apply this
correction C to whatever our state correction C to whatever our state correction C to whatever our state
becomes after the air e becomes after the air e becomes after the air e
occurs the answer is that the state that occurs the answer is that the state that occurs the answer is that the state that
we obtain is C * e multiplied to the we obtain is C * e multiplied to the we obtain is C * e multiplied to the
original encoding because we first original encoding because we first original encoding because we first
multiply by E that was the error and multiply by E that was the error and multiply by E that was the error and
then we multiplied by C to try to then we multiplied by C to try to then we multiplied by C to try to
correct it and this product must be in correct it and this product must be in correct it and this product must be in
the stabilizer or proportional to an the stabilizer or proportional to an the stabilizer or proportional to an
element of the stabilizer and here's element of the stabilizer and here's element of the stabilizer and here's
why first because e and C give us the why first because e and C give us the why first because e and C give us the
same syndrome C * e must give us the all same syndrome C * e must give us the all same syndrome C * e must give us the all
plus one plus one plus one
syndrome that's because for a given syndrome that's because for a given syndrome that's because for a given
stabilizer generator either c and e both stabilizer generator either c and e both stabilizer generator either c and e both
commute with that generator or they both commute with that generator or they both commute with that generator or they both
anticommute it has to be the same for anticommute it has to be the same for anticommute it has to be the same for
both because the syndromes are the same both because the syndromes are the same both because the syndromes are the same
and if you think about that for just a and if you think about that for just a and if you think about that for just a
moment it implies that c * e must moment it implies that c * e must moment it implies that c * e must
commute with all the stabiliz commute with all the stabiliz commute with all the stabiliz
generators and finally C * e has to be generators and finally C * e has to be generators and finally C * e has to be
down in the stabilizer because its down in the stabilizer because its down in the stabilizer because its
weight can't be larger than the sum of weight can't be larger than the sum of weight can't be larger than the sum of
the weights of c and e and so its weight the weights of c and e and so its weight the weights of c and e and so its weight
is strictly less than D and this means is strictly less than D and this means is strictly less than D and this means
that we successfully correct e because that we successfully correct e because that we successfully correct e because
applying e followed by C does nothing to applying e followed by C does nothing to applying e followed by C does nothing to
the code space so that's great but the code space so that's great but the code space so that's great but
there's just one there's just one there's just one
problem and that is that for stabilizer problem and that is that for stabilizer problem and that is that for stabilizer
codes in general it's a computationally codes in general it's a computationally codes in general it's a computationally
difficult problem to compute the lowest difficult problem to compute the lowest difficult problem to compute the lowest
weight operation causing a given weight operation causing a given weight operation causing a given
syndrome and indeed that's true even for syndrome and indeed that's true even for syndrome and indeed that's true even for
classical codes which in this context we classical codes which in this context we classical codes which in this context we
can think about as stabilizer codes can think about as stabilizer codes can think about as stabilizer codes
where we only have identity matrices and where we only have identity matrices and where we only have identity matrices and
polyc matrices appearing as tensor polyc matrices appearing as tensor polyc matrices appearing as tensor
factors within the stabilizer factors within the stabilizer factors within the stabilizer
generators so unlike the encoding step generators so unlike the encoding step generators so unlike the encoding step
Clifford operations don't come to our Clifford operations don't come to our Clifford operations don't come to our
rescue this time and the only reasonable rescue this time and the only reasonable rescue this time and the only reasonable
solution is to choose specific codes for solution is to choose specific codes for solution is to choose specific codes for
which good Corrections can be computed which good Corrections can be computed which good Corrections can be computed
efficiently and there's no recipe for efficiently and there's no recipe for efficiently and there's no recipe for
this simply put devising stabilizer this simply put devising stabilizer this simply put devising stabilizer
codes like this is part of the Artistry codes like this is part of the Artistry codes like this is part of the Artistry
of Quantum Code of Quantum Code of Quantum Code
design and that is a good place to stop design and that is a good place to stop design and that is a good place to stop
for this lesson I hope you will join me for this lesson I hope you will join me for this lesson I hope you will join me
for the next lesson in which we'll take for the next lesson in which we'll take for the next lesson in which we'll take
a look at some specific Quantum Code a look at some specific Quantum Code a look at some specific Quantum Code
constructions including codes that constructions including codes that constructions including codes that
illustrate the Artistry of Quantum Code illustrate the Artistry of Quantum Code illustrate the Artistry of Quantum Code
design in a pretty spectacular way design in a pretty spectacular way design in a pretty spectacular way
goodbye for now

## Understanding Quantum Information & Computation ｜ Course Overview

- Hello and welcome to Understanding Quantum
Information and Computation. This is a series that explains
how quantum information and computation work, at a
detailed mathematical level. It's meant for anyone that wants to learn about quantum computing at
a level that's comparable to what's taught in universities around the world to students
at the advanced undergraduate, or introductory graduate level. My name is John Watrous, and I'm the Technical Director
of Education at IBM Quantum. I first learned about
quantum computing in 1994, and I've been studying, researching, and teaching it ever since then. The idea behind this
series is pretty simple. There are a lot of people,
all around the world, that want to learn
about quantum computing, and ultimately to make contributions of their own to its
development and its usage. My goal for this series is to
make this content available to anyone that wishes to make use of it. Now, the series surely won't
have value for everyone. If you're not interested
in getting into the details and learning about the mathematics
behind quantum computing, it probably won't appeal to you. To follow these videos, you'll
need to understand the basics of linear algebra, including
vectors and matrices, and how to work with them, as well as notions such as linear
independence, bases, and dimension. You also need to understand
how complex numbers work, and you need to be comfortable with some basic mathematical concepts such as sets and functions. There's no expectation that
you have any background in quantum computing, quantum mechanics, or physics in general. Unit one of the series explains the basics of quantum information,
including quantum states, measurements and operations,
how quantum circuits work, and some important examples
such as quantum teleportation. In unit two, we'll explore
quantum algorithms, including Shor's algorithm
for factoring integers, as well as some algorithms
that are better suited to the quantum devices that
are available to us today, which are steadily improving,
but still somewhat limited. In unit three, we'll dive a bit deeper into quantum information, and we'll talk about how it can be described
using density matrices, and we'll explore why this
is such an important tool for studying quantum information. Finally, in unit four, we'll move on to how we can understand and mitigate the effects of
noise on quantum computers. Thank you for watching,
and I hope you'll join me for the first lesson of the series.

## Understanding Quantum Information & Computation ｜ Series Trailer

- Quantum computing has
attracted the attention and sparked the imaginations
of people around the globe. And whether based on fact or fiction, the excitement for quantum
computing is growing every day. So let me welcome you to Understanding Quantum
Information and Computation, a free course on quantum computing explained at a detailed
mathematical level. My name is John Watrous, and I invite you to join
me starting November 1st for an exploration of quantum algorithms, the mathematics of quantum information, techniques to mitigate noise
in quantum devices, and more. You'll find the course right here on the Qiskit YouTube channel. I hope to see you then.

## What ＊is＊ a qubit？

Qubits sound really complicated but&nbsp;
they really don't have to be. Like this&nbsp;&nbsp; laser is a qubit and an electron by itself&nbsp;
is a qubit, a trapped ion can be a qubit,&nbsp;&nbsp; there can be superconducting qubits, there are&nbsp;
so many different things that can be qubits.&nbsp;&nbsp; Fundamentally the only thing that matters is if&nbsp;
it is a two level quantum system and all two level&nbsp;&nbsp; quantum systems are are qubits and they're&nbsp;
basically the same, at least mathematically. But when you're making them in the lab, you&nbsp;
might decide to use one type of qubit over the&nbsp;&nbsp; other because it's more stable or it's easier&nbsp;
to manipulate, whatever it is. Fundamentally,&nbsp;&nbsp; at the mathematics level, they're all the same.&nbsp;
So let's talk about what a qubit is. So this laser&nbsp;&nbsp; pointer here has a little polarizer in front&nbsp;
of it and that polarizer makes sure that all&nbsp;&nbsp; of the light is polarized in this direction&nbsp;
when it comes out of this laser pointer. So this is polarized this way but I could&nbsp;
rotate it now it's polarized that way.&nbsp;&nbsp; Etc. And so there are many many different&nbsp;
states that this light can be in but if I&nbsp;&nbsp; was to measure the Polarization then there will&nbsp;
only be two outcomes that this measurement can&nbsp;&nbsp; resolve So here's an example this block&nbsp;
of calcite measures polarization So if I&nbsp;&nbsp; put this in front of my laser It splits it up&nbsp;
into two dots and those two dots have opposite&nbsp;&nbsp; polarizations to each other So what I mean&nbsp;
by opposite is that they're 90 degrees away&nbsp;&nbsp; from each other So one of them is at 45&nbsp;
degrees and the other is at 130 degrees. So you could think of this calcite&nbsp;
as measuring the polarization,&nbsp;&nbsp; but it only allows two options. It can&nbsp;
allow light to go through that now has&nbsp;&nbsp; 45 degree polarization or 130 degree polarization.&nbsp;
But what if I rotated this calcite by a bit? So&nbsp;&nbsp; if I rotated it like this, now this calcite&nbsp;
measures a different set of polarizations. In fact, if it was measuring 45 degree and&nbsp;
135 degree light. and I rotated it like this,&nbsp;&nbsp; then now it's measuring horizontal and vertical&nbsp;
light. This is why, even though this light has so&nbsp;&nbsp; many different options for what state it's&nbsp;
in, we call this light a two level system&nbsp;&nbsp; because when you measure it, there's always two&nbsp;
outcomes, no matter which way you measure it. there's always going to be just two.&nbsp;
And so fundamentally this system is&nbsp;&nbsp; two dimensional. So here's what I mean&nbsp;
by that. Let's suppose I start off with&nbsp;&nbsp; my light pointing in this direction so&nbsp;
its state is represented by this. By&nbsp;&nbsp; the way I'm going to put that inside of&nbsp;
a thing called a ket. That's just a way&nbsp;&nbsp; in quantum mechanics of marking that this&nbsp;
thing represents the state of this thing. So this kind of means a state. What will&nbsp;
happen when I measure this light with this&nbsp;&nbsp; calcite that's oriented like this? This&nbsp;
calcite breaks up light into two options,&nbsp;&nbsp; horizontal or vertical. So that's light&nbsp;
that's pointing this way or light that's&nbsp;&nbsp; pointing that way. Now one of the key concepts in&nbsp;
quantum mechanics is this idea of superposition. We say that this light is really a mixture of&nbsp;
both of these types of light at the same time.&nbsp;&nbsp; Like this light is clearly pushing somewhat to&nbsp;
this direction, but it's also pushing upwards,&nbsp;&nbsp; and so really it's a combination of&nbsp;
these two. To decide how much of each&nbsp;&nbsp; of those two options we have, we need to&nbsp;
put coefficients in front of these vectors. In this example, I've put one on square root&nbsp;
two in front of both of them, and this just&nbsp;&nbsp; means that there's an equal amount of both. But&nbsp;
what if we chose to measure this light in some&nbsp;&nbsp; other direction? So instead of horizontal&nbsp;
and vertical, we're Let's say we rotated&nbsp;&nbsp; it kind of like this. So now we have to write&nbsp;
that same light in terms of two other options. So now we have to figure out what these&nbsp;
coefficients are going to be. And you can&nbsp;&nbsp; see that this coefficient should be pretty&nbsp;
big because this is pretty much the same as&nbsp;&nbsp; that. But this one should be quite small.&nbsp;
This is called a change of basis. So we've&nbsp;&nbsp; just changed how we look at this same light. So&nbsp;
this light is equal to being a bit horizontal&nbsp;&nbsp; and a bit vertical, but it's also a bit&nbsp;
equal to this and this at the same time. And in fact there's never anything special&nbsp;
about which direction you choose to do the&nbsp;&nbsp; measurements in. For quantum computing,&nbsp;
we choose one direction as our sort of&nbsp;&nbsp; like fundamental direction, not for any&nbsp;
actually good reasons, just we choose one,&nbsp;&nbsp; and then we go, okay, so in this basis,&nbsp;
option one, we're going to call that zero. And in this basis, option two is going to&nbsp;
be called one. So let's say that I chose&nbsp;&nbsp; horizontal and vertical to be the 0&nbsp;
and 1. I've just relabelled these,&nbsp;&nbsp; I haven't actually changed the physics&nbsp;
of that light in any way. But now you&nbsp;&nbsp; can kind of see why you might call this a&nbsp;
qubit, right? Because there's a 0 and a 1&nbsp;&nbsp; and we're saying that this light is in some sort&nbsp;
of combination of both 0 and 1 at the same time,&nbsp;&nbsp; which is what you might have&nbsp;
heard people say about qubits. But an important thing to note though is that&nbsp;
this state really is both zero and one at the&nbsp;&nbsp; same time, right? Like if it was just zero, it&nbsp;
would be fully horizontal. If it was just one,&nbsp;&nbsp; it would just be vertical. But we know it's&nbsp;
something else. It's something kind of in between.&nbsp;&nbsp; And so that's all people mean when they say that&nbsp;
qubits are doing two things at the same time. Okay, sorry to interrupt, but I have an&nbsp;
announcement. There's going to be another&nbsp;&nbsp; cohort of the live version of this course in&nbsp;
January. So it's a four week course and once&nbsp;&nbsp; a week we meet to go over the homework&nbsp;
problems and the students get to like&nbsp;&nbsp; ask any questions they want. So I just&nbsp;
finished a cohort and it was so much fun. And a lot of the students emailed me to say that&nbsp;
they thought they understood quantum mechanics,&nbsp;&nbsp; at least somewhat before doing&nbsp;
the problems and realizing that&nbsp;&nbsp; there's actually more to it. I really&nbsp;
believe that doing problems is the best&nbsp;&nbsp; way to learn physics. So that's&nbsp;
why I came up with this course. So if you're interested in doing that in&nbsp;
January, then there's going to be a link&nbsp;&nbsp; in the description. Light isn't the only example&nbsp;
of a qubit though. Electrons can also be qubits,&nbsp;&nbsp; and that's by using their property&nbsp;
called spin. So an electron's spin&nbsp;&nbsp; points in a particular direction. And&nbsp;
just like with the calcite and light,&nbsp;&nbsp; we can measure the spin of this&nbsp;
electron in many different directions. So the way that you measure spin is using a&nbsp;
Stern Gerlach machine, and those machines can be&nbsp;&nbsp; oriented in different ways. As it's oriented here,&nbsp;
It's going to measure whether the spin is up. or&nbsp;&nbsp; down. So again, there's only two options for this&nbsp;
particle, even though actually the spin could be&nbsp;&nbsp; pointing in any direction at all, when you do a&nbsp;
measurement it collapses it to one of two options. And that's again the sort of two&nbsp;
dimensionality of this state. So&nbsp;&nbsp; when I'm measuring this electron in terms of&nbsp;
up and down, then I can think of its state&nbsp;&nbsp; as some combination of up. and down. But&nbsp;
I could also orient the Skrngalak machine&nbsp;&nbsp; in the left right direction and write our&nbsp;
state as a combination of left and right. Both of these two options for writing this state&nbsp;
are equally valid, both of them are true, but I&nbsp;&nbsp; might decide that really I am more interested in&nbsp;
up and down and I might want to make them the sort&nbsp;&nbsp; of canonical directions. And so just like with&nbsp;
the light, I might re label up and down to zero&nbsp;&nbsp; and one. But light and electrons are not your&nbsp;
only choices when it comes to making a qubit. And actually, if you're making a quantum&nbsp;
computer, there are much better choices.&nbsp;&nbsp; And that's because you want your qubits to&nbsp;
be very stable, but also very controllable.&nbsp;&nbsp; And I don't think that the light or spin are, um,&nbsp;
really either of those. And so instead people use&nbsp;&nbsp; things like trapped ions. So for example,&nbsp;
a trapped ion can be made into a qubit. Okay so I just looked this up this morning,&nbsp;
ytterbium, apparently that is a atom that you&nbsp;&nbsp; can make into a trapped ion. So you take&nbsp;
the positive version of this and you trap&nbsp;&nbsp; it in like a magnetic trap. And then it&nbsp;
has two ground states. So a ground state&nbsp;&nbsp; is sort of the lowest energy state. And&nbsp;
when I say that there's two ground states,&nbsp;&nbsp; what I really mean is that there are&nbsp;
two that are very, very close in energy. It's like a hyperfine splitting of the ground&nbsp;
state. And so those two energies can kind of&nbsp;&nbsp; act as your two options for the qubit, so one of&nbsp;
them is zero and one of them is one. The reason&nbsp;&nbsp; why that's apparently a good choice is because you&nbsp;
can use microwave radiation to jump between those&nbsp;&nbsp; two or go into superpositions, and so all of the&nbsp;
gates in the quantum computer are superpositioned. are made using uh, yeah, microwaves&nbsp;
to manipulate the state. So yeah,&nbsp;&nbsp; that's a really nice example because it&nbsp;
technically isn't a two level system. Like,&nbsp;&nbsp; yes, there are these two energies that are very&nbsp;
close to each other, but there's loads of other&nbsp;&nbsp; energies for this atom as well, right? Um, like&nbsp;
the electron could always jump to higher energies,&nbsp;&nbsp; but they can safely ignore those higher&nbsp;
energies because all of the manipulations&nbsp;&nbsp; they're going to do are only going to&nbsp;
be between those two lowest energies. So yeah, that's a really cool way to make a&nbsp;
qubit because most things aren't naturally&nbsp;&nbsp; two level systems. So if you saw a state like this&nbsp;
written down with a combination of zero and one,&nbsp;&nbsp; you wouldn't be able to tell whether we're talking&nbsp;&nbsp; about light here or we're talking&nbsp;
about electrons or something else. And that's kind of by design. The purpose of&nbsp;
writing zero and one is to abstract away all&nbsp;&nbsp; of the messy physics and just bring it down to&nbsp;
what's fundamentally important about this state,&nbsp;&nbsp; that it is some combination of these two&nbsp;
directions. This also helps us draw the state as&nbsp;&nbsp; a two dimensional vector, exactly like we did with&nbsp;
light, no matter what the actual qubit is made of. And that's really useful. If I have the state&nbsp;
a times zero plus b times one, then all I need&nbsp;&nbsp; to do to draw it is to draw zero and one here&nbsp;
and then I want to take a steps in the zero&nbsp;&nbsp; direction and b steps in the one direction.&nbsp;
So maybe two here. And then I link them up. This is my way of drawing the state, which&nbsp;
completely abstracts away the physics. And so,&nbsp;&nbsp; no matter what type of quantum computer you're&nbsp;
talking about, everyone can use the same&nbsp;&nbsp; language. This also leads to a bunch of common&nbsp;
shorthands in the quantum computing community&nbsp;&nbsp; that I really like. Like for example, the plus&nbsp;
state is an equal superposition of 0 and 1. What do you think the minus state is? It's zero minus one. You might be&nbsp;
thinking how can this be negative?&nbsp;&nbsp; Well actually it makes perfect sense&nbsp;
if we think about it as a vector. If&nbsp;&nbsp; we have positive zero but minus one then it&nbsp;
would be pointing in this direction. And so&nbsp;&nbsp; a combination of zero and minus one looks&nbsp;
like this. whereas plus looks like this. And again, if we go back to the light example,&nbsp;
that makes perfect sense. This is plus and&nbsp;&nbsp; this is minus. But you can have even weirder&nbsp;
things. So these numbers can actually be complex&nbsp;&nbsp; numbers. So this is the imaginary number.&nbsp;
I. Now you might freak out and think like,&nbsp;&nbsp; what are complex numbers doing here?&nbsp;
But it actually can make a lot of sense. Let's go back to thinking about the spin of&nbsp;
this electron. We said that the spin of the&nbsp;&nbsp; electron could be oriented all kinds of different&nbsp;
directions. But what about this direction? Or&nbsp;&nbsp; something like this? Why aren't those allowed?&nbsp;
Well, they are. And the way to represent them&nbsp;&nbsp; is through complex numbers. So for example, this&nbsp;
state is represented by zero plus i times one. And that kind of makes sense if you think of&nbsp;
the i direction as like the third dimension.&nbsp;&nbsp; So you have your one pointing this way and your&nbsp;
negative one pointing this way. In between you&nbsp;&nbsp; have i. And that's pointing upwards.&nbsp;
So that's how it works out. With light&nbsp;&nbsp; it's a little bit more complicated. Like,&nbsp;
what would this state look like for light? Because we said that we're allowed to&nbsp;
rotate this light around in any direction,&nbsp;&nbsp; and that's all fine, but those are&nbsp;
all just like the real ones. What&nbsp;&nbsp; would this state look like? do. Unlike with the&nbsp;
electron, we can't just sort of rotate the laser,&nbsp;&nbsp; because that doesn't really work. Like,&nbsp;
polarization is about the direction of the&nbsp;&nbsp; polarization relative to the direction of motion&nbsp;
of the light, and so only these ones seem valid. So all of the polarization we've talked&nbsp;
about so far is linearly polarized light,&nbsp;&nbsp; which is always pointing in one direction. But&nbsp;
there's circularly polarized light, which is which&nbsp;&nbsp; changes direction of polarisation as it travels&nbsp;
forward. So that is what this I represents and&nbsp;&nbsp; different complex numbers would represent&nbsp;
being more or less circularly polarised. So yeah, for every qubit that you can write&nbsp;
down, when you translate that back into the&nbsp;&nbsp; physical system, there is some way to&nbsp;
get this object to behave so that its&nbsp;&nbsp; state is actually equal to this. And that&nbsp;
is why it's so useful to abstract away and&nbsp;&nbsp; write things like this, rather than always&nbsp;
thinking about the actual physical system. Because, no matter what architecture you use&nbsp;
for quantum computing, there you will always&nbsp;&nbsp; be able to write states like this and manipulate&nbsp;
them and guarantee that there is some way that&nbsp;&nbsp; you can make this all happen in the real&nbsp;
world. And that is someone else's problem.

## But what is quantum computing？  (Grover's Algorithm)

A lot of pop science outlets give a certain summary of quantum computing that I can almost guarantee leads to misconceptions. The summary goes something like this. In a classical computer, data is stored with bits, some sequence of ones and zeros, but in a quantum computer, you are able to represent every possible sequence of bits of some fixed length all at once in one big thing known as a superposition. And sometimes the implication of these summaries is that quantum computers can be faster by basically doing whatever a classical computer would do, but to all of these sequences in parallel. Now, this does gesture at something that's kind of true, but let me see if I can prove to you why I think this leads to misconceptions, and I'll prove it using a quiz. To set it up, I want you to imagine that I have a mystery function, and I tell you that there's a certain secret number among all the numbers from 0 up to n-1, where if you plug that value into my function, it returns true, but if you were to plug in any other value, it returns false. And let's say you can't look at the innards of the function to learn anything about it, the only thing you're allowed to do with it is just try it out on numbers. The warmup question is, how many times, on average, would you have to apply this mystery function in order to find the secret key? Well, if the setup was in an ordinary classical computer, there's really nothing better that you can do than guess and check. You go through all the numbers, and maybe you're lucky and find it early, maybe you're unlucky and it doesn't come up until later, but on average, with a list of n possibilities, it takes one half of n attempts to find the key. Now, in computer science, people care about how runtimes scale. If you had a list ten times as big, how much longer would it take? And computer scientists have a way of categorizing runtimes. They would call this one O of n, where that big O communicates that maybe there's some constants like the one half, or maybe some factors that grow slower than n, but the factor of n is what explains how quickly it scales as n grows. If n goes up by a factor of ten, the runtime also goes up by a factor of ten. Now, here's your quiz. For the equivalent of this setup, but in a quantum computer, what's the best runtime for finding the secret key? I've asked a variant of this quiz many times in a certain lecture that I've given over the years, and the options I typically offer are O of square root of n, O of log of n, O of log of log of n, and O of one, where here O of one would mean that the runtime is just some constant that doesn't actually grow as n grows. Now, to be fair, I have not defined quantum computing. In fact, doing so from the ground up is going to be the goal of this video. So without showing you what this mystery function would look like in that setting, it's kind of an incoherent question. But it's not really meant as a quiz that I'm grading you on or anything, it's just meant to be a gut check of intuition before we dive in. In principle, it's the same task. Finding a needle in a haystack, where you want to discover which value out of many options uniquely triggers some function. I asked this as a YouTube post last month, which 100,000 of you kindly answered. The most recent time I gave it live was to a group of Stanford students. I also posed it to those attending the International Math Olympiad. And in all of these and many other instances, the answer distribution looks very similar. The most common answer is always O of one. And this is wrong, and I'm pretty sure that it stems from that misleading summary. That summary implies that you would put all of the n values that you need to search into this mysterious superposition, and then you would process them all in parallel, and then somehow the answer would be revealed. The second most common answer is typically O of log of n, and this is also wrong. You would call this an exponential speedup. For example, if you increase the size of the list by factors of 10, an O of log n runtime would only tick up by the same additive increment each time. Now this wrong answer I suspect stems from a misconception about how much better quantum computers are in general. There are very certain special problems where you can achieve an exponential speedup. The most famous case is probably Shor's algorithm for factoring large numbers. But most problems are not like that. In this case, the correct answer is O of square root of n. And this is a lot more representative of the typical speedup that you could get with a quantum computer. In 1994, it was proven that a quantum computer could not possibly do any better than O of square root of n on this task. And then two years later, Lav Grover found a specific procedure that actually achieves that runtime. So searching through a bag of a million options takes on the order of a thousand steps. A bag of a trillion options takes on the order of a million. This big O actually hides something that's pretty fun, which is the constant of pi fourths in the precise runtime, and that pi has its own whole fun story to tell, which I'll get to later. You might think that this puzzle with a mystery function triggered by one specific value is super contrived. But I want you to keep in mind this is meant to be a generic stand-in for any problem where you know how to quickly verify a solution, even if you don't know how to find that solution in the first place. This describes an enormous class of problems in computer science known as NP problems. So while a square root speedup is frankly not as earth-shattering as an exponential speedup would be, and while big O runtimes are often a lot less important than other practical considerations, it is thought-provoking that something like Grover's algorithm is even possible at all, providing this catch-all method for speeding up any NP problem. My goal with this lesson is to build up to a step-by-step walkthrough of how that algorithm works. It's actually very geometric and very beautiful. But we need to build up a lot of background in order to get there. The first two-thirds or so of the video will be spent building up the fundamentals of quantum computing, not with a set of analogies, which as we've seen can lead to misconceptions, but as a piece of math, which I think offers you a pair of glasses through which you can see a much more honest depiction of the whole field. This is one of those topics that has a few premises that are just going to feel a little bit strange at first, and I should warn you they take a little getting used to. My current plan is to follow this lesson with another one about some of the underlying physics, which hopefully can help motivate a few of the odd-looking rules that you'll see here. But today the goal is to provide the minimal viable path to seeing a genuine, bona fide quantum algorithm. Let me pull up again that contrast between classical computing and quantum computing, and let's see if we can build up something of a more representative mental model. It is true of course that data in a classical computer looks like a series of ones and zeros, and at a higher layer of abstraction that might represent an actual data type, like an integer or some text, and at a lower layer of abstraction, those ones and zeros represent some actual thing in the physical world, like voltages across a capacitor or something like that. These same layers of abstraction provide a pretty helpful framing when we discuss quantum computing. Over there, there's also some underlying physical measurement, and again you represent the outcome of that measurement with some sequence of ones and zeros, and again this might implement some actual data type that you care about, like a number. This symbol that I'm showing by the way is called a ket. I'll explain it properly in a couple minutes, but for right now just think of it as conveying that something is coming from a quantum computer. Let's start things off with a description of quantum computing through the lens of that middle layer of abstraction, meaning we're going to postpone all of the underlying physics for now, which is a little bit like teaching computer science without discussing hardware. In a classical computer, there's no need to distinguish between the state of memory and what you read out from the memory, both of them just look like the same sequence of bits, but it's a very different story in a quantum computer. Our main job today is going to be to understand something called the state vector, which is continuous, this is the thing the computer actually operates on, but it has a very unusual relationship with the values that you actually read out, those discrete sequences of bits. Before I can define this state vector, you need to know one other key difference from classical computers, which is that this value that you read out, which again just looks like some sequence of ones and zeros, is random. Or to be a little more accurate, I should say it's typically random. The way you can think about this is that if you run a program on a quantum computer, that program doesn't necessarily determine a particular output, instead it determines a probability distribution across all possible outputs. So for the example I'm showing on screen, this would be a very small quantum computer where the thing that you read out has four bits, meaning that there are 2 to the 4, or 16 possible outputs, and the specific program you run determines some kind of distribution across all those possible outputs. Some programs might manage to concentrate more probability on just one of those outputs, but other programs might give a more even spread across everything. This example, by the way, where the thing you read out has four bits, would be called a 4-qubit quantum computer, and more generally, if you have a k-qubit quantum computer, that means there are 2 to the k distinct possible outputs, and any program gives a distribution across all of those, and the thing that you read out has k distinct bits. That word qubit, by the way, is another thing I'm going to define more precisely in just a minute. I do want to emphasize that this distribution is implicit. You never actually see it directly, you instead infer what it must be based on the program that you run. You never see all bit strings coexisting at once in some kind of way. You just see one of them, drawn at random, according to this distribution. At a lower layer of abstraction, what I'm describing as reading out from memory looks like a physical measurement, and the randomness stems from the laws of quantum mechanics. If you're curious about the physics, that lower layer of abstraction, that's exactly what the next video is for. Up in this layer, you just think about probability distributions over all possible bit strings. One more funny rule here, which does bubble up from the underlying quantum mechanics, is that after you read out from memory, and you see some particular value, the underlying state of the computer changes such that now all of the probability is concentrated on whatever value you read out. So if you kept reading out from memory over and over, you would just keep seeing that same value. You might imagine these programs as creating a very delicate and sensitive probability distribution, where the moment you look at it, sampling from that distribution, the whole thing collapses to one value. Now you might be wondering, where does this distribution come from? This is both the most important and the most confusing part. You think of the state of the computer as being described by a big vector. Right now when I say the word vector, you can just think big list of numbers, although as you'll see later on, it can be helpful to think of this as a direction in some super high dimensional space. Each component of this vector corresponds to one of the possible values you might read out, one of those distinct bit strings. So in this example where what you read out has 4 bits, the state vector would have 16 distinct components. The state vector is not the same thing as the probability distribution over all possible outputs, but it is very closely related. The fundamental rule, which I admit is going to look very strange at first, is that if you take the magnitude of each component in that state vector and you square it, that gives you the probability of seeing the corresponding output, the corresponding bit string. Let me just say up front, a lot of people learning quantum computing find this state vector a bit weird. What is it actually, and why are we squaring things to get probability? As it is, for the sake of simplicity, there's a certain important detail that I'm neglecting until the end of the video here. I just want to flag that for most people, this takes a little getting used to. And to be super clear on what I mean with the fundamental rule here, let's suppose that after this program processes this vector, maybe the component of it associated with some specific bit string, like 0011, happened to be 0.5. Then when you square that value, 0.5 squared is 0.25, so the observable implication of this is that when you read out from memory, you have a 25% chance of seeing that bit string, 0011. One thing I'll highlight is that it is perfectly valid for the values in this state vector to be negative, and at first you might think that has no real impact, since flipping the sign doesn't change the square, and therefore all the probabilities stay the same. It is true that the probabilities stay the same, but we absolutely consider this to be a distinct state, and as you'll see, the idea of flipping signs plays a very central role in Grover's algorithm. Here, this example with four qubits has kind of a lot on screen, with not a lot of visualization to back it up, so let's scale things down to the smallest possible case where the computer has just two possible outputs, represented with a 0 and a 1. In this simplest possible case, the state vector would only be two-dimensional, so we can actually represent it geometrically as an arrow inside a 2D space. In this case, the x-coordinate corresponds to the outcome 0, in the sense that the square of that coordinate tells you the probability that when you read out from the computer, you would read a 0. Maybe it's helpful if I add a little bar to show that probability. You'll notice that as the vector points more in the horizontal direction, more of that probability mass is concentrated on the 0. Similarly, the y-coordinate corresponds to a 1 in the same way. A more vertical state vector means you're more likely to see a 1 when you read out from the computer. Now, notice, because the two probabilities should add up to 1, after all, something is going to happen, x squared plus y squared should equal 1. Geometrically, this means that the state vector has a length of 1, so you could think of it as being confined to a unit circle. More generally, the state vector for a quantum computer will always have a length of 1, and you can think of it as living on some very high-dimensional unit sphere. This two-dimensional example has a special name, which I've already mentioned. It's called a qubit, short for quantum bit. The analogy with a classical bit is that when you read out from the computer, you see either a 0 or a 1, but other than that, it is a completely different animal. Mathematically, a qubit is a unit vector in a two-dimensional space, together with a coordinate system where these two perpendicular x and y directions correspond to the two values that you might read out when you measure. You should know there is that added bit of complexity that I'm postponing, but this is 90% of the right idea. And again, you have this funny rule where when you measure the qubit, seeing either a 0 or a 1, the vector then collapses to fall onto that corresponding direction. So unless something is done to prepare that qubit back into a diagonal direction, any follow-on observations that you make are always going to show the same outcome. It's very possible that at this point you're thinking something like, okay, grant, this is a super bizarre set of premises you're asking me to accept, and if so, you are not alone. What I'm describing, as you can no doubt probably tell, are basically the postulates of quantum mechanics. There are many systems throughout physics, like the spin of an electron or the polarization of a photon, that have this property, where the outcome of a measurement is random, and our best laws of physics have us model the state of that system using a vector, just like the one I'm describing here, where squaring the magnitudes of that vector's components give you the probabilities for seeing various possible outcomes. That actually has a special name, it's called the Born rule. This idea of a qubit is basically meant to be an abstraction over many possible systems like this, in just the same way that a bit is meant to be an abstraction over many possible physical systems that can toggle one of two directions. The symbol I've been showing, by the way, is used throughout any subject with the word quantum in its name to refer to a unit vector in this state space. What you put inside that ket is often going to give some kind of readable meaning for what that vector represents. So in our example with a qubit, the unit vector to the right is often shown with a zero inside the ket, because if that's the state vector, it means you deterministically read out a zero from the computer. Likewise, the unit vector in the vertical direction is represented with a ket that has a one inside of it. And if you go on and read more about this, something that you'll very commonly see is that instead of writing down a general qubit with a column vector, the way I've been showing you, a lot of people like to write it as an explicit weighted sum of these two unit vectors in the two coordinate directions. That's a very physicist kind of convention. Now classical computing has this idea of logic gates, certain basic operations like AND, OR, and NOT that you can use to process bits and that you can string together to create arbitrarily complicated functions. Analogously we have what are called quantum gates, which are certain fundamental operations that you can apply to a qubit or to a system of multiple qubits. And they always look like somehow flipping or rotating the state vector. Now I'm not going to delve too deeply into the details of all the different quantum gates, but if you are curious, I'll show you an example of what one of them looks like. Here's a very standard one known as a Hadamard gate. What it does is it maps the unit vector in that horizontal zero direction into the diagonal northeast direction, and it maps the unit vector in the vertical one direction into that kind of diagonal southeast direction. You would very commonly use this to take a deterministic state, something that's either a zero or a one, and turn it into something with a 50-50 equal balance, or vice versa, too. This is just one example, but there are a number of others forming the building blocks for quantum computing, and the art of writing an algorithm in this setting is to somehow compose a bunch of different quantum gates together that will progressively manipulate and flip and massage this vector until it points almost entirely in one particular coordinate direction, presumably one that actually answers a question you care about. Now down with the simplest example of a qubit, you only have two coordinate directions to work with, so you would be constrained to answer simple yes-no questions. And although I can't illustrate a geometric vector with more than three dimensions, in principle, a system with k qubits is going to have 2 to the k distinct coordinate directions, one for each bit string. So if you can somehow manage to coerce this vector to point along just one of those directions, you could potentially answer some more interesting question, carrying more information. Maybe one of them represents a prime divisor in a very large number you're trying to factor, or maybe one of them represents that secret key value from the opening puzzle of this video. Even though the potential power of quantum computers has a long tradition now of being greatly exaggerated, insofar as there really is potentially more power there, one of the key reasons is that the size of the state vector grows exponentially. As few as 100 qubits would already imply a mind-bogglingly massive state vector. But the catch is that you have no direct access to the values inside this vector. It's effectively invisible to you. The only way it can be useful is if you have a way to manipulate it in such a way that all of the probability, or at least most of it, gets concentrated on one single component, and if that component corresponds to an answer to a question that you care about. Grover's algorithm offers us a really great example to actually see how this looks, and it's high time that we get there. Let me offer you a very high-level preview for how it looks. I promise I will explain all of this in more detail, but here's the bird's eye view. It initializes this state vector in such a way that there's an equal balance of probability across all possible outcomes. One of those outcomes is going to be the secret key that you're searching for, and the tool that you'll have available, which I promise to motivate later, is to flip the sign of the state vector at that coordinate. This doesn't immediately affect the probabilities, but when you interleave this with a certain other operation, and you kind of go back and forth between the two of these, what happens is that the probability mass slowly starts to get concentrated over that secret key value, and at a certain point, almost all of it will be there, so when you read out from the computer, you will almost certainly see the secret key you're looking for. Okay, so that's the high level, but let's unpack it in some more detail. The first thing to address is this idea of flipping the sign of the component associated with the secret key. That might feel a little bit weird. Why would we assume that that operation is available to us? Backing up, remember that Grover's algorithm is meant to apply to any problem where you can verify a solution quickly, even if finding a solution in the first place is hard. Examples here would include solving Sudokus, finding a valid coloring of a map where no two border regions share a color, or countless tasks throughout cryptography, where security often depends on a certain value being hard to find, even though for pragmatism it has to be easily verifiable. We began this video with a generic stand-in for all of these problems, where you imagine some function that takes in any number from 0 to n-1 and returns true on one and only one of those. In principle, we'll think of such a function as being built out of a bunch of classical logic gates. Those logic gates act on some binary representation of the inputs, and the final output is either 0 or 1. Now here's the key point. Grover knew that given any ensemble of logic gates like this, you can translate it into a system of quantum gates so that if in the classical case the function takes in some binary input and returns a 1, for true, then in the quantum case the effect of all of these gates is to flip the sign of that state, the state associated with the same bit string. Similarly, if in the classical case the function maps some binary input to 0, for false, then in this quantum translation the effect on the corresponding state would be to leave it unchanged. More generally, because all of these quantum operations are linear, if the state is a combination of multiple pure coordinate directions, then the effect is to simply flip the sign for the component associated with whatever bit string triggers that classical function. It means that if you have any NP problem, anything where you can quickly verify solutions, you're able to create an operation on a quantum computer that flips the sign of a state vector at the position corresponding to a solution of that problem. This might feel kind of useless at first. After all, flipping signs doesn't affect the probabilities. But Grover realized that this could be used in conjunction with another step that slowly amplifies the probability of that key value. There's actually a really nice way to visualize his algorithm. To set it up, let's imagine that our state vector has only three dimensions. Obviously in principle it would be way bigger, but this lets me draw the first picture. The three directions here would correspond to the values 0, 1, and 2, and the whole problem statement here is that one of those values would be a secret key that we're searching for. Many different quantum algorithms will begin by putting that state vector into a kind of equal balance, where all of the components have the same value. I want to give that equal balance vector a name. Let's call it B. I hope you don't object too much to me simply declaring that this is possible without dwelling on the underlying quantum gates that make it happen. In this case it essentially looks like a big pile of Hadamard gates, but all you need to know is that this equal balance direction is abundantly accessible. So starting from here, the goal is to somehow coerce this vector to instead point up in that secret key direction. And what's very helpful for the visualization purposes here is that throughout Grover's algorithm, the vector only ever moves inside the 2D plane that's spanned by these two vectors. So what I'm going to do is draw everything on that two-dimensional slice. And this is going to give us a faithful representation even when the full dimension is way too big for me to draw literally. The convention I'll use here in drawing that slice will be to put the secret key direction, whatever it is, along this y-axis, and then the x-axis is going to represent something that's an equal balance of all of the other states, the non-key states. So in our very small three-dimensional example, if that secret key was the 2 state up in the z direction, that would mean this perpendicular is an equal balance of 0 and 1, which sits on the xy plane perpendicular to that z-axis. Notice the fully equally balanced state, b, has some component in that secret key direction, since by its definition it has a little bit of that secret key within it. Instead of drawing this slice from three dimensions, here's what it would look like if it was taken from some larger number of dimensions. It's almost identical, but the main difference is that that equal balance state vector gets closer and closer to being perpendicular to the secret key direction. Crucially though, this angle is never quite 90 degrees, since that balance state always has a little bit of that secret key value inside of it. In fact, calculating this angle is going to be essential for understanding the runtime of Grover's algorithm. This is the main bit of math you actually have to do for it. You can find this angle by taking a dot product between the balance state and the key direction. The components of that balance state vector are all going to look like 1 divided by the square root of n, since remember it needs to be true that when you add the squares of all of these, it would give you 1. Since the key vector is 0 almost everywhere but 1 in one of the components, it means that the dot product between these two is 1 divided by the square root of n. If you know a little about dot products, you'll know that this is also the cosine of the angle between those two vectors. I'm going to translate this fact a little to make it more useful for our purposes later. This is the same as saying the sine of the complementary angle down over here, which I'll call theta, is 1 over the square root of n. For really small angles, the sine of theta and theta are approximately the same, so if n is very large, we can safely say that this angle is about 1 divided by the square root of n, as long as it's measured in radians. This value for theta is ultimately where the square root in the runtime is going to come from. So let's remember it. I'll throw it up in the corner here. Okay, having spent a while setting up the actual picture, let me show you the procedure here. It's surprisingly simple. Remember that key operation we were looking at earlier? The one where you take the verification function for whatever NP problem you're trying to solve, and you translate it into some quantum gates, and the effect is to flip the sign associated with that key value? Well, what does that look like inside our diagram? In this diagram, if you flip the sign for the component associated with the key value, but all other components stay the same, what it looks like is flipping about the x-axis. And the final ingredient you need to know is that it is also possible to flip this state vector around this equal balance direction. I realize it might be a little unsatisfying for me to keep declaring that certain operations are available, but one thing to know is that in general, if you can clearly describe and access one of these state vectors, it's also perfectly possible to reflect around it. There are some quantum gates, they let you flip around this balance direction, but trust me that dwelling on the details of how those look isn't really going to add much clarity or intuition to the whole algorithm. The insight really just comes from geometry. Notice how if you first flip around the x-axis, and then flip around this off-diagonal direction, the state now points slightly more in the vertical direction. If you were to read out from memory now, you would have a slightly larger chance of seeing the secret key value than all the others. Here, maybe it's helpful if I show the actual coordinates, in this case where n equals 100, along with some probability bars based on the squares of those coordinates. Notice how each time I flip around the x-axis, and then flip around this off-diagonal axis, the component associated with that secret key gets a little bit larger. So Grover's algorithm is remarkably simple. All you do is repeat this over and over until the vector points as close as it can get to the secret key direction. The final bit of reasoning you have to do is to figure out how many repetitions that should be. A wonderful fact from geometry is that if you successively flip about two different lines like this, the overall effect is the same as a rotation, more specifically a rotation by two times the angle between those two lines. So in our case, applying the two operations we have available one after the other, the net effect is to rotate the state vector by two times theta, where again theta is that little angle that we calculated earlier, approximately one over the square root of n. The ultimate goal is to rotate our initial state a little under 90 degrees, or about pi halves radians. This means the optimal number of repetitions looks like pi halves divided by two theta, which is pi fourths times one over theta, and critically, because theta is about one divided by the square root of n, it means the total number of steps looks like pi fourths times the square root of n. So what Grover's algorithm says is first find whatever whole number is closest to this value and then repeat your two available flips that specific number of times. As a concrete example, let's say that n was two to the power of twenty, meaning you're searching for a secret key out of about a million options. You would be running this on a computer with at least twenty qubits, and what the algorithm would say is first compute pi fourths times the square root of this number, which is around eight hundred and four, meaning you now repeat those two operations you have available, the one flipping the sign of the key state and the one that's flipping around the balance direction eight hundred and four times. Now remember, this state vector is invisible to you. There is nothing that you can do that will read out the values that it has. You instead have to infer where it must be through reasoning, and in this case, through all the geometric reasoning, you can conclude that after this specific number of times, the vector should be pointed almost entirely in that secret key direction. So when you read out from the computer, you are almost guaranteed to see that secret key value. Now to be clear, you're not guaranteed to see it. There is some small chance that after reading out, you would see something that's not the secret key. So to be sure, you could always quickly verify the answer, since after all, the whole premise of this situation is that you have some quick way of verifying answers. You can just do that on a classical computer. Worst case, if you got unlucky and sampled a different number, you run the whole thing again, and it becomes vanishingly unlikely that you would ever need to run this more than just a couple times. To wrap up, I want to come clean about a lie that I've been telling you, and also reflect a little bit about where this speedup came from, and then highlight a surprising analogy. Before I do, now might be as good a time as any to say a thanks to the community of channel supporters on Patreon. Putting together visualized lessons like this takes an enormous amount of time. As you probably know, most YouTubers monetize their content with in-video sponsorships, but for many years now that's something that I've opted to decline. I think it makes the videos better, and the only reason it's not a wildly costly decision is that enough viewers who agree with that directly support the channel through Patreon. In exchange, I offer supporters early views of new content, which is actually very helpful for developing it, and there's other perks in there too. In general though, if you like this content, it would mean a lot if you considered joining. No pressure though, one of the big values is that the content can be free. Anyway, back to those three finishing points. The lie is a lie by omission. I've been showing these state vectors with positive and negative real number values, but more generally, these components can be complex numbers. Now my hope is to motivate why that's the case in the follow-on video about physics. The general idea is that any time you're working with waves, you care about both amplitude and phase, and a complex number is a really elegant way to encode an amplitude and phase together. So if you look at one of the components inside the state vector, the fuller picture for how to think about it is that it has some magnitude and some phase. The magnitude is the thing that you square to get the probability, and the phase is essentially a more general version of our whole discussion here around positive and negative values. Changing phase doesn't immediately affect the probabilities, but it does affect the state, which in turn affects how it gets processed and how it interacts with the world. Now needless to say, throwing in a bunch of complex numbers adds a lot of potential confusion to an already complicated topic. That's why I avoided it. But we were safe to ignore complex values for the purposes of Grover's algorithm, since very mercifully, during that algorithm, you only ever see positive and negative values. I want you to know, though, that this availability of complex values plays a crucial role in other quantum algorithms, like Shor's for factoring numbers. Next, even if you understood everything that I described with this whole algorithm, it's not easy to summarize where exactly the speedup came from. The fact that you begin by applying a certain operation to this equal balance state makes it very tempting to say that the speedup comes from parallelizing the operation over all possible inputs. But like I mentioned at the start, that summary, at least to me, just really doesn't feel right, and it definitely leads to misconceptions. As you now know, that step, on its own, does nothing to reveal the key value. I'll leave it up to your interpretation whether it feels apt to describe this first step as applying a function to many inputs in parallel, or if it feels better to say that the balance state is just its own new thing, and the function we have always applies to individual inputs one at a time, never multiple at once. It's just that those inputs are now a new kind of thing. In my view, for this algorithm, if you want the one-word summary for where the speedup comes from, I think a better choice would be Pythagoras. As a loose analogy, if you want to get from one corner of a unit square to the opposite, if you're limited to move only in the x and y directions, you have to walk two units. But if you're allowed to move diagonally, you can get there in the square root of two. And more generally, if you're up in n dimensions, you would have to walk n units to get from one corner of a cube to the opposite one if you can only move along the edges, but if you can go diagonally, you can get there in the square root of n. In the worldview of quantum mechanics, different observable states all represent perpendicular directions in some state space. So viewed from this framework, what the classical deterministic world looks like is one where you only have access to these pure coordinate directions. If you think about it, anytime you're doing computation, your computer is somehow walking through a series of different states that are available to it, and algorithmic runtime is all about understanding how many steps you have to walk in the space of all possible states. So from this quantum worldview, where classical states look like pure coordinate directions, the key difference with quantum computing is that you now also have available to you a panoply of additional diagonal directions that you can work with. Now, to be clear, the analogy is not too literal, you should take it with a grain of salt. It's not like runtime necessarily looks like a distance travelled through this particular state space. But it is true that if you follow the state vector throughout Grover's algorithm, what it's doing is slowly walking along a quarter circle arc from an initial condition to the target condition, tracing a path that would be entirely unavailable if you were limited to move only in pure coordinate directions. And the effect is to provide this square root-sized shortcut. And as the very last point before I let you go, regular viewers will know that one of my reasons for covering this topic is because of a promised analogy between this algorithm and something we covered last video, about two colliding blocks that can compute pi. In that video, we are also studying a point in a certain two-dimensional state space bouncing around a circle, and in fact, the series of bounces that it took is essentially identical to what we just saw here with Grover's algorithm. The story here is that when a physicist friend of mine, Adam Brown, saw the first version of that video, he had recently been reading up on Grover's algorithm and immediately realized that both processes were identical. Now I had this whole cockamamie plan for this video where I was going to explain quantum computing and Grover's algorithm in the context of that analogy, but it turned out to be just a terrible idea. The whole thing only really made sense if you already understood Grover's algorithm. So instead, now that you have seen both topics in isolation, what I'll do is leave up a rough outline of how the analogy looks. Think of this as an open-ended homework puzzle, where the task is to draw the connection for yourself. As an answer key of sorts, I will link to the very delightful paper by Adam Brown outlining precisely what that analogy looks like. If you want to learn more of the fundamentals of quantum computing, several years ago two very smart friends of mine, Andy Matuszczak and Michael Nielsen, put together a really nice resource for learning the topic, offering a pretty unique approach to making sure that you actually remember it in the long term. To learn some of the fundamental quantum mechanics, Mithina Yoganathan from the channel Looking Glass Universe has been putting together a very beginner-friendly course on the topic. She was actually the one to teach me how Grover's algorithm works many years ago, and honestly I owe a lot of the content of this video to many helpful conversations with her. And as a very last note to end on, one of the conversations I had while making this video was with Scott Aaronson, a very widely respected researcher and utterly delightful author on the topic. I recorded the Zoom call for notes, and there's one little piece of it which I just wanted to share with you. You know, I have this dream of writing a science fiction novel where I know it's going to be the climactic scene, where the heroes will be, you know, Ron and Grover's algorithm to try to find this cryptographic key, right? The whole fate of the world will depend on whether they can find it, OK? The bad guys have surrounded their base, you know, they're bashing down the walls. But Grover's algorithm is still running, and it has only like a 30 percent probability of giving you the solution if you measure. So the question is, like, do you measure now or do you let it run for another minute? Right? And if you measure now, you know, and you don't get it, then you've lost everything. Then you have to restart from the beginning. So this is this is not a plot that you could have with any classical algorithm. Right.

## Where my explanation of Grover’s algorithm failed

Last week I put up a video introducing quantum computing, and in the final section we were stepping through something known as Grover's algorithm. And based on the comments that I saw, I think there was a very common point of confusion that reveals I clearly could have done a better job explaining a core piece of it. Right here I wanted to throw together a very quick supplement in the hopes of adding a bit of clarity. The premise was to have a function which is somehow triggered by a unique value out of a big bag of options, and the puzzle is basically to figure out how do you find that unique value just by applying the function on various inputs. Now in a classical setting, it's not a very interesting question, the best you can do is guess and check, but what we walked through was a completely different approach that you can take that becomes possible in the setting of a quantum computer. When we did this, there was a certain key step where, if I'm understanding the comments correctly, it looked to a lot of people like in order to apply this key step, you would have to already know the value that you're searching for, which would of course defeat the whole purpose of the algorithm. More specifically, we had this very high dimensional vector space, and one of the axes in that space corresponded to the value that we're searching for, and this step of the algorithm looked like flipping along that axis, multiplying any component of a vector in that direction by negative one. Now viewers were essentially asking, whoa whoa whoa, how could you know how to do that without already knowing which axis you're searching for? I genuinely tried to forestall that objection, but I think I failed, so backing up, I think the whole discussion might be clearer if we focus on a very concrete example, something like solving a sudoku. On your normal classical computer, it's not hard to write a function that checks whether a proposed solution follows all of the sudoku rules and solves the puzzle. You know, it would check the rows, the columns, squares for duplicates, things like that. If you have written this function, just because you know how to verify a solution, it's not at all obvious what the solution is in the first place. This is, after all, why a sudoku is a puzzle. The rules alone don't reveal the answer. There are other situations where this is actually a much stronger assumption. The function SHA-256, for example, is what's called a cryptographic hash function. That basically means if you want to find what input gives you a particular output, it is strongly believed that you really can't gain much insight by looking at how the function is implemented. If someone could reverse engineer it, they would be mining all of the bitcoin in the world and breaking numerous other cryptographic schemes. But it's believed that the best thing you can do when you're searching for a particular output is guess and check. So it's not that the key value is like hiding inside the function behind some curtain, it's more of a difficult-to-find emergent phenomenon of the function itself. Now the idea with Grover's algorithm is that if you have this sort of verifier function for some hard problem and you translate it into the language of quantum computing, there is a method for sifting out valid solutions which requires fewer steps than simple guessing and checking over all the possibilities. Now to be clear, it is not dramatically faster, it's only a quadratic speedup, and given the overheads for making quantum computing work, this frankly has questionable utility. In fact, let's talk a little bit more about that at the end. First, to the clarification, if you have this Sudoku verifying function, it's not like you can just run it on a quantum computer. After all, quantum computers speak an entirely different language, it's a totally different framework for computing that looks a lot more like vector manipulation. The first step to port over this verification function into the new context is to imagine that we've kind of compiled your verifier into a bunch of logic gates, things like AND, OR, and NOT. So for any proposed Sudoku solution, you would represent it all in binary, all of those bits would be processed by your web of logic gates, and the output would be a 1 if it's a valid Sudoku solution, and a 0 for all the invalid ones. And again, being able to assemble these logic gates does not require knowing ahead of time which input solves the puzzle. The logic gates distill the rules of the game, but not the strategy. Now I'm assuming everyone has watched the main video, in particular the core section about the fundamentals of the state vector, but as a quick recap, the upshot is that you think of every possible bit string as a unit vector along a coordinate axis in some high dimensional space. In the language of linear algebra, you would call these the basis vectors of your coordinate system. For example, with a 2-qubit quantum computer, you would have 4 possible bit strings, and these would all look like basis directions in some 4-dimensional space. These state vectors get very big very fast. If you have a k-qubit quantum computer, that gives you 2 to the k possible bit strings, and you think of each one of them as being a coordinate direction in some very high dimensional vector space. Now operations on a quantum computer don't spit out true or false the way you can see on a classical computer. Instead they take in a vector and spit out a new vector, both of which live in the same space. And like I said last video, you often think about them as somehow flipping or rotating the vectors in that space. Now here's the crux of the confusion. I mentioned how if you have this classical verifier function, something like a Sudoku checker that spits out a 1 or a 0, it is possible to translate it into an operation on a quantum computer that has the following behavior. If a bit string returns 1 for true up in the classical case, then down in the quantum case, the corresponding basis vector gets multiplied by negative 1, effectively flipping 180 degrees. And then if in the classical case a bit string outputs 0 for false, then in the quantum translation, the corresponding basis vector is unchanged. Now I can see three reasons that this step might have caused some confusion. First of all, I didn't explain how it actually works. I didn't step through the translation. Now my hope with that video was that it's just not too huge a leap to have you accept that in principle there exists this correspondence between returning true and false up in the classical world and multiplying by negative 1 or positive 1 down in the quantum world with vectors. Now maybe that is a leap. I could preview for you what that translation looks like. The relevant search term here is quantum compilation, but I'm going to be honest, I don't think it would add much clarity, in the same way that knowing the logic gates that implement addition don't really teach you much about how to add two numbers. Now it's not exactly like this, but loosely speaking, every time you see an AND gate, you translate it into a quantum operation that looks kind of like an AND. Every time you see a NOT gate, there's a quantum analogue that does something kind of like a NOT. I suspect the real cause of confusion originates not from a lack of low-level detail, but from how I had framed the entire setup. I opened that video by having you imagine that there was some mystery number that we're searching for, and as one commenter helpfully pointed out, this made it seem like the computer kind of knows the answer ahead of time, it's just hiding it from us. And this is almost certainly exacerbated by me briefly flashing an example function that just checks if the input is 12, and saying that we were going to treat the function as a black box. That's on me. That's a misleading way to open things. What I wanted to foreshadow is how, with Grover's algorithm, the only way you use the new quantum function is by trying it out on inputs, as opposed to maybe like reverse engineering it. So in that sense, it's treated as a black box. But to be clear, in order to translate the classical verifier into the quantum version, you absolutely need to get into the guts of the function. And if this is going to be a compelling example, it would be very silly if the only thing the function did was just check if the input equals some hidden number. The Sudoku example is much better, and a cryptographic hash like SHA-256 would be much better still. In these contexts, there is one value that will trigger the function, and we don't know what it is, but the computer also doesn't know what it is. It's not like the key value is just hiding in the source code. Whether we're up here in the classical setting, where triggering the function means returning true, or down in the quantum setting, where triggering the function means multiplying by negative one, which specific key input does this is a difficult to find and emergent property of those logic gates. It's not something that's baked in ahead of time. The other potential source of confusion, I suspect, is that I didn't appropriately emphasize the idea of linearity. In fact, this is a central enough feature of quantum computing and quantum mechanics that half of my reason for making this whole follow-up video is as an excuse to talk about it. So most vectors don't look like a pure basis direction, they look like some weighted sum of all the different basis vectors. One way you can represent this is with a column vector, where we think of each component as being associated with one of the possible bitstrings. The more common convention among physicists is to write general vectors as an explicit weighted sum of all the basis directions, each one represented with a ket. When the state vector for a computer looks like this, you say that it's in superposition, meaning it has some non-zero component associated with multiple distinct bitstrings. It's a lot like saying if someone is walking northeast, their velocity is a superposition of north and east, they're travelling both directions at the same time. A core idea from the last video is that you never actually see the coordinates of a state vector in superposition like this. When you read out from the computer, all you see is one of the bitstrings at random, and the probability of seeing it is equal to the square of the magnitude of the component of the state vector associated with that value. I'm saying magnitude here with the absolute value signs, because in general, these components can be complex numbers, but for simplicity, I'm only going to be showing real values. When I say that operations in quantum computers are linear, what I mean is that if you pass in one of these weighted sums of the different basis directions, that is to say, a superposition, then the output looks like the same weighted sum, but of the transformed versions of each vector. So here's a very small example. On a single qubit, there's an operation that we call a z-gate. What it does is it leaves the 0 direction unchanged, but it multiplies that vertical 1 direction by negative 1. These are only two out of the infinitely many possible state vectors, but they're all you need to know. If you pass in a superposition of those two, something that has a little bit of 0 plus a little bit of 1, what you do is look at what the z-gate does to each part separately, and then add those together. Again, with the same components. In this case, that means flipping the sign associated with the 1 component. Geometrically, when you draw this vector in a 2D space, the action of a z-gate looks like flipping around the x-axis. The z-gate is simple enough that just by looking at the definition, you can clearly see which direction gets flipped. But keep in mind, for more complicated functions, the definition alone might not so easily reveal how it behaves. Take a look back at the Sudoku verification function and its translation onto a quantum computer, and then say that the state of your computer is not one of those clean basis directions, but it is a combination of all the basis vectors, a superposition of every possible bit string in this context representing every possible solution to the Sudoku. To figure out what our verifier does to this new vector, what you do is look at what it would do to each basis separately, and then the output is going to be the same scaled sum of the result. In this case, most parts of that sum stay unchanged, but one of them, the one that's associated with the key input that represents a Sudoku solution, that part is going to have it sign flipped. When you do this, it's very tempting to look at it and say that the function is acting on every possible basis vector at once in parallel, and then adding the results. Now that might be true, but I invite you to reflect on whether that's necessarily a fair way to summarize it. As an analogy, if a hiker is walking northeast, and you tell him to rotate 90 degrees, that rotation is a linear operation. In other words, the final direction is the same as what you would get by rotating the north vector 90 degrees, rotating the east vector 90 degrees, and adding the two results. But that doesn't mean that you have to perform two separate rotations in parallel in order to move the hiker. The linearity is a property of the transformation, it's not necessarily a set of instructions on how to do it. It's very similar over here. The effect of the quantum translation for our verifier looks like adding together what its effect would be on all of the basis vectors, but I will leave it to your interpretation whether this means that it is necessarily acting on all 2 to the k possible bitstrings at once. Here, I've been writing the vector the physicist way, but if I write it with a column vector instead, what this operation looks like is taking one component of that vector and multiplying it by negative 1, specifically whichever component corresponds to the sudoku solution. So stepping back, hopefully this helps clear up some of the confusion there was in that last video. The starting point of Grover's algorithm is to assume that you're given a function that flips one component of a vector like this, though you don't know which one. The puzzle is to somehow figure out which direction is getting flipped, where all you're allowed to do is apply this function to some well-chosen set of inputs, and in Grover's case, it involves interleaving it with a certain other operation in the toolkit of quantum computing. I get it, that is a really weird place to start from. And it doesn't help that this is a famously confusing topic. If you find it weird to work with a state vector whose components you never actually observe and which instead acts like a kind of square root of a probability distribution, you're not alone. Everyone finds that very weird. At this point I genuinely can't tell if I'm overexplaining things or still under-explaining them, but there is one final aspect of the explanation from last time that may have added to this confusion. When we visualized the algorithm, we chose to view everything on a certain two-dimensional slice of the enormous n-dimensional vector space where all these state vectors live, and this slice, by definition, included the axis associated with that mystery value we're searching for. Now, in case that left anyone with the impression that part of the algorithm was to choose that slice, let me be very clear, it's not. The algorithm is just doing what it's going to do. It interleaves two operations that go back and forth. It doesn't have a care in the world how you and I choose to visualize it. The fact that the state vector stays confined to this particular plane, that's a happy emergent property of the algorithm. It is in no way part of the instruction set that we're giving to the computer. One very final thing that I do think deserves some added reflection is how Grover's algorithm, while very thought-provoking, is maybe just not really useful. Take the Sudoku example. The number of possible solutions will depend on how many of those 81 squares start off blank, but let's call it something like 9 to the power 60. If you tried to use your classical verifier function to find a solution by brute force, there are way, way too many possibilities to check. But even if you had a fully functioning quantum computer, sitting on your desk right now with ample qubits and no issues maintaining coherence and all that, using Grover's algorithm, this would still take around 9 to the 30 steps, which is a much smaller number, but it's still enormous. In this case, there's obviously many smarter ways to solve a Sudoku than a brute force search, but if you take something like SHA-256, inverting it by brute force takes 2 to the 256 steps on a classical computer. Using Grover's algorithm, that becomes 2 to the 128, at least up to that pi-fourths constant that we saw last video. So even in some sci-fi future where quantum computers are as far along as today's classical computers, that is still an infeasibly large number of steps. The way some people write about quantum computing, it makes it sound like the moment they arrive, everything is going to change and all of cryptography will break. And it's true that there are specific problems that have exponential speedups, and especially with RSA, some of those are relevant to cryptography, but that's not true in general. This quadratic speedup is much more representative of what you get for most problems. It's really cool, and the math is just beautiful, and there continues to be lots of interesting research in the field, but one of my hopes with this whole project is that you now have enough background to maybe see through some of the hyperbole that certain outlets are so fond of. Thank you.